<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: Parallel Universes</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Parallel Universes">
<meta name="Date" content="2003-02-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Parallel Universes</h1>
<!-- received="Wed Feb 12 00:10:49 2003" -->
<!-- isoreceived="20030212071049" -->
<!-- sent="Wed, 12 Feb 2003 02:10:41 -0500" -->
<!-- isosent="20030212071041" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Parallel Universes" -->
<!-- id="3E49F371.2000407@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20030212064416.GA30084@weidai.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Parallel%20Universes"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Feb 12 2003 - 00:10:41 MST
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2588.html">Damien Broderick: "RE: skyhooks again"</a>
<ul>
<li><strong>Previous message:</strong> <a href="2586.html">Lee Corbin: "RE: Where the I is"</a>
<li><strong>In reply to:</strong> <a href="2584.html">Wei Dai: "Re: Parallel Universes"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2592.html">Max M: "Spam (was: Re: Parallel Universes)"</a>
<li><strong>Reply:</strong> <a href="2592.html">Max M: "Spam (was: Re: Parallel Universes)"</a>
<li><strong>Reply:</strong> <a href="2599.html">Wei Dai: "Re: Parallel Universes"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2587">[ date ]</a>
<a href="index.html#2587">[ thread ]</a>
<a href="subject.html#2587">[ subject ]</a>
<a href="author.html#2587">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Wei Dai wrote:
<br>
<em>&gt; On Tue, Feb 11, 2003 at 11:13:32PM -0500, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;If he chooses to apply Bayes's rule, then his expected *global* utility is 
</em><br>
<em>&gt;&gt;p(1)*u(.99m rewarded) + p(1)*u(.01m punished), while his expected *local* 
</em><br>
<em>&gt;&gt;utility is p(.99)*(1 rewarded) + p(.01)*(1 punished).  
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I assume an altruist-Platonist is supposed to try to maximize his global
</em><br>
<em>&gt; utility. What role is the local utility supposed to play? BTW, the only
</em><br>
<em>&gt; difference between your &quot;global&quot; utility and my utility is that I
</em><br>
<em>&gt; subtracted out the parts of the multiverse that can't be causally affected
</em><br>
<em>&gt; by the decision maker, so they're really equivalent. But for the argument
</em><br>
<em>&gt; below I'll use your definition.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;If he sees X=0 
</em><br>
<em>&gt;&gt;locally it doesn't change his estimate of the global truth that .99m 
</em><br>
<em>&gt;&gt;observers see the true value of X and .01m observers see a false value of 
</em><br>
<em>&gt;&gt;X.  Roughly speaking, if a Bayesian altruist-Platonist sees X=0, his 
</em><br>
<em>&gt;&gt;expected global utility is:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;p(.99)*u(.99m observers see 0 =&gt; .99m observers choose 0 =&gt; .99m observers 
</em><br>
<em>&gt;&gt;are rewarded &amp;&amp; .01m observers see 1 =&gt; .01m observers choose 1 =&gt; .01m 
</em><br>
<em>&gt;&gt;observers are punished)
</em><br>
<em>&gt;&gt;+
</em><br>
<em>&gt;&gt;p(.01)*u(.99m observers see 1 =&gt; .99m observers choose 1 =&gt; .99m observers 
</em><br>
<em>&gt;&gt;are rewarded &amp;&amp; .01m observers see 0 =&gt; .01m observers choose 0 =&gt; .01m 
</em><br>
<em>&gt;&gt;observers are punished)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Remember in my last post I wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Note that if he did apply Bayes's rule, then his expected utility would
</em><br>
<em>&gt; instead become .99*U(.99m people rewarded) + .01*U(.01m people punished)
</em><br>
<em>&gt; which would weight the reward too heavily. It doesn't matter in this case
</em><br>
<em>&gt; but would matter in other situations.
</em><br>
<p>No, Wei Dai, this does *not* happen.  I see the mistake you're making but 
<br>
I honestly don't know why you're making it, which makes it hard to 
<br>
correct.  If you see 1 it affects your Bayesian probability that the real 
<br>
digit is 1 or 0; it does not affect your estimate of the global outcome of 
<br>
people following a Bayesian strategy.  You seem to be assuming that 
<br>
Bayesian reasoners think that all observers see the same digit... or 
<br>
something.  Actually, I don't know what you're assuming.  I can't make the 
<br>
math come out your way, period.
<br>
<p><em>&gt; You're basically making exactly this mistake,
</em><br>
<p>Please actually read my numbers.
<br>
<p><em> &gt; and I'll describe one of the
</em><br>
<em>&gt; &quot;other situations&quot; where it's easy to see that it is a mistake.  Suppose
</em><br>
<em>&gt; in the thought experiment, the reward for guessing 1 if X=1 is 1000 times
</em><br>
<em>&gt; as high as the reward for guessing 0 if X=0 (and the punishments stay the
</em><br>
<em>&gt; same). Wouldn't you agree that in this case, you should guess 1 even if
</em><br>
<em>&gt; the printout says 0?
</em><br>
<p>Yes.
<br>
<p><em> &gt; The way you compute global utility, however, your
</em><br>
<em>&gt; utility is still higher if you guess 0 when you see 0.
</em><br>
<p>No.
<br>
<p><em>&gt; Here're the calculations. Let R stand for &quot;observers rewarded&quot;, E stand
</em><br>
<em>&gt; for &quot;observers rewarded extra&quot;, i.e. rewarded 1000 times more, and P for
</em><br>
<em>&gt; &quot;observers punished&quot;. And let u(E) = 1000*u(R) = -1000*u(P) = 1000.  
</em><br>
<em>&gt; According to your utility function, the expected utilities of choosing 0
</em><br>
<em>&gt; or 1 are:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; EU(choose 0)
</em><br>
<em>&gt; .99*u(.99m R &amp; .01m P) + .01*u(.99m E &amp; .01m P)
</em><br>
<em>&gt; =
</em><br>
<em>&gt; .99*.98 + .01*(990-.01)
</em><br>
<em>&gt; =
</em><br>
<em>&gt; 10.8701
</em><br>
<p>Um... I just don't see where you get these numbers.  Are you talking about 
<br>
the expected total utility of choosing 0 every time, or the expected 
<br>
utility of choosing 0 given that you saw 0?
<br>
<p>Given that you see 0, the expected utility of all reasoners choosing 0 is:
<br>
<p>.99*u(1.0m R) + 0.01u(1.0m P)
<br>
=
<br>
.99 - .01
<br>
<p>Given that you see 0, the expected utility of all reasoners choosing 1 is:
<br>
<p>.99*u(1.0m P) + 0.01u(1.0m E)
<br>
=
<br>
-.99 + 10
<br>
<p><em>&gt; If you do the same computations without applying Bayes's rule first,
</em><br>
<em>&gt; you'll see that EU(choose 1) &gt; EU(choose 0), but I won't go through the
</em><br>
<em>&gt; details. What's going on here is that by applying Bayes's rule, you're
</em><br>
<em>&gt; discounting the extra reward twice, once through the probability, and
</em><br>
<em>&gt; another time through the measure of observers, so the extra reward is
</em><br>
<em>&gt; discounted by a factor of 10000 instead of 100. That's why I choose to
</em><br>
<em>&gt; make the extra reward worth 1000 times the normal reward.
</em><br>
<p>You know, there are days when you want to just give up and say:  &quot;Don't 
<br>
tell ME how Bayes' Theorem works, foolish mortal.&quot;  Anyway, for some 
<br>
strange reason you appear to be applying Bayes adjustments to the measure 
<br>
of observers and *and* to those observers' subjective probabilities. 
<br>
That's where the double discount is coming from in your strange 
<br>
calculations.  You're applying numbers to the measure of observers that 
<br>
just don't go there.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2588.html">Damien Broderick: "RE: skyhooks again"</a>
<li><strong>Previous message:</strong> <a href="2586.html">Lee Corbin: "RE: Where the I is"</a>
<li><strong>In reply to:</strong> <a href="2584.html">Wei Dai: "Re: Parallel Universes"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2592.html">Max M: "Spam (was: Re: Parallel Universes)"</a>
<li><strong>Reply:</strong> <a href="2592.html">Max M: "Spam (was: Re: Parallel Universes)"</a>
<li><strong>Reply:</strong> <a href="2599.html">Wei Dai: "Re: Parallel Universes"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2587">[ date ]</a>
<a href="index.html#2587">[ thread ]</a>
<a href="subject.html#2587">[ subject ]</a>
<a href="author.html#2587">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Feb 12 2003 - 00:12:59 MST
</em></small></p>
</body>
</html>
