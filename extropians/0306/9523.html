<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: RE: The Simulation Argument again</title>
<meta name="Author" content="Dan Fabulich (dfabulich@warpmail.net)">
<meta name="Subject" content="RE: The Simulation Argument again">
<meta name="Date" content="2003-06-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: The Simulation Argument again</h1>
<!-- received="Thu Jun  5 14:56:43 2003" -->
<!-- isoreceived="20030605205643" -->
<!-- sent="Thu, 5 Jun 2003 13:56:28 -0700 (Pacific Daylight Time)" -->
<!-- isosent="20030605205628" -->
<!-- name="Dan Fabulich" -->
<!-- email="dfabulich@warpmail.net" -->
<!-- subject="RE: The Simulation Argument again" -->
<!-- id="Pine.WNT.4.50.0306051312420.1948-100000@dfab.na.plumtree.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="FEEHKEIJDJKMGAAFLAIMMEFAGAAA.mail@HarveyNewstrom.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Dan Fabulich (<a href="mailto:dfabulich@warpmail.net?Subject=RE:%20The%20Simulation%20Argument%20again"><em>dfabulich@warpmail.net</em></a>)<br>
<strong>Date:</strong> Thu Jun 05 2003 - 14:56:28 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9524.html">Alfio Puglisi: "Re: Political labels (was: neocons (WAS IRAQ: Weapons of Mass Delusion))"</a>
<ul>
<li><strong>Previous message:</strong> <a href="9522.html">gts: "RE: Martha Stewart and her Merrill Lynch Broker"</a>
<li><strong>In reply to:</strong> <a href="9480.html">Harvey Newstrom: "RE: The Simulation Argument again"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9452.html">Hal Finney: "RE: The Simulation Argument again"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9523">[ date ]</a>
<a href="index.html#9523">[ thread ]</a>
<a href="subject.html#9523">[ subject ]</a>
<a href="author.html#9523">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Harvey Newstrom wrote:
<br>
<p><em>&gt; I am equating technological advances and virtual reality toys.  We can't
</em><br>
<em>&gt; simulate entire universes in our current existence.  But the Simulators
</em><br>
<em>&gt; obviously can.  Their level of virtual reality, computing power, and
</em><br>
<em>&gt; technological advancement must be immeasurably superior to ours.
</em><br>
<em>&gt; Whatever you find exciting about our escalating level of technology,
</em><br>
<em>&gt; they've been there, done that, and moved on. That's what I call
</em><br>
<em>&gt; exciting!  On any continuum between pre-singularity and
</em><br>
<em>&gt; post-singularity, I would assume that the previous world is more boring
</em><br>
<em>&gt; than the later world!
</em><br>
<p>Even if I concede to you that the world-as-it-is is inherently less
<br>
exciting to posthumanity, I insist that the question is whether our world
<br>
is exciting *enough*.  Everything I know about the Mesozoic era tells me
<br>
that most everything about it was incredibly boring, but there are
<br>
obviously some people who spend a lot of time trying to simulate it... it
<br>
may not be exciting to me, or even &quot;exciting&quot; in general, but it's
<br>
exciting *enough* for some people.
<br>
<p><em>&gt; &gt; As for the argument about suffering, I think this is argument is utterly
</em><br>
<em>&gt; &gt; misplaced.  I don't need a posthuman theodicy to think that there's a
</em><br>
<em>&gt; &gt; reasonable chance that an interested posthuman with arbitrarily large
</em><br>
<em>&gt; &gt; resources might rehearse the brief history of life on Earth in ver mind,
</em><br>
<em>&gt; &gt; perhaps in great detail, and that's all it takes to get the sim argument
</em><br>
<em>&gt; &gt; off the ground.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But to give us the ability to feel pain, to make us conscious of our
</em><br>
<em>&gt; pain, and then to submit us all into the world of suffering
</em><br>
<em>&gt; needlessly....  That's hard to imagine.  I don't think any of us would
</em><br>
<em>&gt; do that.  If we are simulations of the Creators' ancestors, then they
</em><br>
<em>&gt; are our descendants in the future.  Can we extrapolate a scenario where
</em><br>
<em>&gt; we will become transhuman, immortal, and ultimately powerful, ...and
</em><br>
<em>&gt; then recreate a world of suffering that we have ourselves escaped?
</em><br>
<em>&gt; This does not seem very likely to me.
</em><br>
<p>I think you're reading too much into what it takes to make a &quot;simulation.&quot;
<br>
Remember, we're talking about (to us) microscopic fractions of the power
<br>
of a posthuman brain.  It seems to me that you're imagining a posthuman
<br>
civilization thinking about doing a simulation, deciding to do it,
<br>
architecting it, planning it in detail, fixing bugs in the plan,
<br>
implementing the plan, and then watching the results, perhaps in real
<br>
time.
<br>
<p>But when you're talking about a millionth of the power of, say, a
<br>
Matrioshka brain, I think this picture is misleading.  For example, I
<br>
invite you to think back to a time when you did or said something that was
<br>
really embarrassing.  Remember how awful that felt?  OK, now imagine how
<br>
things might have gone a bit differently: if you had caught yourself a few
<br>
seconds beforehand, or if you had done something praiseworthy at that
<br>
point instead.  Much better, yes?
<br>
<p>OK, now note: did you architect/plan your recollection?  Did you spend
<br>
much time fixing bugs in your plan?  Did you watch the results, or did you
<br>
even consider the results as part of a separate simulation mechanism?
<br>
I'd argue that, if you're like most people, you simply rehearsed the
<br>
memory in your mind, changing bits and employing your imagination as need
<br>
be, without separately *planning* a complex simulation.  You *could* plan
<br>
something more elaborate, but it's so trivially easy to just &quot;throw
<br>
something together&quot; mentally that you probably didn't even bother.
<br>
Indeed, if you're like me, you often find yourself mentally rehearsing
<br>
emotional events (both happy and sad) by *accident*: that's how easy they
<br>
are.
<br>
<p>Now, there's not much that I can show on the basis of this introspective
<br>
&quot;experiment.&quot;  Certainly I'd be a fool to argue that posthumans would
<br>
simulate Earth's natural history accidentally, or even that they probably
<br>
would.  But the point is that OUR simulations come so quickly, easily, and
<br>
naturally to us that it seems to me that, given the trivial expenditure of
<br>
effort required from a posthuman, it's not inconceivable that one might
<br>
find vimself doing so *accidentally*.  THAT's how little effort/complexity
<br>
we're talking about here.
<br>
<p>You ask: &quot;why would they bother?&quot; as if it would be difficult or costly.
<br>
Similarly, you suggest that it would be morally wrong to imagine/rehearse
<br>
our history in detail, on account of the suffering of the sims. I don't
<br>
see that a posthuman would so obviously forbear from these kinds of
<br>
detailed simulations merely on account of the suffering of the sims: when
<br>
it gets to be *that* easy, why bother stopping?  Would you simulate an ant
<br>
farm that included ants that reacted to simulated pain?
<br>
<p><em>&gt; &gt; This
</em><br>
<em>&gt; &gt; argument amounts to saying that in most sim universes, movies like The
</em><br>
<em>&gt; &gt; Matrix would be prevented by physics itself.  A cool superhero movie would
</em><br>
<em>&gt; &gt; be possible, but considering the brain-in-a-vat case wouldn't...?  This
</em><br>
<em>&gt; &gt; seems absurd.  Why wouldn't they just let us consider the possibility
</em><br>
<em>&gt; &gt; without giving us a practical use for this information?
</em><br>
<em>&gt;
</em><br>
<em>&gt; But this would break the simulation.  Because in the real world, people
</em><br>
<em>&gt; might ponder such things uselessly.  They aren't in a simulation, so
</em><br>
<em>&gt; none of their attempts to detect the simulation, prove it, and break it
</em><br>
<em>&gt; would work.  But in a simulation, these very same actions could cause
</em><br>
<em>&gt; problems.  We might discover proof and redirect the history of the
</em><br>
<em>&gt; simulation as we demonstrate this to everybody.  We might find a way to
</em><br>
<em>&gt; hack the simulation and start reprogramming it or try to communicate to
</em><br>
<em>&gt; the real world.  If we couldn't get that far, we could all just commit
</em><br>
<em>&gt; suicide to end the simulation, or all act inappropriately to ruin the
</em><br>
<em>&gt; historical recreation, or overload the system with a denial of service
</em><br>
<em>&gt; attack by doing a lot of things that are much harder to calculate and
</em><br>
<em>&gt; simulate.  These actions must be possible, or, they must be prevented by
</em><br>
<em>&gt; security features in the simulation.  If these security features are
</em><br>
<em>&gt; there, we should be able to detect them as our attempts fail.  But it
</em><br>
<em>&gt; seems much simpler and more direct in design if we were merely prevented
</em><br>
<em>&gt; from having the idea of hacking our way out of the simulation in the
</em><br>
<em>&gt; first place.
</em><br>
<p>No, no, no.  All that's needed here is an incapacity to *detect* that
<br>
we're in a simulation.  You don't need any more anti-hacking mechanisms
<br>
than that, if your obfuscation mechanism is in order.  THAT'S by far the
<br>
most &quot;direct&quot; design, especially if human behavior is, itself, what you're
<br>
trying to simulate.  Again, what the hell kind of crappy simulation are we
<br>
talking about here if movies like the Matrix could never get made, or even
<br>
thought about?
<br>
<p><em>&gt; I find that argument to be unconvincing.  If we can't effect how the
</em><br>
<em>&gt; simulation is being run by getting the interest or attention of our
</em><br>
<em>&gt; Simulators, then it would be futile to alter our behavior.  On the other
</em><br>
<em>&gt; hand, if they did tweak the simulation based on our actions, we would
</em><br>
<em>&gt; have examples of miracles or non-sequiturs all the time.  Maybe they
</em><br>
<em>&gt; could carefully modify the game without it being noticed.  But this
</em><br>
<em>&gt; would change the argument from a simulation to a directly-controlled
</em><br>
<em>&gt; scenario or game.  I think such control would make our universe appear
</em><br>
<em>&gt; less arbitrary or more directed or would be detectable in some way.
</em><br>
<p>Not that I think that we should take this seriously, but if the wrong kind
<br>
of theist heard this argument, I think they'd take precisely the opposite
<br>
point here.  ;)
<br>
<p>Still, I agree that it's almost certainly futile to follow Hanson's advice
<br>
in a detailed long-term posthuman simulation.
<br>
<p><em>&gt; You need a professional hacker!  They are great for figuring out how to
</em><br>
<em>&gt; detect, probe, evaluate, and ultimately manipulate remote unseen
</em><br>
<em>&gt; computing forces by using ordinary communications and interactions in
</em><br>
<em>&gt; such a way as to get unexpected results.  If we are in a simulation, a
</em><br>
<em>&gt; hacker should be able to figure it out.
</em><br>
<em>&gt;
</em><br>
<em>&gt; (Hmmm...  unless the hackers capable of doing this are programmed to
</em><br>
<em>&gt; disbelieve that we are in a simulation so they never try! Nah...!)
</em><br>
<p>I think you put too much confidence in mortal hackers.  ;)  Don't get me
<br>
wrong, some of our hackers are quite clever, but posthumanity is expected
<br>
to be, you know, much cleverer.
<br>
<p>Think of it this way: could we build a simulation so good that a dog
<br>
couldn't tell the difference?  What about the brightest of dogs, who like
<br>
to dig holes under fences and find lost objects?  Does it matter if we
<br>
switch to cats here? ;)
<br>
<p>-Dan
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-unless you love someone-
<br>
&nbsp;&nbsp;&nbsp;&nbsp;-nothing else makes any sense-
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e.e. cummings
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9524.html">Alfio Puglisi: "Re: Political labels (was: neocons (WAS IRAQ: Weapons of Mass Delusion))"</a>
<li><strong>Previous message:</strong> <a href="9522.html">gts: "RE: Martha Stewart and her Merrill Lynch Broker"</a>
<li><strong>In reply to:</strong> <a href="9480.html">Harvey Newstrom: "RE: The Simulation Argument again"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9452.html">Hal Finney: "RE: The Simulation Argument again"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9523">[ date ]</a>
<a href="index.html#9523">[ thread ]</a>
<a href="subject.html#9523">[ subject ]</a>
<a href="author.html#9523">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Thu Jun 05 2003 - 15:06:47 MDT
</em></small></p>
</body>
</html>
