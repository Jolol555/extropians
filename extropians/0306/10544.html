<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: The Future of Secrecy</title>
<meta name="Author" content="Hal Finney (hal@finney.org)">
<meta name="Subject" content="Re: The Future of Secrecy">
<meta name="Date" content="2003-06-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Future of Secrecy</h1>
<!-- received="Wed Jun 18 12:22:13 2003" -->
<!-- isoreceived="20030618182213" -->
<!-- sent="Wed, 18 Jun 2003 11:21:04 -0700" -->
<!-- isosent="20030618182104" -->
<!-- name="Hal Finney" -->
<!-- email="hal@finney.org" -->
<!-- subject="Re: The Future of Secrecy" -->
<!-- id="200306181821.h5IIL4C25309@finney.org" -->
<!-- inreplyto="The Future of Secrecy" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Hal Finney (<a href="mailto:hal@finney.org?Subject=Re:%20The%20Future%20of%20Secrecy"><em>hal@finney.org</em></a>)<br>
<strong>Date:</strong> Wed Jun 18 2003 - 12:21:04 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10545.html">Michael Wiik: "Re: The Future of Secrecy"</a>
<ul>
<li><strong>Previous message:</strong> <a href="10543.html">Olga Bourlin: "Re: Rand and IRAQ"</a>
<li><strong>Maybe in reply to:</strong> <a href="10531.html">Robin Hanson: "The Future of Secrecy"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10551.html">Robin Hanson: "Re: The Future of Secrecy"</a>
<li><strong>Reply:</strong> <a href="10551.html">Robin Hanson: "Re: The Future of Secrecy"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10544">[ date ]</a>
<a href="index.html#10544">[ thread ]</a>
<a href="subject.html#10544">[ subject ]</a>
<a href="author.html#10544">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Robin writes:
<br>
<p><em>&gt; How leaky will our distant descendants be?  How far will they want to go,
</em><br>
<em>&gt; and be able to go, in agreeing to reveal their secrets to each other, to
</em><br>
<em>&gt; avoid the social problems that secrets cause?  It seems plausible that our
</em><br>
<em>&gt; descendants will be constructed so that they can allow outsiders to directly
</em><br>
<em>&gt; inspect the internal state of their minds, to verify the absence of certain
</em><br>
<em>&gt; harmful secrets.
</em><br>
<p>Such a practice is common among some of the members of the society
<br>
described in John C. Wright's book, The Golden Age, which we discussed
<br>
a few months ago.  Influenced by that and also some of the debate about
<br>
Microsoft's Palladium initiative, I wrote a message titled &quot;World of
<br>
Knights&quot; that explored some of the possibilities if people were able to
<br>
prove their honesty in this way:
<br>
<p><a href="http://forum.javien.com/XMLmessage.php?id=id::GQ1aUElJ-EF94-GX4k-C3Z0-PmNcO0MRbkoj">http://forum.javien.com/XMLmessage.php?id=id::GQ1aUElJ-EF94-GX4k-C3Z0-PmNcO0MRbkoj</a>
<br>
<p>Robin points out that economic pressures could strongly encourage the
<br>
voluntary adoption of these policies, making it harder for &quot;Knaves&quot; (those
<br>
who refuse to adopt this mental transparency) to compete and survive
<br>
economically once this becomes possible.  Indeed, in Wright's book the
<br>
&quot;Knights&quot; appear to be the most successful members of society, although it
<br>
is not clear what is cause and what is effect.
<br>
<p>The tie-in to Palladium I saw:
<br>
<p>: In some ways, Palladium-style &quot;trusted computing&quot; technology provides a
<br>
: preview of such a world, in a small domain. It lets you convincingly
<br>
: prove to a remote system that you are running a particular program,
<br>
: which means that your computer's behavior is trustworthy from the remote
<br>
: point of view. That's why you may be eventually forced to run Palladium
<br>
: systems in order to legally download movies and music, because only
<br>
: this kind of public commitment to &quot;honest&quot; or &quot;trustworthy&quot; behavior
<br>
: will win the confidence of the content companies.
<br>
<p>I suggested that our experiences with &quot;Trusted Computing&quot; technologies over
<br>
the next few years might give us a preview of the future world much along
<br>
the lines that Robin describes.
<br>
<p>Robin does raise one cautionary note:
<br>
<p><em>&gt; And one disturbing implication of this is that
</em><br>
<em>&gt; we may well evolve to become even *more* self-deceived than we are now,
</em><br>
<em>&gt; as believing one thing and thinking another becomes even harder than now.
</em><br>
<p>(Actually I think the idea has many disturbing aspects, but that's just
<br>
an emotional reaction!)
<br>
<p>Wei Dai had made a somewhat similar point in response to my message, at:
<br>
<p><a href="http://forum.javien.com/XMLmessage.php?id=id::Wh1DfB54-bjhA-bksC-UyYU-Ghl9HQFcbBUw">http://forum.javien.com/XMLmessage.php?id=id::Wh1DfB54-bjhA-bksC-UyYU-Ghl9HQFcbBUw</a>
<br>
<p>: Does
<br>
: it prevent you from believing in self-serving rationalizations? Our
<br>
: current notions of rationality depends on the assumption that not only are
<br>
: your beliefs private, but there is no way you can convince others that you
<br>
: truly believe them. If you *are* able to convince others of what your
<br>
: beliefs are, it's no longer in your self-interest to only believe in what
<br>
: is true. We already see this to some degree because humans are not able to
<br>
: lie costlessly. The incentive for self-serving rationalizations becomes
<br>
: much higher when lying is impossible. It's not clear whether this could be
<br>
: prevented by any kind of technology. 
<br>
<p>I'm not sure though that Robin's and Wei's point(s) are necessarily
<br>
valid; just as people could become economically compelled to tell the
<br>
truth, they might feel equal pressures to seek the truth, that is, to
<br>
not self-deceive.  It doesn't do me much good to know that you won't
<br>
lie to me, if I can't tell if you're lying to yourself.
<br>
<p>There also might be pressure, along the lines that Robin describes
<br>
for standardization, towards mental structures which are relatively
<br>
transparent and don't allow for self-deception.  A simple man is more
<br>
trustworthy than one who has layer upon layer of contradictory thoughts.
<br>
As long as this &quot;motivational simplicity&quot; is consistent with highly
<br>
intelligent and rational analysis, these kinds of minds should achieve
<br>
economic success.
<br>
<p>Hal
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10545.html">Michael Wiik: "Re: The Future of Secrecy"</a>
<li><strong>Previous message:</strong> <a href="10543.html">Olga Bourlin: "Re: Rand and IRAQ"</a>
<li><strong>Maybe in reply to:</strong> <a href="10531.html">Robin Hanson: "The Future of Secrecy"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10551.html">Robin Hanson: "Re: The Future of Secrecy"</a>
<li><strong>Reply:</strong> <a href="10551.html">Robin Hanson: "Re: The Future of Secrecy"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10544">[ date ]</a>
<a href="index.html#10544">[ thread ]</a>
<a href="subject.html#10544">[ subject ]</a>
<a href="author.html#10544">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jun 18 2003 - 12:32:40 MDT
</em></small></p>
</body>
</html>
