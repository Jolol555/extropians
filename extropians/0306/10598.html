<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: greatest threats to survival (was: why believe the truth?)</title>
<meta name="Author" content="Brett Paatsch (paatschb@optusnet.com.au)">
<meta name="Subject" content="Re: greatest threats to survival (was: why believe the truth?)">
<meta name="Date" content="2003-06-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: greatest threats to survival (was: why believe the truth?)</h1>
<!-- received="Thu Jun 19 07:05:42 2003" -->
<!-- isoreceived="20030619130542" -->
<!-- sent="Thu, 19 Jun 2003 23:07:19 +1000" -->
<!-- isosent="20030619130719" -->
<!-- name="Brett Paatsch" -->
<!-- email="paatschb@optusnet.com.au" -->
<!-- subject="Re: greatest threats to survival (was: why believe the truth?)" -->
<!-- id="00a801c33663$b784c5e0$87818ec6@brett" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="OJEHKDIANIFPAJPDBDGLEEFLCFAA.rafal@smigrodzki.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brett Paatsch (<a href="mailto:paatschb@optusnet.com.au?Subject=Re:%20greatest%20threats%20to%20survival%20(was:%20why%20believe%20the%20truth?)"><em>paatschb@optusnet.com.au</em></a>)<br>
<strong>Date:</strong> Thu Jun 19 2003 - 07:07:19 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10599.html">Olga Bourlin: "Re: OOPs. Re: Offlist   Re: List Moderator Suggestions"</a>
<ul>
<li><strong>Previous message:</strong> <a href="10597.html">Dehede011@aol.com: "Re: Rand and IRAQ"</a>
<li><strong>In reply to:</strong> <a href="10572.html">Rafal Smigrodzki: "RE: greatest threats to survival (was: why believe the truth?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10405.html">Robert J. Bradbury: "RE: greatest threats to survival (was: why believe the truth?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10598">[ date ]</a>
<a href="index.html#10598">[ thread ]</a>
<a href="subject.html#10598">[ subject ]</a>
<a href="author.html#10598">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Rafal writes:
<br>
<em>&gt; Brett wrote:
</em><br>
<em>&gt; &gt; Rafal Smigrodzki wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt; Brett wrote:
</em><br>
<em>&gt; &gt;&gt;&gt;
</em><br>
<em>&gt; &gt;&gt;&gt; PS:  If I *knew* what the greatest threat to survival
</em><br>
<em>&gt; &gt;&gt;&gt; to (say 150) was for the average healthy 36 year old
</em><br>
<em>&gt; &gt;&gt;&gt; Australian male was that might focus my energies
</em><br>
<em>&gt; &gt;&gt;&gt; wonderfully.
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt; ### UFAI.
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt; I think it could happen within the next 20 to 40 years,
</em><br>
<em>&gt; &gt;&gt; with a higher probability than the sum total of the 
</em><br>
<em>&gt; &gt;&gt; prosaic causes of death killing you over the same 
</em><br>
<em>&gt; &gt;&gt; time period.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Un-friendly AI ?  That *is* interesting.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; ..a 36 year old would be 56 to 76 in the timeframe you
</em><br>
<em>&gt; &gt; nominate and 76 is probably slightly over the average
</em><br>
<em>&gt; &gt; lifespan expected ...you really think unfriendly AI is
</em><br>
<em>&gt; &gt; that big a risk?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### If you are leading a healthy life, long-lived parents,
</em><br>
<em>&gt; and medical progress doesn't slow down, you should
</em><br>
<em>&gt; have a 70 - 80 % chance of making it to 80 years. 
</em><br>
<p>20 - 30 % by 80 (assuming no major surprises).
<br>
<p><em>&gt;  As to the AI - see below.
</em><br>
<em>&gt; -------------------------------
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Not being an AI enthusiast of the pedigree of certain
</em><br>
<em>&gt; &gt; others on this list I wonder:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; 1) What is the probability of General AI in the next
</em><br>
<em>&gt; &gt; 20 years of *either* friendly or unfriendly variety? 
</em><br>
<em>&gt; &gt; (I'm thinking about the massive parallelism of brains
</em><br>
<em>&gt; &gt;  and that maybe a subjective is a necessary pre-
</em><br>
<em>&gt; &gt; requisite for &quot;I&quot; and might be not so trivial to 
</em><br>
<em>&gt; &gt; engineer.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### My uneducated guess is 30 - 40%.
</em><br>
<p>Yep. In 20. So going higher over the following 20 yrs. 
<br>
<p>So 20 - 30 % mortality by 80yrs with pretty low X factor.
<br>
vs.  30 - 40 % mortality by 60yrs but with v.high X factor.
<br>
<p>X factor (unknown unknowns) seems too high for me to
<br>
rationally chose to spend my time lobbying to stop AI 
<br>
(defensively), when my potential opponents would be 
<br>
numerous and hard to identify, rather than pursue anti-
<br>
aging (pro-actively) when my potential allies are numerous
<br>
and pretty easy to find. 
<br>
<p>Perhaps a third option is to join the friendly AI race.
<br>
<p>But it would seem to make little sense to throw ones 
<br>
resources into joining such a race without a good reason
<br>
for believing (A) that &quot;friendly&quot; AI is not inherently absurd
<br>
and (B) that &quot;friendly&quot; AI is not going to be intrinsically at
<br>
a disadvantage to UFAI.  Perhaps one has to be a zebra
<br>
to spend time trying to build a vegetarian lion?
<br>
<p><em>&gt; ----------------------------
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; 2) How would this probability be figured? What
</em><br>
<em>&gt; &gt; assumptions are required?  (I am an open-minded
</em><br>
<em>&gt; &gt; AI sceptic. But then I am an &quot;I&quot; sceptic too so that's
</em><br>
<em>&gt; &gt; not saying a great deal.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### Fuzzy thinking about Moravec, Moore, maybe
</em><br>
<em>&gt; some future markets.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Is there a market future on the possibility of AGI 
</em><br>
<em>&gt; available on ideafutures?
</em><br>
<p>I'll leave that one to Robin. 
<br>
<p><em>&gt; -----------------------------------
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; 3) Can friendly AI be built that would be competitive
</em><br>
<em>&gt; &gt; with un-friendly AI or would the friendly AI be at the
</em><br>
<em>&gt; &gt; same sort of competitive/selective disadvantage as a
</em><br>
<em>&gt; &gt; lion that wastes time and sentiment (resources)
</em><br>
<em>&gt; &gt; making friends with zebras?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### Hard to tell. If some humans try to build FAI, we
</em><br>
<em>&gt; will have better chances than if nobody does, but I
</em><br>
<em>&gt;  wouldn't place any bets yet.
</em><br>
<p>I wonder about this. What if some person A's friendly AI
<br>
turns out to be person B's unfriendly AI?
<br>
<p>Quite a bit seems to turn on the question of what is 
<br>
&quot;friendly&quot; in the *context* of an AI, where would the
<br>
boundaries of it's &quot;friendliness&quot; end, or appear to various
<br>
people to end.
<br>
<p>Weren't Oppenheimer and General Groves trying to 
<br>
build a 'friendly' nuke BEFORE Von Braun and
<br>
Heisenberg's 'leader' built one that he'd regard with
<br>
fondness?
<br>
<p>Regards,
<br>
Brett Paatsch
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10599.html">Olga Bourlin: "Re: OOPs. Re: Offlist   Re: List Moderator Suggestions"</a>
<li><strong>Previous message:</strong> <a href="10597.html">Dehede011@aol.com: "Re: Rand and IRAQ"</a>
<li><strong>In reply to:</strong> <a href="10572.html">Rafal Smigrodzki: "RE: greatest threats to survival (was: why believe the truth?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10405.html">Robert J. Bradbury: "RE: greatest threats to survival (was: why believe the truth?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10598">[ date ]</a>
<a href="index.html#10598">[ thread ]</a>
<a href="subject.html#10598">[ subject ]</a>
<a href="author.html#10598">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Thu Jun 19 2003 - 07:16:12 MDT
</em></small></p>
</body>
</html>
