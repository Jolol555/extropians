<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: AI Risks, was Re: Rulers and Famine in Poor Countries (was Obesity)</title>
<meta name="Author" content="Adrian Tymes (wingcat@pacbell.net)">
<meta name="Subject" content="Re: AI Risks, was Re: Rulers and Famine in Poor Countries (was Obesity)">
<meta name="Date" content="2003-03-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI Risks, was Re: Rulers and Famine in Poor Countries (was Obesity)</h1>
<!-- received="Sun Mar  9 19:46:45 2003" -->
<!-- isoreceived="20030310024645" -->
<!-- sent="Sun, 9 Mar 2003 18:46:36 -0800 (PST)" -->
<!-- isosent="20030310024636" -->
<!-- name="Adrian Tymes" -->
<!-- email="wingcat@pacbell.net" -->
<!-- subject="Re: AI Risks, was Re: Rulers and Famine in Poor Countries (was Obesity)" -->
<!-- id="20030310024636.19245.qmail@web80109.mail.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="oprlsg1w1smkbtpc@spies.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Adrian Tymes (<a href="mailto:wingcat@pacbell.net?Subject=Re:%20AI%20Risks,%20was%20Re:%20Rulers%20and%20Famine%20in%20Poor%20Countries%20(was%20Obesity)"><em>wingcat@pacbell.net</em></a>)<br>
<strong>Date:</strong> Sun Mar 09 2003 - 19:46:36 MST
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4074.html">Spudboy100@aol.com: "Cleanest Car?"</a>
<ul>
<li><strong>Previous message:</strong> <a href="4072.html">spike66: "welcome to the singularity"</a>
<li><strong>In reply to:</strong> <a href="4054.html">Michael M. Butler: "AI Risks, was Re: Rulers and Famine in Poor Countries (was Obesity)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3854.html">Anders Sandberg: "Re: Obesity (was Extropic Priniciples)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4073">[ date ]</a>
<a href="index.html#4073">[ thread ]</a>
<a href="subject.html#4073">[ subject ]</a>
<a href="author.html#4073">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- &quot;Michael M. Butler&quot; &lt;<a href="mailto:mmb@spies.com?Subject=Re:%20AI%20Risks,%20was%20Re:%20Rulers%20and%20Famine%20in%20Poor%20Countries%20(was%20Obesity)">mmb@spies.com</a>&gt; wrote:
<br>
<em>&gt; Adrian Tymes &lt;<a href="mailto:wingcat@pacbell.net?Subject=Re:%20AI%20Risks,%20was%20Re:%20Rulers%20and%20Famine%20in%20Poor%20Countries%20(was%20Obesity)">wingcat@pacbell.net</a>&gt; wrote:
</em><br>
<em>&gt; &gt; You have a *chance* of dying.  You could, totally
</em><br>
<em>&gt; by
</em><br>
<em>&gt; &gt; accident, wind up doing the right thing anyway. 
</em><br>
<em>&gt; This
</em><br>
<em>&gt; &gt; is not the same thing as guaranteed death.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And the combinatorics could wind up resulting in a
</em><br>
<em>&gt; likelihood smaller than 
</em><br>
<em>&gt; the quantity 1 over the number of seconds that have
</em><br>
<em>&gt; elapsed since the Big 
</em><br>
<em>&gt; Bang. That's not the same as guaranteed death, but
</em><br>
<em>&gt; it's the way to bet if 
</em><br>
<em>&gt; those are the odds. So are odds of 1 in 100.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Adrian, just for my couriosity's sake, how do you
</em><br>
<em>&gt; figure
</em><br>
<em>&gt; the odds? I'm not just asking that question
</em><br>
<em>&gt; rhetorically.
</em><br>
<p>To be honest, I don't.  I haven't gone through the
<br>
effort to make a hard statistical analysis, largely
<br>
because most of the data this analysis would be based
<br>
on is so fuzzy as to make the analysis near-useless
<br>
for
<br>
convincing anyone.
<br>
<p>What I have done, however, is draw parallels to my own
<br>
knowledge and study of intelligence and power, and
<br>
make
<br>
the assumption that AIs of any import to the world
<br>
would live in the same reality as I have.  Which means
<br>
that similar solutions for granting other people power
<br>
work, for much the same reasons, if you make the
<br>
assumption (which I believe most people would admit)
<br>
that AIs are, essentially, &quot;people&quot; with certain
<br>
enhancements (for instance, the ability to calculate
<br>
much much faster, the ability to hold more points of
<br>
data - though still a finite number, however large -
<br>
in
<br>
their minds, and so forth; the exact list can be
<br>
argued).
<br>
<p>Which means that an AI could be sociopathic (i.e.,
<br>
uninterested in mere human survival, or even opposed
<br>
to
<br>
it as in so many Hollywood flicks), greedy,
<br>
solipsistic
<br>
(i.e., refusing to believe that we humans are
<br>
sentient,
<br>
even if it were proven that we created it), and so
<br>
forth, like any person.  If that is the case, then
<br>
planning on the case where you make an AI without
<br>
those
<br>
flaws is, itself, flawed: while you could never
<br>
succeed
<br>
in that goal, you *could* be pressured to declare
<br>
success, pretend you have succeeded (and thus, do not
<br>
need to defend against those flaws), and move on to
<br>
grant the AI power.  Which is a scenario for disaster.
<br>
<p>On the other hand, if you account for that and treat
<br>
the AI like a human being, with potentials for flaws
<br>
imagined and unimagined, then you can still set up
<br>
systems where, no matter how intelligent the AI is, it
<br>
can not magically outthink you and gain freedom until
<br>
you, or someone, trusts it.  And to gain your trust,
<br>
it
<br>
would have to emulate friendly (dare I even say
<br>
Friendly?) practices.  The only real data point we
<br>
have
<br>
for such scenarios is human behavior, and the human
<br>
data shows that, the vast majority (easily over 99%)
<br>
of
<br>
the time, a human being who fundamentally acts
<br>
friendly
<br>
really is friendly.  This trend does not go down with
<br>
increasing intelligence; if anything, it goes up -
<br>
which would seem that the odds of a seemingly-friendly
<br>
AI actually being friendly would be even higher.
<br>
<p>Besides, any plan for dealing with and developing AIs
<br>
must take into account the abilities of the humans
<br>
developing it, else the plan can not be put into
<br>
practice (though, as described above, fatally flawed
<br>
versions of the plan can be; sadly, this happens all
<br>
the time in politics and business).  Humans do not
<br>
always have the ability to thoroughly review and
<br>
understand programs, especially nontrivial programs.
<br>
(For this, I cite my own professional knowledge.)  But
<br>
they do have the ability to evaluate human-like beings
<br>
as if they were humans (cite anthropormorphism
<br>
throughout the ages).
<br>
<p><em>&gt; Against that, I think a case can be made that, for
</em><br>
<em>&gt; issues as hairy as this, 
</em><br>
<em>&gt; Bayes isn't applicable in a vacuum; we don't get to
</em><br>
<em>&gt; refine our estimates
</em><br>
<em>&gt; over multiple trials that include catastrophe--maybe
</em><br>
<em>&gt; the human race does 
</em><br>
<em>&gt; (if
</em><br>
<em>&gt; we luck out), maybe not, but probably not we here
</em><br>
<em>&gt; assembled. Conflict 
</em><br>
<em>&gt; levels
</em><br>
<em>&gt; of 11 (that's 10^11 casualties--hundreds of millions
</em><br>
<em>&gt; dead) /or more/ also 
</em><br>
<em>&gt; surely change the weight.
</em><br>
<p>And what are the odds of such a conflict if we do
<br>
*not*
<br>
go this route?  Neither way prevents or guarantees it;
<br>
it just makes the outcome more or less likely, over
<br>
whatever time span you wish to designate significant.
<br>
<p><em>&gt; &gt;&gt; Do you have any ideas for dealing with this
</em><br>
<em>&gt; besides
</em><br>
<em>&gt; &gt;&gt; building FAI first? Because as far as I can tell,
</em><br>
<em>&gt; humanity is in
</em><br>
<em>&gt; &gt;&gt; serious, serious trouble.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Yes.  Build an AI that you can trust, even if it
</em><br>
<em>&gt; &gt; quickly goes beyond your control.
</em><br>
<em>&gt; ...
</em><br>
<em>&gt; &gt; perspective), but if you can trust someone else to
</em><br>
<em>&gt; be
</em><br>
<em>&gt; &gt; gifted with the capabilities that an AI would
</em><br>
<em>&gt; have...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I agree with what you're saying but (and perhaps
</em><br>
<em>&gt; this is also what you're 
</em><br>
<em>&gt; saying) the only way I can see to apply the level of
</em><br>
<em>&gt; trust I give to a 
</em><br>
<em>&gt; person is to guarantee the physical limitations and
</em><br>
<em>&gt; the
</em><br>
<em>&gt; developmental psychological ground of an actual
</em><br>
<em>&gt; human.
</em><br>
<p>That's certainly one way to do it.  In fact, one
<br>
scenario I think rather likely to work - and to
<br>
increase our chances of this being done in a way that
<br>
guarantees the survival of most presently human
<br>
sentient beings - is to find a way to augment human
<br>
intelligence into AIs.  Certain elites will gain this
<br>
capability before most people; that can not feasably
<br>
be
<br>
avoided.  But if this power can be allowed to spread
<br>
rapidly and freely (despite the inevitable
<br>
restrictions
<br>
some of these early adopters, or non-adopter elites
<br>
afraid of the power this could bring to non-elites,
<br>
will impose), then IMO the only significant risk of
<br>
human extinction would be to the mere biological
<br>
shells
<br>
we presently inhabit.  And even a good number of those
<br>
would likely survive, at least for a time, by uploads
<br>
with a sentimental attachment to the past.  (If
<br>
nothing
<br>
else.  Mmaybe the first uploads will be upgraded
<br>
organic circuitry, using some of the best life-support
<br>
equipment for such things we have available today:
<br>
already grown, living bodies, like the ones the brains
<br>
came from in the first place.)
<br>
<p><em>&gt; I wonder how likely this is, given the fraction of
</em><br>
<em>&gt; people with technical 
</em><br>
<em>&gt; chops that would dismiss this out of hand as
</em><br>
<em>&gt; [quasi]paranoid.
</em><br>
<em>&gt; We internet-enable hot tubs, for goodness' sake. So
</em><br>
<em>&gt; the first aspect 
</em><br>
<em>&gt; (physical limitations) is likely to be ignored in
</em><br>
<em>&gt; some way by even the 
</em><br>
<em>&gt; &quot;embodied consciousness&quot; folks.
</em><br>
<p>You also have to consider their arguments in light of
<br>
their likelihood of sucess.  Intelligence, or at least
<br>
that which we are trying to replicate here (whatever
<br>
its true nature, and whether or not it really has a
<br>
name as yet), is defined as a uniquely human trait (at
<br>
the moment).  Biomimicry has proven successful in many
<br>
other fields, and I would argue that AI is no
<br>
exception
<br>
- if you do not define Eliza and similar chatbots as
<br>
&quot;successful&quot;.  The most robust, adaptable, and (again,
<br>
I would argue) true intelligence to date has been the
<br>
very close mimicing of bugs' control algorithms, and
<br>
there is every reason to believe that this approach
<br>
will scale up at least to human levels (existence
<br>
proof: it's done so with organic evolved products, and
<br>
the main limitation to doing it in artificial products
<br>
is one which Moore's Law looks like it will beat down
<br>
in due time - assuming it holds long enough, and
<br>
despite the hue and cry from certain sectors about its
<br>
imminent failure, most of the evidence suggests that
<br>
it
<br>
will).
<br>
<p>All of which is to say, when true AIs finally do
<br>
arrive, the odds are that they will have something
<br>
like
<br>
a human upbringing.  And given that the odds favor
<br>
this
<br>
outcome as it is, actions (including strenuous debate)
<br>
to prevent any other approach are of diminished value
<br>
in achieving their approach, while their costs
<br>
(including, for some actions, retarding potential
<br>
advances along any approach to AI, even the preferred
<br>
one) remain unaffected.  Therefore, it is best to
<br>
allow
<br>
even the &quot;likely to produce monsters&quot; paths to
<br>
succeed,
<br>
such that the &quot;likely to produce what we want&quot; paths
<br>
may benefit from the results.
<br>
<p>All of which is a long way to say: Don't Panic.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4074.html">Spudboy100@aol.com: "Cleanest Car?"</a>
<li><strong>Previous message:</strong> <a href="4072.html">spike66: "welcome to the singularity"</a>
<li><strong>In reply to:</strong> <a href="4054.html">Michael M. Butler: "AI Risks, was Re: Rulers and Famine in Poor Countries (was Obesity)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3854.html">Anders Sandberg: "Re: Obesity (was Extropic Priniciples)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4073">[ date ]</a>
<a href="index.html#4073">[ thread ]</a>
<a href="subject.html#4073">[ subject ]</a>
<a href="author.html#4073">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Sun Mar 09 2003 - 19:51:51 MST
</em></small></p>
</body>
</html>
