<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: Why Does Self-Discovery Require a Journey?</title>
<meta name="Author" content="Dan Fabulich (dfabulich@warpmail.net)">
<meta name="Subject" content="Re: Why Does Self-Discovery Require a Journey?">
<meta name="Date" content="2003-07-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Why Does Self-Discovery Require a Journey?</h1>
<!-- received="Mon Jul 14 03:11:46 2003" -->
<!-- isoreceived="20030714091146" -->
<!-- sent="Mon, 14 Jul 2003 02:11:21 -0700 (Pacific Daylight Time)" -->
<!-- isosent="20030714091121" -->
<!-- name="Dan Fabulich" -->
<!-- email="dfabulich@warpmail.net" -->
<!-- subject="Re: Why Does Self-Discovery Require a Journey?" -->
<!-- id="Pine.WNT.4.50.0307140046170.1132-100000@darkforge" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="5.2.1.1.2.20030713225858.01d08c40@mail.gmu.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Dan Fabulich (<a href="mailto:dfabulich@warpmail.net?Subject=Re:%20Why%20Does%20Self-Discovery%20Require%20a%20Journey?"><em>dfabulich@warpmail.net</em></a>)<br>
<strong>Date:</strong> Mon Jul 14 2003 - 03:11:21 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11455.html">Eliezer S. Yudkowsky: "Re: Why Does Self-Discovery Require a Journey?"</a>
<ul>
<li><strong>Previous message:</strong> <a href="11453.html">ABlainey@aol.com: "list problems?"</a>
<li><strong>In reply to:</strong> <a href="11447.html">Robin Hanson: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11455.html">Eliezer S. Yudkowsky: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Reply:</strong> <a href="11455.html">Eliezer S. Yudkowsky: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Reply:</strong> <a href="11576.html">Robin Hanson: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11454">[ date ]</a>
<a href="index.html#11454">[ thread ]</a>
<a href="subject.html#11454">[ subject ]</a>
<a href="author.html#11454">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Robin Hanson wrote:
<br>
<p><em>&gt; On 7/13/2003, Dan Fabulich wrote:
</em><br>
<em>&gt; &gt; &gt; A happiness metric does not require that people be informed about the
</em><br>
<em>&gt; &gt; &gt; consequences of their choice.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;Of course not.  Perhaps I'm misunderstanding your argument, but I thought
</em><br>
<em>&gt; &gt;the claim was: &quot;We can think of two ways of identifying what you'd
</em><br>
<em>&gt; &gt;'really' want.  First, we can see if you'd be happier if you got it.
</em><br>
<em>&gt; &gt;Second, we can see if you'd choose to get it when they are briefly and
</em><br>
<em>&gt; &gt;privately informed.  In the case of betraying the tribe, the person would
</em><br>
<em>&gt; &gt;do this if briefly and privately informed.  Therefore, this is what they
</em><br>
<em>&gt; &gt;really want.&quot;
</em><br>
<em>&gt; &gt;I'm claiming, against this: &quot;But it wouldn't, in fact, make them happy.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The question was about the choice between power/status and doing good for
</em><br>
<em>&gt; the tribe.  We have been assuming that typically when faced with this choice
</em><br>
<em>&gt; people fool themselves into thinking they are actually doing good for the
</em><br>
<em>&gt; tribe.  We have been asking if this is what they &quot;really want&quot;, and I
</em><br>
<em>&gt; proposed considering two standard ways to define what we &quot;want.&quot;  I claim
</em><br>
<em>&gt; that people are happier in the situations where they get power/status,
</em><br>
<em>&gt; relative to the situations where the tribe has been done good to, in part
</em><br>
<em>&gt; because they can fool themselves into thinking they are doing good by
</em><br>
<em>&gt; getting power/status.
</em><br>
<p>Ah, I see.  So, would you agree that, if I am right that most people who
<br>
make a briefly informed betrayal would be miserable, your happiness metric
<br>
would, at least, be in contradiction with the &quot;informed choice&quot; metric in
<br>
those cases?
<br>
<p>Of course, the opposing results may also be reversed: even in cases where
<br>
people are actually blindly obliviously happier when their tribe is worse
<br>
off, it may still be the case that the &quot;informed choice&quot; *would* lead them
<br>
in an entirely different path, rejecting their earlier programming, were
<br>
it to be applied.
<br>
<p>Having established that it's possible, there's the question of
<br>
establishing the real answer: just how common *are* these contradiction
<br>
cases?  Eliezer and I can only provide prima facie back-of-the-envelope
<br>
responses, e.g. Eliezer claims that he can't think of anybody who has been
<br>
informed of their programming and hasn't tried to shake it off; he also
<br>
claims that most people who choose betrayal in an informed way turn out
<br>
miserable from guilt.  Eliezer's arguments seem right to me, but actually
<br>
testing this requires real research to which I can only give an
<br>
&quot;intelligent layman's&quot; response.
<br>
<p>Regardless, the latter argument I'm making allows you to win this claim
<br>
without forcing me to conclude that you've identified &quot;real&quot; wants; on
<br>
that point, it's philosophy.  [In which, you know, I actually have a
<br>
degree. ;)]
<br>
<p><em>&gt; &gt;Your argument is that we're self-decieved: we are &quot;really&quot;
</em><br>
<em>&gt; &gt;untrustworthy, in the sense that we &quot;really&quot; want things that no one would
</em><br>
<em>&gt; &gt;trust us if they knew we wanted them (and that we would act to get them).
</em><br>
<em>&gt; &gt;You follow that up with the claim that if individuals become honest to and
</em><br>
<em>&gt; &gt;with themselves, they'll suffer serious consequences.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;Any plausible normative corollary to this claim would require that either:
</em><br>
<em>&gt; &gt;1 we should cast off our &quot;self-deception&quot; and be honest to ourselves, come
</em><br>
<em>&gt; &gt;what may, accepting that we all really want terrible things and that no
</em><br>
<em>&gt; &gt;one should be trusted in the ways that they've been pressured to claim, or
</em><br>
<em>&gt; &gt;2 we shouldn't believe the truth, as you argued in an earlier thread.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I have so far avoided making any moral claims; I have just been making
</em><br>
<em>&gt; claims about what people &quot;really want&quot;.
</em><br>
<p>I was actually quite aware of that; I went back and reviewed your postings
<br>
on this matter and was surprised by how little you had to say about the
<br>
practical component of this argument.
<br>
<p>Still, I stand by my claim that there are basically two plausible
<br>
normative implications of your claim: 1 accept it honestly, &quot;come what
<br>
may&quot; or 2 reject it, even though it is true; while you needn't commit
<br>
yourself to either one in particular, (e.g. if I utterly destroy 2, you
<br>
may still take refuge in 1, and vice versa,) I think you must ultimately
<br>
agree with *one* of those two normative corollaries if you accept the
<br>
arguments you're making about what people &quot;really want.&quot;
<br>
<p>And if both of the possible moral implications of doing this are bad, when
<br>
there's another option which doesn't have these bad moral implications, I
<br>
can just say &quot;Look, accepting this claim would require me to act
<br>
sub-optimally, or perhaps in a way that is deontologically forbidden;
<br>
furthermore, there's an alternative proposition I can accept that doesn't
<br>
lead me down the wrong moral road.  Therefore, I should accept the
<br>
alternative proposition instead, and I should morally exhort others to do
<br>
the same.&quot;  (Which is, of course, what I've actually done.)
<br>
<p><em>&gt; &gt;As you may recall, I argued that 2 was self-contradictory in the
</em><br>
<em>&gt; &gt;&quot;Should we believe the truth&quot; thread: the short version of that
</em><br>
<em>&gt; &gt;argument was that it was contrary to 2's own normative principles to
</em><br>
<em>&gt; &gt;believe that it was true.  So, if it were true, we shouldn't believe
</em><br>
<em>&gt; &gt;it: it would be its own first casualty.  I claimed further that 2 was
</em><br>
<em>&gt; &gt;in violation of normative logic: that, rather, the truth is what we
</em><br>
<em>&gt; &gt;should believe.  (This is because I shouldn't believe that &quot;there is
</em><br>
<em>&gt; &gt;some X such that X is true but I shouldn't believe X.&quot;)
</em><br>
<em>&gt;
</em><br>
<em>&gt; You have generalized your 2 far more than need be.  One might instead
</em><br>
<em>&gt; claim that we shouldn't believe certain particular truths.  That claim
</em><br>
<em>&gt; would not be self-contradictory.
</em><br>
<p>That doesn't seem right at all...  Supposing you did try to argue that
<br>
this was just one particular truth that we shouldn't believe, you'd at
<br>
least fall under the *domain* of the second argument from normative logic,
<br>
since you'd be claiming that &quot;there is some X that's true but I shouldn't
<br>
believe it.&quot;  You may come to reject my second argument, but I don't think
<br>
you could claim that it &quot;doesn't apply&quot; to your argument because my
<br>
argument is generalized whereas your claim is specific.
<br>
<p>Similarly, I'd argue that any &quot;specific&quot; argument you might make would get
<br>
hit by the first argument as well.  By arguing that your claim about our
<br>
&quot;real desires&quot; is true, but you shouldn't believe it, you'd be forced to
<br>
accept the logical inference that there is, therefore, some truth that you
<br>
shouldn't believe;  that you shouldn't always believe the truth.  But this
<br>
was precisely the claim I was responding to in the first place.  (Perhaps
<br>
you thought I was merely arguing against a case where you should
<br>
disbelieve the truth &quot;most of the time,&quot; but I take my argument to be
<br>
adequately general that it could apply even when you propose that there's
<br>
just a few white lies we should believe.)
<br>
<p><em>&gt; &gt;... being anti-realistic about the set of desires D, those which
</em><br>
<em>&gt; &gt;the economist seems to find that we have, allows us to act correctly on
</em><br>
<em>&gt; &gt;the economic data without believing anything that would lead us to have to
</em><br>
<em>&gt; &gt;accept 1 or 2.  We can say, without contradiction, that the economist has
</em><br>
<em>&gt; &gt;found a set of things that might be called desires but aren't &quot;really&quot;
</em><br>
<em>&gt; &gt;what we desire at all.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It seems to me you are going through these contortions for the mere purpose
</em><br>
<em>&gt; of being able at the end to say that we really want to be moral.  Sure, you
</em><br>
<em>&gt; say, in the ordinary sense of want we don't want to be moral, but in this
</em><br>
<em>&gt; new spiffy philosophical sense of want, that we just invented for this
</em><br>
<em>&gt; purpose, we do want to be moral.  Why not just accept that people don't want
</em><br>
<em>&gt; to be moral?
</em><br>
<p>[I'd like to open with an aside on rhetoric here.  When two people discuss
<br>
X and ~X, &quot;Why don't you believe that X?&quot; is a perfectly reasonable
<br>
question, whereas &quot;Why don't you accept that X?&quot; is an extremely loaded
<br>
question.
<br>
<p>In the minds of many readers, the latter question asks the interlocutor to
<br>
explain what psychological hang-ups are preventing him from accepting the
<br>
truth, X.  The former question asks for rational reasons why one wouldn't
<br>
switch positions.
<br>
<p>I'm calling attention to this not because I think you're trying to load
<br>
the question, so much as because I think if you'd noticed this
<br>
connotation, as I did, you probably wouldn't use it.  I'd also like to
<br>
politely ask that we avoid using that particular phrase in the future.]
<br>
<p>OK, with that aside, I'll now actually answer your question: Why not just
<br>
believe that people don't want to be moral?
<br>
<p>You answered this question yourself at the opening of the thread:
<br>
<p>On Mon, 07 Jul 2003, Robin Hanson wrote:
<br>
<p><em>&gt; Real journeys of self-discovery would largely be dark affairs, wherein
</em><br>
<em>&gt; mounting evidence forced people to believe ignoble things about
</em><br>
<em>&gt; themselves that they would rather not tell others.  And those who do
</em><br>
<em>&gt; struggle over decades to learn the truth about what people want, and who
</em><br>
<em>&gt; are willing to tell others, would face largely indifferent or hostile
</em><br>
<em>&gt; audiences.
</em><br>
<p>It's easy to see how life will be miserable for anyone who believes and
<br>
espouses this extremely cynical view; I take your depiction here to be a
<br>
pretty gross understatement as to how bad life would really be for someone
<br>
who took this philosophy truly seriously.  Just imagine how much
<br>
association you'd like to have with someone who was in the habit of saying
<br>
things this: &quot;I've gone through a terrible self-discovery process, and
<br>
discovered that I don't really like other people, and, what's more, I
<br>
realized that almost no one else does either.  I'd rather be famous than
<br>
help anybody; I believe most intimate relationships are based on commonly
<br>
accepted lies, and that most people believe in our common mores on account
<br>
of comforting self-deception.&quot;
<br>
<p>But even supposing that it would ONLY be as bad as you depicted earlier,
<br>
well, hey, that's pretty bad.  If it turns out that I have the choice of
<br>
doing that or living a much better life, without compromising my
<br>
intellectual honesty, it seems obvious to me which I (or anyone else)
<br>
should prefer.  Supposing I DID have to go through contortions to get it,
<br>
it seems to me they would be worth it!
<br>
<p>Furthermore, I think you are in no way licensed to conclude that you've
<br>
correctly captured the notion of &quot;want&quot; that agrees with &quot;the ordinary
<br>
sense&quot;; indeed, if anyone's more licensed to make the claim to agree with
<br>
&quot;ordinary common sense&quot; here, it would have to be me: you're the one
<br>
proposing that almost everyone is basically wrong (self-deceived) in the
<br>
way that they use the word &quot;want&quot;.
<br>
<p>This brings me back to Davidsonian style arguments: if you were
<br>
interpreting the &quot;ordinary sense of want&quot; correctly, which is to say
<br>
charitably, you would never accept an error theory like the one you
<br>
currently propose.  Interpretative theories should have the possibility
<br>
for mistakes, but that's a world of difference from near-universal
<br>
self-deception; if that's your conclusion, it seems that you obviously
<br>
failed to apply enough of the principle of charity.
<br>
<p><em>&gt; I don't think I follow you here, but it seems irrelevant to me; if need
</em><br>
<em>&gt; be I'll just put on my philosopher's hat and claim to be in both roles.
</em><br>
<p>If you like; I maintain that your economist's hat won't help you much in
<br>
the realism/anti-realism debate, but we're cosmopolitan these days; you
<br>
can wear any hat you like. ;)
<br>
<p>-Dan
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-unless you love someone-
<br>
&nbsp;&nbsp;&nbsp;&nbsp;-nothing else makes any sense-
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e.e. cummings
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11455.html">Eliezer S. Yudkowsky: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Previous message:</strong> <a href="11453.html">ABlainey@aol.com: "list problems?"</a>
<li><strong>In reply to:</strong> <a href="11447.html">Robin Hanson: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11455.html">Eliezer S. Yudkowsky: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Reply:</strong> <a href="11455.html">Eliezer S. Yudkowsky: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Reply:</strong> <a href="11576.html">Robin Hanson: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11454">[ date ]</a>
<a href="index.html#11454">[ thread ]</a>
<a href="subject.html#11454">[ subject ]</a>
<a href="author.html#11454">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Mon Jul 14 2003 - 03:21:48 MDT
</em></small></p>
</body>
</html>
