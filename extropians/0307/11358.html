<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: Why Does Self-Discovery Require a Journey?</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Why Does Self-Discovery Require a Journey?">
<meta name="Date" content="2003-07-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Why Does Self-Discovery Require a Journey?</h1>
<!-- received="Fri Jul 11 00:52:52 2003" -->
<!-- isoreceived="20030711065252" -->
<!-- sent="Fri, 11 Jul 2003 02:50:41 -0400" -->
<!-- isosent="20030711065041" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Why Does Self-Discovery Require a Journey?" -->
<!-- id="3F0E5E41.5040001@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="5.2.1.1.2.20030708070030.01c880f0@mail.gmu.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Why%20Does%20Self-Discovery%20Require%20a%20Journey?"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri Jul 11 2003 - 00:50:41 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11359.html">Jeff Davis: "RE: HUMANS and low genetic complexity"</a>
<ul>
<li><strong>Previous message:</strong> <a href="11357.html">Spike: "RE: Number of carbon atoms in the Earth's biomass"</a>
<li><strong>In reply to:</strong> <a href="11258.html">Robin Hanson: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11360.html">Wei Dai: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Reply:</strong> <a href="11360.html">Wei Dai: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Reply:</strong> <a href="11368.html">Robin Hanson: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11358">[ date ]</a>
<a href="index.html#11358">[ thread ]</a>
<a href="subject.html#11358">[ subject ]</a>
<a href="author.html#11358">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Robin Hanson wrote:
<br>
<em>&gt; At 10:02 PM 7/7/2003 -0400, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; I am objecting to your phraseology here because it seems to 
</em><br>
<em>&gt;&gt; preemptively settle the issue by identifying people's built-in 
</em><br>
<em>&gt;&gt; emotional reinforcers as their real wants, while dismissing their 
</em><br>
<em>&gt;&gt; cognitively held hopes and aspirations and personal philosophy as a 
</em><br>
<em>&gt;&gt; foreign force interfering with their true selves.  One could just as
</em><br>
<em>&gt;&gt; easily view the system from the opposite perspective.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I do agree that this is a subtle question, whose answer is not 
</em><br>
<em>&gt; immediately obvious.  The topic of self-deception can be a conceptual 
</em><br>
<em>&gt; morass, as our usual anchors are not as available.   Nevertheless, I do
</em><br>
<em>&gt; want to argue for the claim you find questionable.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If people have contradictory beliefs, how can we say which ones are the
</em><br>
<em>&gt; &quot;real&quot; beliefs?  By reference to the basic schema of self-deception,
</em><br>
<em>&gt; in which the real beliefs tend to determine less visible actions with
</em><br>
<em>&gt; more fundamental consequences, and the false beliefs tend to determine
</em><br>
<em>&gt; what we tell others and ourselves about ourselves, and the most
</em><br>
<em>&gt; socially visible actions with the least fundamental consequences.
</em><br>
<p>When you deal with a human, you are dealing with at least three
<br>
intertwined goal-ish systems.
<br>
<p>There's the hardware wiring that determines which thoughts and experiences
<br>
are pleasurably reinforced, or painful.  This wiring is by no means
<br>
simple; it has more modes of action than just &quot;pleasure&quot; or &quot;pain&quot;,
<br>
despite that usual division.  Still, it overall tends to shift people's
<br>
thoughts in a goal-ish way.
<br>
<p>There are people's declaratively held purposes and goals.
<br>
<p>And there is evolution, which is also a goal-ish system, and which acts to
<br>
select among {heriditary information in the initial conditions of the
<br>
other two systems} based solely on the correlation between {the ultimate
<br>
outcome of the interaction of thought and emotion} and the number of
<br>
surviving grandchildren.
<br>
<p>If you go around determining real beliefs by the *outcomes* of people's 
<br>
actions, you run the risk of confusing evolutionary motives with cognitive 
<br>
ones, the classic mistake in evolutionary psychology.  Every atom of 
<br>
complex hereditary information in humans was constructed by evolution 
<br>
based on the sole and only criterion of reproductive success, yet the 
<br>
explicit wish for children is only one among hundreds of explicitly 
<br>
represented cognitive reinforcers, woven by memes and philosophies into 
<br>
individually unique personal philosophies.
<br>
<p><em>&gt; People may want to produce art to gain social approval, wealth, mates,
</em><br>
<em>&gt; etc., but want to be thought of as doing it just for the art. People
</em><br>
<em>&gt; may want to advocate positions that make them seem clever and 
</em><br>
<em>&gt; compassionate, and get them social accepted by the right folks, but
</em><br>
<em>&gt; want to be thought of as wanting only to tell the truth.  People may
</em><br>
<em>&gt; want to be unfair when serving as a neutral judge, but want to thought
</em><br>
<em>&gt; of as fair.  These examples should be familiar to everyone; is there
</em><br>
<em>&gt; anyone here to whom these are &quot;news&quot;?
</em><br>
<p>Aren't these instances of the classic error?
<br>
<p>People have emotional hardware and cognitive representations leading them 
<br>
to be devoted to art for its own sake, because people who, in the past, 
<br>
possessed the hardware for art, or fell into the philosophical attractor 
<br>
for personal philosophies exalting art as the result of their innate 
<br>
tendencies and biases, gained social approval, wealth, and mates.  And 
<br>
even these are proximal goals, don't forget; all evolution cared about was 
<br>
the grandchildren.
<br>
<p>People have emotions leading them to honestly advocate positions that 
<br>
people applaud as clever and compassionate, because their ancestors who 
<br>
did the same were socially accepted by the right folks.  They say they're 
<br>
just telling the truth, and they *are* just telling the truth, because in 
<br>
the evolutionary arms race of liars and lie-detectors, it's easier and 
<br>
more reliable to deceive the phenotype you're building than to have your 
<br>
phenotype deliberately deceive other genomes' phenotypes.  People think 
<br>
they're honest and they are, but what think is the truth is output by 
<br>
biased reasoning hardware that was constructed according to the sole and 
<br>
only criterion of the number of surviving grandchildren to which that 
<br>
hardware's outputs led.
<br>
<p>Over and over, people seize power &quot;for the good of the community&quot;.  Are 
<br>
they being self-deceptive?  No, they are being evolutionary deceived.  The 
<br>
universality of this motif suggests to me that it is the result of 
<br>
selection over evolutionary time among imperfectly deceptive social 
<br>
organisms who argued linguistically about each other's motives in adaptive 
<br>
political contexts.  There is a warp applied between the selection of 
<br>
goals and the selection of subgoals, because both operations are carried 
<br>
out by evolutionarily constructed, emotionally influenced brainware.  I 
<br>
hypothesize (this is not a generally accepted truth in evolutionary 
<br>
psychology, as far as I know) the following evolutionarily constructed 
<br>
template of cognitive, emotional, and socioenvironmental interactions, 
<br>
which was statistically likely to lead to reproduction in the ancestral 
<br>
environment, and which today has become a famous motif in large-scale 
<br>
human events.
<br>
<p>1.  People (emotional hardware) care about the community they live in. 
<br>
(See &quot;Any Animal Whatever&quot; by Flack and de Waal for an argument that 
<br>
&quot;community concern can be observed in primates.)
<br>
<p>2.  People (emotional hardware) react to perceived abuses of power by the 
<br>
tribal chief with righteous indignation.  Note that the evolutionary force 
<br>
underlying the rise of this emotional perception rests on the possibility 
<br>
of installing a less abusive tribal chief *or* the possibility of 
<br>
achieving higher social status in the post-revolutionary order, *not* the 
<br>
factual degree of the tribal chief's abuses, nor the degree to which 
<br>
opposing the tribal chief benefits the *community*.
<br>
<p>3.  People who are reacting to perceived abuses of power by the tribal 
<br>
chief with righteous indignation, and see the opportunity to personally 
<br>
overthrow the tribal chief or do so with a clique of friends, are biased 
<br>
to see this as a *good subgoal* of serving the community.  This is the 
<br>
first appearance of an explicit rationalization warp in the hypothesis.
<br>
<p>So what's a rationalization warp?  Okay, as all good decision theorists 
<br>
know, you get the desirability D(a) of an action A by summing over the 
<br>
utility times the probability, U(x)P(x|a), for all x of interest (i.e., 
<br>
that the system can afford to compute).  What I'm saying is that in this 
<br>
case, the adaptive bias is being applied to the computation p(x|a) rather 
<br>
than U(x).  It's not that people &quot;really want&quot; to take over the tribe. 
<br>
They really want to promote the good of the community, and they really 
<br>
believe that they can do so by taking over the tribe.  It would not even 
<br>
be accurate to say that the people are being deceived about their &quot;real 
<br>
motives&quot;; they are being deceived about which means correspond to which 
<br>
ends.  In other words, the rationalization warp looks like this:
<br>
<p>Evolutionary end, i.e., subgoal of reproduction:  Y.  (Status, power...)
<br>
Cognitively held end which is socially acceptable:  X.  (Good of the tribe.)
<br>
So evolution is applying a bias to the computation of p(x|a) such that 
<br>
people find A to appear very plausible as a subgoal of X, given that it is 
<br>
*actually* a subgoal of Y.  In other words, p(x|a) will be computed as 
<br>
higher than it should be, given that p(y|a) is *in fact* high.
<br>
<p>This is not being carried out by a deliberative process that evaluates 
<br>
p(y|a) before evaluating p(x|a), any more than people stop to think about 
<br>
their number of surviving grandchildren before eating tasty berries or 
<br>
tasty candy bars.  There may be emotional ties from statistical correlates 
<br>
of p(y|a) to the evaluation of p(x|a), and so on, but there's not a 
<br>
deliberate deception involved.  It's an emergent outcome of evolutionary 
<br>
selection on hereditary information that biases p(x|a) and correlates to 
<br>
reproductive success after *everything*, the whole deal, is finished.
<br>
<p>The end result of this is that acting on the evolutionary biased 
<br>
computation U(x)p(x|a) tends to actually maximize Y instead of X, 
<br>
providing that all goes as planned and you are in the ancestral 
<br>
environment.  Stalin, as far as I know, did not have a spectacularly large 
<br>
number of surviving grandchildren - was no great success from an 
<br>
evolutionary standpoint.  Had he just been in one tribe, though, less 
<br>
educated and literate, with less grand plans, he probably would have done 
<br>
better from a reproductive standpoint.  The point is that this is the 
<br>
machinery that sent Stalin bad.  And Robespierre.  And... so on.  See here 
<br>
the roots of the human species; it is still better than not caring at all.
<br>
<p>Step 4, of course, is another rationalization warp to believing that, 
<br>
having taken power, people are then emotionally and cognitively biased to 
<br>
find plausible the proposition that power must be concentrated into their 
<br>
own hands, again for the good of the community.
<br>
<p>That this template has been carried out successfully so frequently in our 
<br>
own, literate, suspicious times, to say nothing of a tribe of 
<br>
non-timebinding hunter-gatherers, suggests that it has been operating as a 
<br>
successful strategy over evolutionary time.  Evolution constructs 
<br>
phenotypes to believe that they are acting for the good of the tribe, 
<br>
because that is what wins public support.
<br>
<p>But the point is that people *really do want* to help others, to create 
<br>
art, to be compassionate.  It's the whole reason why we find the 
<br>
evolutionary puppet strings so horrifying once we become aware of them; 
<br>
evolution didn't plan for that, any more than it planned for the 
<br>
introduction of contraceptives.  Ultimately DNA is just an 
<br>
information-theoretic history of *who did in fact* reproduce, not a plan 
<br>
to make organisms reproduce in the future.
<br>
<p>We genuinely *are* altruists.  Tainted by shadow, perhaps, but true 
<br>
altruists nonetheless.  Evolution, acting purely on the criterion of 
<br>
covariance of hereditary information in dynamic cognitive processes with 
<br>
reproductive success, managed to construct people who truly care about 
<br>
each other, as ends in themselves, and not as means.
<br>
<p>I have often thought that the rationalization warp is the greatest mixed 
<br>
blessing in the entire history of the universe.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11359.html">Jeff Davis: "RE: HUMANS and low genetic complexity"</a>
<li><strong>Previous message:</strong> <a href="11357.html">Spike: "RE: Number of carbon atoms in the Earth's biomass"</a>
<li><strong>In reply to:</strong> <a href="11258.html">Robin Hanson: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11360.html">Wei Dai: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Reply:</strong> <a href="11360.html">Wei Dai: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Reply:</strong> <a href="11368.html">Robin Hanson: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11358">[ date ]</a>
<a href="index.html#11358">[ thread ]</a>
<a href="subject.html#11358">[ subject ]</a>
<a href="author.html#11358">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Fri Jul 11 2003 - 01:03:14 MDT
</em></small></p>
</body>
</html>
