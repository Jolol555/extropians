<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: Why Does Self-Discovery Require a Journey?</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Why Does Self-Discovery Require a Journey?">
<meta name="Date" content="2003-07-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Why Does Self-Discovery Require a Journey?</h1>
<!-- received="Sat Jul 12 05:03:25 2003" -->
<!-- isoreceived="20030712110325" -->
<!-- sent="Sat, 12 Jul 2003 07:01:18 -0400" -->
<!-- isosent="20030712110118" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Why Does Self-Discovery Require a Journey?" -->
<!-- id="3F0FEA7E.6080909@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="5.2.1.1.2.20030712035618.01d24618@mail.gmu.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Why%20Does%20Self-Discovery%20Require%20a%20Journey?"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jul 12 2003 - 05:01:18 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11402.html">Anders Sandberg: "A vision"</a>
<ul>
<li><strong>Previous message:</strong> <a href="11400.html">Steve Davies: "Re: Taking Children Seriously"</a>
<li><strong>In reply to:</strong> <a href="11396.html">Robin Hanson: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11575.html">Robin Hanson: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Reply:</strong> <a href="11575.html">Robin Hanson: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11401">[ date ]</a>
<a href="index.html#11401">[ thread ]</a>
<a href="subject.html#11401">[ subject ]</a>
<a href="author.html#11401">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Robin Hanson wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt;&gt; ... when you start telling me that people do not really want what they 
</em><br>
<em>&gt;&gt; say they want, I become worried for several reasons.
</em><br>
<em>&gt;&gt; One, you're assuming that wanting-ness is a simple natural category, 
</em><br>
<em>&gt;&gt; which distracts attention away from the task of arriving at a 
</em><br>
<em>&gt;&gt; functional decomposition of decision-making into a surprisingly weird 
</em><br>
<em>&gt;&gt; evolutionary layer-cake with human icing on top.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Doesn't your claim that people do want what they say they want assume 
</em><br>
<em>&gt; just as much simplicity of a natural category?
</em><br>
<p>Guilty.  Rephrasing, I would say that when research subjects report on the 
<br>
presence of a morally and emotionally valent belief, there probably really 
<br>
is a belief there, and that belief as a piece of information is a natural 
<br>
category with respect to the human mind's content.  If that's the end of 
<br>
the story, i.e., if people's observed behaviors are consistent with the 
<br>
implications of that moral belief, then I'd probably identify that belief 
<br>
as what people &quot;want&quot;, at least for purposes of conversation, if not 
<br>
Friendly AI.  If anything more complicated is going on, then yes, I 
<br>
oversimplified.
<br>
<p>Look at how much damage has been done to the field of self-deception by 
<br>
the epistemological disputes over whether people &quot;really believe&quot; p, or 
<br>
~p, or whatever.  It was a mistake made in founding the field, and a 
<br>
predictable mistake, in the sense that I guessed before looking up the 
<br>
subfield that that's what the original history of the field would look 
<br>
like, whether or not people had moved beyond it as of 2003.  Apparently, 
<br>
Mele at least has caught on, but energy is still going into arguing 
<br>
against the basic oversimplification of self-deception as involving 
<br>
primary belief that p, and secondary belief that ~p.
<br>
<p>If you specify what you mean by &quot;belief&quot; in terms of events and 
<br>
information present in people's mental content, rather than by attempted 
<br>
functional tests (which I would regard as secondary outcomes rather than 
<br>
primary causes), then the self-deceived have a declarative false belief p 
<br>
and accurately report its presence as their belief; they also have adapted 
<br>
biases leading them to arrive at the declarative false belief p, rather 
<br>
than motivational biases stemming from ~p represented in a hidden 
<br>
secondary control center.  So the false belief p is there, and the true 
<br>
belief ~p is not - if you take &quot;belief&quot; as referring to the declarative 
<br>
statements stored in memory, then people believe just p, and not ~p.  But 
<br>
really it would be much wiser not to invoke propositional logic to begin 
<br>
with.  The real events take place in a complex brain that does not 
<br>
actually run on this kind of logic.  If the people who started the debate 
<br>
on self-deception had not defined it as simultaneous belief in p and ~p, 
<br>
i.e., a secondary reported belief in p, and a primary hidden belief in ~p, 
<br>
a lot of energy would have been spared, and the field's leaders could be 
<br>
focusing on uncovering the cognitive mechanisms of self-deception rather 
<br>
than arguing against the basic mistake made at the start of the debate.
<br>
<p><em>&gt;&gt; Two, you're setting off people's cheater-detectors in a way that 
</em><br>
<em>&gt;&gt; invokes an implicit theory of mind that I think is oversimplified, 
</em><br>
<em>&gt;&gt; false-to-fact, and carves the mind at the wrong joints; ...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I hope you are not blaming me for people's cheater-detectors having the 
</em><br>
<em>&gt; wrong implicit theory of mind.
</em><br>
<p>Cheater detectors have the right theory of mind with respect to tobacco PR 
<br>
departments, and the wrong theory of mind with respect to adaptive 
<br>
self-deception, so I'm blaming the analogy.
<br>
<p><em>&gt;&gt; Three, you're making a preemptive philosophical judgment ...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm making a judgement, but I'm not sure what you mean it is &quot;pre&quot; to.  
</em><br>
<em>&gt; It is post consideration of many issues, but surely pre to more 
</em><br>
<em>&gt; consideration.
</em><br>
<p>True.  I have no idea how much thought you've put into your philosophical 
<br>
judgment.  However, I do still think it's wrong.  If I were writing the 
<br>
same paper about the same evidence I would title it:
<br>
<p>&quot;How Evolution Deceives You To Prevent You From Living Up To Your Ideals.&quot;
<br>
<p>Implicit assumptions in that title:
<br>
<p>1:  &quot;You&quot; are identified with your ideals.
<br>
2:  Evolution is identified as an external, infringing force.
<br>
3:  The deceptive part of self-deception is carried out by evolution as an 
<br>
agent - any knowledge of p, used in constructing the lie ~p, is identified 
<br>
as residing in evolution as the liar.
<br>
4:  Ideals are to be &quot;lived up&quot; to; one's current actions are to be 
<br>
interpreted as an imperfect approximation to one's ideals, in the same way 
<br>
that human rationality is interpreted as being a Bayesian wannabe; one's 
<br>
ideals can be determined to some degree independently of one's ability to 
<br>
live up to them; failure to live up to your ideals does not imply that 
<br>
your ideals are really something else.
<br>
<p><em>&gt;&gt; Five, there's no good reason to mess with points 1-4 - they are 
</em><br>
<em>&gt;&gt; totally extraneous to the real substance of your theory.  You can 
</em><br>
<em>&gt;&gt; declare yourself to be studying &quot;adaptive gross inconsistencies in 
</em><br>
<em>&gt;&gt; moral belief and real actions&quot;, and get the benefit of intersection 
</em><br>
<em>&gt;&gt; with both evolutionary psychology and experimental psychology, without 
</em><br>
<em>&gt;&gt; ever needing to take a stance about what people &quot;really&quot; &quot;want&quot;, or 
</em><br>
<em>&gt;&gt; presuming a particular functional decomposition of the mechanisms 
</em><br>
<em>&gt;&gt; involved.  Modularize away those contrarian points...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I agree we can consider these issues separately, but I am really 
</em><br>
<em>&gt; interested in what people really want; it is basic to normative 
</em><br>
<em>&gt; analysis, which economists do a lot of.
</em><br>
<p>I think a full theory of volition is unnecessary overkill if you're 
<br>
dealing with human-level economic issues.  However, speaking from within 
<br>
my unpublished thoughts on volition, there isn't a register in people's 
<br>
minds that stores what they &quot;really want&quot;.  You can ask about what people 
<br>
really *are*, and get back information about what emotional brainware they 
<br>
really have, and what moral beliefs are really stored in their memory, but 
<br>
to ask what people &quot;really want&quot; is a different order of question 
<br>
entirely.  The theory of volition I'm constructing to handle Friendly AI 
<br>
doesn't have the concept of a &quot;really want&quot; that's hidden in the brain 
<br>
somewhere; it has the idea of extrapolating someone's reactions and 
<br>
determining the spread in them.  If your reactions have a small amount of 
<br>
spread, they may compound in such a way as to converge on a single 
<br>
strongly determined answer to a given question.  But if this is not the 
<br>
case, then for a given problem, there may not yet be something that you 
<br>
&quot;really want&quot; in the sense that you mean it.
<br>
<p><em>&gt;&gt;&gt; ... We've seen how people behave if briefly informed.   We haven't 
</em><br>
<em>&gt;&gt;&gt; seen your ideal of fully informed post-upheaval choice.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; When have we seen how people behave if briefly informed?  How can you 
</em><br>
<em>&gt;&gt; briefly inform someone of something they don't believe to be true?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; For example, in the area of disagreement, we can see how people respond 
</em><br>
<em>&gt; to being privately persuaded that persistent disagreement is irrational 
</em><br>
<em>&gt; - they continue to disagree.  We can see that people who are privately 
</em><br>
<em>&gt; informed that they probably over-estimate their own abilities soon 
</em><br>
<em>&gt; continue to do so.
</em><br>
<p>I'd read this as:  &quot;'Briefly informing' people doesn't work.&quot;  It 
<br>
certainly wouldn't work in FAI - death with a filtered-out warning label 
<br>
on it is not friendly.
<br>
<p><em>&gt;&gt; ... I have to say that I was not surprised by my brief survey - it 
</em><br>
<em>&gt;&gt; looks pretty much like what I expected.  Is there any particular area 
</em><br>
<em>&gt;&gt; or result of which you worry I am ignorant?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In economics it goes by the name of &quot;social welfare analysis&quot;.
</em><br>
<p>Do you mean social choice theory?
<br>
<p><em>&gt;&gt; But if I had a genie built using your definition of &quot;wanting&quot;, I would 
</em><br>
<em>&gt;&gt; never, ever make a wish to it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You would apparently also not wish for it to make you happy, or to make 
</em><br>
<em>&gt; the choice you would make if you were privately informed.
</em><br>
<p>I would not make any wish to a genie built using your definition of 
<br>
&quot;wanting&quot;, because it identifies the person known as Eliezer in a 
<br>
different place than I identify myself.  The way the genie interprets what 
<br>
makes *me* happy, or the choice that *I* would make if *I* were privately 
<br>
informed, rests on a construction of volition that I don't trust and in 
<br>
fact actively disagree with.  It doesn't matter how many meta-wishes I 
<br>
use; I can't trust the root-level interpreter.
<br>
<p>Incidentally, I'm not discriminating against your genie in particular; I 
<br>
wouldn't go anywhere near any genie unless it satisfied a considerable 
<br>
number of nonobvious properties.  It's an FAI-complete problem.  To give 
<br>
an example of a failure that would result from the specific points under 
<br>
debate, the genie might decide that what I &quot;really want&quot; is to be 
<br>
wireheaded.  That I would verbally object to this, if fully informed, 
<br>
might not strike the genie as relevant, given that I would in fact, 
<br>
presented with the wire, be deliriously happy - who says that my verbal 
<br>
objections are what count, when my &quot;real wants&quot; are clearly for the wire?
<br>
<p>It's worth noting that the definition of volition I'm using is constructed 
<br>
around this specific problem of helping people - not around an economic 
<br>
problem, or around the disembodied philosophical question of what people 
<br>
&quot;really want&quot;.  When in doubt, the question I step back and ask myself is 
<br>
&quot;But would this really be helpful?&quot;  A Friendly AI looking at Robin Hanson 
<br>
pondering the abstract philosophical question of how to define the phrase 
<br>
&quot;really want&quot; might see that you would unambiguously converge on a single 
<br>
definition, given that you were fully informed about volitional theory. 
<br>
Or not.  I don't much believe in arguing over the definitions of words - 
<br>
there's no ready observable for &quot;really want&quot;, and there may be no good 
<br>
definition at all without the context provided by a meta-problem such as 
<br>
interpersonal interactions in granting wishes.
<br>
<p><em>&gt; I had in mind examples that look much less like deliberate deception.  A 
</em><br>
<em>&gt; corporation (or non-profit org) can start out with principles that it 
</em><br>
<em>&gt; declares, and that the top individuals in it believe it follows, but 
</em><br>
<em>&gt; market selection pressures can end up making it violate those 
</em><br>
<em>&gt; principles.  It might be company policy not to pollute, or to always 
</em><br>
<em>&gt; tell the costumer the truth, but the company may not give individual 
</em><br>
<em>&gt; employees the incentive to follow those policies.  Those low level 
</em><br>
<em>&gt; employees may engage in deliberate deception, but the corporation as a 
</em><br>
<em>&gt; whole is may be better described as engaging in self-deception.  The CEO 
</em><br>
<em>&gt; may not realize that the incentives are off, but that is because he has 
</em><br>
<em>&gt; not paid sufficient attention to such issues.  Each person may think it 
</em><br>
<em>&gt; is someone else's job to deal with that problem.  And the corporate PR 
</em><br>
<em>&gt; department may be even less away of incentive issues than most parts of 
</em><br>
<em>&gt; the company.
</em><br>
<p>This organizational pathology sounds like something entirely separate from 
<br>
human self-deception, operating through different mechanisms to produce 
<br>
results with surface similarity.  If I were analyzing the two systems I 
<br>
would be very careful to do so separately, to avoid fearsome confusion and 
<br>
dismay.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11402.html">Anders Sandberg: "A vision"</a>
<li><strong>Previous message:</strong> <a href="11400.html">Steve Davies: "Re: Taking Children Seriously"</a>
<li><strong>In reply to:</strong> <a href="11396.html">Robin Hanson: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11575.html">Robin Hanson: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Reply:</strong> <a href="11575.html">Robin Hanson: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11401">[ date ]</a>
<a href="index.html#11401">[ thread ]</a>
<a href="subject.html#11401">[ subject ]</a>
<a href="author.html#11401">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Sat Jul 12 2003 - 05:12:50 MDT
</em></small></p>
</body>
</html>
