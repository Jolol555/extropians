<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: Why Does Self-Discovery Require a Journey?</title>
<meta name="Author" content="Robin Hanson (rhanson@gmu.edu)">
<meta name="Subject" content="Re: Why Does Self-Discovery Require a Journey?">
<meta name="Date" content="2003-07-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Why Does Self-Discovery Require a Journey?</h1>
<!-- received="Fri Jul 11 08:52:17 2003" -->
<!-- isoreceived="20030711145217" -->
<!-- sent="Fri, 11 Jul 2003 10:52:10 -0400" -->
<!-- isosent="20030711145210" -->
<!-- name="Robin Hanson" -->
<!-- email="rhanson@gmu.edu" -->
<!-- subject="Re: Why Does Self-Discovery Require a Journey?" -->
<!-- id="5.2.1.1.2.20030711092552.01c7d9f8@mail.gmu.edu" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3F0E5E41.5040001@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Robin Hanson (<a href="mailto:rhanson@gmu.edu?Subject=Re:%20Why%20Does%20Self-Discovery%20Require%20a%20Journey?"><em>rhanson@gmu.edu</em></a>)<br>
<strong>Date:</strong> Fri Jul 11 2003 - 08:52:10 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11369.html">Natasha Vita-More: "Re: Cooperation between All Transhumanist Organizations (Was Re:  ExI/WTA)"</a>
<ul>
<li><strong>Previous message:</strong> <a href="11367.html">Spike: "RE: Number of carbon atoms in the Earth's biomass"</a>
<li><strong>In reply to:</strong> <a href="11358.html">Eliezer S. Yudkowsky: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11371.html">Eliezer S. Yudkowsky: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Reply:</strong> <a href="11371.html">Eliezer S. Yudkowsky: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Reply:</strong> <a href="11374.html">Jeff Davis: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11368">[ date ]</a>
<a href="index.html#11368">[ thread ]</a>
<a href="subject.html#11368">[ subject ]</a>
<a href="author.html#11368">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 7/11/2003, Eliezer S. Yudkowsky wrote:
<br>
<em>&gt;&gt;&gt;... your phraseology ... seems to preemptively settle the issue by 
</em><br>
<em>&gt;&gt;&gt;identifying people's built-in emotional reinforcers as their real wants, 
</em><br>
<em>&gt;&gt;&gt;while dismissing their cognitively held hopes and aspirations and 
</em><br>
<em>&gt;&gt;&gt;personal philosophy as a foreign force interfering with their true selves.  ...
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;If people have contradictory beliefs, how can we say which ones are the 
</em><br>
<em>&gt;&gt;&quot;real&quot; beliefs?  By reference to the basic schema of self-deception, in 
</em><br>
<em>&gt;&gt;which the real beliefs tend to determine less visible actions with more 
</em><br>
<em>&gt;&gt;fundamental consequences, and the false beliefs tend to determine what we 
</em><br>
<em>&gt;&gt;tell others and ourselves about ourselves, and the most socially visible 
</em><br>
<em>&gt;&gt;actions with the least fundamental consequences.
</em><br>
<em>&gt;
</em><br>
<em>&gt;... If you go around determining real beliefs by the *outcomes* of 
</em><br>
<em>&gt;people's actions, you run the risk of confusing evolutionary motives with 
</em><br>
<em>&gt;cognitive ones, the classic mistake in evolutionary psychology.  ...
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;People may want to produce art to gain social approval, wealth, mates, 
</em><br>
<em>&gt;&gt;etc., but want to be thought of as doing it just for the art. People
</em><br>
<em>&gt;&gt;may want to advocate positions that make them seem clever and 
</em><br>
<em>&gt;&gt;compassionate, and get them social accepted by the right folks, but want 
</em><br>
<em>&gt;&gt;to be thought of as wanting only to tell the truth.  People may want to 
</em><br>
<em>&gt;&gt;be unfair when serving as a neutral judge, but want to thought of as fair.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Aren't these instances of the classic error?  People have emotional 
</em><br>
<em>&gt;hardware and cognitive representations leading them to be devoted to art 
</em><br>
<em>&gt;for its own sake, ... People have emotions leading them to honestly 
</em><br>
<em>&gt;advocate positions that people applaud as clever and compassionate, 
</em><br>
<em>&gt;...  People think they're honest and they are, but what think is the truth 
</em><br>
<em>&gt;is output by biased reasoning hardware ... Over and over, people seize 
</em><br>
<em>&gt;power &quot;for the good of the community&quot;. ... What I'm saying is that in this 
</em><br>
<em>&gt;case, the adaptive bias is being applied to the computation p(x|a) rather 
</em><br>
<em>&gt;than U(x).  ... It would not even be accurate to say that the people are 
</em><br>
<em>&gt;being deceived about their &quot;real motives&quot;; they are being deceived about 
</em><br>
<em>&gt;which means correspond to which ends.  In other words, the rationalization 
</em><br>
<em>&gt;warp looks like this:
</em><br>
<em>&gt;
</em><br>
<em>&gt;Evolutionary end, i.e., subgoal of reproduction:  Y.  (Status, power...)
</em><br>
<em>&gt;Cognitively held end which is socially acceptable:  X.  (Good of the tribe.)
</em><br>
<em>&gt;So evolution is applying a bias to the computation of p(x|a) such that 
</em><br>
<em>&gt;people find A to appear very plausible as a subgoal of X, given that it is 
</em><br>
<em>&gt;*actually* a subgoal of Y.  In other words, p(x|a) will be computed as 
</em><br>
<em>&gt;higher than it should be, given that p(y|a) is *in fact* high. ...
</em><br>
<em>&gt;But the point is that people *really do want* to help others, to create 
</em><br>
<em>&gt;art, to be compassionate.  It's the whole reason why we find the 
</em><br>
<em>&gt;evolutionary puppet strings so horrifying once we become aware of them; ...
</em><br>
<p>[FYI, there are many papers/books in philosophy and psychology, and fewer 
<br>
evolutionary psychology, on self-deception.  And classic literature has 
<br>
many things to say about it.  (I'm co-hosting a small invitation-only 
<br>
interdisciplinary conference on the subject here in October.)  This is 
<br>
exactly the sort of topic that extropians shouldn't try to reinvent before 
<br>
surveying the existing literature.  RH]
<br>
<p>There are many possible mechanisms of self-deception, one of which is as 
<br>
you describe.  The whole system of calculating actions from goals and 
<br>
beliefs has many entry points for motivational bias.  Some of these points 
<br>
are focused more on beliefs, others more on goals.  The system also has 
<br>
many rich layers of protection from situations that might to remove such 
<br>
bias.  We are much better at spotting self-deception in others than in 
<br>
ourselves, because we have ways of avoiding looking at the relevant 
<br>
evidence, and rationalizing it away when others point it out.
<br>
<p>There are two classic ways to determine what people &quot;really&quot; want.  One is 
<br>
based on &quot;happiness,&quot; the other on informed choice.  In your example, the 
<br>
happiness metric asks if people are happier when they get status/power 
<br>
versus when they actually do good for the tribe, without getting such 
<br>
status/power.  The informed choice metric asks whether people would choose 
<br>
status/power or good for the tribe if they were briefly and privately 
<br>
informed, via enough evidence to typically be persuasive to a neutral 
<br>
observer, that this is actually what they are choosing between.  (I say 
<br>
briefly so that they can quickly forget the conversation every happened and 
<br>
revert to the state where they actually believe they are doing good for the 
<br>
tribe.)
<br>
<p>My reading of human behavior in most of the contexts in which 
<br>
self-deception is an issue is that most people are happier with the 
<br>
status/power type option, versus the doing good for the tribe type option, 
<br>
and that this is what they usually actually choose when briefly and 
<br>
privately informed.  I agree that most people do believe that they want to 
<br>
do good for the tribe.  My claim is that this belief is relatively isolated 
<br>
and ineffectual; it is allowed to influence what people say and some 
<br>
actions that influence social perceptions, but is otherwise little used.
<br>
<p>Consider that today most people around here would say that the think the 
<br>
world is their tribe, but they give almost no money to help poor people in 
<br>
Africa, even when they believe that such aid would make the world a better 
<br>
place overall.
<br>
<p>If I still haven't convinced you, I suggest we consider the more familiar 
<br>
and tractable question I suggested before, namely how we can tell what a 
<br>
corporation &quot;really&quot; wants:
<br>
<p><em>&gt;If a corporation polluted a lot, but had a public relations department 
</em><br>
<em>&gt;that insisted that it did not pollute, and that PR department managed to 
</em><br>
<em>&gt;make sure that no pollution was obvious during public tours of corporate 
</em><br>
<em>&gt;facilities, I'd say the corporation wanted to pollute but did not want to 
</em><br>
<em>&gt;be thought of as polluting.
</em><br>
<p>When would you say that a corporation that consistently continues to 
<br>
pollute, even though its PR denies it, &quot;really wants&quot; to not pollute?
<br>
<p><p><p><p><p><p>Robin Hanson  <a href="mailto:rhanson@gmu.edu?Subject=Re:%20Why%20Does%20Self-Discovery%20Require%20a%20Journey?">rhanson@gmu.edu</a>  <a href="http://hanson.gmu.edu">http://hanson.gmu.edu</a>
<br>
Assistant Professor of Economics, George Mason University
<br>
MSN 1D3, Carow Hall, Fairfax VA 22030-4444
<br>
703-993-2326  FAX: 703-993-2323 
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11369.html">Natasha Vita-More: "Re: Cooperation between All Transhumanist Organizations (Was Re:  ExI/WTA)"</a>
<li><strong>Previous message:</strong> <a href="11367.html">Spike: "RE: Number of carbon atoms in the Earth's biomass"</a>
<li><strong>In reply to:</strong> <a href="11358.html">Eliezer S. Yudkowsky: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11371.html">Eliezer S. Yudkowsky: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Reply:</strong> <a href="11371.html">Eliezer S. Yudkowsky: "Re: Why Does Self-Discovery Require a Journey?"</a>
<li><strong>Reply:</strong> <a href="11374.html">Jeff Davis: "Re: Why Does Self-Discovery Require a Journey?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11368">[ date ]</a>
<a href="index.html#11368">[ thread ]</a>
<a href="subject.html#11368">[ subject ]</a>
<a href="author.html#11368">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Fri Jul 11 2003 - 09:02:35 MDT
</em></small></p>
</body>
</html>
