<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: RE: Fermi &quot;Paradox&quot;</title>
<meta name="Author" content="Rafal Smigrodzki (rafal@smigrodzki.org)">
<meta name="Subject" content="RE: Fermi &quot;Paradox&quot;">
<meta name="Date" content="2003-07-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Fermi &quot;Paradox&quot;</h1>
<!-- received="Thu Jul 24 09:24:29 2003" -->
<!-- isoreceived="20030724152429" -->
<!-- sent="Thu, 24 Jul 2003 11:25:55 -0700" -->
<!-- isosent="20030724182555" -->
<!-- name="Rafal Smigrodzki" -->
<!-- email="rafal@smigrodzki.org" -->
<!-- subject="RE: Fermi &quot;Paradox&quot;" -->
<!-- id="OJEHKDIANIFPAJPDBDGLMEBHCHAA.rafal@smigrodzki.org" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="5.2.1.1.2.20030723203526.01c23f60@mail.gmu.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Rafal Smigrodzki (<a href="mailto:rafal@smigrodzki.org?Subject=RE:%20Fermi%20&quot;Paradox&quot;"><em>rafal@smigrodzki.org</em></a>)<br>
<strong>Date:</strong> Thu Jul 24 2003 - 12:25:55 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11939.html">Rafal Smigrodzki: "RE: Fermi &quot;Paradox&quot;"</a>
<ul>
<li><strong>Previous message:</strong> <a href="11937.html">Arthur T. Murray: "Re: Robotic nation"</a>
<li><strong>In reply to:</strong> <a href="11909.html">Robin Hanson: "RE: Fermi &quot;Paradox&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11912.html">Damien Broderick: "Re: Fermi &quot;Paradox&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11938">[ date ]</a>
<a href="index.html#11938">[ thread ]</a>
<a href="subject.html#11938">[ subject ]</a>
<a href="author.html#11938">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Robin wrote:
<br>
<em>&gt; On 7/23/2003, Rafal Smigrodzki wrote:
</em><br>
<em>&gt;&gt; I remember reading about an experiment on the growth of a colony of
</em><br>
<em>&gt;&gt; mice in a limited space. ... the mice became too crowded ... After a
</em><br>
<em>&gt;&gt; few more months all of them died. ... Let's say that a reliable
</em><br>
<em>&gt;&gt; modeling method predicts that the existence of more than N
</em><br>
<em>&gt;&gt; independent volitional systems (minds with independent,
</em><br>
<em>&gt;&gt; self-modifiable goals) within a volume of space inevitably results
</em><br>
<em>&gt;&gt; in a destructive chain reaction of internecine warfare. In that
</em><br>
<em>&gt;&gt; case, a coalition of minds might form, perhaps aided by techniques
</em><br>
<em>&gt;&gt; for assuring transparency of motives, to keep the population below
</em><br>
<em>&gt;&gt; N, perhaps a very low number, much less than the physical carrying
</em><br>
<em>&gt;&gt; capacity of the substrate. If the coalition fails, all minds die
</em><br>
<em>&gt;&gt; like stupid mice. ... I don't think this is the case, I tend to
</em><br>
<em>&gt;&gt; think that expansion and proliferation will be decisive for
</em><br>
<em>&gt;&gt; successful survival at superhuman levels of intelligence as well,
</em><br>
<em>&gt;&gt; but one never knows.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I agree your scenario is logically possible, but it would need quite
</em><br>
<em>&gt; a bit more detail to make it plausible.  If it were true, I'd expect
</em><br>
<em>&gt; the creation of N large &quot;borgs&quot;, which still use up most of the
</em><br>
<em>&gt; physical carrying capacity. And note that this scenario is one of
</em><br>
<em>&gt; straight evolutionary selection, and not really of &quot;self-directed
</em><br>
<em>&gt; evolution&quot;.
</em><br>
<p>### I think there would be an element of both straight evolutionary
<br>
selection (at the level of groups of beings either collectively succeeding
<br>
or failing at preventing death by fratricide), and an element of
<br>
self-directed evolution (at the level of individuals making decisions about
<br>
removing/suppressing parts of their volitional system and joining the
<br>
coalition).
<br>
<p>To add detail - suppose that once nanotechnology develops a bit more (;-),
<br>
it would be possible for any individual to produce a doomsday weapon with
<br>
little effort, maybe a nucleus of ice-9. In that case, the probability of a
<br>
civilization with N individuals surviving for another day would be P=l exp
<br>
N, where l is the probability that the average individual does not make the
<br>
fateful choice. For sufficiently large N/l, the survival of the civilization
<br>
would be very unlikely. You can decrease the risk by increasing l
<br>
(transparency, choice of stable mental architecture), or by limiting N.
<br>
Today's proponents of technological relinquishment are advocates of
<br>
increasing l (by limiting technical abilities of individuals), but at large
<br>
enough N even a very high l offers little reassurance. This is very similar
<br>
to the considerations of nuclear MAD played between two superpowers vs. the
<br>
same game between a few dozens of smaller nations.
<br>
<p>The success of the strategy of limiting N would be of course dependent on
<br>
the ability of the minds forming the coalition to deny the use of physical
<br>
resources to possible defectors. However, whether this denial would take the
<br>
form of incorporating the resources Borg-wise into themselves, or merely
<br>
monitoring them with the ability to detect and destroy disallowed use, is
<br>
hard to predict.
<br>
<p>I agree, it *is* quite difficult to decide which future is plausible without
<br>
knowing the technical details.
<br>
<p>Rafal
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11939.html">Rafal Smigrodzki: "RE: Fermi &quot;Paradox&quot;"</a>
<li><strong>Previous message:</strong> <a href="11937.html">Arthur T. Murray: "Re: Robotic nation"</a>
<li><strong>In reply to:</strong> <a href="11909.html">Robin Hanson: "RE: Fermi &quot;Paradox&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11912.html">Damien Broderick: "Re: Fermi &quot;Paradox&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11938">[ date ]</a>
<a href="index.html#11938">[ thread ]</a>
<a href="subject.html#11938">[ subject ]</a>
<a href="author.html#11938">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Thu Jul 24 2003 - 09:32:40 MDT
</em></small></p>
</body>
</html>
