<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: ExI principles: people left behind?</title>
<meta name="Author" content="Robert J. Bradbury (bradbury@aeiveos.com)">
<meta name="Subject" content="Re: ExI principles: people left behind?">
<meta name="Date" content="2003-07-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ExI principles: people left behind?</h1>
<!-- received="Fri Jul 18 20:55:43 2003" -->
<!-- isoreceived="20030719025543" -->
<!-- sent="Fri, 18 Jul 2003 19:55:40 -0700 (PDT)" -->
<!-- isosent="20030719025540" -->
<!-- name="Robert J. Bradbury" -->
<!-- email="bradbury@aeiveos.com" -->
<!-- subject="Re: ExI principles: people left behind?" -->
<!-- id="Pine.LNX.4.44.0307181822210.15760-100000@server.aeiveos.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3F18750F.4090407@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Robert J. Bradbury (<a href="mailto:bradbury@aeiveos.com?Subject=Re:%20ExI%20principles:%20people%20left%20behind?"><em>bradbury@aeiveos.com</em></a>)<br>
<strong>Date:</strong> Fri Jul 18 2003 - 20:55:40 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11633.html">Dehede011@aol.com: "Re: flame wars"</a>
<ul>
<li><strong>Previous message:</strong> <a href="11631.html">Dan Fabulich: "Re: so here's what I think this list needs..."</a>
<li><strong>In reply to:</strong> <a href="11618.html">Eliezer S. Yudkowsky: "Re: ExI principles: people left behind?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11640.html">Spike: "RE: ExI principles: people left behind?"</a>
<li><strong>Reply:</strong> <a href="11640.html">Spike: "RE: ExI principles: people left behind?"</a>
<li><strong>Reply:</strong> <a href="11644.html">Lee Corbin: "RE: ExI principles: people left behind?"</a>
<li><strong>Reply:</strong> <a href="11652.html">Anders Sandberg: "Re: ExI principles: people left behind?"</a>
<li><strong>Reply:</strong> <a href="11696.html">Samantha Atkins: "Re: ExI principles: people left behind?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11632">[ date ]</a>
<a href="index.html#11632">[ thread ]</a>
<a href="subject.html#11632">[ subject ]</a>
<a href="author.html#11632">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Fri, 18 Jul 2003, Eliezer S. Yudkowsky wrote (responding to
<br>
my rather extreme utilitarian simplification of things):
<br>
<p><em>&gt; Are you autistic?
</em><br>
<p>No, but I do seem to have a rather high Aspergers quotient so
<br>
it may be easier for me to make such proposals than the average
<br>
individual.  (Careful how you phrase things -- you may be abusing
<br>
a &quot;disabled&quot; person (me) -- lord knows what legal swampland that gets
<br>
you into in the U.S. ...)  semi - :-;
<br>
<p><em>&gt; Historical villains have killed millions of people in
</em><br>
<em>&gt; terrible causes, but the idea that it's too inconvenient to think about
</em><br>
<em>&gt; the subject, and that dropping nukes would save time and aggravation, may
</em><br>
<em>&gt; well represent a new low for the human species.
</em><br>
<p>Ah, but the debate must change if the &quot;killing of millions of people&quot;
<br>
is in the name of a &quot;good&quot; cause.  I do not note in your message
<br>
a schema for the valuation of &quot;lives&quot;.  Say even an AI life vs.
<br>
a human life.  This is not a new debate -- it goes way back to
<br>
the question of whether one has the right to kill (shut off, erase)
<br>
ones copies -- (even *if* they have given you &quot;well informed&quot;
<br>
permission to do so in advance).
<br>
<p>And the &quot;valuation of lives&quot; goes to the crux of the matter.  Nick
<br>
in his paper suggested there were some alternate perspectives
<br>
of utilitarian evaluations, Anders expanded on this quite a bit in
<br>
his comments (much to my education).
<br>
<p>But the problem is not simple and it doesn't go away (just because we
<br>
find the discussion repulsive).
<br>
<p>I do agree that villains have abused their power and that millions of
<br>
innocent people have died as a result.  I would also probably agree
<br>
that my suggestion would also result in similar negentropic casualties.
<br>
But the point I am trying to get at is *when* the negentropic losses
<br>
are acceptable?  Is the saving of a single human life worth a sacrifice
<br>
by humanity?  In medicine this is known as &quot;triage&quot; -- and it involves
<br>
some very difficult decisions as to how one optimizes who one saves.
<br>
<p><em>&gt; I doubt you could kill a single human being at close quarters.
</em><br>
<p>Eliezer, given a few situations that I have been through in my life I have
<br>
no doubt that I could kill a single human being at close quarters (or even
<br>
multiple human beings) [these were primarily self-defense situations].  I
<br>
will observe that this is a very different position from the one I held
<br>
around your age (when I was fighting with my father to avoid returning my
<br>
draft card to the government during the declining days of the Vietnam
<br>
War). I would also note that in order to do this I would have to make a
<br>
very short term analysis as to whether my life is worth more or less than
<br>
the individual that might be killed.  (I would be likely be willing to
<br>
sacrifice my life if I thought the other individual had a more extropic
<br>
vector.)  It seems likely that that would be a very error prone process.
<br>
But one has to base survival decisions on what one is given.
<br>
<p><em>&gt; because knifing a person sets off our built-in instincts and pressing
</em><br>
<em>&gt; a button does not.
</em><br>
<p>I was trying to go beyond that.  I was trying to determine whether
<br>
or not there is a moral framework for the net worth of human lives
<br>
and whether that justifies a &quot;way of being&quot;?  For example, the
<br>
Buddhist perpective on &quot;lives&quot; provides a &quot;way of being&quot; -- the
<br>
extropic principles may not (at least in some aspects).  And perhaps
<br>
more importantly the extropic perspective may *never* generate a
<br>
schema that trumps the Buddhist perspective.  That is why I raised
<br>
the question of how one achieves the shortest path to ones goals.
<br>
<p>(Or being pragmatic -- we will not nuke anyone -- we will simply
<br>
deny access to Life Extending Technologies to anyone who is clearly
<br>
a &quot;luddite&quot; attempting to discourage the development of such LET.
<br>
Someday those of us who support and use LET will triumph.)
<br>
<p><em>&gt; Technological distance is emotional distance, as Dave Grossman put
</em><br>
<em>&gt; it in &quot;On Killing&quot;.
</em><br>
<p>I would disagree.  If that were true I would not have contributed
<br>
tens of thousands of dollars to The Hunger Project over two decades.
<br>
I never met the people who may have been helped by my support.
<br>
I simply supported them because it seemed like the right thing to do
<br>
(i.e. it seemed extropic before I ever heard of ExI).
<br>
<p><em>&gt; And how easy it is for people who can't distinguish word games from
</em><br>
<em>&gt; reality to arrange a few thoughts in the right order and decide to
</em><br>
<em>&gt; commit genocide.  The human mind has no safety catch.
</em><br>
<p>I am not playing word games.  My comment was very serious (though
<br>
I may currently regret posting it).  It was an effort to question
<br>
&quot;at what rate&quot; and &quot;how&quot; do you want humanity to evolve?
<br>
<p><em>&gt; Because you genuinely seem to be serious.  I wish I could say I don't
</em><br>
<em>&gt; understand it, but I do, and I'm sad, and frightened, because you were
</em><br>
<em>&gt; someone I used to respect.  Even if you don't understand what you're saying,
</em><br>
<em>&gt; even if it has no connection to reality for you, you said it, and I can't
</em><br>
<em>&gt; make it unreal to myself.
</em><br>
<p>If it is of any help, reframe it in terms of &quot;can you erase your copies&quot;?
<br>
It seems to be a reasonable proposal that an evolving technological
<br>
civilization that allows the erasing of copies would advance faster
<br>
than one that does not (simply due to the expense of the memory
<br>
requirements of preserving inactive copies -- ignoring the question
<br>
of whether copies must be allowed some slice of the global CPU time).
<br>
<p>So making the great &quot;leap&quot; that one human is pretty much like another
<br>
human (I mean really -- if a 1 cm^3 nanocomputer can support 100,000+
<br>
human minds our &quot;individuality&quot; is probably overrated) one begins to
<br>
get into the question of the &quot;survival of humanity&quot;.  This isn't a
<br>
new topic -- it has been discussed by Robin in his &quot;If Uploads Come
<br>
First&quot; paper (<a href="http://hanson.gmu.edu/uploads.html">http://hanson.gmu.edu/uploads.html</a>).
<br>
<p>All I am saying, and I am sad that it makes you &quot;sad, and frightened&quot;
<br>
but someone has to face what I perceive as the spectre of the Pied Piper,
<br>
is that the philosophy, belief system, what we promote, etc. may be
<br>
very incomplete unless we deal with the fact that a society that
<br>
allows the deletion of copies may out-evolve a society that does not.
<br>
<p>And particularly for you Eliezer -- I have not noticed (though I
<br>
will admit not having read what you have written extensively --
<br>
it is a rather great volume) any focus on addressing the issue
<br>
of what one preserves as an AI evolves.
<br>
<p>Ok, it is reasonable to delete the memories and functional
<br>
algorithms of an AI with the intelligence of someone with
<br>
Down's syndrome but it is unreasonable to delete the memories
<br>
and functional algorithms of an AI with the intelligence of
<br>
Einstein.
<br>
<p>Where do you stand on the &quot;Deep Blue&quot; preservation effort?
<br>
Should we not anticipate it being only a few decades before
<br>
this software (?intelligence?) is lost forever?
<br>
<p>After all it did defeat the best human chess player in the world...
<br>
<p>Robert
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11633.html">Dehede011@aol.com: "Re: flame wars"</a>
<li><strong>Previous message:</strong> <a href="11631.html">Dan Fabulich: "Re: so here's what I think this list needs..."</a>
<li><strong>In reply to:</strong> <a href="11618.html">Eliezer S. Yudkowsky: "Re: ExI principles: people left behind?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11640.html">Spike: "RE: ExI principles: people left behind?"</a>
<li><strong>Reply:</strong> <a href="11640.html">Spike: "RE: ExI principles: people left behind?"</a>
<li><strong>Reply:</strong> <a href="11644.html">Lee Corbin: "RE: ExI principles: people left behind?"</a>
<li><strong>Reply:</strong> <a href="11652.html">Anders Sandberg: "Re: ExI principles: people left behind?"</a>
<li><strong>Reply:</strong> <a href="11696.html">Samantha Atkins: "Re: ExI principles: people left behind?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11632">[ date ]</a>
<a href="index.html#11632">[ thread ]</a>
<a href="subject.html#11632">[ subject ]</a>
<a href="author.html#11632">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Fri Jul 18 2003 - 21:05:07 MDT
</em></small></p>
</body>
</html>
