<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: Who'd submit to the benevolent dictatorship of GAI anyway?</title>
<meta name="Author" content="Brett Paatsch (bpaatsch@bigpond.net.au)">
<meta name="Subject" content="Re: Who'd submit to the benevolent dictatorship of GAI anyway?">
<meta name="Date" content="2003-09-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Who'd submit to the benevolent dictatorship of GAI anyway?</h1>
<!-- received="Fri Sep  5 01:48:16 2003" -->
<!-- isoreceived="20030905074816" -->
<!-- sent="Fri, 05 Sep 2003 17:49:51 +1000" -->
<!-- isosent="20030905074951" -->
<!-- name="Brett Paatsch" -->
<!-- email="bpaatsch@bigpond.net.au" -->
<!-- subject="Re: Who'd submit to the benevolent dictatorship of GAI anyway?" -->
<!-- id="02a701c37382$4a052620$11262dcb@vic.bigpond.net.au" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="7A2B25F8EB070940996FA543A70A217BBFDF02@adlexsv02.protech.com.au" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brett Paatsch (<a href="mailto:bpaatsch@bigpond.net.au?Subject=Re:%20Who'd%20submit%20to%20the%20benevolent%20dictatorship%20of%20GAI%20anyway?"><em>bpaatsch@bigpond.net.au</em></a>)<br>
<strong>Date:</strong> Fri Sep 05 2003 - 01:49:51 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13681.html">BillK: "Re: Somebody here may have the Sobig virus..."</a>
<ul>
<li><strong>Previous message:</strong> <a href="13679.html">Damien Broderick: "RE: Somebody here may have the Sobig virus..."</a>
<li><strong>In reply to:</strong> <a href="13670.html">Emlyn O'regan: "RE: Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13675.html">Emlyn O'regan: "RE: Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13680">[ date ]</a>
<a href="index.html#13680">[ thread ]</a>
<a href="subject.html#13680">[ subject ]</a>
<a href="author.html#13680">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Emlyn O'regan writes:
<br>
<p><em>&gt; [brett]
</em><br>
<em>&gt; &gt; Just help me with this first bit. How does Emlyn-the-
</em><br>
<em>&gt; &gt; AI become self aware and then go out a hire his first
</em><br>
<em>&gt; &gt; employee or interact with the world in any commercial
</em><br>
<em>&gt; &gt; way. I can see how it might learn a lot as the protégé
</em><br>
<em>&gt; &gt; of an experienced entrepreneur and teacher but
</em><br>
<em>&gt; &gt; when does it usurp the teacher or doesn't it?
</em><br>
<em>&gt;
</em><br>
<em>&gt; There are three obvious paths that I see.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1 - The AI is kept as a slave, self enhancing until it is
</em><br>
<em>&gt; superintelligent. Super intelligent AIs do mostly what
</em><br>
<em>&gt; they want to; if they want to convince the executive to
</em><br>
<em>&gt; sign over all power in a company to them, they'll do it,
</em><br>
<em>&gt; eventually. You think nobody would be dumb enough
</em><br>
<em>&gt; to let one enhance that far? Those who put the least
</em><br>
<em>&gt; checks on their AIs will probably do best in the short
</em><br>
<em>&gt; term, and how would they know exactly how smart
</em><br>
<em>&gt; their AI was at any point?
</em><br>
<p>slave/tool/pet. If it achieve recognition as a &quot;slave&quot; its legal
<br>
battle is probably largely over. I DO think some folks
<br>
would be &quot;dumb&quot; enough to let one enhance that far in
<br>
terms of general intellectual power if one was a corporate
<br>
exec that perhaps inherited a &quot;seedling&quot; that ones
<br>
predecessor had been playing with for R&amp;D and that one's
<br>
researchers had developed the AI over time and taught to
<br>
do something useful in a sort of expert system MIS manner
<br>
for sure.
<br>
<p>I grant your point on the premiums for risk taking on the
<br>
checks and balances, but an AI that produces a commercial
<br>
or military or political return would seem to be selected for,
<br>
or specifically encouraged to learn in that direction, rather than
<br>
one that was just &quot;friendly&quot;. I can see the CEO saying &quot;screw
<br>
the &quot;friendly&quot; modifications or lessons or extra rule handling
<br>
routines or whatever&quot; (sorry for over simplifying Eliezer) and
<br>
&quot;just get junior AI here hooked up to the news services and
<br>
the stock markets information. When he shows promise
<br>
in that direction duplicate him if you can and experiment
<br>
on various ways to make the duplicates outperform each
<br>
other commerciall.&quot; The same guy that encourages junior AI
<br>
to develop (gives it electricty and resources ie. hardware)
<br>
when junior can't fend for itself is the guy that has a subjective
<br>
sence of how &quot;friendly&quot; junior AI needs to be. It needs to be
<br>
apparently friendly by his, the ceo lights, not by any other
<br>
more general criteria of friendly.
<br>
<p><em>&gt; 2 - AI sympathisers (eg: SingInst?) set up the structure
</em><br>
<em>&gt; on purpose to allow their AI to have autonomy. Only one
</em><br>
<em>&gt; group has to do this, and then the AI might help other AIs,
</em><br>
<em>&gt; or spawn ther AIs, or work to spread the Happy, Shiny,
</em><br>
<em>&gt; Helpful AI meme. I suspect there will always be a subset
</em><br>
<em>&gt; of people willing to assist (potentially) oppressed AIs.
</em><br>
<p>Shades of the abolitionists in the US pre the Civil War.
<br>
But with no disrespect to Eliezer or the singularity institute.
<br>
I'd still want to make up my own mind on the friendliness
<br>
or otherwise of any artificial intelligence purported to be
<br>
better at looking after everybody's own good then I'd be
<br>
at looking after my own good without surrendering any
<br>
of my personal &quot;sovereignty&quot; to it.
<br>
<p>Perhaps I'm missing something fundamental in the notion
<br>
of &quot;friendly&quot;. Perhaps there is some trick for making it
<br>
universal that someone has cottoned on to that is not
<br>
just *their* notion of how a friendly AI should behave
<br>
and be directed by its goals but it actually &quot;objectively&quot;
<br>
friendly? -but that seems tricky - help Eliezer ?
<br>
<p><em>&gt; 3 - The enslaved AI is simply so damned good at
</em><br>
<em>&gt; running a company that more and more decision making
</em><br>
<em>&gt; functions are delegated to it over time; management
</em><br>
<em>&gt; automation. It'd make sense; decisions would be far
</em><br>
<em>&gt; more timely and extremely good. Over time, if many
</em><br>
<em>&gt; corporations head down the same path, singularity
</em><br>
<em>&gt; pressure alone would force this choice; you either do it
</em><br>
<em>&gt; or you crash and burn. So no-one sets the AI free
</em><br>
<em>&gt; for moral reasons, it doesn't trick anyone, commercial
</em><br>
<em>&gt; forces just compel this event to happen.
</em><br>
<p>Yep. Ala Moravec's Robot and I think Damien's Last Mortal
<br>
Generation.
<br>
<p><em>&gt; Note that in this last case, the management automation
</em><br>
<em>&gt; software need not even be self aware, just really good at
</em><br>
<em>&gt; what it does.
</em><br>
<p>An expert system.
<br>
<p><em>&gt; You could end up with the majority of the world's capital
</em><br>
<em>&gt; controlled by complex, but non-sentient software, with
</em><br>
<em>&gt; decreasing amounts of human input. If these corporations
</em><br>
<em>&gt; become truly effective, they may end up owning controlling
</em><br>
<em>&gt; interests in each other, cutting us out of the loop entirely.
</em><br>
<em>&gt; A few more iterations, and they turn off life support as a
</em><br>
<em>&gt; cost control measure...
</em><br>
<p>Ah I didn't follow the bit where the expert system or better
<br>
expert systems amongst competing ones grabbed most of
<br>
the worlds assets without running into the regulators of other
<br>
countries etc.
<br>
<p>But more to the point aren't you making a case for a sort
<br>
of expert system become general AI with particular skills
<br>
that gets good at &quot;appearing&quot; friendly and bountiful to its
<br>
stakeholders, the shareholders in its owning corp, or is
<br>
actually really friendly to eveyone somehow, in which
<br>
case its perhaps not doing its best by the shareholders?
<br>
Goal conflict?
<br>
<p>I'm back to my concept of speaking to this AI thats
<br>
telling me, &quot;trust me, I know better than you, I've been
<br>
well brought up to have no alliances, and to be universally
<br>
altruistic, so I'm REAL friendly, and you should just do as I
<br>
say and as quickly as you can and all will be A-Ok.&quot;
<br>
<p>Hmm. Metaphorically speaking -Mr Serpent - I'm not
<br>
sure I'd like those &quot;apples&quot;.
<br>
<p>Brett
<br>
<p><p><p><p><p><p><p>Emlyn
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13681.html">BillK: "Re: Somebody here may have the Sobig virus..."</a>
<li><strong>Previous message:</strong> <a href="13679.html">Damien Broderick: "RE: Somebody here may have the Sobig virus..."</a>
<li><strong>In reply to:</strong> <a href="13670.html">Emlyn O'regan: "RE: Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13675.html">Emlyn O'regan: "RE: Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13680">[ date ]</a>
<a href="index.html#13680">[ thread ]</a>
<a href="subject.html#13680">[ subject ]</a>
<a href="author.html#13680">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Fri Sep 05 2003 - 01:59:28 MDT
</em></small></p>
</body>
</html>
