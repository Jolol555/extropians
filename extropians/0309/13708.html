<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: Who'd submit to the benevolent dictatorship of GAI anyway?</title>
<meta name="Author" content="Brett Paatsch (bpaatsch@bigpond.net.au)">
<meta name="Subject" content="Re: Who'd submit to the benevolent dictatorship of GAI anyway?">
<meta name="Date" content="2003-09-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Who'd submit to the benevolent dictatorship of GAI anyway?</h1>
<!-- received="Fri Sep  5 23:36:22 2003" -->
<!-- isoreceived="20030906053622" -->
<!-- sent="Sat, 06 Sep 2003 15:39:42 +1000" -->
<!-- isosent="20030906053942" -->
<!-- name="Brett Paatsch" -->
<!-- email="bpaatsch@bigpond.net.au" -->
<!-- subject="Re: Who'd submit to the benevolent dictatorship of GAI anyway?" -->
<!-- id="00e101c37439$45cb11c0$11262dcb@vic.bigpond.net.au" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20030905100853.GA19618@akira.nada.kth.se" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brett Paatsch (<a href="mailto:bpaatsch@bigpond.net.au?Subject=Re:%20Who'd%20submit%20to%20the%20benevolent%20dictatorship%20of%20GAI%20anyway?"><em>bpaatsch@bigpond.net.au</em></a>)<br>
<strong>Date:</strong> Fri Sep 05 2003 - 23:39:42 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13709.html">Damien Broderick: "Re: BOOKS: Has anyone read this book?"</a>
<ul>
<li><strong>Previous message:</strong> <a href="13707.html">Spike: "RE: SPACE: Loss of the Saturn V"</a>
<li><strong>In reply to:</strong> <a href="13685.html">Anders Sandberg: "Re: Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13632.html">Spike: "RE: Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13708">[ date ]</a>
<a href="index.html#13708">[ thread ]</a>
<a href="subject.html#13708">[ subject ]</a>
<a href="author.html#13708">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Anders writes:
<br>
<p>[Brett]
<br>
<em>&gt; &gt;...whether its Joe person,
</em><br>
<em>&gt; &gt; Joe the group or corporate or Joe the country the concerns
</em><br>
<em>&gt; &gt; are still the same.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It still assumes that one group is the only one with access to
</em><br>
<em>&gt; a certain technology.
</em><br>
<p><em>&gt; .... the strong takeoff position claims that this is enough to
</em><br>
<em>&gt; make the monopoly last indefinitely, but I'm not so sure about
</em><br>
<em>&gt; that (or the strong takeoff).
</em><br>
<p>Me either I'm extremely sceptical but its worth considering the
<br>
arguments for a strong take-off in my view for one to make
<br>
good decisions about where it is prudent to spend ones time.
<br>
<p><em>&gt; IMHO a much more plausible situation has AI of various
</em><br>
<em>&gt; calibers in the hands of many groups, and Joe just happens
</em><br>
<em>&gt; to have the most powerful one.
</em><br>
<p>Agreed. Though I'd note that Joe-groups structure may have
<br>
some implications too. One Joe person might design a
<br>
Joe-group (as a set of private corporation(s) but that's an
<br>
aside.
<br>
<p><em>&gt; What remains to be analysed is how much power that
</em><br>
<em>&gt; translates into.
</em><br>
<p>Yes. Excellent question.
<br>
<p><em>&gt; &gt; Where in all this does the friendly bit of AI
</em><br>
<em>&gt; &gt; get to emerge that is genuinely universally friendly? Seems
</em><br>
<em>&gt; &gt; to me it quite possibly doesn't. Universal friendliness is not
</em><br>
<em>&gt; &gt; something that the AI developers get a premium for
</em><br>
<em>&gt; &gt; &quot;growing from a seed&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It seems likely that many would aim for something like
</em><br>
<em>&gt; friendliness, but  indeed not universal friendliness.
</em><br>
<p>Again, I'd like to hear the case well argued for the possibility
<br>
of universal friendliness being designed in, from someone who
<br>
thinks it can be. I thought Eliezer may be such a person but I
<br>
am quite possibly misunderstanding his position. And that would
<br>
be my own fault substantially for not having the time to come
<br>
up to speed. It could also be Eliezer's position that friendliness
<br>
is not garanteed to be engineerable but the risk of not checking
<br>
it out (if it is) is very much not worth ignoring. The singularity
<br>
could be a &quot;black&quot; (negative to humanity) singularity. (Apologies
<br>
again Eliezer if I'm misunderstanding).
<br>
<p><em>&gt; &gt; &gt; Maybe one could make an analysis looking at different &quot;Gini
</em><br>
<em>&gt; &gt; &gt; coefficients&quot; of AI intelligence distribution.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Sorry. Must plead ignorance to &quot;gini coefficients&quot; but some risk
</em><br>
<em>&gt; &gt; analysis sounds real healthy 'cause if it turns out that by running
</em><br>
<em>&gt; &gt; some fairly easy game theory notions that the first AI's are going
</em><br>
<em>&gt; &gt; to face a backlash because they are not human and can't be
</em><br>
<em>&gt; &gt; agents or treated as persons (where would society draw the
</em><br>
<em>&gt; &gt; line?) then we may do well to look at the consequences for this
</em><br>
<em>&gt; &gt; on any singularity take off time.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't think game theory says anything about a backlash, that is just
</em><br>
<em>&gt; psychology.
</em><br>
<p>Fair enough I think you and I probably have slightly different perceptions
<br>
of the ambit of game theory, and that your usage is the more conventional.
<br>
I'd include psychology and forms of political transaction analysis (some
<br>
of my own orignial stuff -so far as I know - based on notions of binary
<br>
and ternary logic) in the broader notion of &quot;game theory&quot; - i.e. what are
<br>
the
<br>
choices that players can actually make given a scenario and how does each
<br>
players choice determine the choices of the other players- but again I
<br>
digress.
<br>
<p><em>&gt; As for the Gini coefficient, it is a measure of inequality.
</em><br>
<em>&gt; It is zero for a population where everyone is equal (all AIs have the
</em><br>
<em>&gt; same power) and one for a population of total inequality (one super AI,
</em><br>
<em>&gt; zero others):
</em><br>
<em>&gt; <a href="http://www.wikipedia.org/wiki/Gini_coefficient">http://www.wikipedia.org/wiki/Gini_coefficient</a>
</em><br>
<em>&gt; <a href="http://mathworld.wolfram.com/GiniCoefficient.html">http://mathworld.wolfram.com/GiniCoefficient.html</a>
</em><br>
<em>&gt;
</em><br>
<p>Thanks!  I console myself, not very effectively, that Einstein
<br>
struggled with math too :-(     Probably from a higher base.
<br>
<p><em>&gt; Imagine that having an AI of power P directly translates into social
</em><br>
<em>&gt; power; the social power SP of group number i with an AI is
</em><br>
<em>&gt; SP_i= P_i / sum_j P_j
</em><br>
<p>Can you please define your terms &quot;power P&quot; and social power SP
<br>
more fully. Intelligence is a matter of some contention even amongst
<br>
psychologist in relation to humans. Artificial intelligence as its
<br>
approximation from various different directions must be more so.
<br>
Can you be sure its meaningful to have a single &quot;power P&quot;. I am
<br>
wary that my math is likely to be marked inferior to yours but
<br>
garbage in garbage out  ;-)
<br>
<p><em>&gt; If a group with social power can win over any group with lesser
</em><br>
<em>&gt; power, then the most powerful group (lets call it 1, and order them
</em><br>
<em>&gt; by power) [that] can run everything is P_1 &gt; sum_j=2^N P_j.
</em><br>
<em>&gt; If we assume the powers follow a power law distribution
</em><br>
<em>&gt; P_j = c/j^k  for some k&gt;0 this can happen above a certain k=K,
</em><br>
<em>&gt; where 2^(1-K)=K.
</em><br>
<em>&gt; So there is a range of power distributions that is not amenable to
</em><br>
<em>&gt; direct takeover.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If we assume P grows exponentially over time (P'=lambda P)
</em><br>
<em>&gt; for everyone there is no relative change in the analysis above,
</em><br>
<em>&gt; which is very interesting. If we only count AI power of course
</em><br>
<em>&gt; the first AI will be dominant even if it is Eliza; in reality social
</em><br>
<em>&gt; power has a non-AI factor F which we have to add in:
</em><br>
<em>&gt; SP_i = (P_i + F_i)/sum_j (P_j + F_j); it can be assumed to be
</em><br>
<em>&gt; constant and we can scale it to 1 for simplicity. A bit of equation
</em><br>
<em>&gt; fiddling does not seem to change things much.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But if the rate of growth is more than proportional to P_i
</em><br>
<em>&gt; (a Vingean singularity) or if the initial distrivution of the non-AI
</em><br>
<em>&gt; power is extreme, then the potential of monopolies increase.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is worth looking into more detail, but now I have to go to
</em><br>
<em>&gt;  lunch.
</em><br>
<p>I agree, but I'm going to have to backfill some neglected math
<br>
concepts to follow you properly above.
<br>
<p>Well at least you didn't do that little bit of analysis &quot;before
<br>
breakfast&quot; ;-)
<br>
<p>Seriously, I see the question of how likely &quot;friendly&quot; and
<br>
&quot;unfriendly AI&quot; is too emerge, if at all, as crucial to planning
<br>
and I imagine others like me would too.
<br>
<p>I feel I can plan quite effectively, assuming only Moore's law
<br>
to say 2012, and the development of expert systems with no
<br>
requirement that they be friendly or generally intelligent at all
<br>
anytime soon. i.e.. I want better protein folding grunt and a few
<br>
other neat IT tools but I don't need super general AI. And
<br>
frankly at this stage super general AI presents to me as more
<br>
of a threat than a benefit. It seems likely to end up in the
<br>
wrong hands and turn distinctly unfriendly even with the best
<br>
of intent by those working on it.
<br>
<p>Regards,
<br>
Brett
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13709.html">Damien Broderick: "Re: BOOKS: Has anyone read this book?"</a>
<li><strong>Previous message:</strong> <a href="13707.html">Spike: "RE: SPACE: Loss of the Saturn V"</a>
<li><strong>In reply to:</strong> <a href="13685.html">Anders Sandberg: "Re: Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13632.html">Spike: "RE: Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13708">[ date ]</a>
<a href="index.html#13708">[ thread ]</a>
<a href="subject.html#13708">[ subject ]</a>
<a href="author.html#13708">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Fri Sep 05 2003 - 23:45:33 MDT
</em></small></p>
</body>
</html>
