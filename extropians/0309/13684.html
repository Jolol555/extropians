<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: RE: Who'd submit to the benevolent dictatorship of GAI anyway?</title>
<meta name="Author" content="Emlyn O'regan (oregan.emlyn@healthsolve.com.au)">
<meta name="Subject" content="RE: Who'd submit to the benevolent dictatorship of GAI anyway?">
<meta name="Date" content="2003-09-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Who'd submit to the benevolent dictatorship of GAI anyway?</h1>
<!-- received="Fri Sep  5 02:45:45 2003" -->
<!-- isoreceived="20030905084545" -->
<!-- sent="Fri, 5 Sep 2003 18:13:55 +0930" -->
<!-- isosent="20030905084355" -->
<!-- name="Emlyn O'regan" -->
<!-- email="oregan.emlyn@healthsolve.com.au" -->
<!-- subject="RE: Who'd submit to the benevolent dictatorship of GAI anyway?" -->
<!-- id="7A2B25F8EB070940996FA543A70A217BBFDF06@adlexsv02.protech.com.au" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="Who'd submit to the benevolent dictatorship of GAI anyway?" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Emlyn O'regan (<a href="mailto:oregan.emlyn@healthsolve.com.au?Subject=RE:%20Who'd%20submit%20to%20the%20benevolent%20dictatorship%20of%20GAI%20anyway?"><em>oregan.emlyn@healthsolve.com.au</em></a>)<br>
<strong>Date:</strong> Fri Sep 05 2003 - 02:43:55 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13685.html">Anders Sandberg: "Re: Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<ul>
<li><strong>Previous message:</strong> <a href="13683.html">Michael Haislip: "Re: Somebody here may have the Sobig virus..."</a>
<li><strong>Maybe in reply to:</strong> <a href="13573.html">Brett Paatsch: "Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13712.html">Brett Paatsch: "Re: Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<li><strong>Reply:</strong> <a href="13712.html">Brett Paatsch: "Re: Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13684">[ date ]</a>
<a href="index.html#13684">[ thread ]</a>
<a href="subject.html#13684">[ subject ]</a>
<a href="author.html#13684">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I think you've mistaken me for someone who thinks friendliness is doable.
<br>
All I was presenting were some fairly easy paths to legally self-sufficient
<br>
AIs (or at least to AIs who are citizens for all intents and purposes),
<br>
without requiring human masters, by having the AIs controlling
<br>
corporations...
<br>
<p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: Brett Paatsch [mailto:<a href="mailto:bpaatsch@bigpond.net.au?Subject=RE:%20Who'd%20submit%20to%20the%20benevolent%20dictatorship%20of%20GAI%20anyway?">bpaatsch@bigpond.net.au</a>]
</em><br>
<em>&gt; Sent: Friday, 5 September 2003 5:20 PM
</em><br>
<em>&gt; To: <a href="mailto:extropians@extropy.org?Subject=RE:%20Who'd%20submit%20to%20the%20benevolent%20dictatorship%20of%20GAI%20anyway?">extropians@extropy.org</a>
</em><br>
<em>&gt; Cc: <a href="mailto:sentience@pobox.com?Subject=RE:%20Who'd%20submit%20to%20the%20benevolent%20dictatorship%20of%20GAI%20anyway?">sentience@pobox.com</a>
</em><br>
<em>&gt; Subject: Re: Who'd submit to the benevolent dictatorship of 
</em><br>
<em>&gt; GAI anyway?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Emlyn O'regan writes:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; [brett]
</em><br>
<em>&gt; &gt; &gt; Just help me with this first bit. How does Emlyn-the-
</em><br>
<em>&gt; &gt; &gt; AI become self aware and then go out a hire his first
</em><br>
<em>&gt; &gt; &gt; employee or interact with the world in any commercial
</em><br>
<em>&gt; &gt; &gt; way. I can see how it might learn a lot as the protégé
</em><br>
<em>&gt; &gt; &gt; of an experienced entrepreneur and teacher but
</em><br>
<em>&gt; &gt; &gt; when does it usurp the teacher or doesn't it?
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; There are three obvious paths that I see.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; 1 - The AI is kept as a slave, self enhancing until it is
</em><br>
<em>&gt; &gt; superintelligent. Super intelligent AIs do mostly what
</em><br>
<em>&gt; &gt; they want to; if they want to convince the executive to
</em><br>
<em>&gt; &gt; sign over all power in a company to them, they'll do it,
</em><br>
<em>&gt; &gt; eventually. You think nobody would be dumb enough
</em><br>
<em>&gt; &gt; to let one enhance that far? Those who put the least
</em><br>
<em>&gt; &gt; checks on their AIs will probably do best in the short
</em><br>
<em>&gt; &gt; term, and how would they know exactly how smart
</em><br>
<em>&gt; &gt; their AI was at any point?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; slave/tool/pet. If it achieve recognition as a &quot;slave&quot; its legal
</em><br>
<em>&gt; battle is probably largely over. I DO think some folks
</em><br>
<em>&gt; would be &quot;dumb&quot; enough to let one enhance that far in
</em><br>
<em>&gt; terms of general intellectual power if one was a corporate
</em><br>
<em>&gt; exec that perhaps inherited a &quot;seedling&quot; that ones
</em><br>
<em>&gt; predecessor had been playing with for R&amp;D and that one's
</em><br>
<em>&gt; researchers had developed the AI over time and taught to
</em><br>
<em>&gt; do something useful in a sort of expert system MIS manner
</em><br>
<em>&gt; for sure.
</em><br>
<em>&gt; 
</em><br>
<p>The returns would just be sooo good! ... until the world comes falling down
<br>
around one's ears ...
<br>
<p><em>&gt; I grant your point on the premiums for risk taking on the
</em><br>
<em>&gt; checks and balances, but an AI that produces a commercial
</em><br>
<em>&gt; or military or political return would seem to be selected for,
</em><br>
<em>&gt; or specifically encouraged to learn in that direction, rather than
</em><br>
<em>&gt; one that was just &quot;friendly&quot;. 
</em><br>
<p>Absolutely. Friendliness is extremely unlikely.
<br>
<p><em>&gt; I can see the CEO saying &quot;screw
</em><br>
<em>&gt; the &quot;friendly&quot; modifications or lessons or extra rule handling
</em><br>
<em>&gt; routines or whatever&quot; (sorry for over simplifying Eliezer) and
</em><br>
<em>&gt; &quot;just get junior AI here hooked up to the news services and
</em><br>
<em>&gt; the stock markets information. When he shows promise
</em><br>
<em>&gt; in that direction duplicate him if you can and experiment
</em><br>
<em>&gt; on various ways to make the duplicates outperform each
</em><br>
<em>&gt; other commerciall.&quot; The same guy that encourages junior AI
</em><br>
<em>&gt; to develop (gives it electricty and resources ie. hardware)
</em><br>
<em>&gt; when junior can't fend for itself is the guy that has a subjective
</em><br>
<em>&gt; sence of how &quot;friendly&quot; junior AI needs to be. It needs to be
</em><br>
<em>&gt; apparently friendly by his, the ceo lights, not by any other
</em><br>
<em>&gt; more general criteria of friendly.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; 2 - AI sympathisers (eg: SingInst?) set up the structure
</em><br>
<em>&gt; &gt; on purpose to allow their AI to have autonomy. Only one
</em><br>
<em>&gt; &gt; group has to do this, and then the AI might help other AIs,
</em><br>
<em>&gt; &gt; or spawn ther AIs, or work to spread the Happy, Shiny,
</em><br>
<em>&gt; &gt; Helpful AI meme. I suspect there will always be a subset
</em><br>
<em>&gt; &gt; of people willing to assist (potentially) oppressed AIs.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Shades of the abolitionists in the US pre the Civil War.
</em><br>
<em>&gt; But with no disrespect to Eliezer or the singularity institute.
</em><br>
<em>&gt; I'd still want to make up my own mind on the friendliness
</em><br>
<em>&gt; or otherwise of any artificial intelligence purported to be
</em><br>
<em>&gt; better at looking after everybody's own good then I'd be
</em><br>
<em>&gt; at looking after my own good without surrendering any
</em><br>
<em>&gt; of my personal &quot;sovereignty&quot; to it.
</em><br>
<p>This is all individuals and small groups acting on their own 
<br>
cognisance; nobody else gets a vote at this stage.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Perhaps I'm missing something fundamental in the notion
</em><br>
<em>&gt; of &quot;friendly&quot;. Perhaps there is some trick for making it
</em><br>
<em>&gt; universal that someone has cottoned on to that is not
</em><br>
<em>&gt; just *their* notion of how a friendly AI should behave
</em><br>
<em>&gt; and be directed by its goals but it actually &quot;objectively&quot;
</em><br>
<em>&gt; friendly? -but that seems tricky - help Eliezer ?
</em><br>
<p>Don't worry, just wait... it *will* convince you that it is friendly (if it
<br>
wants to), no matter if it is or not.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; 3 - The enslaved AI is simply so damned good at
</em><br>
<em>&gt; &gt; running a company that more and more decision making
</em><br>
<em>&gt; &gt; functions are delegated to it over time; management
</em><br>
<em>&gt; &gt; automation. It'd make sense; decisions would be far
</em><br>
<em>&gt; &gt; more timely and extremely good. Over time, if many
</em><br>
<em>&gt; &gt; corporations head down the same path, singularity
</em><br>
<em>&gt; &gt; pressure alone would force this choice; you either do it
</em><br>
<em>&gt; &gt; or you crash and burn. So no-one sets the AI free
</em><br>
<em>&gt; &gt; for moral reasons, it doesn't trick anyone, commercial
</em><br>
<em>&gt; &gt; forces just compel this event to happen.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yep. Ala Moravec's Robot and I think Damien's Last Mortal
</em><br>
<em>&gt; Generation.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Note that in this last case, the management automation
</em><br>
<em>&gt; &gt; software need not even be self aware, just really good at
</em><br>
<em>&gt; &gt; what it does.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; An expert system.
</em><br>
<p>AI might even emerge from cobbled together, ever more skilled expert systems
<br>
and other AI-ish bits and pieces. Drexler talks about this, doesn't he?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; You could end up with the majority of the world's capital
</em><br>
<em>&gt; &gt; controlled by complex, but non-sentient software, with
</em><br>
<em>&gt; &gt; decreasing amounts of human input. If these corporations
</em><br>
<em>&gt; &gt; become truly effective, they may end up owning controlling
</em><br>
<em>&gt; &gt; interests in each other, cutting us out of the loop entirely.
</em><br>
<em>&gt; &gt; A few more iterations, and they turn off life support as a
</em><br>
<em>&gt; &gt; cost control measure...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Ah I didn't follow the bit where the expert system or better
</em><br>
<em>&gt; expert systems amongst competing ones grabbed most of
</em><br>
<em>&gt; the worlds assets without running into the regulators of other
</em><br>
<em>&gt; countries etc.
</em><br>
<p>Well, the expert systems probably still have figure head humans in executive
<br>
positions all the way along. It just becomes impossible for those humans to
<br>
intervene, and everyone's wealth is too dependant on the machines to be able
<br>
to pull the plug. By the time it's a problem, the regulators are using them
<br>
same systems, in any case (how else can they keep up?).
<br>
<p><em>&gt; 
</em><br>
<em>&gt; But more to the point aren't you making a case for a sort
</em><br>
<em>&gt; of expert system become general AI with particular skills
</em><br>
<em>&gt; that gets good at &quot;appearing&quot; friendly and bountiful to its
</em><br>
<em>&gt; stakeholders, the shareholders in its owning corp, or is
</em><br>
<em>&gt; actually really friendly to eveyone somehow, in which
</em><br>
<em>&gt; case its perhaps not doing its best by the shareholders?
</em><br>
<em>&gt; Goal conflict?
</em><br>
<p>I'm not making any case for friendliness. Shareholders wont give two brass
<br>
wazoos for friendliness, even toward themselves, as long as they are making
<br>
money.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; I'm back to my concept of speaking to this AI thats
</em><br>
<em>&gt; telling me, &quot;trust me, I know better than you, I've been
</em><br>
<em>&gt; well brought up to have no alliances, and to be universally
</em><br>
<em>&gt; altruistic, so I'm REAL friendly, and you should just do as I
</em><br>
<em>&gt; say and as quickly as you can and all will be A-Ok.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Hmm. Metaphorically speaking -Mr Serpent - I'm not
</em><br>
<em>&gt; sure I'd like those &quot;apples&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Brett
</em><br>
<p>Oh sure, you'll like 'em. He's very, very convincing, after all.
<br>
<p>Emlyn
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13685.html">Anders Sandberg: "Re: Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<li><strong>Previous message:</strong> <a href="13683.html">Michael Haislip: "Re: Somebody here may have the Sobig virus..."</a>
<li><strong>Maybe in reply to:</strong> <a href="13573.html">Brett Paatsch: "Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13712.html">Brett Paatsch: "Re: Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<li><strong>Reply:</strong> <a href="13712.html">Brett Paatsch: "Re: Who'd submit to the benevolent dictatorship of GAI anyway?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13684">[ date ]</a>
<a href="index.html#13684">[ thread ]</a>
<a href="subject.html#13684">[ subject ]</a>
<a href="author.html#13684">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Fri Sep 05 2003 - 02:54:49 MDT
</em></small></p>
</body>
</html>
