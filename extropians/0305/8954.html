<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: RE: Status of Superrationality</title>
<meta name="Author" content="Rafal Smigrodzki (rafal@smigrodzki.org)">
<meta name="Subject" content="RE: Status of Superrationality">
<meta name="Date" content="2003-05-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Status of Superrationality</h1>
<!-- received="Wed May 28 14:49:26 2003" -->
<!-- isoreceived="20030528204926" -->
<!-- sent="Wed, 28 May 2003 16:49:51 -0700" -->
<!-- isosent="20030528234951" -->
<!-- name="Rafal Smigrodzki" -->
<!-- email="rafal@smigrodzki.org" -->
<!-- subject="RE: Status of Superrationality" -->
<!-- id="OJEHKDIANIFPAJPDBDGLMEAACEAA.rafal@smigrodzki.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="JEEOJKGAIKFKEHJONHMPCEHLEHAA.lcorbin@tsoft.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Rafal Smigrodzki (<a href="mailto:rafal@smigrodzki.org?Subject=RE:%20Status%20of%20Superrationality"><em>rafal@smigrodzki.org</em></a>)<br>
<strong>Date:</strong> Wed May 28 2003 - 17:49:51 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8955.html">Hal Finney: "Re: PHYSICS: Black holes on demand?"</a>
<ul>
<li><strong>Previous message:</strong> <a href="8953.html">Dennis Fantoni: "Re: Extropy.org and Folding@Home"</a>
<li><strong>In reply to:</strong> <a href="8907.html">Lee Corbin: "RE: Status of Superrationality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8960.html">Eliezer S. Yudkowsky: "Re: Status of Superrationality"</a>
<li><strong>Reply:</strong> <a href="8960.html">Eliezer S. Yudkowsky: "Re: Status of Superrationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8954">[ date ]</a>
<a href="index.html#8954">[ thread ]</a>
<a href="subject.html#8954">[ subject ]</a>
<a href="author.html#8954">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<a href="mailto:owner-extropians@extropy.org?Subject=RE:%20Status%20of%20Superrationality">owner-extropians@extropy.org</a> wrote:
<br>
<em>&gt; Rafal writes
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; Hal Finney wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; Without analyzing it in detail, I think this level of honesty,
</em><br>
<em>&gt;&gt;&gt; in conjunction with the usual game theory assumption of rationality,
</em><br>
<em>&gt;&gt;&gt; would be enough to imply the result that the two parties can't
</em><br>
<em>&gt;&gt;&gt; disagree. Basically the argument is the same, that since you both
</em><br>
<em>&gt;&gt;&gt; have the same goals and (arguably) the same priors, the fact that
</em><br>
<em>&gt;&gt;&gt; the other party judges an outcome differently than you must make
</em><br>
<em>&gt;&gt;&gt; you no more likely to believe your own estimation than his.  Since
</em><br>
<em>&gt;&gt;&gt; the game theory matrix makes the estimated utilities for each
</em><br>
<em>&gt;&gt;&gt; outcome common knowledge, the two estimates must be equal, for each
</em><br>
<em>&gt;&gt;&gt; outcome.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; ### But isn't the main problem an irreconcilable difference in the
</em><br>
<em>&gt;&gt; goals between players, the difference in weighing outcomes? The
</em><br>
<em>&gt;&gt; simplified depiction of the averagist vs. the totalist is just the
</em><br>
<em>&gt;&gt; beginning: you could imagine all kinds of global payoff matrices,
</em><br>
<em>&gt;&gt; describing attitudes towards outcomes affecting all objects of
</em><br>
<em>&gt;&gt; value, and even differences in what may be considered an object of
</em><br>
<em>&gt;&gt; value. There are those who favor asymmetric relationships between
</em><br>
<em>&gt;&gt; wishes and their fulfillment (meaning that while the total rather
</em><br>
<em>&gt;&gt; than average utility is to be maximized, at the same time a limited
</em><br>
<em>&gt;&gt; list of outcomes must be minimized). There are fundamental
</em><br>
<em>&gt;&gt; differences the lists of subjects whose preferences are to be
</em><br>
<em>&gt;&gt; entered into the ethical equation, and the methods for relative
</em><br>
<em>&gt;&gt; weighing of such preferences.
</em><br>
<em>&gt;
</em><br>
<em>&gt; At this stage, I'm not going to claim that I understand what you
</em><br>
<em>&gt; have written.  But would you care to comment upon
</em><br>
<em>&gt;
</em><br>
<em>&gt; <a href="http://hanson.gmu.edu/deceive.pdf">http://hanson.gmu.edu/deceive.pdf</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; It mentions the annoying result that &quot;if two or more Bayesians
</em><br>
<em>&gt; would believe the same thing given the same information (i.e.,
</em><br>
<em>&gt; have &quot;common priors&quot;), then those individuals cannot knowingly
</em><br>
<em>&gt; disagree.  Merely knowing someone else's opinion provides a
</em><br>
<em>&gt; powerful summary of everything that person knows, powerful
</em><br>
<em>&gt; enough to eliminate any differences of opinion due to differing
</em><br>
<em>&gt; information.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I could certainly use a hand in getting to the bottom of this.
</em><br>
<em>&gt;
</em><br>
<p>### Certainly a quite complex article. I think that what you quoted above
<br>
means that the Bayesian would treat the output of another Bayesian as data
<br>
of the same validity as the output of his own reasoning. If you know that a
<br>
fellow Bayesian sincerely believes in flying saucers, you have to believe in
<br>
them, too, unless your priors are wildly divergent (&quot;having a memory of
<br>
seeing a flying saucer as clear as my memory of seeing my car is sufficient
<br>
to profess belief in flying saucers&quot; vs. &quot;no amount of subjective visual
<br>
experience is sufficient to profess belief in flying saucers&quot;). If the
<br>
honest Bayesian says he saw a flying saucer, you have to believe him, or
<br>
else assume he is not Bayesian at all, or has a higher visual/cortical
<br>
malfunction rate than you (i.e. is less Bayesian than you). Barring these
<br>
doubts, you would become as convinced about the existence of flying saucers
<br>
as the person who actually saw them, despite not having the direct sensory
<br>
input that he had. In effect, his beliefs are as valid an input for your
<br>
future reasoning as your own sensory and logical subsystem outputs.
<br>
<p>All this, as I mentioned before, has little bearing on the discussions where
<br>
the disagreements are about goals, rather than facts, and may be persistent.
<br>
<p>Rafal
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8955.html">Hal Finney: "Re: PHYSICS: Black holes on demand?"</a>
<li><strong>Previous message:</strong> <a href="8953.html">Dennis Fantoni: "Re: Extropy.org and Folding@Home"</a>
<li><strong>In reply to:</strong> <a href="8907.html">Lee Corbin: "RE: Status of Superrationality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8960.html">Eliezer S. Yudkowsky: "Re: Status of Superrationality"</a>
<li><strong>Reply:</strong> <a href="8960.html">Eliezer S. Yudkowsky: "Re: Status of Superrationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8954">[ date ]</a>
<a href="index.html#8954">[ thread ]</a>
<a href="subject.html#8954">[ subject ]</a>
<a href="author.html#8954">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed May 28 2003 - 15:02:50 MDT
</em></small></p>
</body>
</html>
