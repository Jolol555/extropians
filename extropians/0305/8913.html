<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: Status of Superrationality</title>
<meta name="Author" content="Jef Allbright (jef@jefallbright.net)">
<meta name="Subject" content="Re: Status of Superrationality">
<meta name="Date" content="2003-05-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Status of Superrationality</h1>
<!-- received="Tue May 27 20:32:56 2003" -->
<!-- isoreceived="20030528023256" -->
<!-- sent="Tue, 27 May 2003 19:32:11 -0700" -->
<!-- isosent="20030528023211" -->
<!-- name="Jef Allbright" -->
<!-- email="jef@jefallbright.net" -->
<!-- subject="Re: Status of Superrationality" -->
<!-- id="001c01c324c1$58095300$6901a8c0@DI" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="JEEOJKGAIKFKEHJONHMPCEHLEHAA.lcorbin@tsoft.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Jef Allbright (<a href="mailto:jef@jefallbright.net?Subject=Re:%20Status%20of%20Superrationality"><em>jef@jefallbright.net</em></a>)<br>
<strong>Date:</strong> Tue May 27 2003 - 20:32:11 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8914.html">Mike Lorrey: "Re: Guns vs. Tyranny"</a>
<ul>
<li><strong>Previous message:</strong> <a href="8912.html">Dehede011@aol.com: "Re: Meaning of &quot;liberal&quot;"</a>
<li><strong>In reply to:</strong> <a href="8907.html">Lee Corbin: "RE: Status of Superrationality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8923.html">Lee Corbin: "RE: Status of Superrationality"</a>
<li><strong>Reply:</strong> <a href="8923.html">Lee Corbin: "RE: Status of Superrationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8913">[ date ]</a>
<a href="index.html#8913">[ thread ]</a>
<a href="subject.html#8913">[ subject ]</a>
<a href="author.html#8913">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Lee Corbin wrote:
<br>
<em>&gt; Rafal writes
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; Hal Finney wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; Without analyzing it in detail, I think this level of honesty,
</em><br>
<em>&gt;&gt;&gt; in conjunction with the usual game theory assumption of rationality,
</em><br>
<em>&gt;&gt;&gt; would be enough to imply the result that the two parties can't
</em><br>
<em>&gt;&gt;&gt; disagree. Basically the argument is the same, that since you both
</em><br>
<em>&gt;&gt;&gt; have the same goals and (arguably) the same priors, the fact that
</em><br>
<em>&gt;&gt;&gt; the other party judges an outcome differently than you must make
</em><br>
<em>&gt;&gt;&gt; you no more likely to believe your own estimation than his.  Since
</em><br>
<em>&gt;&gt;&gt; the game theory matrix makes the estimated utilities for each
</em><br>
<em>&gt;&gt;&gt; outcome common knowledge, the two estimates must be equal, for each
</em><br>
<em>&gt;&gt;&gt; outcome.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; ### But isn't the main problem an irreconcilable difference in the
</em><br>
<em>&gt;&gt; goals between players, the difference in weighing outcomes? The
</em><br>
<em>&gt;&gt; simplified depiction of the averagist vs. the totalist is just the
</em><br>
<em>&gt;&gt; beginning: you could imagine all kinds of global payoff matrices,
</em><br>
<em>&gt;&gt; describing attitudes towards outcomes affecting all objects of
</em><br>
<em>&gt;&gt; value, and even differences in what may be considered an object of
</em><br>
<em>&gt;&gt; value. There are those who favor asymmetric relationships between
</em><br>
<em>&gt;&gt; wishes and their fulfillment (meaning that while the total rather
</em><br>
<em>&gt;&gt; than average utility is to be maximized, at the same time a limited
</em><br>
<em>&gt;&gt; list of outcomes must be minimized). There are fundamental
</em><br>
<em>&gt;&gt; differences the lists of subjects whose preferences are to be
</em><br>
<em>&gt;&gt; entered into the ethical equation, and the methods for relative
</em><br>
<em>&gt;&gt; weighing of such preferences.
</em><br>
<em>&gt;
</em><br>
<em>&gt; At this stage, I'm not going to claim that I understand what you
</em><br>
<em>&gt; have written.  But would you care to comment upon
</em><br>
<em>&gt;
</em><br>
<em>&gt; <a href="http://hanson.gmu.edu/deceive.pdf">http://hanson.gmu.edu/deceive.pdf</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; It mentions the annoying result that &quot;if two or more Bayesians
</em><br>
<em>&gt; would believe the same thing given the same information (i.e.,
</em><br>
<em>&gt; have &quot;common priors&quot;), then those individuals cannot knowingly
</em><br>
<em>&gt; disagree.  Merely knowing someone else's opinion provides a
</em><br>
<em>&gt; powerful summary of everything that person knows, powerful
</em><br>
<em>&gt; enough to eliminate any differences of opinion due to differing
</em><br>
<em>&gt; information.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I could certainly use a hand in getting to the bottom of this.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Lee
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; I would contend that even perfectly rational altruists could differ
</em><br>
<em>&gt;&gt; significantly about their recipes for the perfect world.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Rafal
</em><br>
<p>To me the problem is simple in concept, but limited in practice.  We can
<br>
never have absolute agreement between any two entities, due to their
<br>
different knowledge bases (experiences.)  However, two rational beings can
<br>
approach agreement as precisely as desired by analyzing and refining their
<br>
differences.  It's interesting to note that all belief systems fit perfectly
<br>
into the total web of beliefs that exist.  It couldn't be otherwise if we
<br>
accept that the universe itself is consistent. From this we might infer that
<br>
superrationality is what you get when you extrapolate any more limited
<br>
concept of rational behavior to a timeless setting. This seems particularly
<br>
appropos to extropians who hope and plan to live forever.
<br>
<p>- Jef
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8914.html">Mike Lorrey: "Re: Guns vs. Tyranny"</a>
<li><strong>Previous message:</strong> <a href="8912.html">Dehede011@aol.com: "Re: Meaning of &quot;liberal&quot;"</a>
<li><strong>In reply to:</strong> <a href="8907.html">Lee Corbin: "RE: Status of Superrationality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8923.html">Lee Corbin: "RE: Status of Superrationality"</a>
<li><strong>Reply:</strong> <a href="8923.html">Lee Corbin: "RE: Status of Superrationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8913">[ date ]</a>
<a href="index.html#8913">[ thread ]</a>
<a href="subject.html#8913">[ subject ]</a>
<a href="author.html#8913">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue May 27 2003 - 20:43:50 MDT
</em></small></p>
</body>
</html>
