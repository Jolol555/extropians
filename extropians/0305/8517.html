<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: Status of Superrationality (was Left/Right... can't we do better than this?)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Status of Superrationality (was Left/Right... can't we do better than this?)">
<meta name="Date" content="2003-05-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Status of Superrationality (was Left/Right... can't we do better than this?)</h1>
<!-- received="Tue May 20 13:50:47 2003" -->
<!-- isoreceived="20030520195047" -->
<!-- sent="Tue, 20 May 2003 15:50:46 -0400" -->
<!-- isosent="20030520195046" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Status of Superrationality (was Left/Right... can't we do better than this?)" -->
<!-- id="3ECA8716.3050409@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="200305201830.h4KIUEL13331@finney.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Status%20of%20Superrationality%20(was%20Left/Right...%20can't%20we%20do%20better%20than%20this?)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue May 20 2003 - 13:50:46 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8518.html">Mike Lorrey: "Re: [GENOME] Ecce Homo?"</a>
<ul>
<li><strong>Previous message:</strong> <a href="8516.html">Adrian Tymes: "Re: Long term self-protection [was Re: BIAS: CNN commits fraud...]"</a>
<li><strong>In reply to:</strong> <a href="8514.html">Hal Finney: "Re: Status of Superrationality (was Left/Right... can't we do better than this?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8530.html">Lee Corbin: "RE: Status of Superrationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8517">[ date ]</a>
<a href="index.html#8517">[ thread ]</a>
<a href="subject.html#8517">[ subject ]</a>
<a href="author.html#8517">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hal Finney wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; A final point; Eliezer points out that the &quot;irrational&quot; action of
</em><br>
<em>&gt; cooperating leads to a greater payoff.  I believe the consensus
</em><br>
<em>&gt; among game theorists is that this does not change the fact that it
</em><br>
<em>&gt; is irrational.  The reasoning is similar to that in Newcomb's paradox
</em><br>
<em>&gt; (take one box or two, but God has arranged that you will get more money
</em><br>
<em>&gt; if you take only one box).  Taking two boxes physically entails getting
</em><br>
<em>&gt; at least as much as in the first box, hence it is the rational action.
</em><br>
<em>&gt; In this paradox, as well, being irrational leads to a greater outcome.
</em><br>
<em>&gt; I copied out some analysis on this issue from a decision theory book at
</em><br>
<em>&gt; <a href="http://www.finney.org/~hal/Newcomb.html">http://www.finney.org/~hal/Newcomb.html</a>.  The argument doesn't go over
</em><br>
<em>&gt; directly to the PD case, but the flavor is the same: it is possible for
</em><br>
<em>&gt; an irrational action to lead to a greater outcome.
</em><br>
<p>Yes, Newcomb's Paradox is a good example of a situation which is very 
<br>
straightforward to rationally resolve for maximum benefit using the Golden 
<br>
Law(*), as discussed on the AGI list.  Game theory has not caught up with 
<br>
this yet, but historically it has often taken game theorists much too long 
<br>
to realize that the &quot;irrational&quot; action of cooperating under situation 
<br>
XYZ, which does in fact deliver a higher payoff, is really rational after 
<br>
all.  In this case solving the problem requires a timeless formulation of 
<br>
decision theory, of which ordinary decision theory is a special case.  Be 
<br>
it noted for the record that Eliezer Yudkowsky disagrees with the 
<br>
consensus of game theorists about what, mathematically speaking, 
<br>
constitutes &quot;rationality&quot;, not just in the case of the Prisoner's Dilemna, 
<br>
but also for Newcomb's Paradox and a wide variety of other situations in 
<br>
which similar or identical decision processes are distantly instantiated. 
<br>
&nbsp;&nbsp;Be it also noted that the actions Eliezer Yudkowsky computes as formally 
<br>
rational are the ones that any sane non-game-theorist would take and that 
<br>
do in fact correlate with maximum payoffs.
<br>
<p>(*) Not to be confused with the prescriptive Golden Rule in human 
<br>
morality, the Golden Law states descriptively that identical or 
<br>
sufficiently similar decision processes, distantly instantiated, return 
<br>
identical or correlated outputs.  The prescriptive formulation of the 
<br>
Golden Law states that you should make any decision between alternatives 
<br>
as if your decision controlled the Platonic output of the mathematical 
<br>
object representing your decision system, rather than acting as if your 
<br>
decision controlled your physical instantiation alone.  Wei Dai's 
<br>
formulation is even simpler; he says that you should choose between 
<br>
alternatives A and B by evaluating your utility function on the state of 
<br>
the multiverse given that your choice is A, versus the state of the 
<br>
multiverse given that your choice is B.
<br>
<p>But again, see the discussion on the AGI list.  (Google on AGI + &quot;Golden 
<br>
Law&quot;.)
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8518.html">Mike Lorrey: "Re: [GENOME] Ecce Homo?"</a>
<li><strong>Previous message:</strong> <a href="8516.html">Adrian Tymes: "Re: Long term self-protection [was Re: BIAS: CNN commits fraud...]"</a>
<li><strong>In reply to:</strong> <a href="8514.html">Hal Finney: "Re: Status of Superrationality (was Left/Right... can't we do better than this?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8530.html">Lee Corbin: "RE: Status of Superrationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8517">[ date ]</a>
<a href="index.html#8517">[ thread ]</a>
<a href="subject.html#8517">[ subject ]</a>
<a href="author.html#8517">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue May 20 2003 - 14:03:17 MDT
</em></small></p>
</body>
</html>
