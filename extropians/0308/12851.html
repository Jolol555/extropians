<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)</title>
<meta name="Author" content="Anders Sandberg (asa@nada.kth.se)">
<meta name="Subject" content="Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)">
<meta name="Date" content="2003-08-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)</h1>
<!-- received="Tue Aug 12 02:06:47 2003" -->
<!-- isoreceived="20030812080647" -->
<!-- sent="Tue, 12 Aug 2003 10:10:33 +0200" -->
<!-- isosent="20030812081033" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)" -->
<!-- id="20030812081033.GA17715@akira.nada.kth.se" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="035401c36084$4bd362c0$11262dcb@vic.bigpond.net.au" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Anders Sandberg (<a href="mailto:asa@nada.kth.se?Subject=Re:%20Is%20this%20safe/prudent%20?%20%20(was%20Re:%20Perl%20AI%20Weblog)"><em>asa@nada.kth.se</em></a>)<br>
<strong>Date:</strong> Tue Aug 12 2003 - 02:10:33 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12852.html">Max M: "Re: META:  List Changes"</a>
<ul>
<li><strong>Previous message:</strong> <a href="12850.html">Eliezer S. Yudkowsky: "Re: Mike Lorrey Eliminated From Extropians List (was RE: META:  List Changes)"</a>
<li><strong>In reply to:</strong> <a href="12842.html">Brett Paatsch: "Is this safe/prudent ?  (was Re: Perl AI Weblog)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12871.html">Brett Paatsch: "Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)"</a>
<li><strong>Reply:</strong> <a href="12871.html">Brett Paatsch: "Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12851">[ date ]</a>
<a href="index.html#12851">[ thread ]</a>
<a href="subject.html#12851">[ subject ]</a>
<a href="author.html#12851">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
A bit of scenario analysis, based on some binary assumption 
<br>
trees:
<br>
<p><p>If the strong autodevelopment scenario of AI is true, then AI
<br>
development is a &quot;first come, first win&quot; situation that promotes
<br>
arms races. But there are two additional assumptions that affect
<br>
things: is there a high complexity threshold to the
<br>
autodevelopment or is it just about finding the right seed, and
<br>
how much would the autodeveloping AI change things - while we
<br>
tend to assume it is a phase transition, it could turn out that
<br>
the top of the development sigmoid is not that high, and the
<br>
resulting ultimate AIs still far from omnipotent. 
<br>
<p>If we make up scenarios based on these assumptions, we get eight 
<br>
possibilities:
<br>
<p>Autodevelopment   Threshold   Effect
<br>
&nbsp;&nbsp;&nbsp;can occur
<br>
<p>1  No                No          Small
<br>
2  No                No          Large
<br>
3  No                Yes         Small
<br>
4  No                Yes         Large
<br>
5  Yes               No          Small
<br>
6  Yes               No          Large
<br>
7  Yes               Yes         Small
<br>
8  Yes               Yes         Large
<br>
<p>The first four are the scenarios where rapid autodevelopment can 
<br>
not happen, because general intelligence turns out to be messy 
<br>
and incompressible. 1 is the case where AI develops 
<br>
incrementally, never taking off or becoming very smart. 2 allows 
<br>
you to push to superintelligence, but it requires a broad 
<br>
research base. 3 and 4 represent situations where it is very hard 
<br>
to get anywhere, and a huge push would be needed - which would be 
<br>
hard to motivate if people believe they are in 3. But lets 
<br>
disregard these for the moment, even if I think we should 
<br>
consider refined versions of them as real possibilities.
<br>
<p>The last four represent the take-off scenarios. 5 &amp; 6 are the 
<br>
&quot;seed is easy&quot; situations and 7 &amp; 8 the &quot;seed is hard&quot; 
<br>
situations. If the seeds have a low initial complexity, then they 
<br>
are possible to do for groups with small resources. Manhattan 
<br>
projects have an advantage, but it is not total. In 7 &amp; 8 
<br>
amateurs are unlikely to get there, and Manhattans will win the 
<br>
game. 
<br>
<p>How large the perceived effect of the AI is will determine
<br>
policy. If AI is seen as &quot;harmless&quot; there will not be a strong
<br>
push to control it from many quarters, while if it is believed to
<br>
be of world domination class stuff people will clamor for
<br>
control. (I made the mistake above of looking at objective power 
<br>
of AI; lets retroactively change the third column to &quot;perceived 
<br>
power&quot; - it is what matters for policy). 
<br>
<p>The Center for Responsible Nanotechnology has written a very
<br>
interesting series of papers on control of nanotechnology, which
<br>
they consider to be relatively easy to bootstrap (&quot;seed yes&quot;)
<br>
once an initial large investment has been achieved (&quot;threshold
<br>
yes&quot;) and then it will change the world (for good or bad). Given
<br>
these assumptions (and that it is likely to be developed *soon*)
<br>
they conclude that the best way to deal with it is a single
<br>
international Manhattan project aimed at getting nanotech first
<br>
and set up the rules for it as a benign monopoly, giving DRM
<br>
limited matter compilers essentially to everyone to forestall the
<br>
need for competing projects. (I'm writing some technology and
<br>
policy comments on the papers which will appear later; I disagree
<br>
with it a lot, but it is a good kind of disagreement :-)
<br>
<p>Compare this to AI. CRN are in scenario 8, and presumably their
<br>
reasoning would run the same for seed AI: we better get a central
<br>
major project to get it first, and competing projects should be
<br>
discouraged until success guarantees that they can be prevented. 
<br>
Of course, getting such a project of the ground assumes
<br>
decisionmakers believe AI will be powerful. It is worth noting
<br>
that if such a project is started for technology X, it is likely
<br>
to be a template for a project dealing with technology Y or even
<br>
extend its domain to that - we get the Technology Authority
<br>
trying to get a monopoly, and nobody else should be allowed to 
<br>
play.
<br>
<p>On the other hand, the nightmare for this scenario is that seeds
<br>
do not have high complexity thresholds but are only about getting
<br>
the right template into order. To get a Technology Authority
<br>
going takes time, and if a myriad amateurs, companies and states
<br>
start playing in the meantime there is a very real risk that
<br>
somebody launches something. Even if it later turns out that the
<br>
AI is not super (it just changes world economy totally, but no
<br>
gods pop up) the perception that it is dangerous is going to
<br>
produce calls on ending these bioweapons-like projects. It is
<br>
worth considering that if the belief that seed AI is possible and
<br>
has a not too high threshold and will be powerful - Eliezers
<br>
position as I understand it - becomes widespread among
<br>
policymakers, then it is likely in the current anti-terror
<br>
climate such AI research would be viewed just as unacceptable and 
<br>
in need of stopping as people working on homebrew bioweapons. 
<br>
Expect marines kicking in doors. It is actually more relaxed in 
<br>
the high threshold belief scenarios, because there the worry 
<br>
would be just other Manhattan projects, amateurs are not seen as 
<br>
risks. 
<br>
<p>On the other hand, if AI is not generally perceived as powerful
<br>
or possible, then the field is clear. No Manhattan projects, no
<br>
Homeland defense raids. That might of course be a mistake in
<br>
scenario 5 and 6.  This is where we are right now; the
<br>
policymakers and public are right now unaware or think it is
<br>
unlikely that seeds or powerful AI will be developed. 
<br>
<p>So where does this put the &quot;AI underground&quot; that believes in seed
<br>
AI? The ordinary academic AI world mostly believes in non-seed AI
<br>
with or without complexity thresholds, so they are not overly
<br>
worried.  But if you think seeds are possible then things become
<br>
more complex. If you believe that there are complexity
<br>
thresholds, then you need a Manhattan-like project (be it the
<br>
Singularity Institute, a popular open source initiative or
<br>
selling out to North Korea). Otherwise just enough brains or luck
<br>
is needed. 
<br>
<p>Should you try to convince people about your views? If you
<br>
believe in the low threshold situation, then you should only do
<br>
it if you think that it is a good idea with antiterror raids on
<br>
AI developers because AI development is too dangerous - if you
<br>
are megalomaniac, think that you could do it right or that AI
<br>
will almost certainly be good/safe, then you better hack away in
<br>
secrecy instead, hoping to be the first. In the almost certainly
<br>
good/safe situation, or if you think that AI will just shake up
<br>
the world a little, then spreading sources around to facilitate
<br>
faster development makes sense. If you believe in a high
<br>
threshold situation you should go public (or more public, since
<br>
the project is still visible) if you think it is likely that you
<br>
would end up in the big centralised project (which you assume to
<br>
be good or at least better than alternatives), or that you have a
<br>
reasonable chance in a race between Manhattans. If you distrust
<br>
the big project and worry about competition, you should be quiet. 
<br>
If the threshold is high, spreading sources won't matter much 
<br>
except possibly by allowing the broad criticism/analysis from 
<br>
others &quot;in the know&quot;. 
<br>
<p>From this analysis it seems that the &quot;AI underground&quot; that
<br>
believes in seed AIs in general would be rather quiet about it,
<br>
and especially not seek to convince the world that the Godseed Is
<br>
Nigh unless they have plenty of connections in Washington. An
<br>
interesting corrolary is that beside the usual suspects on or
<br>
around this list, there are likely many others who have thought
<br>
about these admittedly obvious issues and reached similar
<br>
conclusions. There are likely many (or at least some) people
<br>
hacking away in cellars at their seeds beside the publicly known 
<br>
developers. If I believed in seeds of low threshold, I would be 
<br>
seriously worried. 
<br>
<p><p><p><pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
<a href="mailto:asa@nada.kth.se?Subject=Re:%20Is%20this%20safe/prudent%20?%20%20(was%20Re:%20Perl%20AI%20Weblog)">asa@nada.kth.se</a>                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12852.html">Max M: "Re: META:  List Changes"</a>
<li><strong>Previous message:</strong> <a href="12850.html">Eliezer S. Yudkowsky: "Re: Mike Lorrey Eliminated From Extropians List (was RE: META:  List Changes)"</a>
<li><strong>In reply to:</strong> <a href="12842.html">Brett Paatsch: "Is this safe/prudent ?  (was Re: Perl AI Weblog)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12871.html">Brett Paatsch: "Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)"</a>
<li><strong>Reply:</strong> <a href="12871.html">Brett Paatsch: "Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12851">[ date ]</a>
<a href="index.html#12851">[ thread ]</a>
<a href="subject.html#12851">[ subject ]</a>
<a href="author.html#12851">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Aug 12 2003 - 02:17:58 MDT
</em></small></p>
</body>
</html>
