<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>Extropians: Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)</title>
<meta name="Author" content="Brett Paatsch (bpaatsch@bigpond.net.au)">
<meta name="Subject" content="Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)">
<meta name="Date" content="2003-08-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)</h1>
<!-- received="Tue Aug 12 10:55:35 2003" -->
<!-- isoreceived="20030812165535" -->
<!-- sent="Wed, 13 Aug 2003 02:58:46 +1000" -->
<!-- isosent="20030812165846" -->
<!-- name="Brett Paatsch" -->
<!-- email="bpaatsch@bigpond.net.au" -->
<!-- subject="Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)" -->
<!-- id="005501c360f2$fe9f22e0$11262dcb@vic.bigpond.net.au" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20030812081033.GA17715@akira.nada.kth.se" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brett Paatsch (<a href="mailto:bpaatsch@bigpond.net.au?Subject=Re:%20Is%20this%20safe/prudent%20?%20%20(was%20Re:%20Perl%20AI%20Weblog)"><em>bpaatsch@bigpond.net.au</em></a>)<br>
<strong>Date:</strong> Tue Aug 12 2003 - 10:58:46 MDT
</p>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12872.html">Rafal Smigrodzki: "RE: FWD [forteana] Health Care: USA, Iraq &amp; Canada"</a>
<ul>
<li><strong>Previous message:</strong> <a href="12870.html">Aubrey de Grey: "Re: META:  List Changes"</a>
<li><strong>In reply to:</strong> <a href="12851.html">Anders Sandberg: "Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12866.html">Robert J. Bradbury: "Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12871">[ date ]</a>
<a href="index.html#12871">[ thread ]</a>
<a href="subject.html#12871">[ subject ]</a>
<a href="author.html#12871">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Anders writes:
<br>
<p><em>&gt; A bit of scenario analysis, based on some binary assumption 
</em><br>
<em>&gt; trees:
</em><br>
<p>Neat analysis Anders. I started to add a couple of columns
<br>
on the left. Column 4. Does this drive the singularity significantly
<br>
and Column 5. Would the govt/military want to classify or
<br>
render patents secret in this instance. I thought that 4 and 5
<br>
might couple in an unfortunate way, that is when the singularity
<br>
is likely to get the biggest boost the govt is also most likely to
<br>
cut in, perhaps cutting it off, but I haven't had the chance to 
<br>
run through it yet to check. 
<br>
<p>I think this sort of binary tree analysis could be taken up a notch
<br>
to produce tables identifying where the best extropic reward for
<br>
effort can be achieved factoring in some reasonable assumptions.
<br>
<p>Sorry haven't the time presently to give this the attention it 
<br>
deserves. 
<br>
<p>Brett 
<br>
<p><em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If the strong autodevelopment scenario of AI is true, then AI
</em><br>
<em>&gt; development is a &quot;first come, first win&quot; situation that promotes
</em><br>
<em>&gt; arms races. But there are two additional assumptions that affect
</em><br>
<em>&gt; things: is there a high complexity threshold to the
</em><br>
<em>&gt; autodevelopment or is it just about finding the right seed, and
</em><br>
<em>&gt; how much would the autodeveloping AI change things - while we
</em><br>
<em>&gt; tend to assume it is a phase transition, it could turn out that
</em><br>
<em>&gt; the top of the development sigmoid is not that high, and the
</em><br>
<em>&gt; resulting ultimate AIs still far from omnipotent. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If we make up scenarios based on these assumptions, we get eight 
</em><br>
<em>&gt; possibilities:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Autodevelopment   Threshold   Effect
</em><br>
<em>&gt;    can occur
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1  No                No          Small
</em><br>
<em>&gt; 2  No                No          Large
</em><br>
<em>&gt; 3  No                Yes         Small
</em><br>
<em>&gt; 4  No                Yes         Large
</em><br>
<em>&gt; 5  Yes               No          Small
</em><br>
<em>&gt; 6  Yes               No          Large
</em><br>
<em>&gt; 7  Yes               Yes         Small
</em><br>
<em>&gt; 8  Yes               Yes         Large
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The first four are the scenarios where rapid autodevelopment can 
</em><br>
<em>&gt; not happen, because general intelligence turns out to be messy 
</em><br>
<em>&gt; and incompressible. 1 is the case where AI develops 
</em><br>
<em>&gt; incrementally, never taking off or becoming very smart. 2 allows 
</em><br>
<em>&gt; you to push to superintelligence, but it requires a broad 
</em><br>
<em>&gt; research base. 3 and 4 represent situations where it is very hard 
</em><br>
<em>&gt; to get anywhere, and a huge push would be needed - which would be 
</em><br>
<em>&gt; hard to motivate if people believe they are in 3. But lets 
</em><br>
<em>&gt; disregard these for the moment, even if I think we should 
</em><br>
<em>&gt; consider refined versions of them as real possibilities.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The last four represent the take-off scenarios. 5 &amp; 6 are the 
</em><br>
<em>&gt; &quot;seed is easy&quot; situations and 7 &amp; 8 the &quot;seed is hard&quot; 
</em><br>
<em>&gt; situations. If the seeds have a low initial complexity, then they 
</em><br>
<em>&gt; are possible to do for groups with small resources. Manhattan 
</em><br>
<em>&gt; projects have an advantage, but it is not total. In 7 &amp; 8 
</em><br>
<em>&gt; amateurs are unlikely to get there, and Manhattans will win the 
</em><br>
<em>&gt; game. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How large the perceived effect of the AI is will determine
</em><br>
<em>&gt; policy. If AI is seen as &quot;harmless&quot; there will not be a strong
</em><br>
<em>&gt; push to control it from many quarters, while if it is believed to
</em><br>
<em>&gt; be of world domination class stuff people will clamor for
</em><br>
<em>&gt; control. (I made the mistake above of looking at objective power 
</em><br>
<em>&gt; of AI; lets retroactively change the third column to &quot;perceived 
</em><br>
<em>&gt; power&quot; - it is what matters for policy). 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The Center for Responsible Nanotechnology has written a very
</em><br>
<em>&gt; interesting series of papers on control of nanotechnology, which
</em><br>
<em>&gt; they consider to be relatively easy to bootstrap (&quot;seed yes&quot;)
</em><br>
<em>&gt; once an initial large investment has been achieved (&quot;threshold
</em><br>
<em>&gt; yes&quot;) and then it will change the world (for good or bad). Given
</em><br>
<em>&gt; these assumptions (and that it is likely to be developed *soon*)
</em><br>
<em>&gt; they conclude that the best way to deal with it is a single
</em><br>
<em>&gt; international Manhattan project aimed at getting nanotech first
</em><br>
<em>&gt; and set up the rules for it as a benign monopoly, giving DRM
</em><br>
<em>&gt; limited matter compilers essentially to everyone to forestall the
</em><br>
<em>&gt; need for competing projects. (I'm writing some technology and
</em><br>
<em>&gt; policy comments on the papers which will appear later; I disagree
</em><br>
<em>&gt; with it a lot, but it is a good kind of disagreement :-)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Compare this to AI. CRN are in scenario 8, and presumably their
</em><br>
<em>&gt; reasoning would run the same for seed AI: we better get a central
</em><br>
<em>&gt; major project to get it first, and competing projects should be
</em><br>
<em>&gt; discouraged until success guarantees that they can be prevented. 
</em><br>
<em>&gt; Of course, getting such a project of the ground assumes
</em><br>
<em>&gt; decisionmakers believe AI will be powerful. It is worth noting
</em><br>
<em>&gt; that if such a project is started for technology X, it is likely
</em><br>
<em>&gt; to be a template for a project dealing with technology Y or even
</em><br>
<em>&gt; extend its domain to that - we get the Technology Authority
</em><br>
<em>&gt; trying to get a monopoly, and nobody else should be allowed to 
</em><br>
<em>&gt; play.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; On the other hand, the nightmare for this scenario is that seeds
</em><br>
<em>&gt; do not have high complexity thresholds but are only about getting
</em><br>
<em>&gt; the right template into order. To get a Technology Authority
</em><br>
<em>&gt; going takes time, and if a myriad amateurs, companies and states
</em><br>
<em>&gt; start playing in the meantime there is a very real risk that
</em><br>
<em>&gt; somebody launches something. Even if it later turns out that the
</em><br>
<em>&gt; AI is not super (it just changes world economy totally, but no
</em><br>
<em>&gt; gods pop up) the perception that it is dangerous is going to
</em><br>
<em>&gt; produce calls on ending these bioweapons-like projects. It is
</em><br>
<em>&gt; worth considering that if the belief that seed AI is possible and
</em><br>
<em>&gt; has a not too high threshold and will be powerful - Eliezers
</em><br>
<em>&gt; position as I understand it - becomes widespread among
</em><br>
<em>&gt; policymakers, then it is likely in the current anti-terror
</em><br>
<em>&gt; climate such AI research would be viewed just as unacceptable and 
</em><br>
<em>&gt; in need of stopping as people working on homebrew bioweapons. 
</em><br>
<em>&gt; Expect marines kicking in doors. It is actually more relaxed in 
</em><br>
<em>&gt; the high threshold belief scenarios, because there the worry 
</em><br>
<em>&gt; would be just other Manhattan projects, amateurs are not seen as 
</em><br>
<em>&gt; risks. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; On the other hand, if AI is not generally perceived as powerful
</em><br>
<em>&gt; or possible, then the field is clear. No Manhattan projects, no
</em><br>
<em>&gt; Homeland defense raids. That might of course be a mistake in
</em><br>
<em>&gt; scenario 5 and 6.  This is where we are right now; the
</em><br>
<em>&gt; policymakers and public are right now unaware or think it is
</em><br>
<em>&gt; unlikely that seeds or powerful AI will be developed. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So where does this put the &quot;AI underground&quot; that believes in seed
</em><br>
<em>&gt; AI? The ordinary academic AI world mostly believes in non-seed AI
</em><br>
<em>&gt; with or without complexity thresholds, so they are not overly
</em><br>
<em>&gt; worried.  But if you think seeds are possible then things become
</em><br>
<em>&gt; more complex. If you believe that there are complexity
</em><br>
<em>&gt; thresholds, then you need a Manhattan-like project (be it the
</em><br>
<em>&gt; Singularity Institute, a popular open source initiative or
</em><br>
<em>&gt; selling out to North Korea). Otherwise just enough brains or luck
</em><br>
<em>&gt; is needed. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Should you try to convince people about your views? If you
</em><br>
<em>&gt; believe in the low threshold situation, then you should only do
</em><br>
<em>&gt; it if you think that it is a good idea with antiterror raids on
</em><br>
<em>&gt; AI developers because AI development is too dangerous - if you
</em><br>
<em>&gt; are megalomaniac, think that you could do it right or that AI
</em><br>
<em>&gt; will almost certainly be good/safe, then you better hack away in
</em><br>
<em>&gt; secrecy instead, hoping to be the first. In the almost certainly
</em><br>
<em>&gt; good/safe situation, or if you think that AI will just shake up
</em><br>
<em>&gt; the world a little, then spreading sources around to facilitate
</em><br>
<em>&gt; faster development makes sense. If you believe in a high
</em><br>
<em>&gt; threshold situation you should go public (or more public, since
</em><br>
<em>&gt; the project is still visible) if you think it is likely that you
</em><br>
<em>&gt; would end up in the big centralised project (which you assume to
</em><br>
<em>&gt; be good or at least better than alternatives), or that you have a
</em><br>
<em>&gt; reasonable chance in a race between Manhattans. If you distrust
</em><br>
<em>&gt; the big project and worry about competition, you should be quiet. 
</em><br>
<em>&gt; If the threshold is high, spreading sources won't matter much 
</em><br>
<em>&gt; except possibly by allowing the broad criticism/analysis from 
</em><br>
<em>&gt; others &quot;in the know&quot;. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; From this analysis it seems that the &quot;AI underground&quot; that
</em><br>
<em>&gt; believes in seed AIs in general would be rather quiet about it,
</em><br>
<em>&gt; and especially not seek to convince the world that the Godseed Is
</em><br>
<em>&gt; Nigh unless they have plenty of connections in Washington. An
</em><br>
<em>&gt; interesting corrolary is that beside the usual suspects on or
</em><br>
<em>&gt; around this list, there are likely many others who have thought
</em><br>
<em>&gt; about these admittedly obvious issues and reached similar
</em><br>
<em>&gt; conclusions. There are likely many (or at least some) people
</em><br>
<em>&gt; hacking away in cellars at their seeds beside the publicly known 
</em><br>
<em>&gt; developers. If I believed in seeds of low threshold, I would be 
</em><br>
<em>&gt; seriously worried. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; -- 
</em><br>
<em>&gt; -----------------------------------------------------------------------
</em><br>
<em>&gt; Anders Sandberg                                      Towards Ascension!
</em><br>
<em>&gt; <a href="mailto:asa@nada.kth.se?Subject=Re:%20Is%20this%20safe/prudent%20?%20%20(was%20Re:%20Perl%20AI%20Weblog)">asa@nada.kth.se</a>                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
</em><br>
<em>&gt; GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</em><br>
<em>&gt; 
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12872.html">Rafal Smigrodzki: "RE: FWD [forteana] Health Care: USA, Iraq &amp; Canada"</a>
<li><strong>Previous message:</strong> <a href="12870.html">Aubrey de Grey: "Re: META:  List Changes"</a>
<li><strong>In reply to:</strong> <a href="12851.html">Anders Sandberg: "Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12866.html">Robert J. Bradbury: "Re: Is this safe/prudent ?  (was Re: Perl AI Weblog)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12871">[ date ]</a>
<a href="index.html#12871">[ thread ]</a>
<a href="subject.html#12871">[ subject ]</a>
<a href="author.html#12871">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Aug 12 2003 - 11:04:41 MDT
</em></small></p>
</body>
</html>
