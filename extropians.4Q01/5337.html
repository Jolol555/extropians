<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Who Owns The Singularity? (was: how shall the singu</title>
<meta name="Author" content="J. R. Molloy (jr@shasta.com)">
<meta name="Subject" content="Who Owns The Singularity? (was: how shall the singularity take off?)">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Who Owns The Singularity? (was: how shall the singularity take off?)</h1>
<!-- received="Thu Dec 27 23:16:25 2001" -->
<!-- isoreceived="20011228061625" -->
<!-- sent="Thu, 27 Dec 2001 22:15:28 -0800" -->
<!-- isosent="20011228061528" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Who Owns The Singularity? (was: how shall the singularity take off?)" -->
<!-- id="02b801c18f67$273b84a0$3f5c2a42@jrmolloy" -->
<!-- inreplyto="3C2BDBB0.2D3A72E7@attglobal.net" -->
<strong>From:</strong> J. R. Molloy (<a href="mailto:jr@shasta.com?Subject=Re:%20Who%20Owns%20The%20Singularity?%20(was:%20how%20shall%20the%20singularity%20take%20off?)&In-Reply-To=&lt;02b801c18f67$273b84a0$3f5c2a42@jrmolloy&gt;"><em>jr@shasta.com</em></a>)<br>
<strong>Date:</strong> Thu Dec 27 2001 - 23:15:28 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5338.html">J. R. Molloy: "Re: some U.S. observations and notes"</a>
<li><strong>Previous message:</strong> <a href="5336.html">John Clark: "Re: Zero-tolerance rules"</a>
<li><strong>In reply to:</strong> <a href="5320.html">Spike Jones: "how shall the singularity take off?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5344.html">Adrian Tymes: "Re: Who Owns The Singularity? (was: how shall the singularity take off?)"</a>
<li><strong>Reply:</strong> <a href="5344.html">Adrian Tymes: "Re: Who Owns The Singularity? (was: how shall the singularity take off?)"</a>
<li><strong>Reply:</strong> <a href="5389.html">J. R. Molloy: "ROBOT: Automatic design and manufacture of robotic lifeforms"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5337">[ date ]</a>
<a href="index.html#5337">[ thread ]</a>
<a href="subject.html#5337">[ subject ]</a>
<a href="author.html#5337">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
From: &quot;Spike Jones&quot; &lt;<a href="mailto:spike66@attglobal.net?Subject=Re:%20Who%20Owns%20The%20Singularity?%20(was:%20how%20shall%20the%20singularity%20take%20off?)&In-Reply-To=&lt;02b801c18f67$273b84a0$3f5c2a42@jrmolloy&gt;">spike66@attglobal.net</a>&gt;
<br>
<em>&gt; While I see that the hard takeoff Singularity is perhaps the most
</em><br>
<em>&gt; well-known model, let us look at possible alternatives and even
</em><br>
<em>&gt; consider deriving closed form equations, if at all possible,
</em><br>
<em>&gt; to describe singularity models.
</em><br>
<p>As Kim Cosmos has eloquently observed, &quot;When you can buy intelligence,
<br>
alliances are more important than ability.&quot; As you know, there are some very
<br>
rich nerds in your neighborhood, Spike, and they will have the first chance to
<br>
decide how the AI phase transition shall proceed. If they decide to go with
<br>
Bill Joy's relinquishment scenario, then expect more movement toward a police
<br>
state and martial law (or perhaps universal robotic law).
<br>
<p><em>&gt; If we did that, I can imagine a term in the growth equation
</em><br>
<em>&gt; which we have no knowledge of, since those terms have
</em><br>
<em>&gt; always been zero in the regime where the growth rates of
</em><br>
<em>&gt; knowledge are on the order we have seen historically and
</em><br>
<em>&gt; today.
</em><br>
<p>Have you noticed the growth rate of litigation in the US?
<br>
It may out distance technological innovation, smothering it in the process.
<br>
Then again, the law may simply appropriate AI and make the phase transition a
<br>
matter of jurisdiction. How about lawyer-bots running everything?
<br>
No, wait! That may have happened already.
<br>
<p><em>&gt; For instance, hard takeoff fans: perhaps there an unknown factor
</em><br>
<em>&gt; where friendly AI begins to develop its own next generation,
</em><br>
<em>&gt; but a sub-AI, not necessarily &quot;unfriendly&quot; but just misguided,
</em><br>
<em>&gt; attempts to halt or reverse the growth of AI.  Could there be
</em><br>
<em>&gt; a machine level equivalent of luddites and reactionary religious
</em><br>
<em>&gt; notions?  Is such a thing even imaginable?  I can imagine it.
</em><br>
<p>I think the idea of &quot;friendly&quot; AI is perhaps the greatest impediment to
<br>
actualizing AI phase transition. Machine intelligence outperforms human
<br>
intelligence simply by transcending irrelevant notions and focussing on
<br>
objective facts. As AI systems become more and more complex and adaptive, I
<br>
think they surpass our ability to imagine the trememdous solutions that they
<br>
can create, and the marvelous answers they can provide. To imagine that
<br>
machines could evolve into luddite-religious-reactionaries, it is first
<br>
necessary to imagine that machines could be &quot;friendly&quot; or malevolent. If we
<br>
can avoid making machines &quot;friendly&quot; then we can successfully avoid making
<br>
them luddite-religious-reactionaries.
<br>
<p><em>&gt; We now have clear examples of human level intelligences (humans)
</em><br>
<em>&gt; who openly wish to turn back the advance of knowledge, destroy
</em><br>
<em>&gt; that which a particularly successful subgroup of humans has created.
</em><br>
<em>&gt; How can we be certain that human equivalent (or greater-
</em><br>
<em>&gt; than-human equivalent) AI will not somehow get the same
</em><br>
<em>&gt; idea?
</em><br>
<p>It is the desire to be certain that motivates much of the wish to turn back
<br>
the advance of knowledge, because the more we know, the more we become
<br>
uncertain. So, we can't be certain what human-competitive AI will create.
<br>
Similarly, we can't be certain that the next generation of children will not
<br>
get infected with the turn-back-the-advance-of-knowledge meme. So, the bottom
<br>
line is that we know humans can become luddite-religious-reactionaries, and we
<br>
do *not* know that machines can become so meme-infected. Therefore, machines
<br>
seem like a better bet.
<br>
<p><em>&gt; Furthermore, how can we be certain that this anti-
</em><br>
<em>&gt; singularity AI would not have much more potential to slow
</em><br>
<em>&gt; or reverse the singularity than the current human-level intelligences
</em><br>
<em>&gt; have?
</em><br>
<p>Or, how do we know that anti-singularity humans would not use human-level AI
<br>
to slow or reverse the singularity? We already have evidence, as you've
<br>
pointed out, that humans (such as Bill Joy) would like to relinquish the
<br>
science that can lead to singluarity. But we don't know if
<br>
anti-singularitarians could create anti-singularity AIs. Once again, the safe
<br>
bet is with the machines, since we don't know if they could become
<br>
anti-singularity, but we do know that humans can become so.
<br>
<p><em>&gt; Would not the net effect would be a soft takeoff singularity,
</em><br>
<em>&gt; or even a series of failed Spikelets before The Spike?
</em><br>
<p>The net effect would be hard takeoff or no takeoff.
<br>
<p>(and don't forget to upload your enteric nervous system)
<br>
<p>---   ---   ---   ---   ---
<br>
<p>Useless hypotheses, etc.:
<br>
&nbsp;consciousness, phlogiston, philosophy, vitalism, mind, free will, qualia,
<br>
analog computing, cultural relativism, GAC, Cyc, Eliza, cryonics, individual
<br>
uniqueness, ego, human values, scientific relinquishment, malevolent AI,
<br>
non-sensory experience, SETI
<br>
<p>We  move into a better future in proportion as science displaces superstition.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5338.html">J. R. Molloy: "Re: some U.S. observations and notes"</a>
<li><strong>Previous message:</strong> <a href="5336.html">John Clark: "Re: Zero-tolerance rules"</a>
<li><strong>In reply to:</strong> <a href="5320.html">Spike Jones: "how shall the singularity take off?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5344.html">Adrian Tymes: "Re: Who Owns The Singularity? (was: how shall the singularity take off?)"</a>
<li><strong>Reply:</strong> <a href="5344.html">Adrian Tymes: "Re: Who Owns The Singularity? (was: how shall the singularity take off?)"</a>
<li><strong>Reply:</strong> <a href="5389.html">J. R. Molloy: "ROBOT: Automatic design and manufacture of robotic lifeforms"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5337">[ date ]</a>
<a href="index.html#5337">[ thread ]</a>
<a href="subject.html#5337">[ subject ]</a>
<a href="author.html#5337">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Sat May 11 2002 - 17:44:32 MDT</em>
</em>
</small>
</body>
</html>
