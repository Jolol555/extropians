<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: correction Fermi 2</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: correction Fermi 2">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: correction Fermi 2</h1>
<!-- received="Fri Nov 16 07:50:36 2001" -->
<!-- isoreceived="20011116145036" -->
<!-- sent="Fri, 16 Nov 2001 09:50:33 -0500" -->
<!-- isosent="20011116145033" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: correction Fermi 2" -->
<!-- id="3BF527B9.C1245A44@pobox.com" -->
<!-- inreplyto="Pine.LNX.4.10.10111160230380.13593-100000@server.aeiveos.com" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20correction%20Fermi%202&In-Reply-To=&lt;3BF527B9.C1245A44@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri Nov 16 2001 - 07:50:33 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2735.html">Spudboy100@aol.com: "Re: correction Fermi 2"</a>
<li><strong>Previous message:</strong> <a href="2733.html">jr@shasta.com: "BOOK: Brave New Brain"</a>
<li><strong>In reply to:</strong> <a href="2730.html">Robert J. Bradbury: "Re: correction Fermi 2"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2736.html">Spike Jones: "Re: correction Fermi 2"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2734">[ date ]</a>
<a href="index.html#2734">[ thread ]</a>
<a href="subject.html#2734">[ subject ]</a>
<a href="author.html#2734">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
&quot;Robert J. Bradbury&quot; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; It is far more likely that colonization is frowned upon
</em><br>
<em>&gt; because colonies are likely to be competitors than
</em><br>
<em>&gt; colonization is prohibitively expensive.
</em><br>
<p>That is a very interesting point.  I have to say that pretty much
<br>
everything I currently know says it's probably an anthropomorphism derived
<br>
from battling human tribes - but, if you could really demonstrate that any
<br>
sufficiently large internal time-lag will cause *any* mind-in-general to
<br>
fragment to the point of its parts taking hostile action against each
<br>
other, or otherwise taking actions that the current central part regards
<br>
as being a net negative, then that would present a strong reason for any
<br>
superintelligence-in-general to collapse into a computational space of
<br>
maximal density rather than attempting to colonize the universe.
<br>
<p>Even so, I don't see why minds would fragment this way, or why temporal
<br>
distance would increase the degree of fragmentation; and even if minor
<br>
divergences in goals were to somehow build up, I would still expect the
<br>
net total outcome to be a net positive, if not an absolute positive, from
<br>
the perspective of a central mind considering fragmentation, or of one
<br>
part regarding another part... then again, maybe the incremental utility
<br>
of fragmentation is less than the incremental disutility, especially if
<br>
the sole point of colonization is to rescue a few pre-Singularity
<br>
civilizations here and there.
<br>
<p>Doesn't sound right.  But it's new to me and it's worth thinking through
<br>
in more detail.  I guess the hypothesis would formally be as follows:
<br>
<p>Hypothesis:  The known negative utility of creating a temporally distant
<br>
submind is large enough to exceed the probabilistic possible utility of
<br>
sending out a colony, and colonization beyond some tight physical boundary
<br>
presents such a limit.  This rule holds for all, or virtually all,
<br>
superintelligences in general.
<br>
<p>Supporting subhypotheses would be these:
<br>
<p>1)  Matter and energy are not conserved resources.  From the perspective
<br>
of a Friendly mind, the sole utility of exploring the Universe is rescuing
<br>
pre-Singularity civilizations.  From the perspective of a nonFriendly
<br>
mind, there is no point in exploring the Universe at all.
<br>
<p>2)  Lightspeed limitations hold.  You can't even send an FTL wormhole
<br>
terminus with a colony ship that travels at sublightspeed.  Either there
<br>
are no General Relativity workarounds or such workarounds do not operate
<br>
over transgalactic distances.
<br>
<p>3)  Given sufficient temporal separation from a child, a mind-in-general
<br>
will expect a degree of fragmentation sufficient to establish that child
<br>
as being slightly suboptimal in some way.  The child mind may still be a
<br>
cooperator rather than a defector, but it will be a competitor, and
<br>
&quot;nonexistent&quot; is preferred to &quot;slightly suboptimal&quot;.  For example, the
<br>
child mind might demand a share of some resource which is unique to a
<br>
given universe.
<br>
<p>4)  The probability of other civilizations undergoing their own
<br>
Singularities and demanding their own shares of a unique resource, or of
<br>
such civilizations otherwise having still more divergent goals, is
<br>
sufficiently small that the known creation of a colonizing entity is a
<br>
greater negative utility than the probabilistic positive utility of
<br>
finding a potential competing civilization before its Singularity.  This
<br>
may not make much sense for the universe as a whole, but it may make sense
<br>
if the problem needs to be considered one star system at a time.
<br>
<p>5)  There is no way to send out non-independently-thinking probes capable
<br>
of incorporating possible competing civilizations.  This is certainly
<br>
reasonable from our perspective, but it must also hold true of all
<br>
possible minds that can be built by a superintelligence.
<br>
<p>6)  Competing fragmented entities will tend to fragment further, creating
<br>
a potential exponential problem if even a single fragmentation occurs. 
<br>
Again, this must hold of minds-in-general.
<br>
<p>Sounds like a pretty hostile picture of the universe...
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2735.html">Spudboy100@aol.com: "Re: correction Fermi 2"</a>
<li><strong>Previous message:</strong> <a href="2733.html">jr@shasta.com: "BOOK: Brave New Brain"</a>
<li><strong>In reply to:</strong> <a href="2730.html">Robert J. Bradbury: "Re: correction Fermi 2"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2736.html">Spike Jones: "Re: correction Fermi 2"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2734">[ date ]</a>
<a href="index.html#2734">[ thread ]</a>
<a href="subject.html#2734">[ subject ]</a>
<a href="author.html#2734">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Sat May 11 2002 - 17:44:19 MDT</em>
</em>
</small>
</body>
</html>
