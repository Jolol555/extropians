<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Anticipatory backfire</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Anticipatory backfire">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Anticipatory backfire</h1>
<!-- received="Thu Nov  8 11:48:41 2001" -->
<!-- isoreceived="20011108184841" -->
<!-- sent="Thu, 08 Nov 2001 13:48:39 -0500" -->
<!-- isosent="20011108184839" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Anticipatory backfire" -->
<!-- id="3BEAD387.6E9A5601@pobox.com" -->
<!-- inreplyto="F1553NFVPcAja5dG1O000025397@hotmail.com" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Anticipatory%20backfire&In-Reply-To=&lt;3BEAD387.6E9A5601@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Nov 08 2001 - 11:48:39 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2326.html">Wei Dai: "Re: MORALITY: right &amp; wrong (was: A Bioethical Foundation for Human Rights)"</a>
<li><strong>Previous message:</strong> <a href="2324.html">natashavita@earthlink.net: "ARTS:  Extropic Ken Greenberg Exhibition"</a>
<li><strong>In reply to:</strong> <a href="2288.html">Mitchell Porter: "Anticipatory backfire"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2471.html">Robert J. Bradbury: "Re: Anticipatory backfire"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2325">[ date ]</a>
<a href="index.html#2325">[ thread ]</a>
<a href="subject.html#2325">[ subject ]</a>
<a href="author.html#2325">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Mitchell Porter wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; The AI example: this time one wants to be able to defend against
</em><br>
<em>&gt; the full range of possible hostile minds. In this case, making a
</em><br>
<em>&gt; simulation is making the thing itself, so if you must do so
</em><br>
<em>&gt; (rather than relying on theory to tell you, a priori, about a
</em><br>
<em>&gt; particular possible mind), it's important that it's trapped high
</em><br>
<em>&gt; in a tower of nested virtual worlds, rather than running at
</em><br>
<em>&gt; the physical 'ground level'. But as above, once the code for such
</em><br>
<em>&gt; an entity exists, it can in principle be implemented at ground
</em><br>
<em>&gt; level, which would give it freedom to act in the real world.
</em><br>
<p>I don't think this is the right protocol.  First, do we really need to
<br>
investigate the full range of possible hostile minds?  I agree that if we
<br>
can get the knowledge without taking any risks, it would be nice to know. 
<br>
I don't see a good way to do that.  And it's not really knowledge we need
<br>
if we can build a Friendly seed AI, do the Singularity, et cetera - if
<br>
that knowledge is still needed, it's knowledge that can be obtained
<br>
post-Singularity by a superintelligence.  We don't necessarily need to
<br>
know about it pre-Singularity.  There's also the question of whether we
<br>
could realistically investigate it at all, pre-Singularity - I think that
<br>
pre-Singularity you just see infrahuman AIs, and investigating a hostile
<br>
infrahuman AI doesn't necessarily give you knowledge that's useful for
<br>
defending against ~human or &gt;human AI.
<br>
<p>For investigating failure of Friendliness - this, you probably *do* want
<br>
to watch in the laboratory - the ideal tool, if at all possible, is a
<br>
Friendly AI pretending to be unFriendly, and taking only those mental
<br>
actions that are consonant with the goal of pretending to be unFriendly
<br>
but not in conflict with actual Friendliness.  You want to investigate
<br>
unFriendliness in complete realism without ever actually creating an
<br>
unFriendly entity; you also want to prevent the &quot;shadow&quot; unFriendly entity
<br>
from initiating real-world unFriendly actions, or unFriendly actions
<br>
internal to the AI mind doing the shadowing.  In particular, you want to
<br>
investigate unFriendliness or failure of Friendliness without letting the
<br>
shadowed mind ever once form the thought &quot;How do I break out of this
<br>
enclosing Friendly mind?&quot;, and the way you do this is by making sure that
<br>
a Friendly mind is carrying out all the actual thoughts of the shadowed
<br>
mind, carrying out actions which are consistent with pretending to be
<br>
unFriendly for purposes of investigating Friendliness, but not those
<br>
actions which are only consistent with actual unFriendliness.  The
<br>
shadowed mind might think &quot;I need a new goal system&quot;, but the real mind
<br>
would think &quot;I need to pretend to build a new goal system&quot;.  And so on.
<br>
<p>I think this would take pretty sophisticated seed AI, possibly too
<br>
sophisticated to be seen very long before the Singularity.  If so, then
<br>
the prehuman AIs being tested probably aren't sophisticated enough to do
<br>
anything horrible to nearby humans.  Even so, you'd want to take a few
<br>
obvious precautions, such as running all the tests in a sealed, separate
<br>
lab, totally erasing the hardware after each run, never reusing the
<br>
hardware for anything except future UFAI tests (crush and melt rather than
<br>
resell), no cellphones in the building, et cetera... but that's just being
<br>
sensible.  A prehuman seed AI is lucky if it can win a game of chess
<br>
against you - it's not going to wipe out humanity, even if you're setting
<br>
up an observable failure of Friendliness under laboratory conditions. 
<br>
Which should not be as trivial as it sounds, if other FAI issues have been
<br>
handled properly - ideally, it should require an enormous amount of
<br>
effort, and the cooperation of the AI, to set up the conditions under
<br>
which anything can break even temporarily.
<br>
<p>For an infrahuman seed AI, the only reason you'd have to worry would be if
<br>
somehow the laboratory-simulated failure of Friendliness contributed to
<br>
some kind of unexpected cognitive breakthrough.  And running the
<br>
disposable experimental version of the AI on a little less hardware, with
<br>
some of the knowledge of self-improvement removed, should easily be enough
<br>
to prevent what is actually a pretty implausible scenario in the first
<br>
place.  Disabling Friendliness shouldn't lead to improved intelligence
<br>
unless something is very drastically and seriously wrong with Friendly AI
<br>
theory.
<br>
<p>I agree with Ken Clements that running an actual hostile transhuman,
<br>
whether in any number of towers of simulation, is too dangerous to ever be
<br>
considered.  A hostile transhuman AI isn't an enemy computer program, it's
<br>
an enemy mind, and fighting an enemy mind isn't the same as fighting a
<br>
computer program.  You can't carry out the actions that would be used to
<br>
control a hostile computer program and expect them to work on a hostile
<br>
transhuman AI; that would be a Hofstadterian level confusion, like baking
<br>
a Recipe Cake.  And besides, even an *absolutely* secure Java
<br>
implementation doesn't help if the humans and the rest of the universe are
<br>
full of security holes.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2326.html">Wei Dai: "Re: MORALITY: right &amp; wrong (was: A Bioethical Foundation for Human Rights)"</a>
<li><strong>Previous message:</strong> <a href="2324.html">natashavita@earthlink.net: "ARTS:  Extropic Ken Greenberg Exhibition"</a>
<li><strong>In reply to:</strong> <a href="2288.html">Mitchell Porter: "Anticipatory backfire"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2471.html">Robert J. Bradbury: "Re: Anticipatory backfire"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2325">[ date ]</a>
<a href="index.html#2325">[ thread ]</a>
<a href="subject.html#2325">[ subject ]</a>
<a href="author.html#2325">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Sat May 11 2002 - 17:44:18 MDT</em>
</em>
</small>
</body>
</html>
