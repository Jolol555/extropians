<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: One Unity, Different Ideologies, all in the sam</title>
<meta name="Author" content="Anders Sandberg (asa@nada.kth.se)">
<meta name="Subject" content="Re: One Unity, Different Ideologies, all in the same universe">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: One Unity, Different Ideologies, all in the same universe</h1>
<!-- received="Wed Dec 26 16:17:34 2001" -->
<!-- isoreceived="20011226231734" -->
<!-- sent="Thu, 27 Dec 2001 00:17:33 +0100" -->
<!-- isosent="20011226231733" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: One Unity, Different Ideologies, all in the same universe" -->
<!-- id="20011227001733.B12862@akira.nada.kth.se" -->
<!-- inreplyto="3C2A3306.8B0D2D72@pobox.com" -->
<strong>From:</strong> Anders Sandberg (<a href="mailto:asa@nada.kth.se?Subject=Re:%20One%20Unity,%20Different%20Ideologies,%20all%20in%20the%20same%20universe&In-Reply-To=&lt;20011227001733.B12862@akira.nada.kth.se&gt;"><em>asa@nada.kth.se</em></a>)<br>
<strong>Date:</strong> Wed Dec 26 2001 - 16:17:33 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5247.html">Dehede011@aol.com: "Re: Dumping"</a>
<li><strong>Previous message:</strong> <a href="5245.html">John Clark: "Re: Dumping"</a>
<li><strong>In reply to:</strong> <a href="5238.html">Eliezer S. Yudkowsky: "Re: One Unity, Different Ideologies, all in the same universe"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5250.html">Eliezer S. Yudkowsky: "Re: One Unity, Different Ideologies, all in the same universe"</a>
<li><strong>Reply:</strong> <a href="5250.html">Eliezer S. Yudkowsky: "Re: One Unity, Different Ideologies, all in the same universe"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5246">[ date ]</a>
<a href="index.html#5246">[ thread ]</a>
<a href="subject.html#5246">[ subject ]</a>
<a href="author.html#5246">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
On Wed, Dec 26, 2001 at 03:28:54PM -0500, Eliezer S. Yudkowsky wrote:
<br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; You are apparently thinking more in terms of AI slavery than political
</em><br>
<em>&gt; &gt; prisoners. Whether the consitution would be about sentient rights or human
</em><br>
<em>&gt; &gt; rights is of course important in the long run, but setting up a system
</em><br>
<em>&gt; &gt; somewhat like the above federation is something we can do in the near
</em><br>
<em>&gt; &gt; future. This system can then adapt to new developments, and if the
</em><br>
<em>&gt; &gt; constitution update process is not unnecessarily rigid it wouldn't be too
</em><br>
<em>&gt; &gt; hard to include general sentient rights as people become more aware of
</em><br>
<em>&gt; &gt; their possibility.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; AI slavery is less expensive than biological slavery, but if you insist
</em><br>
<em>&gt; that there is any difference whatever between the two, it's easy enough to
</em><br>
<em>&gt; imagine Osama bin Laden biologically cloning seventy-two helpless
</em><br>
<em>&gt; virgins.  From my perspective, these are people too and they have just as
</em><br>
<em>&gt; much claim on my sympathy as you or anyone else.  If it's worth saving the
</em><br>
<em>&gt; world, it's worth saving them too.
</em><br>
<p>Sure. Do you think Osamaland would be accepted by any reasonable federative
<br>
constitution? (OK, the UN does allow pretty much anything as a member
<br>
state) Even if it is technically legal by using some loopholes, it is darn
<br>
likely most other members would take steps to close those loopholes.  My
<br>
point is that we are talking about getting real people set up real
<br>
political systems in the real world, and while you and I think that AI will
<br>
be relevant sometime soon, we better convince people about that before
<br>
making suggestions and political visions contingent upon our specific
<br>
assumptions about AI. Even if this federation when it is set up does not
<br>
have the least legal protection of AIs or clones, if the basic constitution
<br>
and the shared assumptions of the forming communities are sane enough, the
<br>
system can include this as it is beginning to seem relevant. Trying to set
<br>
up a perfect system from the start is bound to get lost in minutiae,
<br>
spending inordinate amounts of energy on possibilities that do not play out
<br>
and would likely end up in the top-down type approach.
<br>
<p><p><em>&gt; &gt; The important thing to remember about systems like this is that we do not
</em><br>
<em>&gt; &gt; have to get everything perfectly right at the first try. Good political
</em><br>
<em>&gt; &gt; solutions are flexible and can be adaptive.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1)  This sounds to me like a set of heuristics unadapted to dealing with
</em><br>
<em>&gt; existential risks (and not just the Bangs, either).  Some errors are
</em><br>
<em>&gt; nonrecoverable.  If Robin Hanson's cosmic-commons colonization race turns
</em><br>
<em>&gt; out to be a Whimper, then we had better not get started down that road,
</em><br>
<em>&gt; because once begun it won't stop.
</em><br>
<p>Which existential risks are relevant? When planning to act in any way you
<br>
have to make estimates of risks. If the risk is too great, you become
<br>
careful and may even avoid certain actions if the risk is unacceptable. In
<br>
many cases both the probability and the severity of the risk are unknown,
<br>
and have to be gradually refined given what we learn. Basing your actions
<br>
on your prior estimate and then never revising it would be irrational. So
<br>
what is needed is systems that allow us to learn and act according to what
<br>
we have learned. 
<br>
<p>The first part of this, learning, clearly benefits from pluralist
<br>
approaches since many different lines of inquiry can be pursued and they
<br>
can compete on the market of ideas. This is far more likely to give better
<br>
estimates of risk than non-pluralist approaches. As for behavior control,
<br>
most communities will behave according to their self-interest and hence not
<br>
take more risks than they estimate is worth it. Now, since these
<br>
communities are linked by not just a constitution but also by the strands
<br>
of an open meta-society there will also be a cross-community interaction
<br>
(through trade etc) that is likely to bias individual communities
<br>
estimateions towards a risk concensus. It is to a large extent a
<br>
distributed agoric system.
<br>
<p>I know some readers will respond with &quot;But this does not guarantee that the
<br>
federation doesn't take risks with Things Man Was Not Meant To Know!&quot;. That
<br>
is true. But would it on average take *more* risks than a reasonably
<br>
rational individual? It doesn't seem likely. The one exception would be if
<br>
some member communities were little linked to the rest and hence did not
<br>
share any risk consensus; in this way the likeliehood of someone taking a
<br>
risk greater than the average person would consider reasonable would be
<br>
higher - it is the usual problem of having many free agents in a system.
<br>
The federation doesn't solve it, but it does ameliorate it by providing one
<br>
linking mechanism. 
<br>
<p>A quick aside:
<br>
<p>I think there is a certain risk involved with the concept of existential
<br>
risks, actually. Given the common misinterpretation of the precautionary
<br>
principle as &quot;do not do anything that has not been proven safe&quot;, even the
<br>
idea of existential risks provides an excellent and rhetorically powerful
<br>
argument for stasis. Leon Kass is essentially using this in his
<br>
anti-posthuman campaign: since there may be existential risks involved with
<br>
posthumanity *it must be prevented*. And since posthumanity depends on
<br>
learning more about certain areas, inquiry into these has to be curtailed -
<br>
which incidentally makes improved risk estimation harder or impossible.
<br>
Even here on this list we sometimes hear totalistic arguments for global
<br>
police forces or worse atrocities supported by the idea that they would be
<br>
necessary to counter an existential risk.
<br>
<p>This is not an argument against discussing existential risks, but rather
<br>
against invoking them without thinking of their epistemological context. 
<br>
<p><p><em>&gt; 2)  The cost in sentient suffering in a single non-federation community,
</em><br>
<em>&gt; under the framework you present, could enormously exceed the sum of all
</em><br>
<em>&gt; sentient suffering in history up until this point.  This is not a trivial
</em><br>
<em>&gt; error.
</em><br>
<p>No, but that is not something the federation was supposed to solve either.
<br>
The fact that there is awful suffering and tyrrany in some countries
<br>
doesn't invalidate the political system of the US. The federation is not
<br>
based on an utilitarist ethical perspective where the goal is to maximize
<br>
global happiness. 
<br>
<p>I distrust the search for global optima and solutions that solve every
<br>
problem. The world is complex, changing and filled with adaptation, making
<br>
any such absolutist solution futile, or worse, limiting. I prefer to view
<br>
every proposed solution as partial and under revision as we learn more. 
<br>
<p><p><pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
<a href="mailto:asa@nada.kth.se?Subject=Re:%20One%20Unity,%20Different%20Ideologies,%20all%20in%20the%20same%20universe&In-Reply-To=&lt;20011227001733.B12862@akira.nada.kth.se&gt;">asa@nada.kth.se</a>                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5247.html">Dehede011@aol.com: "Re: Dumping"</a>
<li><strong>Previous message:</strong> <a href="5245.html">John Clark: "Re: Dumping"</a>
<li><strong>In reply to:</strong> <a href="5238.html">Eliezer S. Yudkowsky: "Re: One Unity, Different Ideologies, all in the same universe"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5250.html">Eliezer S. Yudkowsky: "Re: One Unity, Different Ideologies, all in the same universe"</a>
<li><strong>Reply:</strong> <a href="5250.html">Eliezer S. Yudkowsky: "Re: One Unity, Different Ideologies, all in the same universe"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5246">[ date ]</a>
<a href="index.html#5246">[ thread ]</a>
<a href="subject.html#5246">[ subject ]</a>
<a href="author.html#5246">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Sat May 11 2002 - 17:44:31 MDT</em>
</em>
</small>
</body>
</html>
