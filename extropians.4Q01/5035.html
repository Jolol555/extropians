<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: TRANSHUMANIST DEFINITIONS; input please</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: TRANSHUMANIST DEFINITIONS; input please">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: TRANSHUMANIST DEFINITIONS; input please</h1>
<!-- received="Mon Dec 24 00:48:10 2001" -->
<!-- isoreceived="20011224074810" -->
<!-- sent="Mon, 24 Dec 2001 02:48:03 -0500" -->
<!-- isosent="20011224074803" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: TRANSHUMANIST DEFINITIONS; input please" -->
<!-- id="3C26DDB3.FE3FCB0D@pobox.com" -->
<!-- inreplyto="5.1.0.14.2.20011223221548.06120b60@mail.speakeasy.net" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20TRANSHUMANIST%20DEFINITIONS;%20input%20please&In-Reply-To=&lt;3C26DDB3.FE3FCB0D@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Dec 24 2001 - 00:48:03 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5036.html">Darin Sunley: "Re: TRANSHUMANIST DEFINITIONS; input please"</a>
<li><strong>Previous message:</strong> <a href="5034.html">Spike Jones: "Re: TRANSHUMANIST DEFINITIONS; input please"</a>
<li><strong>In reply to:</strong> <a href="5031.html">Max More: "TRANSHUMANIST DEFINITIONS; input please"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5039.html">John Grigg: "Re: TRANSHUMANIST DEFINITIONS; input please"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5035">[ date ]</a>
<a href="index.html#5035">[ thread ]</a>
<a href="subject.html#5035">[ subject ]</a>
<a href="author.html#5035">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
I guess I'd better at least do mine.  I'm also tossing in a few other
<br>
definitions, including some from the glossaries of &quot;General Intelligence
<br>
and Seed AI&quot; and &quot;Creating and Friendly AI&quot;, as long as I'm at it.  I
<br>
don't know the attributions for some of those, so feel free to contribute
<br>
them.
<br>
<p>BTW, there a reason why ExI can't borrow Anders Sandberg's glossary?
<br>
<p><pre>
--
<p>COMPUTRONIUM:  Matter that has been transformed from its natural state
into a computer of the maximum physically achievable efficiency.  (A true
Extropian would argue that this &lt;i&gt;is&lt;/i&gt; matter's &quot;natural state&quot;.)  What
constitutes &quot;computronium&quot; varies with the level of postulated technology;
a rod-logic &lt;gl&gt;nanocomputer&lt;/gl&gt; is probably too primitive, since the
basic elements consist of hundreds or thousands of atoms.  More likely
forms of computronium include three-dimensional quantum cellular automata,
or exotic forms of matter such as neutronium, Higgsium, and monopolium.
<p>FRIENDLY AI:  An AI which is, broadly speaking, one of the good guys; an
AI which operates roughly within humanity's moral frame of reference; an
AI which has the potential and the will to become at least as
philosophically enlightened, from our perspective, as intelligence derived
from a human or group of humans; an AI sufficiently advanced to engage in
independent real-world planning, which makes human-benefiting, non-human
harming decisions.  [&lt;a href=&quot;<a href="http://singinst.org/friendly/">http://singinst.org/friendly/</a>&quot;&gt;Eliezer
Yudkowsky&lt;/a&gt;, 2000.]
<p>GREY GOO:  Out-of-control replicating &lt;gl&gt;nanotechnology&lt;/gl&gt;; some &lt;a
href=&quot;<a href="http://www.foresight.org/NanoRev/Ecophagy.html">http://www.foresight.org/NanoRev/Ecophagy.html</a>&quot;&gt;calculations&lt;/a&gt;
indicate that the entire ecosphere could be consumed within weeks or
days.  One of the primary risks threatening the complete destruction of
humanity.  [Eric Drexler, 1986.]  Perhaps an even more dangerous variant
is &quot;red goo&quot;, or military nanotechnology.
<p>HARD TAKEOFF:  A &lt;gl&gt;Singularity&lt;/gl&gt; occurring with extreme speed and
rapidity, over the course of hours or weeks rather than months or
decades.  Most hard takeoff scenarios involve Artificial Intelligence
because of the probable ability of an AI to rapidly absorb enormous
amounts of computing power, run on basic computing elements with limiting
serial speeds of 2GHz (as opposed to 200Hz neurons), and recursively
self-improve by rewriting one's own source code; however, it is also
conceivable that a hard takeoff scenario could develop out of
brain-computer interfaces.
<p>NANOCOMPUTER:  A computer built using &lt;gl&gt;nanotechnology&lt;/gl&gt;
(manufacturing to molecular specifications).  A lower bound on
nanocomputing speeds has been set by calculating the speed of an acoustic
computer using &quot;rod logics&quot; and messages that travel at the speed of
sound; a one-kilogram rod logic, occupying one cubic centimeter, can
contain 10^12 CPUs each operating at 1000 MIPS for a total of ten thousand
billion billion operations per second.  Note that rod logics are the
nanotech equivalent of vacuum tubes; electronic nanocomputers would be
substantially faster.  [Eric Drexler, 1986, 1992.]
<p>NANOTECHNOLOGY:  As used by venture capitalists, technology which operates
in the nanometer (billionth of a meter) scale.  As used by transhumanists,
technology which uses precise positional control of reactants to
mechanically synthesize large-scale structures to exact molecular
specifications - &quot;positional chemistry&quot; or &quot;mechanosynthesis&quot;.  Molecular
nanotechnology is distinguished by the observation that in theory, it can
produce virtually any material object, including a duplicate of itself,
and can moreover operate on a scale that is small relative to human
biology - allowing medical technology verging on total control of biology,
including the halting or reversal of aging.  [Eric Drexler, 1986, 1992.]
<p>NEUROHACK:  A broad term covering most forms of biologically based
intelligence enhancement, including brain-computer interfaces, genetic
engineering for higher intelligence, addition of extra brain tissue,
various proposed neurosurgical methods, et cetera; may also be used to
refer to a sufficiently unusual and extreme natural perturbation to
cognitive processing.  [Eliezer Yudkowsky, 1998.]
<p>SEED AI:  An AI designed for recursive self-improvement; that is,
improvement followed by another round of improvement at that higher level
of intelligence.  Rather than building a mind which is superintelligent
from the start, the theory holds that only some bounded level of
intelligence need be achieved in order for the AI to become capable of
open-ended improvement of its own source code.  [Eliezer Yudkowsky &lt;a
href=&quot;<a href="http://singinst.org/GISAI/">http://singinst.org/GISAI/</a>&quot;&gt;1998&lt;/a&gt;, &lt;a
href=&quot;<a href="http://singinst.org/seedAI/">http://singinst.org/seedAI/</a>&quot;&gt;2000&lt;/a&gt;, &lt;a
href=&quot;<a href="http://singinst.org/seedAI/seedAI.html">http://singinst.org/seedAI/seedAI.html</a>&quot;&gt;2001&lt;/a&gt;.]
<p>SINGULARITARIAN:  Originally defined by Mark Plus to mean &quot;one who
believes the concept of a Singularity&quot;, this term has since been redefined
to mean &quot;Singularity activist&quot; or &quot;friend of the Singularity&quot;; that is,
one who acts so as to bring about a Singularity.  [Mark Plus, 1991; &lt;a
href=&quot;<a href="http://sysopmind.com/sing/principles.html">http://sysopmind.com/sing/principles.html</a>&quot;&gt;Eliezer Yudkowsky&lt;/a&gt;,
2000.]
<p>--              --              --              --              -- 
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5036.html">Darin Sunley: "Re: TRANSHUMANIST DEFINITIONS; input please"</a>
<li><strong>Previous message:</strong> <a href="5034.html">Spike Jones: "Re: TRANSHUMANIST DEFINITIONS; input please"</a>
<li><strong>In reply to:</strong> <a href="5031.html">Max More: "TRANSHUMANIST DEFINITIONS; input please"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5039.html">John Grigg: "Re: TRANSHUMANIST DEFINITIONS; input please"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5035">[ date ]</a>
<a href="index.html#5035">[ thread ]</a>
<a href="subject.html#5035">[ subject ]</a>
<a href="author.html#5035">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Sat May 11 2002 - 17:44:30 MDT</em>
</em>
</small>
</body>
</html>
