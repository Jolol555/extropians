<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Posthuman Politics</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Posthuman Politics">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Posthuman Politics</h1>
<!-- received="Wed Oct 17 01:32:30 2001" -->
<!-- isoreceived="20011017073230" -->
<!-- sent="Wed, 17 Oct 2001 03:32:25 -0400" -->
<!-- isosent="20011017073225" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Posthuman Politics" -->
<!-- id="3BCD3409.2B6C23A0@pobox.com" -->
<!-- inreplyto="Pine.GSO.4.32.0110162203130.2925-100000@jane.itd.umich.edu" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Posthuman%20Politics&In-Reply-To=&lt;3BCD3409.2B6C23A0@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Oct 17 2001 - 01:32:25 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1056.html">Emlyn O'regan: "RE: Humans doomed without space colonies, says Hawking"</a>
<li><strong>Previous message:</strong> <a href="1054.html">Colin Hales: "RE: Anthrax addendum."</a>
<li><strong>In reply to:</strong> <a href="1042.html">Alex F. Bokov: "Re: Posthuman Politics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1154.html">J. R. Molloy: "Re: Posthuman Politics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1055">[ date ]</a>
<a href="index.html#1055">[ thread ]</a>
<a href="subject.html#1055">[ subject ]</a>
<a href="author.html#1055">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
&quot;Alex F. Bokov&quot; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; In fact, we already have evidence that superhuman entitites will not
</em><br>
<em>&gt; be friendly. Governments, mega-corporations, and [other] terrorist
</em><br>
<em>&gt; groups demonstrate that collective intelligences (CIs from now on) are
</em><br>
<em>&gt; capable of being....
</em><br>
<em>&gt;
</em><br>
<em>&gt; It ain't looking good, folks. Of course, Eliezer would argue that it's
</em><br>
<em>&gt; because CIs were designed by humans and inherited our evolutionary,
</em><br>
<em>&gt; atavistic, aggressive tendencies.
</em><br>
<p>Actually, I'll argue that CIs are just human individuals.  They do not
<br>
exhibit greater-than-human intelligence.  A corporation of Neanderthals
<br>
cannot outsmart a human.  It takes an Earthweb a la Stiegler to even start
<br>
exhibiting flashes of transhuman ability, and even then, it's still not a
<br>
gap of the order that separated us from our Neanderthal cousins.  An
<br>
Earthweb certainly is not a superintelligence... and a corporation is not
<br>
an Earthweb.
<br>
<p>I don't see a corporation as a cognitive system at all.  The Earthweb has
<br>
some claim to being a cognitive system but it is still limited to those
<br>
thoughts that a human can originate and represent, even though it can
<br>
string a large number of independently originated good ideas into what
<br>
looks like superhuman deliberate reasoning.
<br>
<p>*None* of these, CIs or even the Earthweb, have any claim to independent
<br>
goal-directed reasoning.
<br>
<p><em>&gt; To which I'll reply that...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1) The social graces that allow us to refrain from beating up people
</em><br>
<em>&gt;    who cut in front of us in line or step on our feet have emerged from
</em><br>
<em>&gt;    evolutionary forces, specifically iterated Prisoner's Dilemma
</em><br>
<p>Right, that is the complexity that a Friendly AI would target for
<br>
absorption.
<br>
<p><em>&gt; 2) It appears that social graces are too complex a behavior to be
</em><br>
<em>&gt;    completely instinctual, and we need childhood and adolescence
</em><br>
<em>&gt;    to fully develop these faculties. Babysitting an adolescent AI that's
</em><br>
<em>&gt;    smarter than you are will be... challenging.
</em><br>
<p>Have you read &quot;The Psychological Foundations of Culture&quot; by John Tooby and
<br>
Leda Cosmides, in &quot;The Adapted Mind&quot; by Barkow, Cosmides, and Tooby?  Just
<br>
because humans make use of childhood and adolescence to grow our innate
<br>
social graces does not mean that an AI must do the same.
<br>
<p><em>&gt; 3) I've met Eliezer and he seems human and shaped by evolutionary
</em><br>
<em>&gt;    pressures, at least insofar as anybody on this list is. I wish
</em><br>
<em>&gt;    him the best in not transferring his human failings onto his
</em><br>
<em>&gt;    brainchild. He's a brilliant guy, so he and his collaborators
</em><br>
<em>&gt;    just might do it, but I'll remain optimally paranoid for now.
</em><br>
<p>Friendly AI is exactly a method whereby imperfect humans, with evolved
<br>
brainware for both good and evil, can transfer only the good parts into an
<br>
AI.  The ideal / design requirement is that the end result should be
<br>
similar to what would happen if an initially altruistic human upload fixed
<br>
up vis own evolutionary failings.
<br>
<p>As FAI problems go, avoiding accidental transfer of human failings is
<br>
pretty straightforward, at least under the CFAI architecture.  A Friendly
<br>
AI is a deliberate rather than an unconscious copycat, and the AI knows
<br>
that programmers can make mistakes.
<br>
<p>PS:  For the record, can anyone here give a specific example of one of my
<br>
human failings?  Note that I'm not claiming I don't have them... just idly
<br>
wondering whether anyone can actually name an example... because I do have
<br>
fewer than usual.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1056.html">Emlyn O'regan: "RE: Humans doomed without space colonies, says Hawking"</a>
<li><strong>Previous message:</strong> <a href="1054.html">Colin Hales: "RE: Anthrax addendum."</a>
<li><strong>In reply to:</strong> <a href="1042.html">Alex F. Bokov: "Re: Posthuman Politics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1154.html">J. R. Molloy: "Re: Posthuman Politics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1055">[ date ]</a>
<a href="index.html#1055">[ thread ]</a>
<a href="subject.html#1055">[ subject ]</a>
<a href="author.html#1055">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Sat May 11 2002 - 17:44:14 MDT</em>
</em>
</small>
</body>
</html>
