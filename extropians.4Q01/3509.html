<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Intelligence scaling (was Re: Posthuman Languag</title>
<meta name="Author" content="James Rogers (jamesr@best.com)">
<meta name="Subject" content="Re: Intelligence scaling (was Re: Posthuman Language)">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Intelligence scaling (was Re: Posthuman Language)</h1>
<!-- received="Wed Nov 28 12:09:59 2001" -->
<!-- isoreceived="20011128190959" -->
<!-- sent="Wed, 28 Nov 2001 11:08:00 -0800" -->
<!-- isosent="20011128190800" -->
<!-- name="James Rogers" -->
<!-- email="jamesr@best.com" -->
<!-- subject="Re: Intelligence scaling (was Re: Posthuman Language)" -->
<!-- id="B82A760F.792C%jamesr@best.com" -->
<!-- inreplyto="Pine.LNX.4.10.10111272151130.22070-100000@server.aeiveos.com" -->
<strong>From:</strong> James Rogers (<a href="mailto:jamesr@best.com?Subject=Re:%20Intelligence%20scaling%20(was%20Re:%20Posthuman%20Language)&In-Reply-To=&lt;B82A760F.792C%25jamesr@best.com&gt;"><em>jamesr@best.com</em></a>)<br>
<strong>Date:</strong> Wed Nov 28 2001 - 12:08:00 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3510.html">BigBooster: "WALK FOR CAPITALISM"</a>
<li><strong>Previous message:</strong> <a href="3508.html">Mike Lorrey: "Re: terrorism, what is and what should never be"</a>
<li><strong>In reply to:</strong> <a href="3486.html">Robert J. Bradbury: "Intelligence scaling (was Re: Posthuman Language)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3493.html">scerir: "Re: Posthuman Language"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3509">[ date ]</a>
<a href="index.html#3509">[ thread ]</a>
<a href="subject.html#3509">[ subject ]</a>
<a href="author.html#3509">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
On 11/27/01 10:10 PM, &quot;Robert J. Bradbury&quot; &lt;<a href="mailto:bradbury@aeiveos.com?Subject=Re:%20Intelligence%20scaling%20(was%20Re:%20Posthuman%20Language)&In-Reply-To=&lt;B82A760F.792C%25jamesr@best.com&gt;">bradbury@aeiveos.com</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; More importantly, we now seem to have some empirical evidence that
</em><br>
<em>&gt; intelligence is not a simple linear scale.  A couple of British
</em><br>
<em>&gt; mathematicians seem to have shown that a relatively small increase
</em><br>
<em>&gt; in &quot;capacity&quot; buys a *lot* in terms of &quot;effective&quot; intelligence.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; One can hope that someone more qualified to comment on the
</em><br>
<em>&gt; results will publish something a bit clearer about the methods.
</em><br>
<em>&gt; (Perhaps someone can find something in the preprint archives...)
</em><br>
<p><p>This is interesting because I arrived at a similar mathematical result
<br>
sometime back in 2000 that I discussed with a few people in private forums.
<br>
While I wasn't aware of the above study, I am still strongly convinced that
<br>
an analysis of the relevant mathematics suggest that a linear increase in
<br>
model capacity (RAM, neurons, etc) generate an exponential increase in the
<br>
size of the model that can be handled by the intelligence with equivalent
<br>
predictive accuracy.  This sort of suggests a different type of &quot;hard
<br>
takeoff&quot; than is normally talked about when discussing a singularity, as
<br>
even modest improvements in hardware capacity can produce vast improvements
<br>
in intelligence; it suggests that an exponential hardware takeoff isn't even
<br>
required for super-intelligent AI.  The consequence of this from an AI
<br>
standpoint is that controlling the effective intelligence of a system may be
<br>
very difficult, as the  resource space between dumb-but-useful AI and
<br>
super-intelligent AI may in fact be quite small, a line that could easily be
<br>
crossed inadvertently.
<br>
<p>The other derivable effect worth noting is that sufficiently small model
<br>
capacity (relative to the complexity of the actual thing that one is
<br>
attempting to model) will generate *worse* predictive results than pure
<br>
chance.  This particular problem is essentially caused by aliasing, and
<br>
could be considered a form of observer bias.  This could explain the
<br>
large-scale religious belief systems in humans if we assume that the human
<br>
brain has only borderline capacity for developing non-aliased models of the
<br>
relatively complex processes of our universe.  I haven't seen this idea
<br>
suggested elsewhere, but it doesn't seem unreasonable either and it actually
<br>
makes a fair amount of sense to me.
<br>
<p><p>-James Rogers
<br>
&nbsp;<a href="mailto:jamesr@best.com?Subject=Re:%20Intelligence%20scaling%20(was%20Re:%20Posthuman%20Language)&In-Reply-To=&lt;B82A760F.792C%25jamesr@best.com&gt;">jamesr@best.com</a>
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3510.html">BigBooster: "WALK FOR CAPITALISM"</a>
<li><strong>Previous message:</strong> <a href="3508.html">Mike Lorrey: "Re: terrorism, what is and what should never be"</a>
<li><strong>In reply to:</strong> <a href="3486.html">Robert J. Bradbury: "Intelligence scaling (was Re: Posthuman Language)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3493.html">scerir: "Re: Posthuman Language"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3509">[ date ]</a>
<a href="index.html#3509">[ thread ]</a>
<a href="subject.html#3509">[ subject ]</a>
<a href="author.html#3509">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Sat May 11 2002 - 17:44:22 MDT</em>
</em>
</small>
</body>
</html>
