<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: correction Fermi 2</title>
<meta name="Author" content="Robert J. Bradbury (bradbury@aeiveos.com)">
<meta name="Subject" content="Re: correction Fermi 2">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: correction Fermi 2</h1>
<!-- received="Sat Nov 17 15:12:15 2001" -->
<!-- isoreceived="20011117221215" -->
<!-- sent="Sat, 17 Nov 2001 14:12:13 -0800 (PST)" -->
<!-- isosent="20011117221213" -->
<!-- name="Robert J. Bradbury" -->
<!-- email="bradbury@aeiveos.com" -->
<!-- subject="Re: correction Fermi 2" -->
<!-- id="Pine.LNX.4.10.10111171304220.14105-100000@server.aeiveos.com" -->
<!-- inreplyto="correction Fermi 2" -->
<strong>From:</strong> Robert J. Bradbury (<a href="mailto:bradbury@aeiveos.com?Subject=Re:%20correction%20Fermi%202&In-Reply-To=&lt;Pine.LNX.4.10.10111171304220.14105-100000@server.aeiveos.com&gt;"><em>bradbury@aeiveos.com</em></a>)<br>
<strong>Date:</strong> Sat Nov 17 2001 - 15:12:13 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2773.html">Curious: "Re: New To Extropian List..."</a>
<li><strong>Previous message:</strong> <a href="2771.html">Michael M. Butler: "Re: SPACE: thinking seriously about space development"</a>
<li><strong>Maybe in reply to:</strong> <a href="2726.html">Spudboy100@aol.com: "correction Fermi 2"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2772">[ date ]</a>
<a href="index.html#2772">[ thread ]</a>
<a href="subject.html#2772">[ subject ]</a>
<a href="author.html#2772">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Eliezer said:
<br>
<p><em>&gt; I don't see why minds would fragment this way, or why temporal
</em><br>
<em>&gt; distance would increase the degree of fragmentation
</em><br>
<p>I would expect the feasibility of finding &quot;optimal&quot; solutions for
<br>
problems is dependent on the degree of autonomy granted to agents.
<br>
If the Mars Rover had to check with Houston every time it encounters
<br>
a rock an ask &quot;Do I go left or right here?&quot; then its going to be
<br>
a pretty slow mission.  For example, the things Eliezer is working
<br>
on now he would probably have been much less able to work on as
<br>
efficiently 1 or 2 years ago due to possible environmental or resource
<br>
constraints.
<br>
<p><em>&gt; maybe the incremental utility of fragmentation is less than the
</em><br>
<em>&gt; incremental disutility, especially if the sole point of colonization
</em><br>
<em>&gt; is to rescue a few pre-Singularity civilizations here and there.
</em><br>
<p>The question is how do you define &quot;utility&quot; to super-intelligences?
<br>
Say our solar system pulls up right next to an adjacent untransformed
<br>
solar system (or a brown dwarf).  You send out a swarm of constructor
<br>
bots to do a rapid foundation building and clone your super-intelligence.
<br>
At that point you have 2 copides of yourself which presumably have
<br>
the same goals, moral perspective, etc.  But as soon as the solar
<br>
systems begin to move apart you presumably begin to diverge.  Your
<br>
next encounter with your former self may not be for a few billion
<br>
years -- at that point it seems unlikely you will recognize your
<br>
former self.  It is doubtful that you can keep yourself &quot;in-sync&quot;
<br>
with each other because as the distance between your &quot;selves&quot;
<br>
increases it becomes increasingly expensive to transmit even a
<br>
small fraction of the new data the clones are constantly creating.
<br>
<p>The only &quot;utility&quot; seems to be that in the process of cloning itself,
<br>
the SI can empty half of its presumably filled up memory.  I doubt
<br>
that it would want to unload the valuable memories, so presumably it
<br>
unloads the less valuable memories.  That means the clone starts
<br>
out rather &quot;handicapped&quot; and potentially has cause for holding a
<br>
grudge against you.  Or you could split everything 50:50 but then
<br>
it would seem that you are rolling the dice with creating an entity
<br>
that could become more powerful than yourself.
<br>
<p>None of this matters at *this* point in the development of the
<br>
Universe since there is a lot of underutilized material to go
<br>
around.  But we *know* at some point push is going to come to
<br>
shove.  Whether SIs develop a moral system that says you never
<br>
cannablize your neighbor, perhaps instead choosing to run
<br>
increasingly slower, remains to be seen.
<br>
<p>I think an argument for colonization to rescue pre-Singularity
<br>
civilizations is extremely anthropocentric.  I think the Zen
<br>
of SIs argues that observing the &quot;process&quot; is what is interesting,
<br>
not selecting the winners and losers.
<br>
<p><em>&gt; The child mind may still be a cooperator rather than a defector,
</em><br>
<em>&gt; but it will be a competitor, and &quot;nonexistent&quot; is preferred to
</em><br>
<em>&gt; &quot;slightly suboptimal&quot;. For example, the child mind might demand
</em><br>
<em>&gt; a share of some resource which is unique to a given universe.
</em><br>
<p>A clone or a child SI is likely to start out as a cooperator
<br>
and seems likely to stay that way assuming the Universe in general
<br>
has resources allowing its continued existence.  Game theoretic
<br>
assumptions of &quot;trustability&quot; would seem to make sense and
<br>
the risks of berserker bots would seem to encourage that.
<br>
However encounters between SIs that allow effective communication
<br>
of any significant fraction of their information content will
<br>
be rare so divergence between &quot;siblings&quot; or parent/child seems likely.
<br>
The only exceptions to this might be families or tribes of SIs that
<br>
have gone to the trouble of arranging their orbits in the galaxy so
<br>
they can fly as a closely packed family or tribe.
<br>
<p><em>&gt; The probability of other civilizations undergoing their own
</em><br>
<em>&gt; Singularities and demanding their own shares of a unique resource, or of
</em><br>
<em>&gt; such civilizations otherwise having still more divergent goals, is
</em><br>
<em>&gt; sufficiently small that the known creation of a colonizing entity is a
</em><br>
<em>&gt; greater negative utility than the probabilistic positive utility of
</em><br>
<em>&gt; finding a potential competing civilization before its Singularity
</em><br>
<p>There would seem to be only 2 resources -- matter (which SIs can
<br>
transmute) and energy (which there is plenty of in interstellar
<br>
hydrogen clouds).  The goals may diverge (in terms of what SIs
<br>
think about or what part of the phase space of to explore) but
<br>
it seems questionable that it should evolve into such differences
<br>
as the Horta, Klingons, Vulcans and Shape-shifters.  Those all
<br>
seem like simple explorations of the &quot;animal&quot; phase space nowhere
<br>
close to the actual physical limits.
<br>
<p>If one located potentially competing civilizations, it seems likely
<br>
that observing them up until the point of the Singularity might
<br>
provide some useful data.  At the current stage of galactic development
<br>
allowing them to go through the Singularity is relatively risk-free as
<br>
well.  Whether they do that seems to depend on the long term utility of
<br>
having had more civilizations explore unique vectors through the
<br>
Singularity into the post-singularity phase space relative to the
<br>
far-distant problem of having more competitors when resources become
<br>
scarce.
<br>
<p><em>&gt; There is no way to send out non-independently-thinking probes capable
</em><br>
<em>&gt; of incorporating possible competing civilizations. This is certainly
</em><br>
<em>&gt; reasonable from our perspective, but it must also hold true of all
</em><br>
<em>&gt; possible minds that can be built by a superintelligence.
</em><br>
<p>This would seem to depend on the potentially competing civilizations.
<br>
This would suggest that as soon as we start to ramp up the Singularity,
<br>
the Galactic Club probe will land in the U.N. courtyard, hand over the
<br>
Galactic Club Rules and say &quot;Sign Here&quot; (or ELSE!).  I suspect we
<br>
would sign, but if we were Klingons we might make a different choice.
<br>
<p><em>&gt; Competing fragmented entities will tend to fragment further, creating
</em><br>
<em>&gt; a potential exponential problem if even a single fragmentation occurs. 
</em><br>
<p>Looks that way to me.  Of course it doesn't grow very fast because
<br>
it depends on encounters with large matter concentrations that can
<br>
support cloning or forking.  But SIs are thinking on billion-to-trillion
<br>
year timescales so they may have more reason for concern than we would.
<br>
<p>I think the question may turn on the utility of having civilizations
<br>
take unique paths through the Singularity balanced with the risks
<br>
that they become rogue SIs post-singularity.  It may be that the
<br>
rate of civilizations going through the Singularity is so low
<br>
that they we are still at the stage of galactic development that 
<br>
we have to be observed to build up the database of what the markers
<br>
are for the development of &quot;good&quot; vs. &quot;evil&quot; SIs.  Then in the future
<br>
the SIs will know in advance which civilizations to &quot;rollback&quot; to
<br>
stages where productive development seems likely.  You can value
<br>
sentience from a moral perspective but the potential destruction
<br>
that could be caused by a rogue SI would seem to weigh in as
<br>
a much greater concern than a few billion human lives (at least
<br>
from an extropic, utilitarian, SI point of view).
<br>
<p>Whether or not SIs clone or fork may in turn depend on whether all
<br>
of the post-singularity SI &quot;thought&quot; we anticipate will be able to
<br>
come up with good solutions to the resource shortage one expects
<br>
in the far distant future.
<br>
<p>Robert
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2773.html">Curious: "Re: New To Extropian List..."</a>
<li><strong>Previous message:</strong> <a href="2771.html">Michael M. Butler: "Re: SPACE: thinking seriously about space development"</a>
<li><strong>Maybe in reply to:</strong> <a href="2726.html">Spudboy100@aol.com: "correction Fermi 2"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2772">[ date ]</a>
<a href="index.html#2772">[ thread ]</a>
<a href="subject.html#2772">[ subject ]</a>
<a href="author.html#2772">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Sat May 11 2002 - 17:44:19 MDT</em>
</em>
</small>
</body>
</html>
