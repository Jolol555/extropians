<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Fwd: Lanier essay of 2001.12.04</title>
<meta name="Author" content="Michael M. Butler (butler@comp-lib.org)">
<meta name="Subject" content="Fwd: Lanier essay of 2001.12.04">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Fwd: Lanier essay of 2001.12.04</h1>
<!-- received="Wed Dec  5 11:23:11 2001" -->
<!-- isoreceived="20011205182311" -->
<!-- sent="Wed, 05 Dec 2001 10:22:47 -0800" -->
<!-- isosent="20011205182247" -->
<!-- name="Michael M. Butler" -->
<!-- email="butler@comp-lib.org" -->
<!-- subject="Fwd: Lanier essay of 2001.12.04" -->
<!-- id="3C0E65F6.53B42942@comp-lib.org" -->
<strong>From:</strong> Michael M. Butler (<a href="mailto:butler@comp-lib.org?Subject=Re:%20Fwd:%20Lanier%20essay%20of%202001.12.04&In-Reply-To=&lt;3C0E65F6.53B42942@comp-lib.org&gt;"><em>butler@comp-lib.org</em></a>)<br>
<strong>Date:</strong> Wed Dec 05 2001 - 11:22:47 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3815.html">David McFadzean: "META: Re: Getting bounces sending to extropians@extropy.org"</a>
<li><strong>Previous message:</strong> <a href="3813.html">Michael M. Butler: "Earth Trojan Asteroids, was Re: Jupiter's L1"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3817.html">Dossy: "Re: Fwd: Lanier essay of 2001.12.04"</a>
<li><strong>Reply:</strong> <a href="3817.html">Dossy: "Re: Fwd: Lanier essay of 2001.12.04"</a>
<li><strong>Maybe reply:</strong> <a href="3974.html">Emlyn O'regan: "RE: Fwd: Lanier essay of 2001.12.04"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3814">[ date ]</a>
<a href="index.html#3814">[ thread ]</a>
<a href="subject.html#3814">[ subject ]</a>
<a href="author.html#3814">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Forwarded from a friend:
<br>
&nbsp;There's some topics of interest on the newly updated Edge.org page. Here's one from Jaron, again.  JARON LANIER: THE
<br>
CENTRAL METAPHOR OF EVERYTHING?[12.4.01] One of the striking things about being a computer scientist in this age is that
<br>
all sorts of other people are happy to tell us that what we do is the central metaphor of everything, which is very ego
<br>
gratifying. We hear from various quarters that our work can serve as the best understanding - if not in the present but
<br>
any minute now because of Moore's law - of everything from biology to the economy to aesthetics, child-rearing, sex, you
<br>
name it. I have found myself being critical of what I view as this overuse as the computational metaphor. My initial
<br>
motivation was because I thought there was naive and poorly constructed philosophy at work. It's as if these people had
<br>
never read philosophy at all and there was no sense of epistemological or other
<br>
problems... <a href="http://www.edge.org/documents/day/day_lanier.html">http://www.edge.org/documents/day/day_lanier.html</a>
<br>
...Then I became concerned for a different reason which was pragmatic and immediate: I became convinced that the overuse
<br>
of the computational metaphor was actually harming the quality of the present-day design of computer systems. One
<br>
example of that, the belief that people and computers are similar, the artificial intelligence mindset, has a tendency
<br>
to create systems that are naively and overly automated. An example of that is the Microsoft word processor that
<br>
attempts to retype what you've just typed, the notion of trying to make computers into people because somehow that
<br>
agenda of making them into people is so important that if you jump the gun it has to be for the greater good, even if it
<br>
makes the current software stupid.
<br>
<p>There's a third reason to be suspicious of the overuse of computer metaphors, and that is that it leads us by reflection
<br>
to have an overly simplistic view of computers. The particular simplification of computers I'm concerned with is
<br>
imagining that Moore's Law applies to software as well as hardware. More specifically, that Moore's Law applies to
<br>
things that have to have complicated interfaces with their surroundings as opposed to things that have simple interfaces
<br>
with their surroundings, which I think is the better distinction.
<br>
<p>Moore's Law is truly an overwhelming phenomenon; it represents the greatest triumph of technology ever, the fact that we
<br>
could keep on this track that was predicted for all these many years and that we have machines that are a million times
<br>
better than they were at the dawn of our work, which was just a half century ago. And yet during that same period of
<br>
time our software has really not kept pace. In fact not only could you argue that software has not improved at the same
<br>
rate as hardware, you could even argue that it's often been in retrograde. It seems to me that our software
<br>
architectures have not even been able to maintain their initial functionality as they've scaled with hardware, so that
<br>
in effect we've had worse and worse software. Most people who use personal computers can experience that effect
<br>
directly, and it's true in most situations.
<br>
<p>But I want to emphasize that the real distinction that I see is between systems with simple interfaces to their
<br>
surroundings and systems with complex interfaces. If you want to have a fancy user interface and you run a bigger thing
<br>
it just gets awful. Windows doesn't scale.
<br>
<p>One question to ask is, why does software suck so badly? There are a number of answers to that. The first thing I would
<br>
say is that I have absolutely no doubt that David Gelernter's framework of streams is fundamentally and overwhelmingly
<br>
superior to the basis in which our current software is designed. The next question is, is that enough to cause it to
<br>
come about? It really becomes a competition between good taste and good judgment on the one hand, and legacy and
<br>
corruption on the other - which are effectively two words for the same thing, in effect. What happens with software
<br>
systems is that the legacy effects end up being the overwhelming determinants of what can happen next as the systems
<br>
scale.
<br>
For instance, there is the idea of the computer file, which was debated up until the early 80s. There was an active
<br>
contingent that thought that the idea of the file wasn't a good thing and we should instead have a massive distributed
<br>
data base with a micro-structure of some sort. The first (unreleased) version of the Macintosh did not have files. But
<br>
Unix jumped the fence from the academic to the business world and it had files, and Macintosh ultimately came out with
<br>
files, and the Microsoft world had files, and basically everything has files. At this point, when we teach
<br>
undergraduates computer science, we do not talk about the file as an invention, but speak of it as if it were a photon,
<br>
because it in effect is more likely to still be around in 50 years than the photon.
<br>
<p>I can imagine physicists coming up with some reasons not to believe in photons any more, but I cannot imagine any way
<br>
that we can tell you not to believe in files. We are stuck with the damn things. That legacy effect is truly
<br>
astonishing, the sort of non-linearity of the costs of undoing decisions that have been made. The remarkable degree to
<br>
which the arrow of time is amplified in software development in its brutalness is extraordinary, and perhaps one of the
<br>
things that really distinguishes software from other phenomena.
<br>
<p>Back to the physics for a second. One of the most remarkable and startling insights in 20th century thought was Claude
<br>
Shannon's connection of information and thermodynamics. Somehow for all of these years working with computers I've been
<br>
looking at these things and I've been thinking, &quot;Are these bits the same bits Shannon was talking about, or is there
<br>
something different?&quot; I still don't know the answer, but I'd like to share my recent thoughts because I think this all
<br>
ties together. If you wish to treat the world as being computational and if you wish to say that the pair of sunglasses
<br>
I am wearing is a computer that has sunglass input and output- if you wish to think of things that way, you would have
<br>
to say that not all of the bits that are potentially measurable are in practice having an effect. Most of them are lost
<br>
in statistical effects, and the situation has to be rather special for a particular bit to matter.
<br>
<p>In fact, bits really do matter. If somebody says &quot;I do&quot; in the right context that means a lot, whereas a similar number
<br>
of bits of information coming in another context might mean much less. Various measurable bits in the universe have
<br>
vastly different potentials to have a causal impact. If you could possibly delineate all the bits you would probably see
<br>
some dramatic power law where there would be a small number of bits that had tremendously greater potential for having
<br>
an effect, and a vast number that had very small potentials. It's those bits that have the potential for great effect
<br>
that are probably the ones that computer scientists are concerned with, and probably Shannon doesn't differentiate
<br>
between those bits as far as he went.
<br>
<p>Then the question is how do we distinguish between the bits; what differentiates one from the other, how can we talk
<br>
about them? One speculation is that legacy effects have something to do with it. If you have a system with a vast
<br>
configuration space, as is our world, and you have some process, perhaps an evolutionary process, that's searching
<br>
through possible configurations, rather than just a meandering random walk, perhaps what we see in nature is a series of
<br>
stair steps where legacies are created that prohibit large numbers of configurations from every being searched again,
<br>
and that there's a series of refinements.
<br>
<p>Once DNA has won out, variants of DNA are very unlikely to appear. Once Windows has appeared, it's stuck around, and so
<br>
forth. Perhaps what happens is that the legacy effect, which is because of the non-linearity of the tremendous expense
<br>
of reversing certain kinds of systems. Legacies that are created are like lenses that amplify certain bits to be more
<br>
important. This suggests that legacies are similar to semantics on some fundamental level. And it suggests that the
<br>
legacy effect might have something to do with the syntax/semantics distinction, to the degree that might be meaningful.
<br>
And it's the first glimmer of a definition of semantics I've ever had, because I've always thought the word didn't mean
<br>
a damn thing except &quot;what we don't understand&quot;. But I'm beginning to think what it might be is the legacies that we're
<br>
stuck with.
<br>
To tie the circle back to the &quot;Rebooting Civilization&quot; question, what I'm hoping might happen is as we start to gain a
<br>
better understanding of how enormously difficult, slow, expensive, tedious and rare an event it is to program a very
<br>
large computer well; as soon as we have a sense and appreciation of that, I think we can overcome the sort of
<br>
intoxication that overcomes us when we think about Moore's Law, and start to apply computation metaphors more soberly to
<br>
both natural science and to metaphorical purposes for society and so forth. A well-appreciated computer that included
<br>
the difficulty of making large software well could serve as a far more beneficial metaphor than the cartoon computer,
<br>
which is based only on Moore's Law; all you have to do is make it fast and everything will suddenly work, and the
<br>
computers-will-become-smarter than-us-if-you just-wait-for-20-years sort of metaphor that has been prevalent lately.The
<br>
really good computer simulations that do exist in biology and in other areas of science, and I've been part of a few
<br>
that count, particularly in surgical prediction and simulation, and in certain neuroscience simulations, have been
<br>
enormously expensive. It took 18 years and 5,000 patients to get the first surgical simulation to the point of testable
<br>
usability. That is what software is, that's what computers are, and we should de-intoxicate ourselves from Moore's Law
<br>
before continuing with the use of this metaphor.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3815.html">David McFadzean: "META: Re: Getting bounces sending to extropians@extropy.org"</a>
<li><strong>Previous message:</strong> <a href="3813.html">Michael M. Butler: "Earth Trojan Asteroids, was Re: Jupiter's L1"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3817.html">Dossy: "Re: Fwd: Lanier essay of 2001.12.04"</a>
<li><strong>Reply:</strong> <a href="3817.html">Dossy: "Re: Fwd: Lanier essay of 2001.12.04"</a>
<li><strong>Maybe reply:</strong> <a href="3974.html">Emlyn O'regan: "RE: Fwd: Lanier essay of 2001.12.04"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3814">[ date ]</a>
<a href="index.html#3814">[ thread ]</a>
<a href="subject.html#3814">[ subject ]</a>
<a href="author.html#3814">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Sat May 11 2002 - 17:44:24 MDT</em>
</em>
</small>
</body>
</html>
