<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: One Unity, Different Ideologies, all in the sam</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: One Unity, Different Ideologies, all in the same universe">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: One Unity, Different Ideologies, all in the same universe</h1>
<!-- received="Wed Dec 26 17:06:40 2001" -->
<!-- isoreceived="20011227000640" -->
<!-- sent="Wed, 26 Dec 2001 19:06:32 -0500" -->
<!-- isosent="20011227000632" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: One Unity, Different Ideologies, all in the same universe" -->
<!-- id="3C2A6608.A7173808@pobox.com" -->
<!-- inreplyto="20011227001733.B12862@akira.nada.kth.se" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20One%20Unity,%20Different%20Ideologies,%20all%20in%20the%20same%20universe&In-Reply-To=&lt;3C2A6608.A7173808@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Dec 26 2001 - 17:06:32 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5251.html">Samantha Atkins: "Re: Dumping"</a>
<li><strong>Previous message:</strong> <a href="5249.html">Kai Becker: "Re: Dumping (was: Local Groups Wanted!)"</a>
<li><strong>In reply to:</strong> <a href="5246.html">Anders Sandberg: "Re: One Unity, Different Ideologies, all in the same universe"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5524.html">Samantha Atkins: "Re: One Unity, Different Ideologies, all in the same universe"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5250">[ date ]</a>
<a href="index.html#5250">[ thread ]</a>
<a href="subject.html#5250">[ subject ]</a>
<a href="author.html#5250">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Anders Sandberg wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; On Wed, Dec 26, 2001 at 03:28:54PM -0500, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; AI slavery is less expensive than biological slavery, but if you insist
</em><br>
<em>&gt; &gt; that there is any difference whatever between the two, it's easy enough to
</em><br>
<em>&gt; &gt; imagine Osama bin Laden biologically cloning seventy-two helpless
</em><br>
<em>&gt; &gt; virgins.  From my perspective, these are people too and they have just as
</em><br>
<em>&gt; &gt; much claim on my sympathy as you or anyone else.  If it's worth saving the
</em><br>
<em>&gt; &gt; world, it's worth saving them too.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Sure. Do you think Osamaland would be accepted by any reasonable federative
</em><br>
<em>&gt; constitution?
</em><br>
<p>I think Osama will not *apply to join* your federative constitution.  I
<br>
think he'll set up his own asteroid cluster (or whatever level of
<br>
technology it is you're postulating) and do whatever the hell he pleases. 
<br>
I want to know what your Federation is going to do about it.
<br>
<p>Until you answer this question you are simply postulating a singleton
<br>
scenario without the singleton; i.e., everyone in the universe has
<br>
mysteriously agreed to adopt the Anders Sandberg Constitution, whose
<br>
bylaws look charmingly like Friendliness's nonviolation of volition.
<br>
<p><em>&gt; My
</em><br>
<em>&gt; point is that we are talking about getting real people set up real
</em><br>
<em>&gt; political systems in the real world
</em><br>
<p>*cough*hopeless*cough*
<br>
<p>Small political changes are one thing.  Total rewriting of the worldwide
<br>
political system, through human social channels, pre-Singularity, seems
<br>
rather unlikely.
<br>
<p><em>&gt; &gt; 1)  This sounds to me like a set of heuristics unadapted to dealing with
</em><br>
<em>&gt; &gt; existential risks (and not just the Bangs, either).  Some errors are
</em><br>
<em>&gt; &gt; nonrecoverable.  If Robin Hanson's cosmic-commons colonization race turns
</em><br>
<em>&gt; &gt; out to be a Whimper, then we had better not get started down that road,
</em><br>
<em>&gt; &gt; because once begun it won't stop.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Which existential risks are relevant? When planning to act in any way you
</em><br>
<em>&gt; have to make estimates of risks. If the risk is too great, you become
</em><br>
<em>&gt; careful and may even avoid certain actions if the risk is unacceptable. In
</em><br>
<em>&gt; many cases both the probability and the severity of the risk are unknown,
</em><br>
<em>&gt; and have to be gradually refined given what we learn. Basing your actions
</em><br>
<em>&gt; on your prior estimate and then never revising it would be irrational. So
</em><br>
<em>&gt; what is needed is systems that allow us to learn and act according to what
</em><br>
<em>&gt; we have learned.
</em><br>
<p>Um... no, what's needed is a system that gets it right the first time. 
<br>
That's kind of the distinguishing quality of existential risks.  Even if
<br>
you drop the stakes a bit, down to the level of say nuclear war, you
<br>
really don't want to plan on building a system that eventually learns to
<br>
manage global thermonuclear wars, after four or five tries.  With
<br>
existential risks, of course, even a single failure is unacceptable.
<br>
<p><em>&gt; I think there is a certain risk involved with the concept of existential
</em><br>
<em>&gt; risks, actually. Given the common misinterpretation of the precautionary
</em><br>
<em>&gt; principle as &quot;do not do anything that has not been proven safe&quot;, even the
</em><br>
<em>&gt; idea of existential risks provides an excellent and rhetorically powerful
</em><br>
<em>&gt; argument for stasis. Leon Kass is essentially using this in his
</em><br>
<em>&gt; anti-posthuman campaign: since there may be existential risks involved with
</em><br>
<em>&gt; posthumanity *it must be prevented*.
</em><br>
<p>The flaw in this idea is simply that minimizing global existential risks
<br>
sometimes involves a necessary existential risk, such as the deliberate
<br>
development of superintelligence.  Not that existential risks are somehow
<br>
okay.  Leon Kass is right to be nervous - our situation *is* every bit as
<br>
precarious as &quot;one single failure leads to total destruction&quot; suggests. 
<br>
It's just that his solution is absolutely unworkable and suicidal, that's
<br>
all.
<br>
<p><em>&gt; &gt; 2)  The cost in sentient suffering in a single non-federation community,
</em><br>
<em>&gt; &gt; under the framework you present, could enormously exceed the sum of all
</em><br>
<em>&gt; &gt; sentient suffering in history up until this point.  This is not a trivial
</em><br>
<em>&gt; &gt; error.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; No, but that is not something the federation was supposed to solve either.
</em><br>
<p>That's going to make the Sandberg Federation rather unattractive if
<br>
there's a strategy that does solve that problem... even if it involves
<br>
(gasp!) a global optimum.
<br>
<p><em>&gt; The fact that there is awful suffering and tyrrany in some countries
</em><br>
<em>&gt; doesn't invalidate the political system of the US. The federation is not
</em><br>
<em>&gt; based on an utilitarist ethical perspective where the goal is to maximize
</em><br>
<em>&gt; global happiness.
</em><br>
<p>Uh... what the heck is it based on then?
<br>
<p><em>&gt; I distrust the search for global optima and solutions that solve every
</em><br>
<em>&gt; problem.
</em><br>
<p>I think you must be confusing &quot;global optimum&quot; and &quot;perfection&quot;. 
<br>
Mistrusting the search for perfection is a common human heuristic, which
<br>
is often quite correct in an imperfect world where predictions of
<br>
perfection are more often produced by wishful thinking than by an unbiased
<br>
model of a real opportunity for perfection.  Of course, exporting this
<br>
heuristic into the superintelligent spaces is more often the product of
<br>
anthropomorphism than of principled analysis.
<br>
<p><em>&gt; The world is complex, changing and filled with adaptation, making
</em><br>
<em>&gt; any such absolutist solution futile, or worse, limiting. I prefer to view
</em><br>
<em>&gt; every proposed solution as partial and under revision as we learn more.
</em><br>
<p>Hm.  Well, your Sandberg Federation must be an absolutist solution too,
<br>
since it doesn't seem to allow for communities disobeying the laws of
<br>
physics.  And if everyone is going to obey the laws of physics anyway, why
<br>
not go the whole hog and use intelligent substrate to add nonviolation of
<br>
volition to the rule set?  You can get just as much variation running on
<br>
top of intelligent substrate as running on top of the laws of physics, and
<br>
it's a lot safer for everyone.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5251.html">Samantha Atkins: "Re: Dumping"</a>
<li><strong>Previous message:</strong> <a href="5249.html">Kai Becker: "Re: Dumping (was: Local Groups Wanted!)"</a>
<li><strong>In reply to:</strong> <a href="5246.html">Anders Sandberg: "Re: One Unity, Different Ideologies, all in the same universe"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5524.html">Samantha Atkins: "Re: One Unity, Different Ideologies, all in the same universe"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5250">[ date ]</a>
<a href="index.html#5250">[ thread ]</a>
<a href="subject.html#5250">[ subject ]</a>
<a href="author.html#5250">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Sat May 11 2002 - 17:44:31 MDT</em>
</em>
</small>
</body>
</html>
