<!-- received="Sat Nov 27 14:25:08 1999 MST" -->
<!-- sent="Sat, 27 Nov 1999 18:54:00 +0100" -->
<!-- name="D.den Otter" -->
<!-- email="otter@transtopia.org" -->
<!-- subject="Re: NANO: Custom molecules (gulp!)" -->
<!-- id="38401AB8.B2E@transtopia.org" -->
<!-- inreplyto="NANO: Custom molecules (gulp!)" -->
<!-- version=1.10, linesinbody=55 -->
<html><head><title>extropians: Re: NANO: Custom molecules (gulp!)</title>
<meta name=author content="D.den Otter">
<link rel=author rev=made href="mailto:otter@transtopia.org" title ="D.den Otter">
</head><body>
<h1>Re: NANO: Custom molecules (gulp!)</h1>
D.den Otter (<i>otter@transtopia.org</i>)<br>
<i>Sat, 27 Nov 1999 18:54:00 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2672">[ date ]</a><a href="index.html#2672">[ thread ]</a><a href="subject.html#2672">[ subject ]</a><a href="author.html#2672">[ author ]</a>
<!-- next="start" -->
<li><a href="2673.html">[ Next ]</a><a href="2671.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2658.html">Spike Jones</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<hr>
<br>
<i> &gt; From: Spike Jones &lt;spike66@ibm.net&gt;</i><br>

<p>
<a href="2658.html#2672qlink1"> &gt; Eliezer S. Yudkowsky wrote:</a><br>
<i> &gt;</i><br>
<i> &gt; &gt; <a href="http://www.eurekalert.org/releases/corn-bmo112399.html">http://www.eurekalert.org/releases/corn-bmo112399.html</a></i><br>
<i> &gt; &gt;</i><br>
<i> &gt; &gt; ...We're all going to die.</i><br>
<i> &gt;</i><br>
<i> &gt; Eliezer, let me make sure I understand your theory.  If humans</i><br>
<i> &gt; develop nanotech before the singularity, then the notion is that</i><br>
<i> &gt; it will get away from us by some means, and we all die from</i><br>
<i> &gt; grey goo?  But if the singularity comes first, then the resulting</i><br>
<i> &gt; AI develops nanotech and we [in some form] have a fighting</i><br>
<i> &gt; chance of survival?</i><br>

<p>
&lt;uuuh, I feel a rant coming up!&gt;

<p>
<a href="2658.html#2672qlink2"> &gt; The notion sounded absurd to me at first, but I must admit</a><br>
<i> &gt; it grows on one with consideration.  spike</i><br>

<p>
<a name="2681qlink1">No, no, NO!</a> For god's sake man, trust your instincts
<a name="2681qlink2">on this one. If an AI singularity comes first, and the
AI *is* fully rational, there is a *major* chance that it
will kill us all. I'm pretty sure it's 99.something</a> %

<p>
<a name="2688qlink1"><a name="2681qlink3">If nanotech comes first, on the other hand, we *will*
have a fighting chance, certainly if we start planning
a space program (as mentioned in the "SPACE:
How hard IS it to get off Earth?" thread)</a>
and related projects (see my Nov. 14 post in the
above thread for some suggestions) ASAP.</a>

<p>
You can run from nanotech, fight it and even
defeat it (eventually), but against a SI god you're
pretty much powerless. If the SI says "die",
you die. And that would be a real shame.

<p>
Fuck objective morality; it's all just mental
masturbation, a chimera. It's *definitely not*
worth dying for. NOTHING IN THE UNIVERSE
IS WORTH DYING FOR! By definition. Survival
is an eternal prerequisite, the No.1 sub-goal,
and "pleasure" is the (interim?) meaning of
life. Period. That which helps you to stay alive
and have fun is "good", that which hinders it
is "bad". That's the essence of rational ethics
based on enlightened self-interest. "Enlightened"
simply means "having foresight, thinking ahead" in
this context.

<p>
<a name="2681qlink4">And yes, there is a "better way", and it's called
synchronized uploading.
</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2673.html">[ Next ]</a><a href="2671.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2658.html">Spike Jones</a>
<!-- nextthread="start" -->
</ul>
</body></html>
