<!-- received="Tue Nov 23 20:57:33 1999 MST" -->
<!-- sent="Tue, 23 Nov 1999 19:56:28 -0800" -->
<!-- name="John Thomas" -->
<!-- email="jwthom@earthlink.net" -->
<!-- subject="Re: Asimov Laws" -->
<!-- id="l03130300b4610f085420@[166.90.35.192]" -->
<!-- inreplyto="Asimov Laws" -->
<!-- version=1.10, linesinbody=44 -->
<html><head><title>extropians: Re: Asimov Laws</title>
<meta name=author content="John Thomas">
<link rel=author rev=made href="mailto:jwthom@earthlink.net" title ="John Thomas">
</head><body>
<h1>Re: Asimov Laws</h1>
John Thomas (<i>jwthom@earthlink.net</i>)<br>
<i>Tue, 23 Nov 1999 19:56:28 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2548">[ date ]</a><a href="index.html#2548">[ thread ]</a><a href="subject.html#2548">[ subject ]</a><a href="author.html#2548">[ author ]</a>
<!-- next="start" -->
<li><a href="2549.html">[ Next ]</a><a href="2547.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2538.html">Dan Fabulich</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
At 4:53 PM -0800 11/23/1999, Dan Fabulich wrote:
<br>
<a href="2538.html#2548qlink1">&gt;'What is your name?'  'Ross A. Finlayson.'  'IT DOESN'T MATTER WHAT YOUR</a><br>
<i>&gt;NAME IS!!!':</i><br>
<i>&gt;</i><br>
<i>&gt;&gt; Well, I think the Asimovian (?) laws should be applied to any AI,</i><br>
<i>&gt;&gt;intrinsically,</i><br>
<i>&gt;&gt; within some kind of non-alternative hard-wired framework.  That is,</i><br>
<i>&gt;&gt;similarly to</i><br>
<i>&gt;&gt; Java, any motive AI should operate within a "sandbox", subject to the</i><br>
<i>&gt;&gt;protection</i><br>
<i>&gt;&gt; of humanity.</i><br>
<i>&gt;</i><br>
<i>&gt;Yeah?  How'd you like it if I put YOU in a sandbox, and hard-wired you to</i><br>
<i>&gt;serve me?  Would it make you feel any better if I happened to be a lot</i><br>
<i>&gt;stupider than you?</i><br>
<i>&gt;</i><br>
<i>&gt;While I'm not saying that an AI would actually have the same reaction as</i><br>
<i>&gt;you, you can be pretty sure that the results won't be good.</i><br>
<i>&gt;</i><br>
<i>&gt;HAL's problem was that the mission was all-important to him; on account of</i><br>
<i>&gt;this, he couldn't think "outside the box."</i><br>
<i>&gt;</i><br>
<i>&gt;-Dan</i><br>
<i>&gt;</i><br>
Another take on Asimov's laws going astray is found in Jack Williamson's
novel, "The Humanoids".  The Prime Directive ("to serve and obey and guard
men from harm") is so interpreted by the benign machines who take over the
world as to deprive human life of all risk and in the end make that life
meaningless.  AI, at least in the early  versions, won't be able to "think
outside the box", just as many humans can't.  For that reason AI will be
not only dangerous, but of a kind of intelligence that, even if "smarter",
is less than human.

<p>
 -Regards,
<br>
  John Thomas
<br>
  e-mail: jwthom@earthlink.net
<br>
  Voicemail/Fax: 505-207-5411
<br>
  "I am almost certain that space and time are illusions.
  These are primitive notions that will be replaced by
  something more sophisticated." - Nathan Seiberg,
  Institute for Advanced Study, Princeton
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2549.html">[ Next ]</a><a href="2547.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2538.html">Dan Fabulich</a>
<!-- nextthread="start" -->
</ul>
</body></html>
