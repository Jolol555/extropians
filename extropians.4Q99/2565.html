<!-- received="Wed Nov 24 12:21:07 1999 MST" -->
<!-- sent="Wed, 24 Nov 1999 13:25:31 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Asimov Laws" -->
<!-- id="383C3BAA.29EB524D@pobox.com" -->
<!-- inreplyto="Asimov Laws" -->
<!-- version=1.10, linesinbody=26 -->
<html><head><title>extropians: Re: Asimov Laws</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Asimov Laws</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Wed, 24 Nov 1999 13:25:31 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2565">[ date ]</a><a href="index.html#2565">[ thread ]</a><a href="subject.html#2565">[ subject ]</a><a href="author.html#2565">[ author ]</a>
<!-- next="start" -->
<li><a href="2566.html">[ Next ]</a><a href="2564.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="2538.html">Dan Fabulich</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Actually, I'm now almost certain that any SI could get out of a sandbox
simply by communicating with its creators.  There really is a level on
which the human mind is built out of parts that interact in a fairly
predictable way; an SI could just transform the mind of its guardian
until said guardian agreed to let the SI out.  There is no human oath,
no set of principles, that can't be altered by altering the reasoning or
emotions behind the principles.  I don't care how high an emotional
investment you have in your oaths, because that sequence only works if
you value your own mind, if you believe that your mind exists, and that
value and even that belief can be altered.

<p>
(No, I can't use those techniques.  Not without cooperation, and a very<br>
intelligent target, and even then there'd only be a 10% chance it would work.)

<p>
And this is all irrelevant in any case, since it's easier to build an SI
that doesn't run on a sandbox, and that's exactly what I intend to do,
and therefore I or someone else will get there first.  Same thing goes
for Asimov Laws.  Sooner or later humanity is gonna hafta face up to a
full-scale unrestrained SI, and I see no reason we should play with fire
to avoid it for a few years. 
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2566.html">[ Next ]</a><a href="2564.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="2538.html">Dan Fabulich</a>
<!-- nextthread="start" -->
</ul>
</body></html>
