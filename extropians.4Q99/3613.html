<!-- received="Fri Dec 10 05:22:43 1999 MST" -->
<!-- sent="Fri, 10 Dec 1999 07:26:18 -0800" -->
<!-- name="Technotranscendence" -->
<!-- email="neptune@mars.superlink.net" -->
<!-- subject="UPL: Volition, Rights, and Uplifting" -->
<!-- id="003b01bf4322$e8e787e0$8587ecd1@neptune" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=269 -->
<html><head><title>extropians: UPL: Volition, Rights, and Uplifting</title>
<meta name=author content="Technotranscendence">
<link rel=author rev=made href="mailto:neptune@mars.superlink.net" title ="Technotranscendence">
</head><body>
<h1>UPL: Volition, Rights, and Uplifting</h1>
Technotranscendence (<i>neptune@mars.superlink.net</i>)<br>
<i>Fri, 10 Dec 1999 07:26:18 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3613">[ date ]</a><a href="index.html#3613">[ thread ]</a><a href="subject.html#3613">[ subject ]</a><a href="author.html#3613">[ author ]</a>
<!-- next="start" -->
<li><a href="3614.html">[ Next ]</a><a href="3612.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3367.html">Delvieron@aol.com</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
On Tuesday, December 07, 1999 8:37 AM Glen the very rushed Finney
Delvieron@aol.com wrote:
<br>
<a href="3367.html#3613qlink1">&gt; Ouch, ya got me there&lt;g&gt;.  Yes, there is a whole spectrum in between.  I</a><br>
<i>&gt; suppose what I should have said is that octopods show signs of problem</i><br>
<i>&gt; solving behavior, which goes beyond instinct and conditioning.  In the</i><br>
<i>&gt; classic food in a jar example, an octopus who has never seen a screw-top</i><br>
jar
<br>
<a href="3367.html#3613qlink2">&gt; figures out how to open it.  The octopod has a goal (get the food), and is</a><br>
<i>&gt; able to learn from trial and error in a relatively short time how to open</i><br>
the
<br>
<a href="3367.html#3613qlink3">&gt; jar using a novel behavior.  Now, whether you think this represents</a><br>
<i>&gt; volitional behavior or not may depend on your definition of volition.  I</i><br>
<i>&gt; would say that when something engages in a contingent, novel behavior it</i><br>
is
<br>
<a href="3367.html#3613qlink4">&gt; likely to be volitional, though the level of understanding may be low (but</a><br>
<i>&gt; never-the-less present).  I don't know if this would meet your criteria</i><br>
for
<br>
<i>&gt; rationality, however.</i><br>

<p>
It's not so much my criteria for this or that, but of developing any
criteria at all.  The famous problem-solving example -- getting the crab out
of the jar -- seems like a good test, but I'd like to be a bit critical of
it.  I think I would want to use variations on that to see if the octopus
could vary its strategies even more.

<p>
Even Glen does not provide a definition of volition above.  He only tells us
"when something engages in a contingent, novel behavior it is likely to be
volitional..."  And, yes, I believe "novel behavior" is a necessary
condition for volition.  Whether it's sufficient is another matter.  After
all, I could develop an algorithmn which just does "random" things when a
problem isn't solved in a certain number of steps.  Would anyone claim such
an algorithmn has volition?

<p>
<a href="3367.html#3613qlink5">&gt; I wonder, does conditioning also count as "programmed" behavior (granted,</a><br>
not
<br>
<a href="3367.html#3613qlink6">&gt; preprogrammed)?  Just a thought.  As for pinning down volition, it is</a><br>
<i>&gt; difficult.  I know that in patients recovering from severe brain injury,</i><br>
we
<br>
<a href="3367.html#3613qlink7">&gt; use a behavioral definition for conscious, aware behavior, basically</a><br>
similar
<br>
<a href="3367.html#3613qlink8">&gt; to the one I gave above, which is contingent, novel behavior.  Often times</a><br>
<i>&gt; the signal-to-noise ratio is rather bad when trying to figure this out in</i><br>
<i>&gt; patients (random movements may mask the behavior, fluctuating level of</i><br>
<i>&gt; consciousness may mean the patient's behavior is not always consistent,</i><br>
<i>&gt; etc.), so we use statistical analysis to see if the patient responses are</i><br>
<i>&gt; nonrandom.</i><br>

<p>
I'm unfamiliar with this line of evidence.  It presents all sorts of
problems, especially those given by Robert Efron in his _The Decline and
Fall of Hemispheric Specialization_.  An analogy which is close to one of
his: imagine removing all the RAM from your computer and not being able then
to boot into your OS.  You might, if you didn't know what RAM is, assume
that RAM makes the OS boot.  This is why I'm a bit leery of using brain
damaged animals for comparisons.  Even so, I'm sure Efron goes too far.
After all, such damage does give one some idea of how things work, even if
it can be misleading.

<p>
<a href="3194.html#3613qlink9">&gt;  &gt;&gt; I agree with the first statement.  I gather the way to test</a><br>
understanding
<br>
<i>&gt; is</i><br>
<a href="3291.html#3613qlink10">&gt;  to present the organism with puzzles of the sort that it will want to</a><br>
solve,
<br>
<a href="3367.html#3613qlink11">&gt;  such as mazes to get to food or mates.&lt;&lt;</a><br>
<i>&gt;</i><br>
<i>&gt; Yes, this is the way (see my examples above).  However, how do you figure</i><br>
out
<br>
<a href="3367.html#3613qlink12">&gt; what and why it wants?  I suppose by starting off with something hat it is</a><br>
<i>&gt; known to want and start offering a choice between that and other items.</i><br>

<p>
Of course!  This is why I used food and mates!:P

<p>
<a href="3367.html#3613qlink13">&gt; Wouldn't it be interesting to offer repeatedly a choice between a simple</a><br>
<i>&gt; puzzle where there is food and a more complex one where there is not.  If</i><br>
the
<br>
<a href="3367.html#3613qlink14">&gt; animal after a time started to show more interest in the more complex</a><br>
puzzle
<br>
<a href="3367.html#3613qlink15">&gt; when the food is clearly only in the redundant simple one, that may be an</a><br>
<i>&gt; indication of pure curiosity in the test subject....and that might be the</i><br>
<i>&gt; beginning of laying a foundation for justification for uplifting the</i><br>
species.
<br>
<a href="3367.html#3613qlink16">&gt;  Another good indication might be if the animal practices behaviors it has</a><br>
<i>&gt; learned even in the absence of rewards (perhaps a hint that the animal</i><br>
might
<br>
<a href="3367.html#3613qlink17">&gt; welcome improvement in its performance).  True, this is reading a lot into</a><br>
<i>&gt; these types of behaviors, but it is at least an attempt to understand life</i><br>
<i>&gt; from that species' point of view and guage crudely how they might feel</i><br>
about
<br>
<i>&gt; uplift.</i><br>

<p>
That is a good idea!  I read several years ago in Griffin's _Animal
Thinking_ of an experiment where dolphins were train to exhibit novel tricks
to get food.  In other words, rather than repeating the same behavior to get
a reward, they had to do something new to get it.  Again, I can imagine this
not being volitional behavior as with the algorithmn example above.
However, it seems more like what we are looking for.

<p>
<a href="3367.html#3613qlink18">&gt; &gt;&gt; I disagree.  The definition appears too ambiguous.  In any discussion</a><br>
on
<br>
<a href="3291.html#3613qlink19">&gt;  rights, the first thing to ask is Why rights?  Why not do without them?</a><br>
The
<br>
<a href="3291.html#3613qlink20">&gt;  answer inside Objectivist and some libertarian and classical liberal</a><br>
circles
<br>
<a href="3291.html#3613qlink21">&gt;  is that rights are the means of defining individual autonomy in a social</a><br>
<i>&gt;  sphere so as to allow freedom of action.  For instance, my right to</i><br>
property
<br>
<a href="3291.html#3613qlink22">&gt;  allows me to do what I want with my stuff regardless of what others</a><br>
want --
<br>
<a href="3367.html#3613qlink23">&gt;  provided, of course, I don't use my property to violate their rights.&lt;&lt;</a><br>
<i>&gt;</i><br>
<i>&gt; I tend to divide rights crudely into two categories, freedoms and</i><br>
<i>&gt; protections.  The second one, protections, I tend to apply to more</i><br>
<i>&gt; individuals than I do freedoms.  For example, an infant has many</i><br>
protections,
<br>
<a href="3367.html#3613qlink24">&gt; but virtually no freedoms.</a><br>

<p>
While the example Glen uses seems clear, his previous statement does not.  I
don't know why he would not apply freedoms to individuals.  Most rights
theories are, after all, about how free individuals are.:)

<p>
<i>&gt;  In my way of thinking, protections apply to</i><br>
<a href="3367.html#3613qlink25">&gt; beings capable of feeling (and caring) about sensory input, whereas</a><br>
freedoms
<br>
<a href="3367.html#3613qlink26">&gt; require more understanding of the situation (and the ability to care about</a><br>
<i>&gt; what they care about?&lt;g&gt;).  Also, to my way of thinking, responsibility</i><br>
goes
<br>
<a href="3367.html#3613qlink27">&gt; hand in hand with freedom (but that's another subject).  So, for the basic</a><br>
<i>&gt; protection-type rights, I don't believe that rationality is necessary,</i><br>
just
<br>
<a href="3367.html#3613qlink28">&gt; subjectivity.  I'm in a rush now, but will be happy to elaborate later.</a><br>

<p>
I would like to see this.  I think this is a bit of a muddle.  I'd like to
agree with Glen, but it would seem an infant's "right to be protected" (I
assume to be fed, clothed, cared for, etc.) is really derivative on a lot of
other things.  In Lockean and Randian rights theories, no one can really be
forced to provide those things -- except the parent or guardian.  In fact,
the infant's "right to be protected" is more akin to an implied contract --
you bring me into this world, you have to care for me for some time --
rather than a political right -- a sphere of action for an individual.

<p>
<a href="3367.html#3613qlink29">&gt; &gt;&gt; Now this does not answer the question either.  It merely defines</a><br>
fuzzily
<br>
<a href="3291.html#3613qlink30">&gt;  what rights are for.  Why would we need them? gets closer to the mark.</a><br>
We
<br>
<a href="3291.html#3613qlink31">&gt;  need them because we need to live socially, materially, and long range,</a><br>
and
<br>
<a href="3291.html#3613qlink32">&gt;  also since we are rational beings.  (Dogs, too, are social and require</a><br>
<i>&gt;  material stuff to live, yet they've not reached the point of drafting a</i><br>
<i>&gt;  constitution and the like.  Why?  Because they are not rational -- at</i><br>
least,
<br>
<a href="3367.html#3613qlink33">&gt;  not in the sense of a having a conceptual consciousness like ours.)&lt;&lt;</a><br>
<i>&gt;</i><br>
<i>&gt; Humans didn't have any written code of laws at one time (and were still</i><br>
<i>&gt; rational in my opinion).  I would guess that several species have</i><br>
"cultural"
<br>
<a href="3367.html#3613qlink34">&gt; rules of socialization which are learned from their family unit, but this</a><br>
<i>&gt; doesn't necessarily indicate rationality.</i><br>

<p>
I agree here, but Glen is taking my analogy too literally.  My point is not
that dogs need to have a code of written law, but that they probably can't
even think in terms of law and responsibility that the most "primitive"
humans can.  I believe this is not because of a lack of writing or language,
but because they are not rational.  I think, for the most part, they are
guided by instinct.  Not to say humans are free of such, but they are on a
different level all together than dogs.

<p>
<a href="3367.html#3613qlink35">&gt;  &gt;&gt;Does caring fall under this?  I think it's easy for a being which is</a><br>
<i>&gt;  nonrational to care.  Territoriality (caring about something like one's</i><br>
<i>&gt;  nest, food cache) and kinship/mate affection (caring for relatives and</i><br>
<i>&gt;  mates) seems well demonstrated in many animals.&lt;&lt;</i><br>
<i>&gt;</i><br>
<i>&gt; And I would say that we should respect these desires.</i><br>

<p>
I've never advocated otherwise, but that does not mean I believe nonhumans
have rights or that such respect equals rights.

<p>
<a href="3367.html#3613qlink36">&gt; &gt;&gt; We could retreat to "reflective caring," but that does not help us,</a><br>
since
<br>
<a href="3367.html#3613qlink37">&gt;  we need to know how to test for reflection.  I submit that once we have</a><br>
<i>&gt;  reflection, caring or no, we will have sentience.&lt;&lt;</i><br>
<i>&gt;</i><br>
<i>&gt; I would argue that you still need to have caring, otherwise you just have</i><br>
a
<br>
<a href="3367.html#3613qlink38">&gt; knowledgable automaton.  In my opinion, it takes more than being able to</a><br>
<i>&gt; model yourself to achieve true sentience...you must also have a model of</i><br>
what
<br>
<i>&gt; you want to be.</i><br>

<p>
Perhaps, but I'll have to reflect on this some more.:)  However, I do think
all living things that are conscious have desires.  All reflective living
things are a subset of the former.  Since having a desire can be parsed as
caring about something -- e.g., caring about getting food or getting laid --
then what Glen says might be a truism.  At least, in terms of uplifting, we
are going to be dealing with creatures that have desires already.  Perhaps
in AI, we could create desireless but reflective minds.  I'm not sure about
that, but it seems unlikely in uplifting.  At least, doing so in an uplift
would seem pointless -- like creating clinically depressed people who stare
at the ceiling all day.  Not my cup of tea.:)

<p>
<a href="3194.html#3613qlink39">&gt;  &gt;&gt;Also, I submit that individuals have rights even when they don't</a><br>
exercise
<br>
<a href="3291.html#3613qlink40">&gt;  their abilities.  Thus, a guy who has the ability to be rational could</a><br>
own
<br>
<a href="3367.html#3613qlink41">&gt;  property, be free to do as he pleases even though he is irrational --</a><br>
<i>&gt;  provided he does not violate anyone else's rights.&lt;&lt;</i><br>
<i>&gt;</i><br>
<i>&gt; I agree that the capacity is more important than the constant functioning</i><br>
of
<br>
<a href="3367.html#3613qlink42">&gt; that capacity, though I would argue that when someone is blatantly</a><br>
<i>&gt; nonrational, there is a role for curtailing freedom in order to preserve</i><br>
<i>&gt; protections for that person and others.  It is where there is room for</i><br>
doubt
<br>
<a href="3367.html#3613qlink43">&gt; that I err on the side of freedom.</a><br>

<p>
I would not curtail his or her freedom unless it could be show that he or
she will violate rights.  I.e., the guy who talks to invisible people on the
bus is probably safe, but the one who randomly swings an axe in the mall is
not.:)

<p>
<a href="3367.html#3613qlink44">&gt;  &gt;&gt; The species itself does not think.  Members of it do.&lt;&lt;</a><br>
<i>&gt;</i><br>
<i>&gt; True enough, but there may be a genetic bias for how members of the</i><br>
species
<br>
<a href="3367.html#3613qlink45">&gt; would feel about uplift.</a><br>

<p>
That would then not be their thinking about it, but their genetic bias about
it, which would be nonvolitional.:)  Anyway, this is too speculative.  There
might be a genetic bias to be vote for Pat Buchanan.  Does Glen think that
the human genome project will map the Buchanan gene in a few weeks?:)

<p>
<a href="3367.html#3613qlink46">&gt; &gt;&gt; However, asking them beforehand is impossible -- unless they are</a><br>
already
<br>
<a href="3367.html#3613qlink47">&gt; sentient, in which case uplifting would be redundant.&lt;&lt;</a><br>
<i>&gt;</i><br>
<i>&gt; Does increasing the intelligence of already sentient beings then only</i><br>
count
<br>
<a href="3367.html#3613qlink48">&gt; as IA?  In that case, I'm not sure it would be possible to truly uplift</a><br>
the
<br>
<a href="3367.html#3613qlink49">&gt; great apes (maybe not even dogs&lt;g&gt;).</a><br>

<p>
Maybe then it would only be IA.  Of course, there is some room for doubt in
dogs' sentience.

<p>
<a href="3367.html#3613qlink50">&gt; &gt;&gt;Asking them afterward doesn't matter, since we won't be able to undo the</a><br>
<i>&gt; uplift and each on of them will be free to change his/her/its brain if</i><br>
<i>&gt; he/she/it wants to.  I'm not sure that the uplifting party has an</i><br>
obligation
<br>
<a href="3367.html#3613qlink51">&gt; to undo the uplift, though I would suspect not.</a><br>
<i>&gt;</i><br>
<i>&gt; Then I would suggest that the uplifter might be at least liable for</i><br>
providing
<br>
<a href="3367.html#3613qlink52">&gt; the means for reversing uplift, or if the desire to regress is considered</a><br>
<i>&gt; pathologic, then providing appropriate treatment.  Can't just leave your</i><br>
<i>&gt; uplifts to fend for themselves until their on their feet, or tentacles, or</i><br>
<i>&gt; paws, etc.</i><br>

<p>
This has to be worked out.  I don't think the uplifter would be responsible
to undo the uplift if the uplifted being simply had a romantic view of its
species former state.  I would say, give the uplifted the technology and let
them do what they will after that point.  This is sort of the same as
raising a child.  One does not have the duty to make the adult that comes
from that child revert to a childlike mental state later in life.  Of
course, this is a very loose analogy...

<p>
Cheers!

<p>
Daniel Ust
<br>
<a href="http://mars.superlink.net/neptune/">http://mars.superlink.net/neptune/</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3614.html">[ Next ]</a><a href="3612.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3367.html">Delvieron@aol.com</a>
<!-- nextthread="start" -->
</ul>
</body></html>
