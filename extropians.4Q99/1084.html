<!-- received="Thu Oct 21 08:26:31 1999 MST" -->
<!-- sent="Thu, 21 Oct 1999 15:28:45 +0100" -->
<!-- name="Rob Harris" -->
<!-- email="rob@hbinternet.co.uk" -->
<!-- subject="technophobes" -->
<!-- id="NDBBJEGIMDDABOBCCDCHOEBOCAAA.rob@hbinternet.co.uk" -->
<!-- inreplyto="199910211210.GAA31188@maxwell.kumo.com" -->
<!-- version=1.10, linesinbody=20 -->
<html><head><title>extropians: technophobes</title>
<meta name=author content="Rob Harris">
<link rel=author rev=made href="mailto:rob@hbinternet.co.uk" title ="Rob Harris">
</head><body>
<h1>technophobes</h1>
Rob Harris (<i>rob@hbinternet.co.uk</i>)<br>
<i>Thu, 21 Oct 1999 15:28:45 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1084">[ date ]</a><a href="index.html#1084">[ thread ]</a><a href="subject.html#1084">[ subject ]</a><a href="author.html#1084">[ author ]</a>
<!-- next="start" -->
<li><a href="1085.html">[ Next ]</a><a href="1083.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1079.html">Jim Fehlinger</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1115.html">Matt Gingell</a>
</ul>
<!-- body="start" -->

<p>
<a href="1079.html#1084qlink1">&gt;So this is the picture of the world in 2050.  A world in which</a><br>
<i>&gt;machines are dominant, where humans, animals after all, are</i><br>
<i>&gt;treated in a similar way to other animals.  Humans are kept</i><br>
<i>&gt;for their usefulness, and those who are not useful are removed.</i><br>
<i>&gt;Humans must do what they are told."</i><br>

<p>
<a name="1108qlink1">I realise the above is from a work of fiction, but what is it with all this
luddite drivel about AI wars all the time?</a> Isn't anyone capable of more than
3 second bursts of half-assed thought these days...?<a name="1115qlink1"> So who's going to
program the AI's with base motivations that involve concepts such as
<a name="1111qlink1">"dominance" and the wish to strive for it, then provide</a> the necessary
faculties/resources to do this? Not me or anyone sane, that's for sure.
</a>
Surely it would be easier for a mad scientist with a grudge against humanity
to launch all the world's nukes somehow, than to design and build a
self-sufficient large scale extermination machine??? Or perhaps it's the old
rogue vacuum cleaner scenario? "The machines got smart".... what a pile of
crap. The "the body can't live without the mind" spontaneous wounds bull
from The Matrix was more feasible.
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1085.html">[ Next ]</a><a href="1083.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1079.html">Jim Fehlinger</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1115.html">Matt Gingell</a>
</ul>
</body></html>
