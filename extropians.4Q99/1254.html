<!-- received="Sun Oct 24 00:24:46 1999 MST" -->
<!-- sent="Sat, 23 Oct 1999 23:24:44 -0700 (PDT)" -->
<!-- name="Robert J. Bradbury" -->
<!-- email="bradbury@www.aeiveos.com" -->
<!-- subject="Preventing AI Breakout [was Genetics, nannotechnology, and , programming]" -->
<!-- id="Pine.SV4.3.91.991023230340.4088A-100000@www.aeiveos.com" -->
<!-- inreplyto="19991024024802.25886.rocketmail@web115.yahoomail.com" -->
<!-- version=1.10, linesinbody=63 -->
<html><head><title>extropians: Preventing AI Breakout [was Genetics, nannotechnology, and , programming]</title>
<meta name=author content="Robert J. Bradbury">
<link rel=author rev=made href="mailto:bradbury@www.aeiveos.com" title ="Robert J. Bradbury">
</head><body>
<h1>Preventing AI Breakout [was Genetics, nannotechnology, and , programming]</h1>
Robert J. Bradbury (<i>bradbury@www.aeiveos.com</i>)<br>
<i>Sat, 23 Oct 1999 23:24:44 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1254">[ date ]</a><a href="index.html#1254">[ thread ]</a><a href="subject.html#1254">[ subject ]</a><a href="author.html#1254">[ author ]</a>
<!-- next="start" -->
<li><a href="1255.html">[ Next ]</a><a href="1253.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1248.html">Skye Howard</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1257.html">Damien Broderick</a>
</ul>
<!-- body="start" -->


<p>
On Sat, 23 Oct 1999, Skye Howard wrote:

<p>
<a name="1276qlink1"><a name="1257qlink1">I believe that Skye may have hit upon something every
interesting that partially solves a major problem
involving self-modifying AI that has plagued the
list and worried me greatly:

<p>
<a href="1248.html#1254qlink1">&gt;      It might be interesting if you could create an</a><br>
<i>&gt; artificial environment where you could test such</i><br>
<i>&gt; things. For example, if you had an artificial human</i><br>
<i>&gt; body existing on some kind of programmed</a> level, you</i><br>
<i>&gt; could instill these devices into it and see if all of</i><br>
<i>&gt; the simulated functions could continue... this would</i><br>
<i>&gt; be a ways beyond modern technology, though, because </i><br>
<i>&gt; an artificial computer model of a genome and the full</i><br>
<i>&gt; animal generated therefrom, not to mention more</i><br>
<i>&gt; processing power and memory space than a medium sized</i><br>
<i>&gt; country, might be necessary:)</i><br>
</a>

<p>
<a name="1259qlink1">The fundamental problem I fear with self-evolving AI
is the connection to the real world as we perceive it.
However,<a name="1267qlink1"> if a self-evolving AI is operating in a
simulation of the real world, then the problem becomes
much more tractable.  First, the changes take place
more slowly so we have a greater ability to monitor them.</a>
Second, if the program shows signs of modifying itself
into self-conscious sabotage of extra-environmental entities
it can be suspended/deleted.</a>

<p>
<a name="1259qlink2">The questions then become:
<br>
<a name="1276qlink2"><a name="1267qlink2"> (a) Can we guarantee that the AI never discovers it is running</a> on,
<pre>
<a name="1267qlink3">     and more importantly escape from,  a simulation machine?</a>
<a name="1267qlink4">     This goes back to the entire thread of whether we can detect *we*
     are running on a simulation or whether our reality is an illusion.</a></a></a>
<a name="1276qlink3"><a name="1259qlink3"> (b)<a name="1267qlink5"> How do we guarantee that everybody understands and
     adheres to the rules that self-evolving AIs are only
     allowed to exist in simulated worlds?</a>   {This is not
     dis-similar from the problem of how do we guarantee
     that petty dictators don't release bioweapons that
     in the process of killing us, come back to "bite" them.}</a>
</a>

</pre>
<p>
<a name="1265qlink1">I think this fundamentally comes down to a core extropian
principle involving rational thought.</a>  Rationale people
presumably seek to preserve themselves and do not undertake
operations that have a significant risk of their own death
in the process of killing others (in contrast to the
tradeoff of saving others lives at the risk of ones own life,
which may be quite rational depending on how you value the
lives involved).

<p>
<a name="1265qlink2">This devolves into 2 basic discussions:
<br>
  (a) whether an AI can discover it is running in a simulation?</a>
<a name="1265qlink3">  (b) whether people (the irrationals) who are willing to sacrifice
<pre>
      themselves can/will create non-simulation environments in
      which to evolve AIs.</a>

</pre>
<p>
<a name="1265qlink4">Many thanks to Skye for providing an interesting solution to
a thorny problem and apologies to the list if this has been
hashed through before.
</a>

<p>
Robert
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1255.html">[ Next ]</a><a href="1253.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1248.html">Skye Howard</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1257.html">Damien Broderick</a>
</ul>
</body></html>
