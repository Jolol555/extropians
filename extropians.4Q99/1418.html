<!-- received="Tue Oct 26 16:05:36 1999 MST" -->
<!-- sent="Tue, 26 Oct 1999 16:05:32 -0600" -->
<!-- name="Joseph Sterlynne" -->
<!-- email="vxs@mailandnews.com" -->
<!-- subject="Re: Preventing AI Breakout" -->
<!-- id="l03130304b43b99ce8121@[209.150.226.181]" -->
<!-- inreplyto="000001bf1f34$d9da3520$6701a8c0@HQDV1" -->
<!-- version=1.10, linesinbody=61 -->
<html><head><title>extropians: Re: Preventing AI Breakout</title>
<meta name=author content="Joseph Sterlynne">
<link rel=author rev=made href="mailto:vxs@mailandnews.com" title ="Joseph Sterlynne">
</head><body>
<h1>Re: Preventing AI Breakout</h1>
Joseph Sterlynne (<i>vxs@mailandnews.com</i>)<br>
<i>Tue, 26 Oct 1999 16:05:32 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1418">[ date ]</a><a href="index.html#1418">[ thread ]</a><a href="subject.html#1418">[ subject ]</a><a href="author.html#1418">[ author ]</a>
<!-- next="start" -->
<li><a href="1419.html">[ Next ]</a><a href="1417.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1360.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1437.html">Billy Brown</a>
</ul>
<!-- body="start" -->


<p>
<i>&gt; Billy Brown</i><br>

<p>
<a href="1360.html#1418qlink1">&gt;I think this whole discussion is wandering off into left field.  The</a><br>
<i>&gt;question here is whether you can control sentient AI (and eventually SI) by</i><br>
<i>&gt;running it in a virtual world instead of letting it interact with the real</i><br>
<i>&gt;one.</i><br>

<p>
Thank you for attempting to maintain focus; it can be easy for main points
and assumptions in discussions (here and elsewhere) to simply get lost.
Which leads to a related problem: if an idea is run in a virtual discussion
can it find its way out?

<p>
<a href="1360.html#1418qlink2">&gt;1) [. . .] The question of whether the AI could think its way out of a</a><br>
<i>&gt;perfect simulation is irrelevant - the real issue is whether you can get</i><br>
<i>&gt;the defect count low enough to be even halfway convincing.</i><br>

<p>
<a name="1437qlink1">If it were raised entirely in even a defective environment it might never
know what would constitute evidence that it was constructed.</a>

<p>
<a href="1360.html#1418qlink3">&gt;2) In order to get useful work out of an AI you need to tell it about the</a><br>
<i>&gt;real world.  That means that for any commercial application the AI will know</i><br>
<i>&gt;all about its real situation, because you'll have to tell it.</i><br>

<p>
<a name="1437qlink2">Not necessarily.  You could (given, we are assuming, adequate resources)
generate a universe for it which is not dissimilar to ours.</a>  The simulation
<a name="1437qlink3">contains the same target problem as the higher-level universe.  The
difference is that the AI is just not directly connected to our world,
which means that it may not know "real" people, places, et cetera;
therefore it will not and cannot attempt to interfere with those things.
The lines out are blocked.</a>

<p>
<a href="1360.html#1418qlink4">&gt;3) [. . .Y]ou are either going to build things that it designs for you, or</a><br>
<i>&gt;follow advice that it gives you, or maybe even (god forbid!) let it write</i><br>
<i>&gt;software for you.  Whichever way you go, this means that the AIs will get</i><br>
<i>&gt;lots and lots of chances to try to break out.</i><br>

<p>
<a name="1437qlink4">If we are gods of the simulation, we should be more or less omniscient.</a>  We
<a name="1437qlink5">could observe the AI's thinking and results and place agents (uploaded
humans, VR-mediated humans, ostensibly-inanimate objects, and so</a> on) within
<a name="1437qlink6">the simulation which would guide the AI's projects.  That aside from more
direct manipulation, which is really what should be available if you had
someone's code right in front of you.  But I guess that if a very</a> clever AI
<a name="1437qlink7">suspects that it is in a simulation and tries to sneak something into a
design there could be undesirable effects.</a>

<p>
<a href="1360.html#1418qlink5">&gt;4) Suppose that VR containment works great for AI 1.0 [. . . and] years</a><br>
<i>&gt;after that you have millions of copies of AI 6.0 (IQ 300, x10,000 time</i><br>
<i>&gt;rate) running on desktop computers.  The longer containment works the</i><br>
<i>&gt;harder it is to maintain, and the worse it will be when it finally gets</i><br>
<i>&gt;breached.</i><br>

<p>
<a name="1437qlink8">I'm not sure I understand the concern in this context.  Why is it the case
that "[t]he longer containment works the harder it is to maintain"?  You
seem to imply that the AIs have the capability to outthink the containment
technology.  And that is in a way just what we were originally debating.
It could be that an AI will never realize its situation or do anything
about it if it did regardless of its intelligence.
</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1419.html">[ Next ]</a><a href="1417.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1360.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1437.html">Billy Brown</a>
</ul>
</body></html>
