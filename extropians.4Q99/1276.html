<!-- received="Sun Oct 24 12:04:22 1999 MST" -->
<!-- sent="Sun, 24 Oct 1999 11:03:46 -0700" -->
<!-- name="hal@finney.org" -->
<!-- email="hal@finney.org" -->
<!-- subject="Re: Preventing AI Breakout [was Genetics, nannotechnology, and , programming]" -->
<!-- id="199910241803.LAA23328@finney.org" -->
<!-- inreplyto="Preventing AI Breakout [was Genetics, nannotechnology, and , programming]" -->
<!-- version=1.10, linesinbody=86 -->
<html><head><title>extropians: Re: Preventing AI Breakout [was Genetics, nannotechnology, and , programming]</title>
<meta name=author content="hal@finney.org">
<link rel=author rev=made href="mailto:hal@finney.org" title ="hal@finney.org">
</head><body>
<h1>Re: Preventing AI Breakout [was Genetics, nannotechnology, and , programming]</h1>
<i>hal@finney.org</i><br>
<i>Sun, 24 Oct 1999 11:03:46 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1276">[ date ]</a><a href="index.html#1276">[ thread ]</a><a href="subject.html#1276">[ subject ]</a><a href="author.html#1276">[ author ]</a>
<!-- next="start" -->
<li><a href="1277.html">[ Next ]</a><a href="1275.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1254.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1285.html">Joseph Sterlynne</a>
</ul>
<!-- body="start" -->

<p>
Robert J. Bradbury, &lt;bradbury@www.aeiveos.com&gt;, writes:
<br>
<a href="1254.html#1276qlink1">&gt; I believe that Skye may have hit upon something every</a><br>
<i>&gt; interesting that partially solves a major problem</i><br>
<i>&gt; involving self-modifying AI that has plagued the</i><br>
<i>&gt; list and worried me greatly:</i><br>
<i>&gt;</i><br>
<i>&gt; &gt;      It might be interesting if you could create an</i><br>
<i>&gt; &gt; artificial environment where you could test such</i><br>
<i>&gt; &gt; things. For example, if you had an artificial human</i><br>
<i>&gt; &gt; body existing on some kind of programmed level, you</i><br>
<i>&gt; &gt; could instill these devices into it and see if all of</i><br>
<i>&gt; &gt; the simulated functions could continue... this would</i><br>
<i>&gt; &gt; be a ways beyond modern technology, though, because </i><br>
<i>&gt; &gt; an artificial computer model of a genome and the full</i><br>
<i>&gt; &gt; animal generated therefrom, not to mention more</i><br>
<i>&gt; &gt; processing power and memory space than a medium sized</i><br>
<i>&gt; &gt; country, might be necessary:)</i><br>

<p>
In the novel Cassini Division, by Ken MacLeod, something similar to this
is done.  Super-advanced life forms have somehow been kept or captured
by humans and are kept in storage, run only in simulated environments
when it is necessary to get some information from them.  In the novel
this appears to have been at least moderately successful.  The humans
have gotten some high technology from the AIs.

<p>
In Greg Bear's novel Eon, a super-intelligent enemy Jart is kept in
a simulated system, however it knows techniques to escape from any
simulation into a higher level system.  It ends up getting out of its
prison and taking over its captor.

<p>
So this idea has been explored in fiction, with opposite outcomes.
<a name="1285qlink1">I suspect that it is possible in principle to keep an AI within a
simulation, but in practice it may be able to notice aspects of the
environment which you have imposed ad hoc and which don't seem "natural".</a>

<p>
<a href="1254.html#1276qlink2">&gt;  (a) Can we guarantee that the AI never discovers it is running on,</a><br>
<i>&gt;      and more importantly escape from,  a simulation machine?</i><br>
<i>&gt;      This goes back to the entire thread of whether we can detect *we*</i><br>
<i>&gt;      are running on a simulation or whether our reality is an illusion.</i><br>

<p>
<a name="1285qlink2">I think if you could actually set up the AI so that it actually evolved
from a starting environment which did not have much more ad hoc features
than our own, you might be able to fool</a> it.  However that might take
billions of years of simulated evolution.  Any<a name="1285qlink3"> shortcuts you take might
</a>
be noticeable by a super-smart AI.<a name="1285qlink4">  Even if you succeeded, it's not
clear what use an AI would be which lived in a world which is probably
</a>
so dissimilar to our own.

<p>
<a name="1285qlink5">In our own case, we do observe a great many coincidences which suggest
that the universe is specially set up for life to be possible.</a>  However
<a name="1285qlink6">few people conclude from this that we are running in an artificially
created simulation.  (The people who do, we call religious, and apparently
we are supposed to bash them.  Bash!  Bash!)</a>

<p>
<a href="1254.html#1276qlink3">&gt;  (b) How do we guarantee that everybody understands and</a><br>
<i>&gt;      adheres to the rules that self-evolving AIs are only</i><br>
<i>&gt;      allowed to exist in simulated worlds?   {This is not</i><br>
<i>&gt;      dis-similar from the problem of how do we guarantee</i><br>
<i>&gt;      that petty dictators don't release bioweapons that</i><br>
<i>&gt;      in the process of killing us, come back to "bite" them.}</i><br>

<p>
I view this as similar to many related ethical issues which will arise
when it becomes possible to create intelligent/conscious simulated
life forms.  Besides the question of creating dangerous life forms,
there is the problem of creating unhappy ones, of creating slaves and
using coercion against intelligent beings.

<p>
How do we guarantee today that people treat their children well and
don't raise them to be sociopaths?  Well, some people do mistreat their
children, and about the only way to catch them is to notice that the
children have something wrong with them when they interact with others.
If children are kept inside all the time and no one knows they exist,
severe mistreatment is possible, and does happen occasionally.

<p>
I don't see any way to enforce such provisions short of some kind of
all pervasive surveillance, which invites a great many abuses of its own.

<p>
Furthermore I don't agree that this trick of running the AI in a
simulation is a panacea.  You're running the AI for a purpose, probably
for a reason related to solving your own problems.  It may be giving
you advice about how to live your life for maximum advantage.  In order
to be effective you'll have to give it so much information about the
outside world that it would basically know the whole story anyway.

<p>
Hal
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1277.html">[ Next ]</a><a href="1275.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1254.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1285.html">Joseph Sterlynne</a>
</ul>
</body></html>
