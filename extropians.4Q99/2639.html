<!-- received="Fri Nov 26 11:52:04 1999 MST" -->
<!-- sent="Fri, 26 Nov 1999 10:52:00 -0800 (PST)" -->
<!-- name="Robert J. Bradbury" -->
<!-- email="bradbury@www.aeiveos.com" -->
<!-- subject="Re: SOC: Tractatus Theopoliticus (was: Is vs. Ought)" -->
<!-- id="Pine.UW2.4.20.9911260903300.22813-100000@www.aeiveos.com" -->
<!-- inreplyto="199911251847.KAA20670@finney.org" -->
<!-- version=1.10, linesinbody=73 -->
<html><head><title>extropians: Re: SOC: Tractatus Theopoliticus (was: Is vs. Ought)</title>
<meta name=author content="Robert J. Bradbury">
<link rel=author rev=made href="mailto:bradbury@www.aeiveos.com" title ="Robert J. Bradbury">
</head><body>
<h1>Re: SOC: Tractatus Theopoliticus (was: Is vs. Ought)</h1>
Robert J. Bradbury (<i>bradbury@www.aeiveos.com</i>)<br>
<i>Fri, 26 Nov 1999 10:52:00 -0800 (PST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2639">[ date ]</a><a href="index.html#2639">[ thread ]</a><a href="subject.html#2639">[ subject ]</a><a href="author.html#2639">[ author ]</a>
<!-- next="start" -->
<li><a href="2640.html">[ Next ]</a><a href="2638.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2601.html">hal@finney.org</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->



<p>
On Thu, 25 Nov 1999 hal@finney.org wrote:

<p>
<i>&gt; </i><br>
<a href="2601.html#2639qlink1">&gt; I'm not sure if his argument works if there is no such thing as Absolute</a><br>
<i>&gt; Morality.  In that case it seems that there is a risk that SIs will</i><br>
<i>&gt; develop their own goals (just as we do) and that their actions will not</i><br>
<i>&gt; be beneficial to the human race.</i><br>
<i>&gt;</i><br>
<i>&gt; The worst outcome would be if the SIs are programmed by Eliezer to have</i><br>
<i>&gt; as their only goal the search for the Holy Grail, that is, Absolute</i><br>
<i>&gt; Morality.  However, smart as they are, they still haven't found it.</i><br>
<i>&gt; They have to be smarter.  And to do that they have to turn all available</i><br>
<i>&gt; mass into SI computational elements, which means, regrettably, wiping out</i><br>
<i>&gt; the human race.</i><br>

<p>
We can use some assumptions and observations to put some constraints
on this.

<p>
  If we assume that current theories of physics (e.g. big bang,
  nucelosynthesis, stellar evolution, etc.) are relatively correct
  then there is "some" observational evidence that SIs exist.

<p>
  If SIs exist and have existed for long enough to have utilized
  our resources (probable) and have not done so, then either:
   (a) they have no interest in doing so (benefit of omission); or
   (b) there may be a moral principle guiding them not to do so
<p>
       (benefit of commission).

<p>
So, at least currently things seem to be going in our favor.
Since we can get to their level within 50 years, at that point
it is more probable that we become part of the community or at
least have a fighting chance of defending ourselves should they
suddenly take an interest in our resource base.

<p>
If instead, we assume that astrophysics needs a lot of adjustment,
there are no SIs and we are first, then we only need worry about our
own failure to chart a proper course of development.

<p>
<i>&gt; </i><br>
<a href="2601.html#2639qlink2">&gt; ... Then, at the end of a millenia-long development effort</a><br>
<i>&gt; that consumes half the galaxy and reaches realms of abstraction we can't</i><br>
<i>&gt; begin to imagine, they finally decide that their is no Absolute Morality.</i><br>

<p>
To consume half the galaxy is going to take a minimum of 100,000 years,
probably an order of magnitude more, unless it is a concerted effort
by a preexisting population of galaxy enveloping SIs.

<p>
<a href="2601.html#2639qlink3">&gt; So they all commit suicide.  Oops.</a><br>

<p>
Thats a bit pessimistic and unless programmed in seems doubtful.
How about -- If no theory-of-absolute-morality-exists then
go play for a trillion years?

<p>
<i>&gt; </i><br>
<a href="2601.html#2639qlink4">&gt; From the point of view of those of us who don't believe in Absolute</a><br>
<i>&gt; Morality, Eliezer's program amounts to building an unachievable goal</i><br>
<i>&gt; into the SIs, a highly dangerous proposition and one we might well oppose.</i><br>
<i>&gt; </i><br>
Before we go rushing off building nanobots to solve this problem
(gotta get a DNA sample from Eliezer first, anyone want to take
this on?) perhaps we should wait and see if there are or are not
SIs present in the universe.  Either way, we would need to do some
serious thinking on the present/not-present implications.  Present
could mean the A.M. is a benevolent one, not present could mean
there is an escape hatch for of our universe.  Both deserve
due consideration.

<p>
Robert
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2640.html">[ Next ]</a><a href="2638.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2601.html">hal@finney.org</a>
<!-- nextthread="start" -->
</ul>
</body></html>
