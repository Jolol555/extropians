<!-- received="Mon Oct 25 06:33:36 1999 MST" -->
<!-- sent="Mon, 25 Oct 1999 05:33:33 -0700 (PDT)" -->
<!-- name="Robert J. Bradbury" -->
<!-- email="bradbury@www.aeiveos.com" -->
<!-- subject="Re: Preventing AI Breakout [was Genetics, nannotechnology, and ,programming]" -->
<!-- id="Pine.SV4.3.91.991025051133.12093A-100000@www.aeiveos.com" -->
<!-- inreplyto="3813D7F9.E038DEF@pobox.com" -->
<!-- version=1.10, linesinbody=62 -->
<html><head><title>extropians: Re: Preventing AI Breakout [was Genetics, nannotechnology, and ,programming]</title>
<meta name=author content="Robert J. Bradbury">
<link rel=author rev=made href="mailto:bradbury@www.aeiveos.com" title ="Robert J. Bradbury">
</head><body>
<h1>Re: Preventing AI Breakout [was Genetics, nannotechnology, and ,programming]</h1>
Robert J. Bradbury (<i>bradbury@www.aeiveos.com</i>)<br>
<i>Mon, 25 Oct 1999 05:33:33 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1304">[ date ]</a><a href="index.html#1304">[ thread ]</a><a href="subject.html#1304">[ subject ]</a><a href="author.html#1304">[ author ]</a>
<!-- next="start" -->
<li><a href="1305.html">[ Next ]</a><a href="1303.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1290.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1317.html">Matt Gingell</a>
</ul>
<!-- body="start" -->



<p>
On Sun, 24 Oct 1999, Eliezer S. Yudkowsky wrote:

<p>
<a href="1290.html#1304qlink1">&gt; Joseph Sterlynne wrote:</a><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; It seems that everyone is confident that an AI of sufficient intelligence</i><br>
<i>&gt; &gt; will be able to discover that it exists within a simulation.</i><br>
  [snip]
<br>
<i>&gt; </i><br>
<a href="1290.html#1304qlink2">&gt; Maybe it'd accept the laws of physics, if it wasn't smart enough to</a><br>
<i>&gt; engage in a-priori ontological reasoning.  But how is the AI supposed to</i><br>
<i>&gt; accept itself?  We all know that the human body and human mind are the</i><br>
<i>&gt; result of evolution, right?  Conscious design would be just as obvious</i><br>
<i>&gt; to the AI.</i><br>

<p>
<a name="1317qlink1">Ah, but bottom-up AI (which is what we are talking about!) involves
a process of "scrambling" the code and selecting the best results.
Now, we as humans are slowly decoding the code and while we may
never have a perfect picture (because many of the steps are "lost")
we *will* be able to have a highly probable set of mutations, chromosome
translocations, etc. that lead to the rise of all existing species.</a>

<p>
I very much doubt that in this process we are going to "discover"
someplace in our evolution, someone "tweaked" the code?  Similarly
how would we ever "discover" that an alien SI "tweaked" the orbit
of a comet and sent it crashing into Earth 65 million years ago?
So long as the people controlling the simulation make the tweaks
in a way "consistent" with the level of noise or chaos in the
system, then it is going to be pretty difficult for the AI to
discover the simulation.  Similarly if the simulation environment
is randomly generated in a manner consistent with the physical
laws of the simulation, I don't see how you can discover that
the simulation was artificial.

<p>
I look around us and see something like the speed of light and
say "now why does light have to travel at that speed"?  I don't
see all of our brilliant physicists suggesting any experiment
that will demonstrate that that speed was arbitrarily set by
the simulation controller.  Hell, the simulation controller may
have simply selected that speed at random.  Unless you can make
an argument that the AI can discover simply by reasoning alone
that the speed of light is "rigged", I don't think it can get
out of the box.

<p>
The arguments thus far seem to imply that once the AI is much
smarter than us it can argue its way out.  But the arguments
seem to rest on the premise that the AI gets "big" relative
to the size of the box (and can recognize the flaws in its matrix).
Interestingly, that argument works as well for us, that when we
get "big" relative to our box (know all the physical laws,
control large amounts of the matter and energy, etc.) that
we will discover the ways our universe is "rigged" and start
to talk our way out of the box.

<p>
If that turns out to be true, then that makes me happy about
our future 10^100 years from now when this universe starts
to get cold and dark.

<p>
Robert
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1305.html">[ Next ]</a><a href="1303.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1290.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1317.html">Matt Gingell</a>
</ul>
</body></html>
