<!-- received="Wed Nov 24 12:45:21 1999 MST" -->
<!-- sent="Wed, 24 Nov 1999 13:49:49 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: AI" -->
<!-- id="383C415A.81DAA2E5@pobox.com" -->
<!-- inreplyto="AI" -->
<!-- version=1.10, linesinbody=33 -->
<html><head><title>extropians: Re: AI</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: AI</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Wed, 24 Nov 1999 13:49:49 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2569">[ date ]</a><a href="index.html#2569">[ thread ]</a><a href="subject.html#2569">[ subject ]</a><a href="author.html#2569">[ author ]</a>
<!-- next="start" -->
<li><a href="2570.html">[ Next ]</a><a href="2568.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2564.html">Rob Harris</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Rob Harris wrote:
<br>
<i>&gt; </i><br>
<a href="2564.html#2569qlink1">&gt; I've posted this same point a million times, but I'm going to do it again</a><br>
<i>&gt; anyway - cos it's not getting through. When you build a system to perform a</i><br>
<i>&gt; certain task, you have to tell it what to do - not what NOT to do. There is</i><br>
<i>&gt; nothing that does everything and has to be constrained down to a set task.</i><br>

<p>
Hear, hear!  This "general motive" fallacy is almost precisely the
complement of the "general reasoning" fallacy that prevented AI from
investigating domain-specific cognition for decades.

<p>
<a href="2564.html#2569qlink2">&gt; Some talk of "seed AI" becoming a self aware nemesis of humanity. Crap. You</a><br>
<i>&gt; see, the idea of "seed AI" is analogous with the evolution of life itself.</i><br>

<p>
<a name="2632qlink1">Okay, I gotta dispute this.  I was the person who invented the term
"seed AI".</a>  *I* certainly am not worrying about it becoming a nemesis of
humanity, and the whole *point* of seed AI is to attain transhuman
intelligence, which requires self-awareness (in the reflexive
self-modeling, not qualia-bearing sense of the word).

<p>
<a name="2632qlink2">A seed AI is an AI that understands its own source code, and is capable
</a>
of rewriting that source code, and of rewriting its own architecture. 
The idea is that the AI redesigns itself to a higher level of
intelligence, and then re-redesigns itself with that new intelligence,
and so on, until the AI reaches either (1) the limits of available
hardware or (2) the intelligence required to create "rapid
infrastructure", i.e. nanotechnology.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2570.html">[ Next ]</a><a href="2568.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2564.html">Rob Harris</a>
<!-- nextthread="start" -->
</ul>
</body></html>
