<!-- received="Tue Nov 23 16:37:39 1999 MST" -->
<!-- sent="Tue, 23 Nov 1999 17:37:27 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@transcient.com" -->
<!-- subject="RE: Why Would Aliens Hide?" -->
<!-- id="000101bf360b$b48a5cd0$6701a8c0@HQDV1" -->
<!-- inreplyto="Pine.UW2.4.20.9911230355130.2733-100000@www.aeiveos.com" -->
<!-- version=1.10, linesinbody=25 -->
<html><head><title>extropians: RE: Why Would Aliens Hide?</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@transcient.com" title ="Billy Brown">
</head><body>
<h1>RE: Why Would Aliens Hide?</h1>
Billy Brown (<i>bbrown@transcient.com</i>)<br>
<i>Tue, 23 Nov 1999 17:37:27 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2536">[ date ]</a><a href="index.html#2536">[ thread ]</a><a href="subject.html#2536">[ subject ]</a><a href="author.html#2536">[ author ]</a>
<!-- next="start" -->
<li><a href="2537.html">[ Next ]</a><a href="2535.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2517.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2287.html">Robin Hanson</a>
</ul>
<!-- body="start" -->

<p>
Robert J. Bradbury wrote:
<br>
<a href="2517.html#2536qlink1">&gt; It may be unclear to others the problem I'm trying to solve.  Since</a><br>
<i>&gt; Extro3, I've been wrestling with the sole question of:</i><br>
<i>&gt;   "What are the limits to longevity?"</i><br>
<i>&gt; or, put another way (more related to the biology of aging):</i><br>
<i>&gt;   "How small can you make your hazard function?"</i><br>
<i>&gt; or, perhaps in more economic terms:</i><br>
<i>&gt;   "When does the cost of reducing your hazard function exceed the</i><br>
<i>&gt;    benefit in increased longevity derived from such a reduction?"</i><br>

<p>
OK, fair enough.

<p>
I think you've made some good points on this topic in the past.  However, I
don't think that you've shown that this is an issue that is likely to
preoccupy an SI.  After all, it seems quite plausible that an SI could be
essentially immune to any sort of natural death (short of the end of the
universe, at least).  There are very few natural phenomena that injure such
an entity, and I can't think of one that would not be easily predictable
with a trivial expenditure of effort.  What sorts of dangers do you see an
SI being concerned about?

<p>
Billy Brown, MCSE+I
<br>
bbrown@transcient.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2537.html">[ Next ]</a><a href="2535.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2517.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2287.html">Robin Hanson</a>
</ul>
</body></html>
