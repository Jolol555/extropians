<!-- received="Sun Nov 28 10:04:51 1999 MST" -->
<!-- sent="Sun, 28 Nov 1999 17:58:01 +0100" -->
<!-- name="D.den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: NANO: Custom molecules (gulp!)" -->
<!-- id="38415F19.7C@geocities.com" -->
<!-- inreplyto="NANO: Custom molecules (gulp!)" -->
<!-- version=1.10, linesinbody=141 -->
<html><head><title>extropians: Re: NANO: Custom molecules (gulp!)</title>
<meta name=author content="D.den Otter">
<link rel=author rev=made href="mailto:neosapient@geocities.com" title ="D.den Otter">
</head><body>
<h1>Re: NANO: Custom molecules (gulp!)</h1>
D.den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Sun, 28 Nov 1999 17:58:01 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2700">[ date ]</a><a href="index.html#2700">[ thread ]</a><a href="subject.html#2700">[ subject ]</a><a href="author.html#2700">[ author ]</a>
<!-- next="start" -->
<li><a href="2701.html">[ Next ]</a><a href="2699.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2681.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<hr>
<br>
<i>&gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</i><br>
<a href="2681.html#2700qlink1">&gt; "D.den Otter" wrote:</a><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; No, no, NO!</i><br>
<i>&gt; </i><br>
<i>&gt; Join us, Spike... don't be afraid...</i><br>

<p>
Siren song...
 
<p>
<a href="2681.html#2700qlink2">&gt; &gt; For god's sake man, trust your instincts</a><br>
<i>&gt; &gt; on this one. If an AI singularity comes first, and the</i><br>
<i>&gt; &gt; AI *is* fully rational, there is a *major* chance that it</i><br>
<i>&gt; &gt; will kill us all. I'm pretty sure it's 99.something %</i><br>
<i>&gt; </i><br>
<i>&gt; Let's apply the Bayesian Probability Theorem.  If, speaking about an</i><br>
<i>&gt; event on which there is great controversy and no access to immediate</i><br>
<i>&gt; evidence, Human A says vis theory is 70% likely to be right and Human B</i><br>
<i>&gt; says vis theory is 99% likely to be right, which one is more likely to</i><br>
<i>&gt; be totally wrong?</i><br>

<p>
<a name="2722qlink1">My estimate is based on the very reasonable assumption that a 
SI wouldn't need anyone else (as the reader may recall we've 
discussed this before and Eliezer was in full agreement back 
then), and wouldn't be bound by redundant evolutionary 
adaptations such as altruism.</a> Add to that the fact that 
<a name="2722qlink2">humans, if allowed to continue freely with their development 
after the SI has ascended, would most likely create and/or 
become superintelligences (i.e. competion for resources and 
a potential threat),</a> and you have a pretty strong argument 
<a name="2722qlink3">for extinction. Now, where does that 70% figure come from??</a>
 
<p>
<a name="2722qlink4"><a href="2681.html#2700qlink3">&gt; Trust me:  I don't think I'm infallible.</a><br>

<p>
But nonetheless you are prepared to act as if you *were* 
infallible...The moment you activate your ASI Golem, you 
and you alone will have passed judgement on the world, 
using your finite wisdom.</a>

<p>
<a href="2681.html#2700qlink4">&gt; &gt; If nanotech comes first, on the other hand, we *will*</a><br>
<i>&gt; &gt; have a fighting chance, certainly if we start planning</i><br>
<i>&gt; &gt; a space program (as mentioned in the "SPACE:</i><br>
<i>&gt; &gt; How hard IS it to get off Earth?" thread)</i><br>
<i>&gt; &gt; and related projects (see my Nov. 14 post in the</i><br>
<i>&gt; &gt; above thread for some suggestions) ASAP.</i><br>
<i>&gt; </i><br>
<i>&gt; Maybe.  But if humanity survives nanotech, sooner or later it'll still</i><br>
<i>&gt; come face-to-face with a greater-than-human intelligence.  As you admit.</i><br>

<p>
<a name="2722qlink5">Of course. But, so what? The primary aim of the ascension 
initiative isn't to save "humanity", but to save *oneself*. 
And don't even try to get sanctimonious on me, Eliezer, as 
saving humanity isn't your primary concern either.</a> Let's 
forget saving the world for a minute, ok, we're talking 
about naked survival here (which may not matter to you, 
but it certainly matters to me and a whole lot of others).
 
<p>
<a href="2681.html#2700qlink5">&gt; &gt; [Wacky rant on "why subjective morality is the objective truth" deleted.]</a><br>

<p>
Rant yes, wacky my ass! Morality is always subjective,
because it only exists in the minds of subjective creatures. 
As there is no objective creature, there can be no objective 
morality. Objective truth (reality), on the other hand, is 
quite real, but not very relevant unless placed in the context 
of some subjective creature's goal system. 

<p>
<a name="2722qlink6">Your obsession with "the Objective" is, IMHO, essentially
religious in nature and has little to do with common sense. 
<a name="2754qlink1">The very fact that you refuse to give survival its rightful 
(top) place indicates that there is a serious flaw in your 
logic department.</a> Usually I don't bother to argue with the 
</a>
religious, but since you are a potentially ASI-spawning genius 
*and* apparently have a considerable influence on many list 
members, I don't really have a choice. 

<p>
<a href="2681.html#2700qlink6">&gt; &gt; And yes, there is a "better way", and it's called</a><br>
<i>&gt; &gt; synchronized uploading.</i><br>
<i>&gt; </i><br>
<i>&gt; I wouldn't trust den Otter on this one, Spike.  </i><br>

<p>
<a name="2722qlink7">Of course not. Trust no-one, Spike, and certainly not Eliezer, 
<a name="2707qlink1">who values the Singularity more than your, mine or anyone else's
</a>existence. Or maybe you *can* "trust" us, as we're both pretty
honest about what could happen. Both our motivations (and those 
of everyone else) are essentially selfish of course; the only 
real difference is that Eliezer is selfishly trying to accomplish
something which probably isn't in his enlightened, rational
("objective") self-interest.</a>

<p>
<i>&gt; Long before he started</i><br>
<a href="2681.html#2700qlink7">&gt; talking about "synchronized uploading" he believed that only one Power</a><br>
<i>&gt; could survive, and he plans to be it.  </i><br>

<p>
<a name="2722qlink8">Only one Power is (obviously) the only stable,</a> and therefore
"ideal", situation from that one Power's perspective. Yes, of
course I'd like to be the one, but...if the choice is (nearcertain)
 death or shared godhood, I'm inclined to choose the 
latter.

<p>
&gt; He'll take your money, and while<br>
<a href="2681.html#2700qlink8">&gt; you're waiting for your journey into the promised land, </a><br>

<p>
<a name="2722qlink9">Hey, you're the Pied Piper here, dude! The ASI-Singularity
is the abyss towards which the lemmings are dancing, 
hypnotized by the gaily tunes of World Peace, Universal Love 
and Unlimited Wisdom. They're in for one hell of a rude, 
albeit mercifully short, awakening...</a>

<p>
<a name="2722qlink10">&gt; he'll jump into<br>
<a href="2681.html#2700qlink9">&gt; the "experimental prototype hardware" and leave you suckers to burn.</a><br>

<p>
Oh, now I'm flattered. Apparently you think that I could 
trick everyone directly involved in the project (which 
could easily be a hundred people or more) and do a "perfect" 
ascension on the sly with some experimental prototype, which, 
foolishly, was left completely unguarded.</a> Somehow I don't think 
<a name="2722qlink11">that the others would be SUCH morons (and if they were, they'd 
*deserve* to get cheated). *Of course* no-one could trust anyone
else *completely*. For example, would you leave anyone on this 
list alone in a room with a fully functional uploader for even 
5 minutes? I sure wouldn't. People betray eachother for the 
most petty reasons, and instant godhood would be a truly 
unprecedented temptation. Consequently, the security protocols 
would have to be "unprecedented" as well. Duh!</a>

<p>
<a name="2722qlink12">Now Eliezer seems to think, or should I say wants *you* to think
(remember, this guy's got his own agenda),</a> that synchronized
<a name="2722qlink13">uploading would only make sense if a bunch of noble, selfless
saints did it. This is clearly not true. In the REAL world, it is 
perfectly feasible to have fruitful cooperation without 100%
mutual trust. It is the very basis of our society, and indeed
nature itself.</a>

<p>
<a name="2722qlink14">The synchronized uploading scenario is the classical prisoner's
dilemma. Of course we can expect some attempts at defection,
and should take the appropriate precautions.</a> No other scenario 
gives you a better fighting chance than this one. Throwing yourself
at the mercy of some artificial god is fit for simple religious
sheep, not proud, individualistic transhumanists. Your 
technophilia has blinded you all.
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2701.html">[ Next ]</a><a href="2699.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2681.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
