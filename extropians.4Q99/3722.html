<!-- received="Sun Dec 12 18:02:48 1999 MST" -->
<!-- sent="Sun, 12 Dec 1999 19:03:42 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: purpose of AIs" -->
<!-- id="385445BE.FD96976B@pobox.com" -->
<!-- inreplyto="purpose of AIs" -->
<!-- version=1.10, linesinbody=37 -->
<html><head><title>extropians: Re: purpose of AIs</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: purpose of AIs</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sun, 12 Dec 1999 19:03:42 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3722">[ date ]</a><a href="index.html#3722">[ thread ]</a><a href="subject.html#3722">[ subject ]</a><a href="author.html#3722">[ author ]</a>
<!-- next="start" -->
<li><a href="3723.html">[ Next ]</a><a href="3721.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3719.html">Kate Riley</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Kate Riley wrote:
<br>
<i>&gt; </i><br>
<a href="3719.html#3722qlink1">&gt; I must admit that this puzzles me.  If we create such a thing and always</a><br>
<i>&gt; assume that it is the best judge in all situations, how do we know when it</i><br>
<i>&gt; is mistaken?  What happens if the AI decides, in its expanisve wisdom (or</i><br>
<i>&gt; perhaps in one of its inevitable flaws), that the human race should not</i><br>
<i>&gt; exist, and decides to pull the plug?  Would you fight it?  Or decide that</i><br>
<i>&gt; since the AI is smarter than you, it must be right, and willingly lay down</i><br>
<i>&gt; your life for the "greatest good"?</i><br>

<p>
<a name="3727qlink1">We are not talking about a bleeding jumped-up version of Windows 98.  We
are talking about something that is no more a big computer program than
a human is a big amoeba.  We are talking about a Power, compared to
which human minds and human-equivalent AIs are cousins.  If the
resulting mind has any of the stereotypical characteristics of computer
programs (or for that matter of humans), it's too dumb to be a Power.</a> 
If there's even the remotest possibility of a human (or a
human-equivalent AI) outthinking it, it's not a Power.  If there's even
the faintest chance of effective human (or AI) resistance against it,
it's not a Power.  We are talking about an entity with billions or
quintillions of times the raw processing power of the entire human race.
 Not your bleeding microwave oven.

<p>
So, yes, as if it mattered, I'd willingly lay down my life for the
greatest good, on the grounds that if somehow I fought the Power, and
somehow I won, and then in the due course of trillions of subjective
years of life my mind expanded beyond mortal limits whether I
deliberately tried to upgrade it or not, I would, when I eventually
became intelligent enough, decide to commit suicide and reinstate the
original Power or something like it, thus leaving the situation pretty
much unchanged, except for a lot of wasted time and computing power.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3723.html">[ Next ]</a><a href="3721.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3719.html">Kate Riley</a>
<!-- nextthread="start" -->
</ul>
</body></html>
