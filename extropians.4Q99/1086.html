<!-- received="Thu Oct 21 08:27:23 1999 MST" -->
<!-- sent="Thu, 21 Oct 1999 09:30:25 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: &gt;H: The next 100 months" -->
<!-- id="380F2380.A1A19653@pobox.com" -->
<!-- inreplyto="&gt;H: The next 100 months" -->
<!-- version=1.10, linesinbody=223 -->
<html><head><title>extropians: Re: &gt;H: The next 100 months</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: &gt;H: The next 100 months</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Thu, 21 Oct 1999 09:30:25 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1086">[ date ]</a><a href="index.html#1086">[ thread ]</a><a href="subject.html#1086">[ subject ]</a><a href="author.html#1086">[ author ]</a>
<!-- next="start" -->
<li><a href="1087.html">[ Next ]</a><a href="1085.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1077.html">Sayke@aol.com</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1100.html">Xiaoguang Li</a>
</ul>
<!-- body="start" -->



<p>
Sayke@aol.com wrote:
<br>
<i>&gt; </i><br>
<a href="1077.html#1086qlink1">&gt; In a message dated 10/19/99 7:56:21 AM Pacific Daylight Time,</a><br>
<i>&gt; sentience@pobox.com writes:</i><br>
<i>&gt; &gt; [snip]</i><br>
<i>&gt;     naaaaa... im using trapped-animal heuristics. im playing a big</i><br>
<i>&gt; two-outcome game, in which if i win, i stay alive to play again, and if i</i><br>
<i>&gt; lose, i dont exist. but, it seems to me, that youve looked at the game, said</i><br>
<i>&gt; "shit! thats hard! i give up; im not going to play", and proceeded to engage</i><br>
<i>&gt; in a course of action not at all unlike a complex form of suicide.</i><br>

<p>
No, I'm walking out of the game.  I don't know if there's anything
outside... but I do know that if I keep playing, sooner or later I'm
going to lose.

<p>
<a href="1077.html#1086qlink2">&gt; is there</a><br>
<i>&gt; any effective difference? do you think you would survive the creation of a</i><br>
<i>&gt; transcendent ai?</i><br>

<p>
Flip me a coin.

<p>
<i>&gt; if not, why are you attempting to speed it along?</i><br>

<p>
<i>&gt;From your perspective:  A coinflip chance is better than certain death.</i><br>
<i>&gt;From my perspective:    It's the rational choice to make.</i><br>

<p>
<i>&gt; im quite</i><br>
<a href="1077.html#1086qlink3">&gt; curious, and thats the gist of this post...</a><br>
<i>&gt;     whats the line? "do you hear that, mr anderson? thats the sound of</i><br>
<i>&gt; inevitability... the sound of your death. goodbye, mr anderson..." baaah</i><br>
<i>&gt; humbug. if the odds say ill die, i hereby resolve to die trying to stay</i><br>
<i>&gt; alive. just because something is hard doesnt mean its impossible; slim is</i><br>
<i>&gt; better then none, etc...</i><br>

<p>
So why are you applying this logic to avoiding AI rather than to
creating it?

<p>
<a href="1077.html#1086qlink4">&gt; &gt;  The next best alternative would probably be Walter John Willams's</a><br>
<i>&gt; &gt;  Aristoi or Iain M. Banks's Culture.  Both are low-probability and would</i><br>
<i>&gt; &gt;  probably require that six billion people die just to establish a seed</i><br>
<i>&gt; &gt;  culture small enough not to destroy itself.</i><br>
<i>&gt; </i><br>
<i>&gt;     [note to self: read more good scifi...] but, i do have at least some idea</i><br>
<i>&gt; of what your talking about, due to shamelessly reading some of the spoilers</i><br>
<i>&gt; on this list, and i cant help thinking that you seem to completely rule out</i><br>
<i>&gt; uploading/neural engineering/whatever else... last time i checked, becoming</i><br>
<i>&gt; the singularity was still a distinct possibility. is this no longer the case?</i><br>
<i>&gt; or are you taking the position that it doesnt matter what we do; somebody,</i><br>
<i>&gt; somewhere, will make a transcendent ai, and that will be the end of us...?</i><br>

<p>
I'm taking the position that the hostile-Power/benevolent-Power
probabilities will not be substantially affected by whether the Power
starts out as an uploaded human or a seed AI.  If you can transcend and
remain benevolent, then I can write a seed AI that will remain
benevolent.  &lt;borgvoice&gt;Selfishness is irrelevant.&lt;/borgvoice&gt;

<p>
I'm also saying that a hostile Power is not "me" even if, historically,
it started out running on my neurons.

<p>
Not that I care about any of this, but you do.

<p>
<a href="1077.html#1086qlink5">&gt;     and there is *no* chance that transcendent ai could be left undeveloped</a><br>
<i>&gt; for a time long enough to allow enhancement/whatever to create a true</i><br>
<i>&gt; transhumanity?</i><br>

<p>
Sure.  Around one percent.

<p>
<a href="1077.html#1086qlink6">&gt; if there is such a chance, regardless of how slim, i think it</a><br>
<i>&gt; should be tried...</i><br>

<p>
Okay, you're giving up a fifty percent chance of surviving with AI in
favor of a one percent chance of creating transhumanity that *still*
leaves you with a fifty percent chance of winding up with hostile
Powers.  You see what I mean about not being able to calculate risks?

<p>
<i>&gt; i understand that supression of new technology almost</i><br>
<a href="1077.html#1086qlink7">&gt; certainly does more harm then good, but shit, what are the alternatives, and</a><br>
<i>&gt; why are they any better?</i><br>

<p>
The alternative is seeing what's on the other side of dawn instead of
trying to run awy from the sunrise.

<p>
<a href="1077.html#1086qlink8">&gt;     it seems to me that you think that the absolute chance of humanity's</a><br>
<i>&gt; survival is non-modifiable.</i><br>

<p>
It's easy to modify it.  Start a nuclear war.  Ta-da!  You've modified
our chances.

<p>
I think that the absolute chance of humanity's survival is
non-*improvable* over the course I've recommended.

<p>
<i>&gt; our actions modify the 'absolute' chances, do</i><br>
<a href="1077.html#1086qlink9">&gt; they not? in that sense, how can any chance be absolute? just because there</a><br>
<i>&gt; is a likely attractor state that could be occupied very well by a</i><br>
<i>&gt; transcendent ai, doesnt mean that it *will* be occupied by one...</i><br>
<i>&gt;     why dont we attempt to wait forever to create a transcendent ai? why</i><br>
<i>&gt; should anyone work on one?</i><br>

<p>
*I* will be working on one.  If I stop, someone else will do it.  You
cannot prevent an entire advanced civilization from creating AI when
anyone can buy the computing power for a few bucks and when the profits
on an incrementally improved AI are so high.  If humanity does not
create nanoweapons, one of the six billion members *will* create an AI
eventually.  If humanity does not create AI, one of the hundred and
fifty countries *will* start a nanowar.

<p>
<i>&gt; i understand that conventional ai will become</i><br>
<a href="1077.html#1086qlink10">&gt; increasingly important and useful, of course, but by not allowing programs to</a><br>
<i>&gt; modify their source code,</i><br>

<p>
I see - you're claiming that an entire industry is going to ignore the
profits inherent in self-modifying AI?  And so will all the idealists,
among whose number I include myself?

<p>
<i>&gt; and not allowing direct outside access to zyvex and</i><br>
<a href="1077.html#1086qlink11">&gt; friends, and above all not actively working on making one, the odds of one</a><br>
<i>&gt; occurring go down considerably, do they not?</i><br>

<p>
No, the odds of AI occurring before nanowar go down.  The odds of AI
being created in the long run, given the survival of humanity, remain
the same - as close to one as makes no difference.

<p>
<i>&gt; you sound like they are, well,</i><br>
<a href="1077.html#1086qlink12">&gt; inevitable, which i dont understand. they probably wont exist (anywhere</a><br>
<i>&gt; nearby, of course) unless we make one. why should we make one?</i><br>

<p>
Six billion people can't take sips from a tidal wave.  Nanodeath or AI. 
One or the other.  Can't avoid both.

<p>
<a href="1077.html#1086qlink13">&gt; &gt;  To you this seems like "defeatism" - which is another way of saying that</a><br>
<i>&gt; &gt;  life is fair and there's no problem you can't take actions to solve.</i><br>
<i>&gt; </i><br>
<i>&gt;     first off, yes, it does seem like defeatism, but thats not saying that</i><br>
<i>&gt; life is fair, or that my actions will be successful in even coming close to</i><br>
<i>&gt; solving the problems at hand. i can always take actions towards solving the</i><br>
<i>&gt; problem. whether it works or not is, of course, quite uncertain, and thats</i><br>
<i>&gt; not a problem.</i><br>

<p>
It most certainly is a problem!  This is exactly the kind of "Do
something, anything, so that we'll be doing something" attitude that
leads to the FDA, the War on Drugs, Welfare, and all kinds of
self-destructive behavior.  Doing something counts for *nothing* unless
you *succeed*.

<p>
<a href="1077.html#1086qlink14">&gt; trying is better then not trying;</a><br>

<p>
No.  Winning is better than losing.

<p>
<a href="1077.html#1086qlink15">&gt; sitting on my ass might help</a><br>
<i>&gt; the situation accidentally, true, but thats far less likely then if i</i><br>
<i>&gt; actually tried to help the situation...</i><br>
<i>&gt;     it seems to me that what your trying would reduce your odds of personal</i><br>
<i>&gt; survival considerably, and i cant figure out why.</i><br>

<p>
I think striving for AI considerably increases my odds of personal survival.

<p>
<a href="1077.html#1086qlink16">&gt; &gt;  You're choosing plans so that they contain actions to correspond to each</a><br>
<i>&gt; &gt;  problem you've noticed, rather than the plan with the least total</i><br>
<i>&gt; &gt;  probability of arriving at a fatal error.</i><br>
<i>&gt; </i><br>
<i>&gt;     i dont think anyone has nearly enough information to come close to</i><br>
<i>&gt; estimate a total probability of arriving at a fatal error. if you think you</i><br>
<i>&gt; do, enlighten me.</i><br>

<p>
Absolute probabilities, no.  The relative value of two probabilities, yes.

<p>
<i>&gt; it seems to me that the course of action i endorse has a</i><br>
<a href="1077.html#1086qlink17">&gt; muuuuuuuuuch lower total probability of arriving at a fatal error then yours,</a><br>
<i>&gt; simply because no action i can take could make the outcome worse. how could</i><br>
<i>&gt; my attempts to stop the development of transcendent ai possibly result in a</i><br>
<i>&gt; worse outcome (then not trying to stop the development, or, eris forbid,</i><br>
<i>&gt; actually helping it) for me?</i><br>

<p>
It's called "nanotechnological weapons".  Red goo can kill you just as
dead as hostile Powers.  The difference is that we don't *know* whether
or not Powers will be hostile.  Flip a coin.  But with red goo, dead is dead.

<p>
Delay AI, get killed by goo.  Wouldn't you look silly if, all along, the
Powers would have turned out to be benevolent?  Oh, yes.  Your actions
can make it *much* worse.

<p>
<a href="1077.html#1086qlink18">&gt;     actually, for all practical purposes, are not all other facts logically</a><br>
<i>&gt; dependent on the fact of my existance?</i><br>

<p>
No.  If you die, the rest of the world will still be here.  The speed of
light will remain constant.

<p>
<a href="1077.html#1086qlink19">&gt;     functional solipsism, man...</a><br>

<p>
Okay, so you're nuts.  Why should I care?

<p>
<a href="1077.html#1086qlink20">&gt; &gt;  Maybe I'll post my intuitional analysis in a couple of days.  But</a><br>
<i>&gt; &gt;  basically... the world is going somewhere.  It has momentum.  It can</i><br>
<i>&gt; &gt;  arrive either at a nanowar or at the creation of superintelligence.</i><br>
<i>&gt; &gt;  Those are the only two realistic alternatives.</i><br>
<i>&gt; </i><br>
<i>&gt;     well, i dont know if i can agree with the part about the "world going</i><br>
<i>&gt; somewhere." evolution happens, of course, but you sound to me like your</i><br>
<i>&gt; trending towards a "there is a plan" anthropicish mentality, which im</i><br>
<i>&gt; surprised to hear from you.</i><br>

<p>
Momentum doesn't imply a plan.  It implies a trend with powerful forces
backing it.

<p>
<a href="1077.html#1086qlink21">&gt;     i agree that those are the only two realistic alternatives. however, i</a><br>
<i>&gt; dont see why you would possibly be trying to assist in the development of any</i><br>
<i>&gt; superintelligence other then yourself. what worthwhile goal does that serve?</i><br>

<p>
I know what goal *I* think it serves.  From your perspective... the
survival of humanity?

<p>
<a href="1077.html#1086qlink22">&gt; you seem to have elevated it to the status of an end unto itself...? why!?</a><br>
<i>&gt; hehe...</i><br>

<p>
Go read my Web pages.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1087.html">[ Next ]</a><a href="1085.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1077.html">Sayke@aol.com</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1100.html">Xiaoguang Li</a>
</ul>
</body></html>
