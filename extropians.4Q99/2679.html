<!-- received="Sat Nov 27 16:40:50 1999 MST" -->
<!-- sent="Sat, 27 Nov 1999 17:45:32 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: NANO: Custom molecules (gulp!)" -->
<!-- id="38406D1B.F0ED5853@pobox.com" -->
<!-- inreplyto="NANO: Custom molecules (gulp!)" -->
<!-- version=1.10, linesinbody=39 -->
<html><head><title>extropians: Re: NANO: Custom molecules (gulp!)</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: NANO: Custom molecules (gulp!)</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sat, 27 Nov 1999 17:45:32 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2679">[ date ]</a><a href="index.html#2679">[ thread ]</a><a href="subject.html#2679">[ subject ]</a><a href="author.html#2679">[ author ]</a>
<!-- next="start" -->
<li><a href="2680.html">[ Next ]</a><a href="2678.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2658.html">Spike Jones</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="2827qlink1">Spike Jones wrote:
<br>
<i>&gt; </i><br>
<a href="2658.html#2679qlink1">&gt; Eliezer S. Yudkowsky wrote:</a><br>
<i>&gt; </i><br>
</a>
<i>&gt; &gt; <a href="http://www.eurekalert.org/releases/corn-bmo112399.html">http://www.eurekalert.org/releases/corn-bmo112399.html</a></i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; ...We're all going to die.</i><br>
<i>&gt; </i><br>
<i>&gt; Eliezer, let me make sure I understand your theory.  If humans</i><br>
<i>&gt; develop nanotech before the singularity, then the notion is that</i><br>
<i>&gt; it will get away from us by some means, and we all die from</i><br>
<i>&gt; grey goo?</i><br>

<p>
I'm not worried about grey goo.  Any half-sane designer should be able
to design systems that simply aren't vulnerable to the problem.

<p>
<a name="2827qlink2">What I'm worried about is nanowar.  Now that I'm finally studying
history (read _The Guns of August_ on WWI, working on _The Rise and Fall
of the Third Reich_ about WWII), I become more and more convinced that
the weapons *will* be developed, and, having been developed, *will* be
used.  Real wars, not little pretend wars like we have nowadays, are
fought with every possible tool.</a>

<p>
<a href="2658.html#2679qlink2">&gt; But if the singularity comes first, then the resulting</a><br>
<i>&gt; AI develops nanotech and we [in some form] have a fighting</i><br>
<i>&gt; chance of survival?</i><br>

<p>
Yep.

<p>
<a href="2658.html#2679qlink3">&gt; The notion sounded absurd to me at first, but I must admit</a><br>
<i>&gt; it grows on one with consideration.  spike</i><br>

<p>
Yep.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2680.html">[ Next ]</a><a href="2678.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2658.html">Spike Jones</a>
<!-- nextthread="start" -->
</ul>
</body></html>
