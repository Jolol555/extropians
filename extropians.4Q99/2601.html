<!-- received="Thu Nov 25 11:46:28 1999 MST" -->
<!-- sent="Thu, 25 Nov 1999 10:47:20 -0800" -->
<!-- name="hal@finney.org" -->
<!-- email="hal@finney.org" -->
<!-- subject="Re: SOC: Tractatus Theopoliticus (was: Is vs. Ought)" -->
<!-- id="199911251847.KAA20670@finney.org" -->
<!-- inreplyto="SOC: Tractatus Theopoliticus (was: Is vs. Ought)" -->
<!-- version=1.10, linesinbody=38 -->
<html><head><title>extropians: Re: SOC: Tractatus Theopoliticus (was: Is vs. Ought)</title>
<meta name=author content="hal@finney.org">
<link rel=author rev=made href="mailto:hal@finney.org" title ="hal@finney.org">
</head><body>
<h1>Re: SOC: Tractatus Theopoliticus (was: Is vs. Ought)</h1>
<i>hal@finney.org</i><br>
<i>Thu, 25 Nov 1999 10:47:20 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2601">[ date ]</a><a href="index.html#2601">[ thread ]</a><a href="subject.html#2601">[ subject ]</a><a href="author.html#2601">[ author ]</a>
<!-- next="start" -->
<li><a href="2602.html">[ Next ]</a><a href="2600.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2597.html">GBurch1@aol.com</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2639.html">Robert J. Bradbury</a>
</ul>
<!-- body="start" -->

<p>
Greg Burch writes:
<br>
<a href="2597.html#2601qlink1">&gt; Eliezer.  Just so that it's clear, are you saying that there is no question </a><br>
<i>&gt; in your mind that letting an SI run human affairs is preferable to any </i><br>
<i>&gt; arrangement of society humans might work out on their own?</i><br>

<p>
I believe Eliezer is working from a model where there is something
called Absolute Morality, and SIs are smart enough to figure it out and
observe it.  Then, by definition, anything the SIs do is "better" than
whatever humans would work out.  Even if the SIs wipe out the human race
and destroy all life in the universe, that was "good" by the standards
of Absolute Morality.

<p>
In this model, SIs do not have goals of their own, hence there is no
danger that they will be selfishly motivated and take actions that
benefit themselves at the expense of humans.  In following the tenets
of Absolute Morality they will do whatever is Right.

<p>
<a name="2639qlink1">I'm not sure if his argument works if there is no such thing as Absolute
Morality.<a name="2624qlink1">  In that case it seems that there is a risk that SIs will
</a>
develop their own goals (just as we do) and that their actions will not
be beneficial to the human race.

<p>
The worst outcome would be if the SIs are programmed by Eliezer to have
as their only goal the search for the Holy Grail, that is, Absolute
Morality.  However, smart as they are, they still haven't found it.
They have to be smarter.  And to do that they have to turn all available
mass into SI computational elements, which means, regrettably, wiping out
<a name="2639qlink2">the human race.  Then, at the end of a millenia-long development effort
</a>
that consumes half the galaxy and reaches realms of abstraction we can't
begin to imagine, they finally decide that their is no Absolute Morality.
</a>
<a name="2639qlink3">So they all commit suicide.  Oops.</a>

<p>
<a name="2639qlink4"><a name="2615qlink1"><i>&gt;From the point of view of those of us who don't believe in Absolute</i><br>
Morality, Eliezer's program amounts to building an unachievable goal
into the SIs, a highly dangerous proposition and one we might well oppose.
</a>

</a>
<p>
Hal
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2602.html">[ Next ]</a><a href="2600.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2597.html">GBurch1@aol.com</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2639.html">Robert J. Bradbury</a>
</ul>
</body></html>
