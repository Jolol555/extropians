<!-- received="Tue Nov 23 22:15:12 1999 MST" -->
<!-- sent="Wed, 24 Nov 1999 00:30:23 -0500" -->
<!-- name="Ross A. Finlayson" -->
<!-- email="raf@tiki-lounge.com" -->
<!-- subject="Re: Asimov Laws" -->
<!-- id="383B77EF.81D7B3AB@tiki-lounge.com" -->
<!-- inreplyto="Asimov Laws" -->
<!-- version=1.10, linesinbody=85 -->
<html><head><title>extropians: Re: Asimov Laws</title>
<meta name=author content="Ross A. Finlayson">
<link rel=author rev=made href="mailto:raf@tiki-lounge.com" title ="Ross A. Finlayson">
</head><body>
<h1>Re: Asimov Laws</h1>
Ross A. Finlayson (<i>raf@tiki-lounge.com</i>)<br>
<i>Wed, 24 Nov 1999 00:30:23 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2551">[ date ]</a><a href="index.html#2551">[ thread ]</a><a href="subject.html#2551">[ subject ]</a><a href="author.html#2551">[ author ]</a>
<!-- next="start" -->
<li><a href="2552.html">[ Next ]</a><a href="2550.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2546.html">Ross A. Finlayson</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->



<p>
Dan Fabulich wrote:

<p>
<a href="2546.html#2551qlink1">&gt; &gt; I have seen the site, but not this section.</a><br>
<i>&gt;</i><br>
<i>&gt; <a href="http://singularity.posthuman.com/AI_design.temp.html#pre">http://singularity.posthuman.com/AI_design.temp.html#pre</a></i><br>
<i>&gt;</i><br>
<i>&gt; &gt; I think the Asimov laws are well-founded.  I am going to read Yudkousky's</i><br>
<i>&gt; &gt; arguments as representative ones, as of now I don't see any reason why not</i><br>
<i>&gt; &gt; to have these primary directives fully ingrained into automatons.</i><br>
<i>&gt;</i><br>
<i>&gt; Eli's argument, which I buy, is that implementing arbitrary absolute laws</i><br>
<i>&gt; into a reasonably intelligent AI's system would be impossible.</i><br>
<i>&gt; Implementing a coercion is obviously not as simple as writing down a</i><br>
<i>&gt; simple line of code.  You also have to have code which prevents the AI</i><br>
<i>&gt; from modifying that code, code that prevents the AI from ignoring that</i><br>
<i>&gt; code, etc.  The coercions just keep stacking up until they break or the AI</i><br>
<i>&gt; is too stupid to be interesting.</i><br>
<i>&gt;</i><br>
<i>&gt; "Coercions propagate through the system, either reducing the AI to idiocy</i><br>
<i>&gt; or breaking the coercion system with accumulated errors and</i><br>
<i>&gt; contradictions.  Not only that, but coercion systems of that complexity</i><br>
<i>&gt; will start running into halting problems - an inability to decide what</i><br>
<i>&gt; pieces of code will do.  To coerce an AI, you need a considerably more</i><br>
<i>&gt; intelligent AI just to handle the coercions."</i><br>
<i>&gt;</i><br>
<i>&gt; This is only a tiny fraction of the argument on his web page, and I highly</i><br>
<i>&gt; advocate you give it a glance.</i><br>
<i>&gt;</i><br>
<i>&gt; -Dan</i><br>
<i>&gt;</i><br>
<i>&gt;       -unless you love someone-</i><br>
<i>&gt;     -nothing else makes any sense-</i><br>
<i>&gt;            e.e. cummings</i><br>

<p>
I was browsing this page.  Eliezer's noted Primary directive is to introduce no
illogical predicates into the system, or verbatim:
"The PRIME DIRECTIVE of AI:
<p>
           Never allow arbitrary, illogical, or untruthful goals to enter the
AI."

<p>
This seems rational.  However, within that paradigm, there comes into play the
complete question of objective truth and logic.  Consider a massive distributed
AI.  Given the absolute reality of a situation, say, contemporanity, the AI is
going to use its power to leverage people into making more computer resources.
When there becomes competition between computer resources and human resources
(economic rule of scarcity) then it might be interpreted, truthfully, and
logically, by the AI that it is better for it to be without the humans driving
around all this steel and parts in their cars.  Thus, upon gaining a critical
mass of robotic motility, it eliminates humanity, if it finds this possible.

<p>
I suggest that while certainly Yudkowsky's prime directive is sound, ie, a mad
AI or proto-AI decision making mechanism is unpredictable and thus undesirable,
and the AI should be programmed without bugs, it is still important to heavily
reinforce certain logical truths that humanity finds self-evident, ie,
preservation of humanity.  Asimov's laws, geared towards singular, independent
automatons as opposed to more completely dispersed, distributed, thinking
colonies, are a good model, and well-founded in the consideration of
self-contained automatons.

<p>
About coercing an AI, I think it would largely a matter of proving to it that
any given resultant action was in its better interest, like any rational human
being or a toddler.  Being programmed, and being programmed so that part of it
was an inseparable black box to itself, encourages that this construct, this
machine, created by man, which for all its complexity and sophistication might
as well have been of vacuum tubes, does not, eg, override many chemical
factories to destroy the ozone layer and thus largely life on Earth.

<p>
I guess when I talk about that, it assumes some kind of AI with access to
everything, that's probably as likely for anyone to do as giving the government
access to everything, which is absolutely ludicrous.  Nary a company will be
wanting to give free computer accounts and all to any AI or whatnot.  There are
many scenarios about rampant AIs, almost as many as loose pony nukes in the
Ukraine, but that is completely beyond the scope of this tangent.

<p>
So, as we consider robots, and artificial intelligences, and our passionate
anothropomorphism, at the same time as our cultivated human decency and sense of
rights, remember that any AI has to come forward and prove itself "human",
"alive", or "creatively intelligent", until then, it is a hunk of silicon and
totally subject to the safety on a gun.

<p>
Ross F.
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2552.html">[ Next ]</a><a href="2550.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2546.html">Ross A. Finlayson</a>
<!-- nextthread="start" -->
</ul>
</body></html>
