<!-- received="Thu Oct 21 19:12:52 1999 MST" -->
<!-- sent="Thu, 21 Oct 1999 21:23:05 -0400" -->
<!-- name="Matt Gingell" -->
<!-- email="mjg223@is7.nyu.edu" -->
<!-- subject="Re: technophobes" -->
<!-- id="16891.991021@is7.nyu.edu" -->
<!-- inreplyto="NDBBJEGIMDDABOBCCDCHOEBOCAAA.rob@hbinternet.co.uk" -->
<!-- version=1.10, linesinbody=29 -->
<html><head><title>extropians: Re: technophobes</title>
<meta name=author content="Matt Gingell">
<link rel=author rev=made href="mailto:mjg223@is7.nyu.edu" title ="Matt Gingell">
</head><body>
<h1>Re: technophobes</h1>
Matt Gingell (<i>mjg223@is7.nyu.edu</i>)<br>
<i>Thu, 21 Oct 1999 21:23:05 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1115">[ date ]</a><a href="index.html#1115">[ thread ]</a><a href="subject.html#1115">[ subject ]</a><a href="author.html#1115">[ author ]</a>
<!-- next="start" -->
<li><a href="1116.html">[ Next ]</a><a href="1114.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1084.html">Rob Harris</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1125.html">Robert J. Bradbury</a>
</ul>
<!-- body="start" -->

<p>
<a name="1125qlink1"><a href="1084.html#1115qlink1">&gt; So who's going to program the AI's with base motivations that</a><br>
<i>&gt; involve concepts such as "dominance" and the wish to strive for it,</i><br>
<i>&gt; then provide the necessary faculties/resources to do this? Not me or</i><br>
<i>&gt; anyone sane, that's for sure.</a></i><br>

<p>
The question of where an AI get's it's motivation is very
interesting. Our primitive motivations are products of our
evolutionary history: sex-drive, survival instinct, pleasure over
pain, etc. None of these are essential to the nature of intelligence
though and they won't necessarily have any correlate in an
artificially engineered brain.

<p>
<a name="1294qlink1"><a name="1125qlink2">Eliezer's pointed out the incoherence of believing you can hard wire
high level beliefs or motivations and I quite agree.</a> You do get to
specify what kind of feedback-inducing behavior gets reinforced or
attenuated though. In humans, for instance, the weight we place on the
</a>
sensation of being burned leads to behaviors like testing the
temperature of the shower before jumping in, an so forth. Perhaps we
<a name="1125qlink3">guide the development of an Ai's value system in the same way.</a>

<p>
Also note that there's no obvious bound to how complex we can make the
reward/punishment system's criteria. We could even have an embedded
limbic-system shaped conscience-Ai or similar that judges what's
desirable and what's not then pokes accordingly.

<p>
-matt
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1116.html">[ Next ]</a><a href="1114.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1084.html">Rob Harris</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1125.html">Robert J. Bradbury</a>
</ul>
</body></html>
