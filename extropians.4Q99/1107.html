<!-- received="Thu Oct 21 14:57:30 1999 MST" -->
<!-- sent="Thu, 21 Oct 1999 16:00:29 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: &gt;H: The next 100 months" -->
<!-- id="380F7EE7.83A19377@pobox.com" -->
<!-- inreplyto="&gt;H: The next 100 months" -->
<!-- version=1.10, linesinbody=81 -->
<html><head><title>extropians: Re: &gt;H: The next 100 months</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: &gt;H: The next 100 months</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Thu, 21 Oct 1999 16:00:29 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1107">[ date ]</a><a href="index.html#1107">[ thread ]</a><a href="subject.html#1107">[ subject ]</a><a href="author.html#1107">[ author ]</a>
<!-- next="start" -->
<li><a href="1108.html">[ Next ]</a><a href="1106.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1100.html">Xiaoguang Li</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Xiaoguang Li wrote:
<br>
<i>&gt; </i><br>
<a href="1100.html#1107qlink1">&gt;         here's my understanding of Eliezer's argument for actively</a><br>
<i>&gt; pursuing the creation of a power:</i><br>
<i>&gt; </i><br>
<i>&gt;         destruction is easier than construction: given humanity's</i><br>
<i>&gt; lackluster record with past technological advances, nanowar (read:</i><br>
<i>&gt; armagedon) is virtually certain.</i><br>
<i>&gt;         evolution is messy: even if humanity survives the advent of</i><br>
<i>&gt; nanotech and gives rise to transhuman powers, these powers would have no</i><br>
<i>&gt; more chance of being benevolent than powers designed de novo. the</i><br>
<i>&gt; dichotomy -- if transhuman powers retained evolutionary baggage such as</i><br>
<i>&gt; emotional attachments or moral inhibitions, then they are prone to</i><br>
<i>&gt; inconsistencies and self-contraditions and are therefore untrustworthy; if</i><br>
<i>&gt; they did not, then initial conditions are insignificant, and there's no</i><br>
<i>&gt; functional difference between transhuman powers or powers who arise</i><br>
<i>&gt; otherwise. the caveat -- AI is easier than IA, so this scenario would</i><br>
<i>&gt; require active suppression of technology, the ills of which are</i><br>
<i>&gt; well-known.</i><br>
<i>&gt;         power before nanowar: creation of a power designed de novo before</i><br>
<i>&gt; nanowar will result in a singularity -- that is, all bets are off (read:</i><br>
<i>&gt; no more house advantage against the survival of humanity).</i><br>

<p>
So far, so good.

<p>
<a href="1100.html#1107qlink2">&gt;         now for my doubts. does the creation of a power really increase</a><br>
<i>&gt; the chances of our survival? it seems that the odds of humanity</i><br>
<i>&gt; surviving a singularity are significantly lower than a coin flip. given</i><br>
<i>&gt; the enormous difference between the creation and its creator, it seems</i><br>
<i>&gt; most likely (&gt; 99%) that humanity would not be significant to a power.</i><br>
<i>&gt; that is, perhaps less significant than virion particles to a human</i><br>
<i>&gt; being.</i><br>

<p>
I honestly don't know.  I really don't.  I certainly wouldn't put the
probability at lower than 10%, or higher than 60%... but, ya know, I
could be missing something either way.  I think we're missing a
fundamental part of the question.  There isn't *any* outcome I can see
for the Singularity, not one of your three possibilities, that could
conceivably have happened a few hundred thousand times previously in the
Universe... which is what I would regard as the absolute minimum number
of intelligent species to expect, given that some galaxies are already
so old as to be burned out.

<p>
What I am fairly certain of, and what my intuitions agree with, is this:


<OL>
  <li>  If benevolent Powers can be created, it will be because there are no
forces or tendencies, internal or external, that would interfere with
their creation - not because we mounted a heroic effort and overcame the
resistance.  Humans and AIs are part of the continuum of "mortal minds",
they are both made of fundamentally the same stuff, and there isn't any
magical reason why humanborn Powers can be benevolent and designed
Powers can't.

  <li>  We can't come up with an evidence-consistent account of what happens
after the Singularity, or what Powers do, because we're not taking some
basic factor into account.  The twentieth century just isn't late enough
for our worldview to be capable of phrasing the question properly.

  <li>  There is a massive momentum, backed by profit-motive, idealism, and
all the forces of generalized technological progress, towards creating
and using ultratechnologies like nanotech and AI.  Neither can be
suppressed indefinitely, because both will just keep getting easier and
easier, until eventually one person can do it.  We can't slow down.  We
can't pick and choose.  We can only steer for one direction or the
other.  Something huge is going to happen on this planet; it's just a
question of what.

  <li>  Uploading is extremely advanced nanotechnology *and* cognitive
science.  Any technology capable of uploading someone has been capable
of frying the planet *or* building an AI for at least five years of
Internet time.

  <li>  This *will* be settled, one way or another, fifty thousand years
from now, no matter what happens in the meantime.  One generation has to
be the last, so it would be irresponsible to refuse to deal with it.
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way

</OL>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1108.html">[ Next ]</a><a href="1106.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1100.html">Xiaoguang Li</a>
<!-- nextthread="start" -->
</ul>
</body></html>
