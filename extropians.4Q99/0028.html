<!-- received="Fri Oct  1 07:32:40 1999 MST" -->
<!-- sent="Fri, 01 Oct 1999 08:34:57 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Transhuman speech processing" -->
<!-- id="37F4B87F.AD893BAC@pobox.com" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=145 -->
<html><head><title>extropians: Transhuman speech processing</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Transhuman speech processing</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Fri, 01 Oct 1999 08:34:57 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#28">[ date ]</a><a href="index.html#28">[ thread ]</a><a href="subject.html#28">[ subject ]</a><a href="author.html#28">[ author ]</a>
<!-- next="start" -->
<li><a href="0029.html">[ Next ]</a><a href="0027.html">[ Previous ]</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0045.html">Robert J. Bradbury</a>
</ul>
<!-- body="start" -->

<p>
<a name="0045qlink1"><a href="http://www.eurekalert.org/releases/usc-nnn093099.</a>html">http://www.eurekalert.org/releases/usc-nnn093099.html</a>

<p>
I knew those discrete models were oversimplified!  It's like I've been
saying all along:  The little tokens in so-called "neural nets" are
*nothing* like actual neurons.

<p>
<a name="0103qlink1"><i>&gt; Novel neural net recognizes spoken</i><br>
</a>
<i>&gt; words better than human listeners </i><br>
<i>&gt; </i><br>
<i>&gt; Machine demonstrates superhuman speech recognition</i><br>
<i>&gt; abilities. University of Southern California biomedical</i><br>
<i>&gt; engineers have created the world's first machine system</i><br>
<i>&gt; that can recognize spoken words better than humans can. A</i><br>
<i>&gt; fundamental rethinking of a long-underperforming computer</i><br>
<i>&gt; architecture led to their achievement. </i><br>
<i>&gt; </i><br>
<i>&gt; The system might soon facilitate voice control of computers</i><br>
<i>&gt; and other machines, help the deaf, aid air traffic controllers</i><br>
<i>&gt; and others who must understand speech in noisy</i><br>
<i>&gt; environments, and instantly produce clean transcripts of</i><br>
<i>&gt; conversations, identifying each of the speakers. The U.S.</i><br>
<i>&gt; Navy, which listens for the sounds of submarines in the</i><br>
<i>&gt; hubbub of the open seas, is another possible user.</i><br>
<i>&gt; Potentially, the system's novel underlying principles could</i><br>
<i>&gt; have applications in such medical areas as patient</i><br>
<i>&gt; monitoring and the reading of electrocardiograms. </i><br>
<i>&gt; </i><br>
<i>&gt; In benchmark testing using just a few spoken words, USC's</i><br>
<i>&gt; Berger-Liaw Neural Network Speaker Independent Speech</i><br>
<i>&gt; Recognition System not only bested all existing computer</i><br>
<i>&gt; speech recognition systems but outperformed the keenest</i><br>
<i>&gt; human ears. </i><br>
<i>&gt; </i><br>
<i>&gt; Neural nets are computing devices that mimic the way brains</i><br>
<i>&gt; process information. Speaker-independent systems can</i><br>
<i>&gt; recognize a word no matter who or what pronounces it. No</i><br>
<i>&gt; previous speaker-independent computer system has ever</i><br>
<i>&gt; outperformed humans in recognizing spoken language, even in</i><br>
<i>&gt; very small test bases, says system co-designer Theodore W.</i><br>
<i>&gt; Berger, Ph.D., a professor of biomedical engineering in the</i><br>
<i>&gt; USC School of Engineering. </i><br>
<i>&gt; </i><br>
<i>&gt; The system can distinguished words in vast amounts of</i><br>
<i>&gt; random "white" noise, noise with amplitude 1,000 times the</i><br>
<i>&gt; strength of the target auditory signal. Human listeners can</i><br>
<i>&gt; deal with only a fraction as much. And the system can pluck</i><br>
<i>&gt; words from the background clutter of other voices, the</i><br>
<i>&gt; hubbub heard in bus stations, theater lobbies and cocktail</i><br>
<i>&gt; parties, for example. Even the best existing systems fail</i><br>
<i>&gt; completely when as little as 10 percent of hubbub masks a</i><br>
<i>&gt; speaker's voice. At slightly higher noise levels, the</i><br>
<i>&gt; likelihood that a human listener can identify spoken test</i><br>
<i>&gt; words is mere chance. By contrast, Berger and Liaw's system</i><br>
<i>&gt; functions at 60 percent recognition with a hubbub level 560</i><br>
<i>&gt; times the strength of the target stimulus. With just a minor</i><br>
<i>&gt; adjustment, the system can identify different speakers of</i><br>
<i>&gt; the same word with superhuman acuity. </i><br>
<i>&gt; </i><br>
<i>&gt; Berger and system co-designer Jim-Shih Liaw, Ph.D.,</i><br>
<i>&gt; achieved this improved performance by paying closer</i><br>
<i>&gt; attention to the signal characteristics used by real</i><br>
<i>&gt; flesh-and-blood brains in processing information. </i><br>
<i>&gt; </i><br>
<i>&gt; First proposed in the 1940s and the subject of intensive</i><br>
<i>&gt; research in the '80s and early '90s, neural nets are</i><br>
<i>&gt; computers configured to imitate the brain's system of</i><br>
<i>&gt; information processing, wherein data are structured not by a</i><br>
<i>&gt; central processing unit but by an interlinked network of</i><br>
<i>&gt; simple units called neurons. Rather than being programmed,</i><br>
<i>&gt; neural nets learn to do tasks through a training regimen in</i><br>
<i>&gt; which desired responses to stimuli are reinforced and</i><br>
<i>&gt; unwanted ones are not. </i><br>
<i>&gt; </i><br>
<i>&gt; "Though mathematical theorists demonstrated that nets</i><br>
<i>&gt; should be highly effective for certain kinds of computation</i><br>
<i>&gt; (particularly pattern recognition), it has been difficult for</i><br>
<i>&gt; artificial neural networks even to approach the power of</i><br>
<i>&gt; biological systems," said Liaw, director of the Laboratory</i><br>
<i>&gt; for Neural Dynamics and a research assistant professor of</i><br>
<i>&gt; biomedical engineering at the USC School of Engineering. </i><br>
<i>&gt; </i><br>
<i>&gt; "Even large nets with more than 1,000 neurons and 10,000</i><br>
<i>&gt; interconnections have shown lackluster results compared</i><br>
<i>&gt; with theoretical capabilities. Deficiencies were often laid to</i><br>
<i>&gt; the fact that even 1,000-neuron networks are tiny, compared</i><br>
<i>&gt; with the millions or billions of neurons in biological</i><br>
<i>&gt; systems." Remarkably, USC's neural net system uses an</i><br>
<i>&gt; architecture consisting of just 11 neurons connected by a</i><br>
<i>&gt; mere 30 links. </i><br>
<i>&gt; </i><br>
<i>&gt; According to Berger, who has spent years studying biological</i><br>
<i>&gt; data-processing systems, previous computer neural nets</i><br>
<i>&gt; went wrong by oversimplifying their biological models,</i><br>
<i>&gt; omitting a crucial dimension. </i><br>
<i>&gt; </i><br>
<a name="0045qlink2"><i>&gt; "Neurons process information structured in time," he</i><br>
<i>&gt; explained. "They communicate with one another in a</i><br>
<i>&gt; 'language' whereby the 'meaning' imparted to the receiving</i><br>
<i>&gt; neuron is coded into the signal's timing. A pair of pulses</i><br>
<i>&gt; separated by a certain time interval excites a certain</i><br>
<i>&gt; neuron, while a pair of pulses separated by a shorter or</i><br>
<i>&gt; longer interval inhibits it. "So far," Berger continued,</i><br>
</a>
<i>&gt; "efforts to create neural networks have had silicon neurons</i><br>
<i>&gt; transmitting only discreet signals of varying intensity, all</i><br>
<i>&gt; clocked the way a computer is clocked, in beats of unvarying</i><br>
<i>&gt; duration. But in living cells, the temporal dimension, both in</i><br>
<i>&gt; the exciting signal and in the response, is as important as</i><br>
<i>&gt; the intensity." </i><br>
<i>&gt; </i><br>
<i>&gt; Berger and Liaw created computer chip neurons that closely</i><br>
<i>&gt; mimic the signaling behavior of living cells, those of the</i><br>
<i>&gt; hippocampus, the brain structure involved in associative</i><br>
<i>&gt; learning. "You might say, we let our cells hear the music,"</i><br>
<i>&gt; Berger said. Berger and Liaw's computer chip neurons were</i><br>
<i>&gt; combined into a small neural network using standard</i><br>
<i>&gt; architecture. While all the neurons shared the same</i><br>
<i>&gt; hippocampus-mimicking general characteristics, each was</i><br>
<i>&gt; randomly given slightly different individual characteristics,</i><br>
<i>&gt; in much the same way that individual hippocampus neurons</i><br>
<i>&gt; would have slightly different individual characteristics. The</i><br>
<i>&gt; network created was then trained, using a procedure as</i><br>
<i>&gt; unique as the neurons , again taken from the biological</i><br>
<i>&gt; model, a learning rule that allows the temporal properties of</i><br>
<i>&gt; the net connections to change. </i><br>
<i>&gt; </i><br>
<i>&gt; The USC research was funded by the Office of Naval</i><br>
<i>&gt; Research; the Defense Department's Advanced Research</i><br>
<i>&gt; Projects Agency; the National Centers for Research</i><br>
<i>&gt; Resources, and the National Institute of Mental Health. The</i><br>
<i>&gt; university has applied for a patent on the system and the</i><br>
<i>&gt; architectural concepts on which it is based. </i><br>
<i>&gt; </i><br>
<i>&gt;                           ###</i><br>
<i>&gt; </i><br>
<i>&gt; RealVideo demonstration at:</i><br>
<i>&gt; <a href="http://www.usc.edu/ext-relations/news_service/real/real_video.html">http://www.usc.edu/ext-relations/news_service/real/real_video.html</a></i><br>
<i>&gt; </i><br>
<i>&gt; </i><br>
&gt; Back to EurekAlert! <br>
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0029.html">[ Next ]</a><a href="0027.html">[ Previous ]</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0045.html">Robert J. Bradbury</a>
</ul>
</body></html>
