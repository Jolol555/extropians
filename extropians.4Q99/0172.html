<!-- received="Sun Oct  3 11:14:28 1999 MST" -->
<!-- sent="Sun, 3 Oct 1999 13:12:24 EDT" -->
<!-- name="GBurch1@aol.com" -->
<!-- email="GBurch1@aol.com" -->
<!-- subject="Re: ETHICS: Mind abuse &amp; technology development (Part One)" -->
<!-- id="647ec6d6.2528e878@aol.com" -->
<!-- inreplyto="ETHICS: Mind abuse &amp; technology development (Part One)" -->
<!-- version=1.10, linesinbody=322 -->
<html><head><title>extropians: Re: ETHICS: Mind abuse &amp; technology development (Part One)</title>
<meta name=author content="GBurch1@aol.com">
<link rel=author rev=made href="mailto:GBurch1@aol.com" title ="GBurch1@aol.com">
</head><body>
<h1>Re: ETHICS: Mind abuse &amp; technology development (Part One)</h1>
<i>GBurch1@aol.com</i><br>
<i>Sun, 3 Oct 1999 13:12:24 EDT</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#172">[ date ]</a><a href="index.html#172">[ thread ]</a><a href="subject.html#172">[ subject ]</a><a href="author.html#172">[ author ]</a>
<!-- next="start" -->
<li><a href="0173.html">[ Next ]</a><a href="0171.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0138.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->


<UL>
  <li>  - - [ This is the first part of a VERY long post ] - - -


</UL>
<p>
In a message dated 99-10-02 17:04:04 EDT, bradbury@www.aeiveos.com (Robert J. 
Bradbury) wrote:

<p>
<a href="0138.html#0172qlink1">&gt;  So, after a month, I'm going to trip over the ropes back into</a><br>
<i>&gt;  the ring with ObiWan Burch regarding mind ethics.</i><br>

<p>
I'm glad you did.  As you said about my original post: This made me think 
hard.

<p>
<a href="0138.html#0172qlink2">&gt;  &gt; On Sun, 29 Aug 1999 GBurch1@aol.com wrote:</a><br>
<i>&gt;  </i><br>
<i>&gt;  &gt; In reponse to my message message dated 99-08-27 07:21:42 EDT </i><br>
<i>&gt; </i><br>
<i>&gt;  &gt; A MORAL theory of mind (which seems to be what you're looking for) may </i><br>
be 
<br>
<a href="0138.html#0172qlink3">&gt;  &gt; dimly perceived in this insight, as applied to the questions you've </a><br>
<i>&gt;  &gt; presented.  As a first pass at formulating such a moral theory of mind, </i><br>
<i>&gt;  &gt; perhaps we can say that an entity should be treated as both a moral </i><br>
<i>&gt;  &gt; subject </i><br>
<i>&gt;  &gt; and a moral object TO THE EXTENT THAT it exhibits more or fewer of the </i><br>
<i>&gt;  &gt; various distinct elements of "mind".  As an example, consider a book or </i><br>
<i>&gt;  &gt; computer hard drive as an instance of memory.  Both are utterly passive </i><br>
<i>&gt;  &gt; repositories of information, incapable by themselves of processing any </i><br>
of 
<br>
<a href="0138.html#0172qlink4">&gt;  &gt; the </a><br>
<i>&gt;  &gt; data they record.</i><br>

<p>
Note that I am at this point proposing "mind" as a moral axiom. I think that 
this is the most basic and least arbitrary possible foundation for a moral 
and ethical system, an idea I've been exploring for a couple of decades.  But 
I think that some of what your post addresses is whether "mind" is a "good" 
moral axiom, which of course is a redundant question: An axiom is just that; 
a "given", a foundation below which analysis isn't "allowed" in the proposed 
system.  

<p>
Of course, it's perfectly legitimate to question the robustness of a moral 
axiom, or whether logical theorems derived from the axiom yield absurd 
results, which some of your questions do.  But the view I'm exploring WOULD 
entail some shifts in moral perspective from those commonly held by most 
people today.  Thus, what we might find to be an "absurd" result may well 
change as we adopt new moral axioms.  

<p>
This has certainly happened before in human history.  Consider, for instance, 
acceptance of individual autonomy as a fundamental moral "good" (if not an 
axiom, per se) and perceptions of the institution of slavery or the legal 
subjugation of women.  Both of those things were vigorously defended by at 
least some people who considered themselves to be moral.  As a matter of 
historical fact (as opposed to rational contemplation and logic), moral 
systems evolve as different values have come to be held as more or less 
fundamental and social practice is thereafter seen as inconsistent with the 
newly valued basic principle. My proposal to make "mind" the most fundamental 
moral axiom would involve such an adjustment, which we see in many of your 
examples.
  
<p>
<a href="0138.html#0172qlink5">&gt;  &gt; Thus, a book or a current computer storage device exhibits </a><br>
<i>&gt;  &gt; only one aspect of mind (and that only dimly).</i><br>
<i>&gt;  </i><br>
<i>&gt;  Maybe, but the complexity is increasing rapidly.  The next step would be</i><br>
<i>&gt;  anticipating what blocks you wanted based on historical trends and</i><br>
<i>&gt;  retreiving them before you request them.  This is an attribute of a</i><br>
<i>&gt;  very efficient secretary.</i><br>

<p>
Yes, and I propose that as the performance of the system begins to approach 
that of a good secretary, you may well have to begin to treat that system 
with as much respect as one would a simple servant.  I'm not saying that a 
simple system that anticipated your data needs based on past patterns would 
be entitled to "wage and hour" equity, but that one would begin to view such 
systems in moral terms; just as one does, say an anthill.  We certainly don't 
accord an anthill the status of moral subject, but we ought to condemn its 
completely arbitrary destruction.
  
<p>
<a href="0138.html#0172qlink6">&gt;  &gt; In the </a><br>
<i>&gt;  &gt; proposed moral theory of mind, we do not consider these examples to be </i><br>
<i>&gt;  &gt; very </i><br>
<i>&gt;  &gt; significant moral objects or subjects; although, interestingly, some </i><br>
<i>&gt;  &gt; people </i><br>
<i>&gt;  &gt; DO consider them to be very slight moral objects, in the sense that </i><br>
there 
<br>
<a href="0138.html#0172qlink7">&gt;  &gt; is </a><br>
<i>&gt;  &gt; a slight moral repugnance to the notion of burning books (no matter who </i><br>
<i>&gt;  &gt; owns </i><br>
<i>&gt;  &gt; them) or, as has been discussed here recently, "wasting CPU cycles".</i><br>
<i>&gt;  </i><br>
<i>&gt;  In an abstract sense it seems that degree of "wrongness" of "burning"</i><br>
<i>&gt;  or "wasting" has to do with the destruction of organized information</i><br>
<i>&gt;  or using CPU cycles for non-useful purposes.  I.e. contributing to</i><br>
<i>&gt;  "entropy" is the basis for the offence.  This seems to apply in the</i><br>
<i>&gt;  abortion debate as well.  I can destroy a relatively unorganized</i><br>
<i>&gt;  potential human (&lt; 3 months) but should not destroy a much more</i><br>
<i>&gt;  organized human (&gt; 6 months).  This seems to be related to the</i><br>
<i>&gt;  destruction of information content.  It also holds up if you</i><br>
<i>&gt;  look at when physicians are allowed to disconnect life-support</i><br>
<i>&gt;  devices -- when it has been determined that the information</i><br>
<i>&gt;  has "left" the system.  Extropians may extend it further by</i><br>
<i>&gt;  recognizing that incinerating or allowing a body to be consumed</i><br>
<i>&gt;  is less morally acceptable than freezing it since we really</i><br>
<i>&gt;  don't have the technology to know for sure whether the information</i><br>
<i>&gt;  is really gone.</i><br>

<p>
Yes, all of this follows, but the key in the idea I propose is the extent to 
which the system in question is an instance of "mind" or potential mind.  I 
realize this begs the question of what a "mind" is, but that is the point of 
my proposal: As we approach the threshold of what we may begin to call 
artificial intelligence or "synthetic minds", we MUST begin to clarify what 
we mean by "mind".  Just as advanced religious moral systems caused people to 
begin to explore what defined a "person" because they were "god's creatures" 
or because they had the potential of "salvation" or "enlightenment", a moral 
system based on "mind" lends a moral imperative to otherwise abstract or 
scientific-technical questions of what a "mind" is.
 
<p>
<a href="0138.html#0172qlink8">&gt;  &gt; From the proposed axiom of "mind morality", one could derive specific </a><br>
<i>&gt;  &gt; propositions of moral imperative.  For instance, it would be morally </i><br>
wrong 
<br>
<a href="0138.html#0172qlink9">&gt;  &gt; to </a><br>
<i>&gt;  &gt; reduce mental capacity in any instance, and the EXTENT of the wrong </i><br>
would 
<br>
<a href="0138.html#0172qlink10">&gt;  &gt; be </a><br>
<i>&gt;  &gt; measured by the capacity of the mental system that is the object of the </i><br>
<i>&gt;  &gt; proposition.</i><br>
<i>&gt;  </i><br>
<i>&gt;  Ah, but you have to be careful if you require an external force to</i><br>
<i>&gt;  implement the morality.  If I personally, wanted to have the left</i><br>
<i>&gt;  half of my brain cut out and subject myself to a round of hormone</i><br>
<i>&gt;  treatments that caused extensive regrowth of stem cells, effectively</i><br>
<i>&gt;  giving me a babies left-brain to go with my old-right brain, you could</i><br>
<i>&gt;  argue that I would be doing something morally wrong by destroying</i><br>
<i>&gt;  the information in my left-brain (actually I would probably have</i><br>
<i>&gt;  it frozen so your argument might not have much weight).  However, if</i><br>
<i>&gt;  you go further and act to prevent me from having my operation, I</i><br>
<i>&gt;  would argue that you are behaving wrongly.  After all its my body damn it!</i><br>
<i>&gt;  We seem to recognize this principle at least to some degree with the</i><br>
<i>&gt;  concept of the "right to die".  This isn't generally recognized in</i><br>
<i>&gt;  our society but I suspect that most extropians (esp. the stronger</i><br>
<i>&gt;  libertarians) would argue this. </i><br>

<p>
This is a good point, but one that moves, in my own philosophical vocabulary, 
from questions of "morals" to ones of "ethics" and thence to "law", where 
each step leads outward from the moral subject to society.  As I use these 
terms, each step involves APPLYING the one before and, thus, involves greater 
complexity and a greater need to balance values.  In particular, the value of 
AUTONOMY, as a moral good, an ethical value and a legal principle, plays an 
increasingly important role as a mediator of application of the moral value 
of mind.

<p>
Note that I do not propose "mind" as the sole moral axiom, although I am 
exploring the implications of holding it as the most fundamental one.  It may 
well be that "autonomy" as a moral, ethical and legal value can be completely 
derived from "mind" as a moral axiom; I'm not ready to make such a 
declaration.  But one cannot, I am sure, declare a "complete" ethical and 
legal system from valuing "mind" alone or in some sort of "pure" or unapplied 
form.

<p>
<a href="0138.html#0172qlink11">&gt;  &gt; Thus, willfully burning a book would be bad, but not very bad, </a><br>
<i>&gt;  &gt; especially if there is a copy of the book that is not destroyed.  It </i><br>
might 
<br>
<a href="0138.html#0172qlink12">&gt;  &gt; be </a><br>
<i>&gt;  &gt; more wrong to kill an ant (depending on the contents of the book with </i><br>
<i>&gt;  &gt; which </i><br>
<i>&gt;  &gt; one was comparing it), but not as wrong as killing a cat or a bat.</i><br>
<i>&gt;  </i><br>
<i>&gt;  Yep, the more information you are "destroying", the wronger it is.</i><br>
<i>&gt;  But we (non-buddists) generally don't consider it over the line to step</i><br>
<i>&gt;  on an ant and we certainly don't get upset over the ants eating plant</i><br>
<i>&gt;  leaves whose cells are self-replicating information stores, but if</i><br>
<i>&gt;  you destroy another person, or worse yet, lots of them, you have</i><br>
<i>&gt;  crossed over the line.</i><br>

<p>
Here I believe you make a simplistic application of the axiom, when you use 
the term "over the line".  Your expression assumes that the world can be 
divided into two zones; "good" and "bad".  That is the methodology of moral 
and ideological absolutism.  Every application of morality to the world (i.e. 
every ethical and legal problem or proposition) involves a balancing act.  
Ultimately, we can only say that things are ON BALANCE "good" or "bad".

<p>
You are also interpreting my proposed axiom as if "mind = information".  That 
certainly can't be right since, as I indicated in my first illustrative 
example, things like books and computer storage devices are instances of only 
one or a very few aspects of "mind".  This compounds the absolutist error.

<p>
<a href="0138.html#0172qlink13">&gt;  &gt; &gt;  Q1: Do you "own" the backup copies?</a><br>
<i>&gt;  &gt; &gt;      (After all, you paid for the process (or did it yourself) and</i><br>
<i>&gt;  &gt; &gt;       it is presumably on hardware that is your property.)</i><br>
<i>&gt;  &gt; </i><br>
<i>&gt;  &gt; I don't think you "own" your backup copies any more than you "own" your </i><br>
<i>&gt;  &gt; children or other dependants.</i><br>
<i>&gt;  </i><br>
<i>&gt;  But I do "own" my left brain and my right brain, my left hand and</i><br>
<i>&gt;  right hand and all of the cells in my body.  Now, in theory I can</i><br>
<i>&gt;  engineer some cells to grow me a hump on my back of unpatterned</i><br>
<i>&gt;  neurons and then send in some nanobots to read my existing synaptic</i><br>
<i>&gt;  connections and rewire the synapses my hump neurons to be virtually</i><br>
<i>&gt;  identical.  The nanobots also route a copy of my sensory inputs</i><br>
<i>&gt;  to my "Brhump" but make the motor outputs no-ops (after all its</i><br>
<i>&gt;  a backup copy).  Are you suggesting that I don't "own" my hump?</i><br>

<p>
I've got to hand it to you, Robert, here you seem to have posed about as hard 
a problem as my idea could face.  First, I think it's necessary to be more 
clear than I previously was about just what we mean when we talk about 
"ownership" in connection with minds, whether our own or not.  While I do 
endorse Max's notion of "self-ownership", I think it's necessary to 
distinguish the nature of that ownership from the relationship we have with 
matter that is not a substrate for mind.  The notion of "self-ownership" is a 
reflexive mapping of the notion of property and serves a useful purpose in 
clarifying the ethical and legal outlines of appropriate boundaries on 
individual autonomy in society. The logical conundrum posed by your example 
relates to another "folding" of that reflexivity.

<p>
At one level, the resolution of your problem might be simple: "If I rightly 
can be said to 'own my self', and my self is my brain, then I own all parts 
of my brain, even ones external to my cranium."  Is your "Brhump" part of 
your brain?  Or is it another brain?  More importantly, is it another "self"? 
 If so, then the principle of self-ownership won't help you: The Brhump "owns 
itself" and has a right to autonomy.

<p>
You say that the pattern of neurons in your "Brhump" are "virtually 
identical" to the ones in your cranium and the sensory inputs appear to BE 
identical as you've posed the hypothetical problem.  If the pattern of 
neurons were PRECISELY identical, then it appears you've succeeded in making 
an exact copy of your mind that is updated in real time to maintain that 
identity.  If so, then it seems you can escape any moral issue: Your "Brhump" 
has no more moral status than your reflection in a mirror.  

<p>
But once any significant diversion of the two instances of mind develops, you 
do have a moral issue, at least in my opinion.  Look at it this way: If your 
brain were removed from your body and somehow sewn onto someone else's back, 
how would you want it to be treated?  What if it was your identical twin's 
back?  We treat Siamese twins as two different (but closely related) 
individuals for moral purposes, and for a good reason; because they are two 
distinguishable moral SUBJECTS.
  
<p>
<a href="0138.html#0172qlink14">&gt;  &gt; In the case of your children, they are built from the "joint</a><br>
<i>&gt;  &gt; intellectual property" of you and your mate and all of the </i><br>
<i>&gt;  &gt; atoms making up their bodies started off as your property, but still we </i><br>
<i>&gt; don't </i><br>
<i>&gt;  &gt; say you "own" your children.  Why?  Because they are complex minds.</i><br>
<i>&gt;  </i><br>
<i>&gt;  So is my "Brhump".  But unlike the example of children, I created</i><br>
<i>&gt;  it entirely and it is a parasite on my body.</i><br>

<p>
I don't think this gets you to the point where you are excused from any moral 
obligation to this new entity you've created: If it is a distinct mind, you 
owe it the duties appropriate to 1) it's current level of mental activity and 
2) it's potential levels of mental activity.
  
<p>
<a href="0138.html#0172qlink15">&gt;  &gt; Now, you may have special rights and duties with regard to minds that</a><br>
<i>&gt;  &gt; have such a special relationship to you, but "ownership" isn't among </i><br>
them. 
<br>
<i>&gt;  </i><br>
<a href="0138.html#0172qlink16">&gt;  Well, since it is a copy, in theory cutting it off and burning it falls</a><br>
<i>&gt;  into the "book burning" moral category.  Since the brain can't feel pain</i><br>
<i>&gt;  there are no "ugly" moral claims that this would be the equivalent of</i><br>
<i>&gt;  torturing or murdering someone.</i><br>

<p>
If it is a precise copy that has never diverged in its "trajectory of 
identity" from the original, then you're right.  In fact, it has no more 
moral quality than smashing a mirror.  But to the extent that it has 
developed a mental identity distinct from your own, how can you distinguish 
your "disposal" of this mind to what some other mind might do to you?

<p>
<a href="0138.html#0172qlink17">&gt;  &gt; Morally, mind is a special sort of "thing".  For one thing, it is a </a><br>
<i>&gt;  &gt; process.  </i><br>
<i>&gt;  &gt; Thus, one might be said to have something more akin to "ownership" in </i><br>
the 
<br>
<a href="0138.html#0172qlink18">&gt;  &gt; stored pattern of one's backup copies, but once they are "run" or </a><br>
"running",
<br>
<a href="0138.html#0172qlink19">&gt;  &gt; they would take on more of the quality of moral subjects as well as </a><br>
moral 
<br>
<a href="0138.html#0172qlink20">&gt;  &gt; objects.  Once a system is capable of being a moral subject, "ownership" </a><br>
<i>&gt;  &gt; ceases to be the right way to consider it as a moral object.</i><br>
<i>&gt;  </i><br>
<i>&gt;  Clearly in the example above, the mind is running (after all what good</i><br>
<i>&gt;  is a backup copy if it isn't up-to-date)?  Now, as an interesting aside</i><br>
<i>&gt;  there is the question of whether "untouched" "Brhumps" (with exactly</i><br>
<i>&gt;  the same inputs) will diverge from your brain and need to be edited</i><br>
<i>&gt;  back into complete equivalence by the nanobots?  Whether or not</i><br>
<i>&gt;  that is necessary, the subject was created by me, for my use and</i><br>
<i>&gt;  literally "is" me (even though it is a second instantiation).</i><br>

<p>
You seem to acknowledge here the moral distinction between an exact copy and 
one that has diverged from you.  I think this is because unless we give a 
special, fundamental moral status to independent minds, we can make no moral 
judgments at all, because we ARE minds.

<p>
<a href="0138.html#0172qlink21">&gt;  Now, you might say that if I give my 2nd instantiation separate</a><br>
<i>&gt;  senses and let it evolve a bit that it now becomes its own unique</i><br>
<i>&gt;  moral subject.  But what if it knew going into the game (since its</i><br>
<i>&gt;  a copy of me) that I have a policy of incinerating "Brhumps" at</i><br>
<i>&gt;  the end of every month?  [Not dissimilar from the knowledge in</i><br>
<i>&gt;  our current world that we don't live to 150 -- its just the</i><br>
<i>&gt;  way it is.]</i><br>
<i>&gt;  </i><br>
<i>&gt;  This comes up a little in Permutation City and to a much greater</i><br>
<i>&gt;  degree in the Saga of The Cuckoo (where people go climbing into</i><br>
<i>&gt;  tachyon teleportation chambers, knowing that the duplicate on</i><br>
<i>&gt;  the receiving end faces almost certain death).  Its kind of a</i><br>
<i>&gt;  going off to war mentality, knowing that you might not come back,</i><br>
<i>&gt;  but you do it anyway if you feel the need is great enough.</i><br>

<p>
Well, this is a different question from "What can I do to minds I create".  
If someone freely chooses to sacrifice themselves for the good of another . . 
. well, that is their choice and no one is harmed but the one doing the 
choosing.  It's hard to see how one could condemn such an action.  I happen 
to be deeply dubious about positions that some people take in discussions 
like this that assume that a copy of me will be willing to sacrifice itself 
for me.  Some years ago we went down this road at great length in discussion 
of "the copy problem".  If I step out of a "copy machine" and encounter the 
original of me asking me to stick a hot poker in my eye so that "I" can find 
out how I'd react to such a situation, I doubt seriously whether I'd just say 
"Oh, OK!" (Use Rick Moranis' voice from "Ghost Busters" there.)

<p>
Whether I might be willing to engage in more dangerous behavior than I 
otherwise would be if I knew that a very recent copy could be reanimated 
should I be destroyed in the process somehow seems PSYCHOLOGICALLY like a 
different question.  The distinction is whether the "I" making the choice 
stands to gain some benefit.  I doubt that I would engage in atmospheric 
re-entry surfboarding if I didn't have a recent copy somewhere.  If I did 
have such a copy, I might just do it, because it would be so COOL.


<UL>
  <li>  * * END OF PART ONE * * *

</UL>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0173.html">[ Next ]</a><a href="0171.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0138.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
</ul>
</body></html>
