<!-- received="Mon Nov 15 22:04:16 1999 MST" -->
<!-- sent="Mon, 15 Nov 1999 23:04:48 -0600" -->
<!-- name="David Blenkinsop" -->
<!-- email="blenl@sk.sympatico.ca" -->
<!-- subject="Re: NANO: Institutional Safety" -->
<!-- id="3830E5F0.5DDC8445@sk.sympatico.ca" -->
<!-- inreplyto="NANO: Institutional Safety" -->
<!-- version=1.10, linesinbody=150 -->
<html><head><title>extropians: Re: NANO: Institutional Safety</title>
<meta name=author content="David Blenkinsop">
<link rel=author rev=made href="mailto:blenl@sk.sympatico.ca" title ="David Blenkinsop">
</head><body>
<h1>Re: NANO: Institutional Safety</h1>
David Blenkinsop (<i>blenl@sk.sympatico.ca</i>)<br>
<i>Mon, 15 Nov 1999 23:04:48 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2216">[ date ]</a><a href="index.html#2216">[ thread ]</a><a href="subject.html#2216">[ subject ]</a><a href="author.html#2216">[ author ]</a>
<!-- next="start" -->
<li><a href="2217.html">[ Next ]</a><a href="2215.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2178.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->


<p>
Earlier, Robert J. Bradbury wrote:

<p>
<i>&gt; </i><br>
<a href="2178.html#2216qlink1">&gt; David Blenkinsop &lt;blenl@sk.sympatico.ca&gt; wrote:</a><br>
<i>&gt; </i><br>
<i>&gt; &gt; In the _Diaspora_ novel, the transhuman societies somehow maintain what</i><br>
<i>&gt; &gt; they think of as a wisely nonexponential or nonexpansive security</i><br>
<i>&gt; &gt; arrangement, where they leave enormous tracts of natural resources</i><br>
<i>&gt; &gt; completely untouched.</i><br>
<i>&gt; </i><br>
<i>&gt; There is a declining Return on Investment as you get bigger because you</i><br>
<i>&gt; have increased communications delays and power costs.  Until we adopt</i><br>
<i>&gt; fundamentally different time scales for entertaining "thoughts" (weeks</i><br>
<i>&gt; or months) it may not make sense to utilize all of the natural resources.</i><br>
<i>&gt; (Sure you can turn all of the asteroids into VR simulations of "you",</i><br>
<i>&gt; but what good does that do "you"?!? . . .</i><br>

<p>
<a name="2307qlink1">OK, check me if I'm wrong, but isn't this a bit like asking why any
group of people or any society would tend to expand based on accessible
resources? For one thing, there is a reproductive, or evolutionary
selection pressure in favor of those who both want to, and are
successful at, expanding.</a> This may sound like circular logic, but that's
evolution for you! We should generally tend to see the success stories
of differential selection, rather than the ones that got left behind in
the race.

<p>
<i>&gt; </i><br>
<a href="2178.html#2216qlink2">&gt; &gt; This despite the fact that they could very readily get</a><br>
<i>&gt; &gt; into colonial competition for settling those resources --</i><br>
<i>&gt; </i><br>
<a name="2307qlink2"><i>&gt; The two main motivating forces I see for colonialism were:</i><br>
<i>&gt;  a) a desire for freedom -- but in a personal VR, you have the</i><br>
<i>&gt;     "ultimate" freedom.</i><br>
<i>&gt;  b) the quest for "rare" natural resources (e.g.  gold, silver,</i><br>
<i>&gt;     spices, etc.) -- these aren't "rare" in a nanotech environment.</i><br>

<p>
To this you should add 

<p>
c) the desire for power, whether over other people or over the material
world</a> in general

<p>
<a name="2307qlink3">d) the desire to become influential in the sense of adding to the
success of the ideas, thoughts, or lifestyles that one values</a> the most

<p>
<a name="2307qlink4">e) the inadvertent, or preconscious tendency that humans share with
other living things, to create a certain "influence on history" for
those who happen to be evolutionarily</a> successful

<p>
<a name="2307qlink5">Note that "colonial" is often used to refer to the 19th century West's
empire building and global resource mining. IMO, the term could also be
appropriately used for any particularly intense modern competition over
disputed resources, especially if the resources are newly accessed, like
space resources or ocean resources.</a> What occurs to me is that maybe it's
just politically incorrect to refer to current or future
territory/resource races as "colonial". A better term would be --?
Anyway, following my concern that future societies could get into
"colonial competition for settling resources", I also mentioned that new
resources might be employed by one side for "building an overwhelming
force of arms".

<p>
Robert J. Bradbury wrote:

<p>
<i>&gt; </i><br>
<a href="2178.html#2216qlink3">&gt; In a nanotech environment, the concept of an "overwhelming"</a><br>
<i>&gt; force of arms is very questionable.  You have to guarantee</i><br>
<i>&gt; that you have disassembled *every* last little bit of nanotech</i><br>
<i>&gt; in an enclave that can have berserker potential.</i><br>

<p>
Well, by that reasoning we had better watch out that the representative
factions from every past war don't find some opportunity to poison our
breakfast cereal tomorrow morning! Seriously, in the relatively long run
scenarios that we are discussing here, active shield systems,
nanomedical upgrades, normal security arrangements, etc., are all
supposed to help prevent us from being anonymously murdered by some
random little bit of nanotech that someone hid away.

<p>
Where I think Robert Bradbury perhaps begins to answer my earlier
concern about exponential arms races is when he says:

<p>
<a href="2178.html#2216qlink4">&gt; No, you understand it -- the physical stuff increases exponentially</a><br>
<i>&gt; but that doesn't mean that "efficient" designs do or your ability</i><br>
<i>&gt; to use it effectively does.</i><br>

<p>
It may well be true that even a fairly advanced, replicable nanofactory
setup would have some definite limits in terms of what sheer volume of
manufactured weaponry would anyone find manageable or maintainable.

<p>
Also:

<p>
<i>&gt; </i><br>
<a href="2178.html#2216qlink5">&gt; Claim: The Resource Base grows so much faster than the population</a><br>
<i>&gt; that there is no incentive to fight over the resources until</i><br>
<i>&gt; uploads or AI arrives.</i><br>
<i>&gt;</i><br>

<p>
Ah, now we've got the whole exponential growth problem back again! Say
that our nanofactory building power seekers have their setup programmed
with sophisticated AI? Or, say that the nano-system software upgrades
itself so reliably that it might as well be a strong AI system in terms
of the control that it gives its builders? This won't happen tomorrow,
but it *may* happen as an outgrowth of true nanofactories, on the other
side of the Nano-Singularity, as it were. Same result if the builders
are keen on upload production in the name of pioneering a transhuman
society, such a society might well hit a biosphere killing exponential
growth curve.

<p>
In opposition to the concern that anyone would engage in an exponential
arms race, Rik van Riel wrote:

<p>
<a href="2187.html#2216qlink6">&gt; Why? We've all seen that in the cold war there were 'enough'</a><br>
<i>&gt; weapons to ensure mutual destruction. With nanotech that point</i><br>
<i>&gt; will be reached much earlier and the 'enough' will be even</i><br>
<i>&gt; more extreme. After that there shouldn't be any incentive to</i><br>
<i>&gt; build more weapons (since there's no need for more).</i><br>

<p>
As I recall, there was a lot of debate about how much was enough and
whether the other side might have a decisive *more*. Even in the absence
of AI, suppose we'd had quite capable nanofactories during the Cold War,
mightn't the mass of weapons have come out worse, maybe even *much*
worse?

<p>
Billy Brown wrote:

<p>
<a href="2185.html#2216qlink7">&gt; No, you haven't missed anything.  Nanotechnology makes it almost impossible</a><br>
<i>&gt; for multiple sovereign powers to coexist on a single planet, because they</i><br>
<i>&gt; will tend to find themselves forced into an exponential arms race that</i><br>
<i>&gt; converts the entire mass of the planet into industry and weaponry.  The fact</i><br>
<i>&gt; that there is no room to deploy effective defenses against many high-tech</i><br>
<i>&gt; weapons makes the problem even worse.</i><br>

<p>
Optimistically, I tend to think that a combination of "active shield"
defense systems plus a strong, world level offensive deterrent, *should*
be able to dissuade anyone from kicking into "hyper" nanoweapons
production. Notice, though, that this is potentially a real problem,
also, it doesn't seem particularly easy to decide just when a
"difficult" state should be hit with a deterrent. For instance, do you
wait until there is evidence that State X has produced one whole
mountain load of weapons, or do you wait for two whole mountain loads,
is the deterrent one of bombing, or of disabling their tech somehow, and
so forth? Certainly, it looks to me as though we can't afford to ignore
such problems, even if it's true that the very worst scenarios require
particularly advanced technologies! In sum, I just don't know, how *do*
we prevent nano arms races -- or is maybe just as well to let such races
run their course, until everyone involved pulls back from the brink,
exhausted?


<p>
David Blenkinsop &lt;blenl@sk.sympatico.ca&gt;
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2217.html">[ Next ]</a><a href="2215.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2178.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
</ul>
</body></html>
