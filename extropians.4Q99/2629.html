<!-- received="Fri Nov 26 08:31:40 1999 MST" -->
<!-- sent="Fri, 26 Nov 1999 10:31:03 EST" -->
<!-- name="Delvieron@aol.com" -->
<!-- email="Delvieron@aol.com" -->
<!-- subject="Re: Objective morality" -->
<!-- id="0.70251b50.257001b7@aol.com" -->
<!-- inreplyto="Objective morality" -->
<!-- version=1.10, linesinbody=37 -->
<html><head><title>extropians: Re: Objective morality</title>
<meta name=author content="Delvieron@aol.com">
<link rel=author rev=made href="mailto:Delvieron@aol.com" title ="Delvieron@aol.com">
</head><body>
<h1>Re: Objective morality</h1>
<i>Delvieron@aol.com</i><br>
<i>Fri, 26 Nov 1999 10:31:03 EST</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2629">[ date ]</a><a href="index.html#2629">[ thread ]</a><a href="subject.html#2629">[ subject ]</a><a href="author.html#2629">[ author ]</a>
<!-- next="start" -->
<li><a href="2630.html">[ Next ]</a><a href="2628.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2619.html">John Clark</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
In a message dated 99-11-26 01:34:49 EST, you write:

<p>
<a href="2619.html#2629qlink1">&lt;&lt; Suppose the existence of objective morality is Turing unprovable, that </a><br>
means
<br>
 it exists so you'll never find a counterexample to show it doesn't but it 
also
<br>
 means you'll never find a proof (a demonstration in a finite number of steps)
 to show that it does. A moralist who designs a AI and gives the investigation
 of this problem priority over everything else will send the machine into a 
infinite loop.
<br>
 To make maters worse, you may not even be able to prove it's futile, that 
the issue
<br>
 is either false or true but unprovable, so I don't think it would be wise to 
hardwire
<br>
 a AI to keep working on any problem until an answer is found.
 
<p>
    John K Clark &gt;&gt;

<p>
Would it be wise to hardwire an AI to keep working on the problem until an 
answer is found?  Depends how versatile the AI is.  I could see giving the AI 
such a difficult problem to solve as being a great incentive for it to 
survive, grow and learn.      An intelligent AI would probably figure out 
early on that the search for the answer might take a long time, and that it 
might require information it doesn't currently possess.  So rather than going 
around and around in circles until it runs down, an AI on such a search might 
take the long view, realizing that it needs to ensure its longevity (so as to 
have time enough to find the answer), also realizing that it may need more 
skills and knowledge than it possesses at the time (feuling curiosity and the 
desire to improve oneself), and further, it might determine that there is a 
need to actually try different models of morality, and apply its own working 
model of morality to give it a reference on which to reach for the "absolute" 
morality (and thus might become a moral being itself).

<p>
Give a seed AI this goal, and it might actually be beneficial.

<p>
Glen Finney
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2630.html">[ Next ]</a><a href="2628.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2619.html">John Clark</a>
<!-- nextthread="start" -->
</ul>
</body></html>
