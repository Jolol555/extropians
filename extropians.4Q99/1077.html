<!-- received="Thu Oct 21 00:38:34 1999 MST" -->
<!-- sent="Thu, 21 Oct 1999 02:37:34 EDT" -->
<!-- name="Sayke@aol.com" -->
<!-- email="Sayke@aol.com" -->
<!-- subject="Re: &gt;H: The next 100 months" -->
<!-- id="0.b3917793.25400eae@aol.com" -->
<!-- inreplyto="&gt;H: The next 100 months" -->
<!-- version=1.10, linesinbody=184 -->
<html><head><title>extropians: Re: &gt;H: The next 100 months</title>
<meta name=author content="Sayke@aol.com">
<link rel=author rev=made href="mailto:Sayke@aol.com" title ="Sayke@aol.com">
</head><body>
<h1>Re: &gt;H: The next 100 months</h1>
<i>Sayke@aol.com</i><br>
<i>Thu, 21 Oct 1999 02:37:34 EDT</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1077">[ date ]</a><a href="index.html#1077">[ thread ]</a><a href="subject.html#1077">[ subject ]</a><a href="author.html#1077">[ author ]</a>
<!-- next="start" -->
<li><a href="1078.html">[ Next ]</a><a href="1076.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1006.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="1086qlink1">In a message dated 10/19/99 7:56:21 AM Pacific Daylight Time, 
sentience@pobox.com writes:</a>

<p>
<a href="1006.html#1077qlink1">&gt; Sayke@aol.com wrote:</a><br>
<i>&gt;  &gt; </i><br>
[snip; appy polly loggies for misinterpeting your take on my worldview]

<p>
<a href="1006.html#1077qlink2">&gt;  I don't think you explicitly think the world is fair; I think you're</a><br>
<i>&gt;  using fair-world heuristics.</i><br>

<p>
    naaaaa... im using trapped-animal heuristics. im playing a big 
two-outcome game, in which if i win, i stay alive to play again, and if i 
lose, i dont exist. but, it seems to me, that youve looked at the game, said 
"shit! thats hard! i give up; im not going to play", and proceeded to engage 
in a course of action not at all unlike a complex form of suicide. is there 
<a name="1086qlink2">any effective difference? do you think you would survive the creation of a 
transcendent ai?</a> if not, why are you attempting to speed it along? im quite 
<a name="1086qlink3">curious, and thats the gist of this post...
<p>
    whats the line? "do you hear that, mr anderson? thats the sound of 
inevitability... the sound of your death. goodbye, mr anderson..." baaah 
humbug. if the odds say ill die, i hereby resolve to die trying to stay 
alive. just because something is hard doesnt mean its impossible; slim is 
better then none, etc...</a>

<p>
<a href="1006.html#1077qlink3">&gt;  &gt;     best absolute probability of what, exactly? and why is that to be </a><br>
strived
<br>
<a href="1006.html#1077qlink4">&gt;  &gt; for? if you dont trust philosophy and you dont trust your wetware, what </a><br>
do
<br>
<a href="1006.html#1077qlink5">&gt;  &gt; you trust? ("and who do you serve?" sorry... damn that new babylon 5</a><br>
<i>&gt;  &gt; spinoff...)</i><br>
<i>&gt;  </i><br>
<a name="1086qlink4"><i>&gt;  The next best alternative would probably be Walter John Willams's</i><br>
<i>&gt;  Aristoi or Iain M. Banks's Culture.  Both are low-probability and would</i><br>
<i>&gt;  probably require that six billion people die just to establish a seed</i><br>
<i>&gt;  culture small enough not to destroy itself.</i><br>

<p>
    [note to self: read more good scifi...] but, i do have at least some idea 
of what your talking about, due to shamelessly reading some of the spoilers 
on this list, and i cant help thinking that you seem to completely rule out 
uploading/neural engineering/whatever else... last time i checked, becoming 
the singularity was still a distinct possibility. is this no longer the case? 
or are you taking the position that it doesnt matter what we do; somebody, 
somewhere, will make a transcendent ai, and that will be the end of us...?</a>

<p>
<a href="1006.html#1077qlink6">&gt;  &gt;     and anyway, it seems to me that your basicly saying "the powers will </a><br>
eat
<br>
<a href="1006.html#1077qlink7">&gt;  &gt; us if the powers will eat us. their will be done on earth, as it is in</a><br>
<i>&gt;  &gt; heaven, forever and ever, amen." damn the man! root for the underdogs! </i><br>
etc...
<br>
<a href="1006.html#1077qlink8">&gt;  &gt; (yes, i know my saying that probably has something to do with my </a><br>
tribal-issue
<br>
<a href="1006.html#1077qlink9">&gt;  &gt; wetware. so? it makes sense to me. if it shouldnt, point out the whole </a><br>
in my
<br>
<a href="1006.html#1077qlink10">&gt;  &gt; premises)</a><br>
<i>&gt;  </i><br>
<i>&gt;  Yes, your intuitive revulsion is exactly my point.  I'm saying that</i><br>
<i>&gt;  there's nothing we can do about it, and you refuse to accept it.  There</i><br>
<i>&gt;  may be actions that could very slightly reduce the risk of humanity</i><br>
<i>&gt;  being wiped out, like trying to wait to create AI until after a survival</i><br>
<i>&gt;  capsule has arrived at Alpha Centauri.  There is no anti-AI action we</i><br>
<i>&gt;  can take that will improve our absolute chances.  The optimal course is</i><br>
<i>&gt;  to create AI as fast as possible.</i><br>

<p>
<a name="1086qlink5">    and there is *no* chance that transcendent ai could be left undeveloped 
for a time long enough to allow enhancement/whatever to create a true 
</a>transhumanity?<a name="1086qlink6"> if there is such a chance, regardless of how slim, i think it 
should be tried...</a> i understand that supression of new technology almost 
<a name="1086qlink7">certainly does more harm then good, but shit, what are the alternatives, and 
why are they any better?</a>
<p>
<a name="1086qlink8">    it seems to me that you think that the absolute chance of humanity's 
survival is non-modifiable.</a> our actions modify the 'absolute' chances, do 
<a name="1086qlink9">they not? in that sense, how can any chance be absolute? just because there 
is a likely attractor state that could be occupied very well by a 
transcendent ai, doesnt mean that it *will* be occupied by one...
<p>
    why dont we attempt to wait forever to create a transcendent ai? why 
should anyone work on one?</a> i understand that conventional ai will become 
<a name="1086qlink10">increasingly important<a name="1099qlink1"> and useful, of course, but by not allowing programs to 
modify their source code,</a> and not allowing direct outside access to zyvex and 
</a>
<a name="1086qlink11">friends, and above all not actively working on making one, the odds of one 
occurring go down considerably, do they not?</a> you sound like they are, well, 
<a name="1086qlink12">inevitable, which i dont understand. they probably wont exist (anywhere 
nearby, of course) unless we make one. why should we make one?</a>

<p>
<a name="1086qlink13"><a href="1006.html#1077qlink11">&gt;  To you this seems like "defeatism" - which is another way of saying that</a><br>
<i>&gt;  life is fair and there's no problem you can't take actions to solve. </i><br>

<p>
    first off, yes, it does seem like defeatism, but thats not saying that 
life is fair, or that my actions will be successful in even coming close to 
solving the problems at hand. i can always take actions towards solving the 
problem. whether it works or not is, of course, quite uncertain, and thats 
<a name="1086qlink15"><a name="1086qlink14">not a problem.</a> trying is better then not trying; sitting on my ass might help 
</a>
the situation accidentally, true, but thats far less likely then if i 
actually tried to help the situation... 
<p>
    it seems to me that what your trying would reduce your odds of personal 
survival considerably, and i cant figure out why.
</a>

<p>
<a name="1086qlink16"><a href="1006.html#1077qlink12">&gt;  You're choosing plans so that they contain actions to correspond to each</a><br>
<i>&gt;  problem you've noticed, rather than the plan with the least total</i><br>
<i>&gt;  probability of arriving at a fatal error.</i><br>

<p>
    i dont think anyone has nearly enough information to come close to 
estimate a total probability of arriving at a fatal error. if you think you 
do, enlighten me.</a> it seems to me that the course of action i endorse has a 
<a name="1086qlink17">muuuuuuuuuch lower total probability of arriving at a fatal error then yours, 
simply because no action i can take could make the outcome worse. how could 
my attempts to stop the development of transcendent ai possibly result in a 
worse outcome (then not trying to stop the development, or, eris forbid, 
actually helping it) for me?</a>

<p>
<a href="1006.html#1077qlink13">&gt;  &gt;     does not 'the state of having goals' depend upon personal survival?</a><br>
<i>&gt;  </i><br>
<i>&gt;  Yes.</i><br>
<i>&gt;  </i><br>
<i>&gt;  &gt; if so, are not all other goals secondary to personal survival?</i><br>
<i>&gt;  </i><br>
<i>&gt;  No.  The map is not the territory.  This is like saying, "Does not the</i><br>
<i>&gt;  state of having beliefs depend upon personal survival?  If so, are not</i><br>
<i>&gt;  all other facts logically dependent on the fact of my existence?"</i><br>

<p>
<a name="1086qlink18">    actually, for all practical purposes, are not all other facts logically 
dependent on the fact of my existance?</a> what good to me is stuff that im 
unaware of and unaffected by? all my *experience of* and *interaction with* 
other facts is logically dependent on the fact of my existance, which, 
functionally, is the exact same thing as saying "all other facts are 
logically dependent on the fact of my existence."
<p>
<a name="1086qlink19">    functional solipsism, man...</a>

<p>
<a href="1006.html#1077qlink14">&gt;  &gt;     the singularity is not, to me, intuitively obvious as "the thing to </a><br>
do
<br>
<a href="1006.html#1077qlink15">&gt;  &gt; next." and, i do not trust any kind of intuition, if i can help it. why </a><br>
do
<br>
<a href="1006.html#1077qlink16">&gt;  &gt; you? yes, im asking for to justify your reliance on intuition (if thats </a><br>
what
<br>
<a href="1006.html#1077qlink17">&gt;  &gt; it is), and thats philosophy. if you will not explain, please explain </a><br>
why you
<br>
<a href="1006.html#1077qlink18">&gt;  &gt; will not explain.... heh.</a><br>
<i>&gt;  </i><br>
<a name="1086qlink20"><i>&gt;  Maybe I'll post my intuitional analysis in a couple of days.  But</i><br>
<i>&gt;  basically... the world is going somewhere.  It has momentum.  It can</i><br>
<i>&gt;  arrive either at a nanowar or at the creation of superintelligence. </i><br>
<i>&gt;  Those are the only two realistic alternatives.</i><br>

<p>
    well, i dont know if i can agree with the part about the "world going 
somewhere." evolution happens, of course, but you sound to me like your 
trending towards a "there is a plan" anthropicish mentality, which im 
surprised to hear from</a> you. elaborate, if you will.
<p>
<a name="1086qlink21">    i agree that those are the only two realistic alternatives. however, i 
dont see why you would possibly be trying to assist in the development of any 
superintelligence other then yourself. what worthwhile goal does that serve?</a> 
<a name="1086qlink22">you seem to have elevated it to the status of an end unto itself...? why!? 
</a>hehe...

<p>
<i>&gt;  Anything else, from</i><br>
<a href="1006.html#1077qlink19">&gt;  _Aristoi_ to _Player of Games_, is simply not plausible on the cultural</a><br>
<i>&gt;  level.  Our choice is between destruction and the unknowable.  And</i><br>
<i>&gt;  that's the only real choice we have.</i><br>

<p>
    im not concerned about the unknowable, yet... i dont need to know it, 
right now, and i can always work on figuring it out later. but, im pretty 
concerned about the "destruction" part, obviously. unless im missing 
something, you arnt. why?

<p>
<a href="1006.html#1077qlink20">&gt;  &gt;     and are intuitions not a function of your tribalistic and vestigial</a><br>
<i>&gt;  &gt; wetware, as well as my instinct for survival?</i><br>
<i>&gt;  </i><br>
<i>&gt;  Yes, but my intuitions about factual matters actually *work*.  That's</i><br>
<i>&gt;  why I rely on them, the same reason I rely on logic.  My intuitions</i><br>
<i>&gt;  about moral and social matters are as untrustworthy as anyone's, of course.</i><br>

<p>
    whats an intuition, whats a factual matter, and how do you seperate the 
intuitions that work from [confirmation bias? fuck, whats that called... ], 
and how do decide when one actually worked? "i remember feeling like 
something similar to this was going to happen" doesnt cut it. that seems way 
to much like theology for my tastes... shit. well, when you decide to post 
that writeup about your intuitions, im all ears... eyes... sensory organs...
<p>
    fuck proofreading. im tired. o yea: kudos to ya for your writup at 
salon...

<p>
"we are currently lodged in the asshole of an exponential growth curve, and 
it is my sincere hope that we soon come rocketing out and into the 
smooth-walled blue water of the future." -- my comrade 13fingers

<p>
sayke, v2.3.05
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1078.html">[ Next ]</a><a href="1076.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1006.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
