<!-- received="Wed Nov  3 15:23:09 1999 MST" -->
<!-- sent="Wed, 3 Nov 1999 17:23:00 -0500 (EST)" -->
<!-- name="Dan Fabulich" -->
<!-- email="daniel.fabulich@yale.edu" -->
<!-- subject="Re: Putnam's kind of realism" -->
<!-- id="Pine.GSO.4.10.9911031616400.14208-100000@morpheus.cis.yale.edu" -->
<!-- inreplyto="382084D4.63ABC99@pobox.com" -->
<!-- version=1.10, linesinbody=116 -->
<html><head><title>extropians: Re: Putnam's kind of realism</title>
<meta name=author content="Dan Fabulich">
<link rel=author rev=made href="mailto:daniel.fabulich@yale.edu" title ="Dan Fabulich">
</head><body>
<h1>Re: Putnam's kind of realism</h1>
Dan Fabulich (<i>daniel.fabulich@yale.edu</i>)<br>
<i>Wed, 3 Nov 1999 17:23:00 -0500 (EST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1806">[ date ]</a><a href="index.html#1806">[ thread ]</a><a href="subject.html#1806">[ subject ]</a><a href="author.html#1806">[ author ]</a>
<!-- next="start" -->
<li><a href="1807.html">[ Next ]</a><a href="1805.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1787.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
'What is your name?'  'Eliezer S. Yudkowsky.'  'IT DOESN'T MATTER WHAT
YOUR NAME IS!!!':

<p>
<a href="1787.html#1806qlink1">&gt; It sounds to me like Putnam, or the person explaining Putnam, or</a><br>
<i>&gt; someone, is failing to clearly distinguish between the question "What is</i><br>
<i>&gt; truth?" and "What is rational?"</i><br>

<p>
This isn't an error.  Putnam explicitly argues for their equivalence.
True statements are those statements which, objectively speaking, we
OUGHT to believe.

<p>
Morality is objective in exactly the same sense and in
exactly the same way that rationality is objective.

<p>
[If it helps, here where I say "morality" insert "meaning of life."  Also 
assume that I'll use rationality and morality interchangeably.  If you
don't believe that rationality is morality, then change every instance of 
the word "rationality" in my argument to "morality" and re-read.]

<p>
I have many midterms due this week, so I'm not going to post on this again
for a while, but...

<p>
<i>&gt; The truth precedes us, generated us, and acts according to laws of</i><br>
<a href="1787.html#1806qlink2">&gt; physics which we cannot specify.  There is an objective answer to the</a><br>
<i>&gt; question "What is truth?".</i><br>

<p>
This seems an odd thing to say, since these two claims are directly
incompatible.  An "answer" is a special kind of statement.  There is no
such thing as a statement which cannot be stated.  (If you think that
there can be, then you're using a Very Different definition of statement
than I am.)  If it can't be specified, then it's not an answer.

<p>
Anyway, I hold that there is an objective answer to the question "what is
truth," one which CAN be stated.  I wholeheartedly agree that it preceded
us, was the cause of our creation, etc.  I also believe that the question
"what is truth" is equivalent to the question "what should I believe?"  I
also believe that questions of morality, questions of "what should I do?"
have objective answers.  Thus, "what is truth" has an objective answer
because "what should I believe" has an objective answer.  Finally, "what
should I believe" is exactly what I mean when I say "what is rational," so
it ALSO has an objective answer.

<p>
<a href="1787.html#1806qlink3">&gt; Rationality is the process whereby we attempt to arrive at the truth.  </a><br>
<i>&gt; There is no objective answer to the question "What is rational?", not</i><br>
<i>&gt; *here*, not without direct access to objective reality.  </i><br>

<p>
If you mean that we simply don't have ACCESS to the objective answer to
the question "what is rational," but that access is possible through
thought/approximation, then we're probably in agreement.  This is very
different from saying that the answer does not exist.

<p>
<i>&gt; Rather, "How should rationality work?" is an engineering question</i><br>
<a href="1787.html#1806qlink4">&gt; about how to create systems that can model and manipulate reality - or</a><br>
<i>&gt; rather, how to create parts of reality whose internal patterns mirror</i><br>
<i>&gt; the whole, to the point that the internal process can predict the</i><br>
<i>&gt; external processes in advance.</i><br>

<p>
Here, again, we're in agreement.  But what makes you think we have access
to the answer to "how should rationality work?"  That use of should drops
this question in the realm of morality.  This question should be as dimly
lit (or as brightly lit) as the question "what is rational."  (Indeed,
under my account, you should have EXACTLY as much certainty about both of
these questions.)

<p>
<a href="1787.html#1806qlink5">&gt; As for AI, it seems to me that the concept of an internalist mental</a><br>
<i>&gt; model is a confusion between "is" and "should".  A reference to "green"</i><br>
<i>&gt; *is* the cognitive concept of "green", but what it *should* be, what the</i><br>
<i>&gt; system tries to make it converge to, is the external reality of green. </i><br>

<p>
But the words "external reality" CANNOT REFER to external reality in the
sense that you use it.  It can refer to your own internal construct which
you call "external reality," but you can't actually cook up a theory of
reference that can make the connection from your words to the right thing
under your conception.

<p>
Having said that, I again assert that positing the existence of an
external reality is a very good idea, we objectively ought to do it, and,
according to my theory of truth, I assert that it is TRUE that external
reality exists, and that our words model it.  We both agree that our
beliefs SHOULD line up with external reality.  However, I know what my
words mean when I say that, HOW they refer to external reality; you do not
appear to.

<p>
<a href="1787.html#1806qlink6">&gt; If you have a wholly internalist system, then the concepts don't</a><br>
<i>&gt; converge to anything.  If the only definition of correctness is the</i><br>
<i>&gt; thought itself, there's no way to correct malfunctions.  The system is</i><br>
<i>&gt; meta-unstable.  The AI thinks "I can't possibly be wrong; anything I</i><br>
<i>&gt; think is by definition correct," and then it gets silly, just like</i><br>
<i>&gt; subjectivist humans.</i><br>

<p>
Again, this is absolutely wrong.  Our beliefs converge onto true beliefs.
(Or, at least, they OUGHT to converge onto true beliefs!)  No AI could
rationally accept the claim that "I can't possibly be wrong."  The claim
is OBVIOUSLY irrational.  The subjectivists buy it, but Putnam doesn't,
and I sure don't.

<p>
Again:

<p>
"Internalism is not a facile relativism that says, 'Anything goes'.
Denying that it makes sense to ask whether our concepts 'match' something
totally uncontaminated by conceptualization is one thing; but to hold that
every conceptual system is therefore just as good as every other would be
something else.  ... Internalism does not deny that there are experiential
*inputs* to knowledge; knowledge is not a story with no constraints except
*internal* coherence; but it does deny that there are any inputs *which
are not themselves to some extent shaped by our concepts*, by the
vocabulary we use to report and describe them, or any inputs *which admit
of only one description, independent of all conceptual choices*."

<p>
-Dan

<p>
      -unless you love someone-
<br>
    -nothing else makes any sense-
<p>
           e.e. cummings
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1807.html">[ Next ]</a><a href="1805.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1787.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
