<!-- received="Wed Nov  3 11:51:09 1999 MST" -->
<!-- sent="Wed, 03 Nov 1999 12:54:13 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Putnam's kind of realism" -->
<!-- id="382084D4.63ABC99@pobox.com" -->
<!-- inreplyto="Putnam's kind of realism" -->
<!-- version=1.10, linesinbody=29 -->
<html><head><title>extropians: Re: Putnam's kind of realism</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Putnam's kind of realism</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Wed, 03 Nov 1999 12:54:13 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1787">[ date ]</a><a href="index.html#1787">[ thread ]</a><a href="subject.html#1787">[ subject ]</a><a href="author.html#1787">[ author ]</a>
<!-- next="start" -->
<li><a href="1788.html">[ Next ]</a><a href="1786.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="1749.html">Damien Broderick</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1806.html">Dan Fabulich</a>
</ul>
<!-- body="start" -->

<p>
<a name="1806qlink1">It sounds to me like Putnam, or the person explaining Putnam, or
someone, is failing to clearly distinguish between the question "What is
truth?" and "What is rational?"</a>  The truth precedes us, generated us,
and acts according to laws<a name="1806qlink2"> of physics which we cannot specify.  There is
an objective answer to the question "What is truth?".</a>  Rationality is
<a name="1806qlink3">the process whereby we attempt to arrive at the truth.  There is no
objective answer to the question "What is rational?", not *here*, not
without direct access to objective reality.</a>  Rather, "How should
rationality work?" is an engineering question<a name="1806qlink4"> about how to create
systems that can model and manipulate reality - or rather, how to create
parts of reality whose internal patterns mirror the whole, to the point
that the internal process can predict the external processes in advance.</a>

<p>
<a name="1806qlink5">As for AI, it seems to me that the concept of an internalist mental
model is a confusion between "is" and "should".  A reference to "green"
<br>
*is* the cognitive concept of "green", but what it *should* be, what the<br>
system tries to make it converge to, is the external reality of green.</a> 
<a name="1806qlink6">If you have a wholly internalist system, then the concepts don't
converge to anything.  If the only definition of correctness is the
thought itself, there's no way to correct malfunctions.  The system is
meta-unstable.  The AI thinks "I can't possibly be wrong; anything I
think is by definition correct," and then it gets silly, just like
subjectivist humans.</a>  Shades of Greg Egan's _Quarantine_.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1788.html">[ Next ]</a><a href="1786.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="1749.html">Damien Broderick</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1806.html">Dan Fabulich</a>
</ul>
</body></html>
