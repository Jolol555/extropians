<!-- received="Wed Dec  8 06:20:18 1999 MST" -->
<!-- sent="Wed, 8 Dec 1999 05:20:09 -0800 (PST)" -->
<!-- name="Robert J. Bradbury" -->
<!-- email="bradbury@www.aeiveos.com" -->
<!-- subject="Re: Uploaded memories" -->
<!-- id="Pine.UW2.4.20.9912080356190.829-100000@www.aeiveos.com" -->
<!-- inreplyto="19991208065234.36479.qmail@hotmail.com" -->
<!-- version=1.10, linesinbody=139 -->
<html><head><title>extropians: Re: Uploaded memories</title>
<meta name=author content="Robert J. Bradbury">
<link rel=author rev=made href="mailto:bradbury@www.aeiveos.com" title ="Robert J. Bradbury">
</head><body>
<h1>Re: Uploaded memories</h1>
Robert J. Bradbury (<i>bradbury@www.aeiveos.com</i>)<br>
<i>Wed, 8 Dec 1999 05:20:09 -0800 (PST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3435">[ date ]</a><a href="index.html#3435">[ thread ]</a><a href="subject.html#3435">[ subject ]</a><a href="author.html#3435">[ author ]</a>
<!-- next="start" -->
<li><a href="3436.html">[ Next ]</a><a href="3434.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3420.html">Kate Riley</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3446.html">Anders Sandberg</a>
</ul>
<!-- body="start" -->



<p>
On Wed, 8 Dec 1999, Kate Riley wrote:

<p>
<a href="3420.html#3435qlink1">&gt; (Please forgive me if this has been asked and answered previously.  I have </a><br>
<i>&gt; been lurking for a bit, but by no means am I familiar with everything that </i><br>
<i>&gt; has been said and discussed.)</i><br>

<p>
<a name="3461qlink1">A high pitched off-stage voice says, "Oh no, a lurker!"
A man in a white coat steps forward, says in a low-pitched, somewhat
menacing voice, "Quick get a lurker trap, we can use them for the
Homo chlorophyllicus experiments and no one will know the difference."</a>

<p>
<i>&gt; </i><br>
<a href="3420.html#3435qlink2">&gt; Concerning uploaded individuals.  Considering that such a being would, </a><br>
<i>&gt; presumably, be immortal, and considering that such a being would wish to </i><br>
<i>&gt; retain all of its memories (and probably with more clarity than is afforded </i><br>
<i>&gt; by our protein brains, which are notorious for storing memories in a </i><br>
<i>&gt; somewhat fuzzy and incomplete manner), there would seem to be some concern </i><br>
<i>&gt; about space.</i><br>

<p>
<a name="3461qlink2">All you people that have been on the list for years, you better look
out, some of these lurkers look like very quick studies.</a>

<p>
<a href="3420.html#3435qlink3">&gt; Certainly my computer could not hold even a significant </a><br>
<i>&gt; fraction of my memories.</i><br>

<p>
<a name="3461qlink3"><a name="3446qlink1">Seriously (can I do that here?), though I can't remember the paper
reference (I do have it, but I have so many papers...) research at
Bell Labs seems to indicate that your computer *could* hold a
large fraction of your memories.  The experiments they have done
seem to indicate you store only a few bits a second.</a>  Working
this out:
<p>
   (a generous 8 bits * 60sec * 60min * 16hrs * 365.25 days * 75+ years)
puts you up in the range of 2.4 gigabytes.  This is a really
conservative estimate, the Bell Labs paper pegged the number
at more like a few hundred megabyutes.  So you *can* buy single
hard drives now (for a few hundred $) now that can store the
brain's recall capacity.  If hard drive's aren't fast enough for
you, the cost for this much computer DRAM memory is about $7000.
We currently have processors that can address this much memory.
[Lookout, while we aren't looking we are becoming obsolete...]
</a>

<p>
<a name="3446qlink2">As I've mentioned in another note, the question remains open
<a name="3483qlink1">whether recall capacity and recognition capacity are the same.
As someone, Hal perhaps?, pointed out, there may be a fair
amount of compression occuring.<a name="3461qlink4">  I don't have to remember
walking to school everyday, I only have to remember odd
events that have occured on a few of the days that I
walked to school on top of a general pattern of walking
to school.</a></a>
</a>

<p>
<a href="3420.html#3435qlink4">&gt;  Now, admittedly, my brain is comparatively small, </a><br>
<i>&gt; and seems to do alright, but most, if not all, of us have witnessed the loss </i><br>
<i>&gt; of memories which tends to occur in the later years of a person's life,</i><br>

<p>
Use it or lose it.

<p>
<a href="3420.html#3435qlink5">&gt; which results partially from the fact that there is a limited space capacity </a><br>
<i>&gt; for memories.</i><br>

<p>
<a name="3446qlink3">I don't think this is proven.  More likely, as I think Anders has
pointed out, you may get loss of memory because you don't exercise
the synaptic connections and they gradually lose strength relative
to other synaptic connections.  You may occasionally "recover" a
memory if you happen to trigger a global pattern that pieces
together a memory from widely separated subfragments across the brain.</a>

<p>
<a name="3461qlink5"><a href="3420.html#3435qlink6">&gt; One would want to retain all memories,</a><br>

<p>
Some of us would.  Spike seems to be hell bent on forgetting them.</a>

<p>
&gt; and therefore would want upgrades, spare hard drives, if you will.<br>
&gt; Is this a concern?<br>

<p>
Perhaps, in my mind it depends on how many *really* good stories you
have.  I am far far away from the point where I have to start
dumping good stories due to memory limitations.  Even when I
get to that point, I suspect I can't rent some of Spike's unused
capacity.  I'll probably have to encrypt the stories though, otherwise
he might go using them as his own...  

<p>
<a name="3461qlink6">Is this more of a concern if you want to know "everything"?
Yes.  But I think most of us are willing to let others be the
"experts" in the things we aren't interested in.  I for example, am
*completely* willing to let Robert and Damien be the experts
on "bathos".</a>

<p>
<a href="3420.html#3435qlink7">&gt; Is it realistic to expect resources to hold out?</a><br>

<p>
It becomes a problem only if we set our memory "valuations" above
the resource base.  One of the things that is interesting in science,
is that as we refine principles, theories and laws, the details that
lead us to those conclusions become less important.  If I have an
abstraction that I know can be used to go generate a lot of
content, then it may be unimportant for me to remember the
content itself.  Think of all of the pretty pictures that you
can get with a fractal equation and a few initial variables.

<p>
The problem with humans as currently structured is that the
memory mechanism is on autopilot with no delete key.  We
aren't consciously saying "Do I really need to remember this?".

<p>
<a href="3420.html#3435qlink8">&gt; How many people are expected to ultimately be uploaded?</a><br>

<p>
All of them that want to be.  My estimates for Matrioshka Brains
(solar system sized nanocomputer clusters) is that you get
something like 10^26 (100 trillion trillion) human brain
processor equivalents and something greater than 10^28
in human recall capacity (memory) equivalents.

<p>
<a name="3461qlink7">The problem isn't with capacity for uploads, the problem is
with capacity for copies.   I can't speak for other list members,
but I think 10^16 copies of myself (which is what everyone can
have) makes no sense at all (it certainly gets Spike worried).</a>  

<p>
So, what you have is uploading, followed by a *huge* increase
in personal capacity, probably involving some mental slow-down
as you become physically really large (much larger than planetary
sized) and then the crystal ball gets really cloudy.

<p>
It may be quite possible, that we all decide to stop growing our
minds at asteroid or moon size because of diminishing returns
or self-management problems.  The excess computronium at that
point gets devoted to offspring that we produce by mind division
(copying?) and mating.  But since we do those things "consciously"
and the phase space for consciousness is huge, it may take quite some
time to figure out how to use those resources.  Some people
on the list would support random evolution of ArtificialBrains
(ABrains) but since we aren't anywhere near a consensus on whether
or not its moral if we want or have to unplug these and whether or
not we can keep them from turning on us, I think the jury is still out
on whether this is a good idea.

<p>
Great observations and questions though.

<p>
Robert
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3436.html">[ Next ]</a><a href="3434.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3420.html">Kate Riley</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3446.html">Anders Sandberg</a>
</ul>
</body></html>
