<!-- received="Fri Jan 23 04:52:35 1998 MDT" -->
<!-- sent="Fri, 23 Jan 1998 11:55:12 +0000" -->
<!-- name="Charlie Stross" -->
<!-- email="charlie@antipope.org" -->
<!-- subject="Re: What is rationality? (Was: Re: Public Relations (and the Extropian Elite))" -->
<!-- id="199801230700.XAA12256@mercury.colossus.net" -->
<!-- inreplyto="34C8281A.E4381EE2@itsa.ucsf.edu" -->
<title>extropians: Re: What is rationality? (Was: Re: Public Relations (and the Extropian Elite))</title>
<h1>Re: What is rationality? (Was: Re: Public Relations (and the Extropian Elite))</h1>
Charlie Stross (<i>charlie@antipope.org</i>)<br>
<i>Fri, 23 Jan 1998 11:55:12 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1062">[ date ]</a><a href="index.html#1062">[ thread ]</a><a href="subject.html#1062">[ subject ]</a><a href="author.html#1062">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1063.html">James Daugherty: "Re: Gov't loves gov't"</a>
<li> <b>Previous message:</b> <a href="1061.html">Anders Sandberg: "Re: 1000 humans in a grain of salt"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Thu, Jan 22, 1998 at 10:18:25PM -0700, Tony Benjamin Csoka wrote:<br>
<i>&gt; </i><br>
<i>&gt; The "vibe" seems pretty rational to me. One could propose that the</i><br>
<i>&gt; primary purpose of any living entity is to increase extropy. By killing</i><br>
<i>&gt; other conscious entities for material or other forms of gain, the net</i><br>
<i>&gt; "extropic content" of the universe is decreased. Personally, I think it</i><br>
<i>&gt; _is_ possible to make a tight link between personal ethics and</i><br>
<i>&gt; "rational" thought.</i><br>
<p>
There are other arguments in favour of a rational basis for personal<br>
ethics. Let's start by assuming  that everyone wants to optimize their<br>
own prospects in social or market exchanges. On that basis, we can<br>
consider life as a rather more complex version of a classic iterated<br>
prisoner's dilemma. To recap: if two people cooperate, they get a small<br>
payoff. If one cooperates and the other defects, the defector gets a<br>
small payoff and the cooperator is stung. If both defect, they both get<br>
stung. Now make this a repeating situation, where both players can<br>
remember the other's previous behaviour: it turns out that the optimal<br>
strategy is to always cooperate _unless_ in the last round the other<br>
player stabbed you in the back. But don't hold grudges -- if they<br>
start cooperating again, follow their example.<br>
<p>
Apply this principle in general and you get something not unlike the<br>
"golden rule"; do unto others what you'd like them to do until you (but<br>
don't turn the other cheek if they spit in your eye).<br>
<p>
Is it rational to want to base your ethics on a game-theoretical<br>
approach to maximizing prosperity?<br>
<p>
<p>
<p>
-- Charlie<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1063.html">James Daugherty: "Re: Gov't loves gov't"</a>
<li> <b>Previous message:</b> <a href="1061.html">Anders Sandberg: "Re: 1000 humans in a grain of salt"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
