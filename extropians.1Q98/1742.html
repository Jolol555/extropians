<!-- received="Thu Feb 12 07:16:08 1998 MDT" -->
<!-- sent="Thu, 12 Feb 1998 08:16:25 -6" -->
<!-- name="Craig Presson" -->
<!-- email="dhr@iname.com" -->
<!-- subject="Re: Truth Machines and Open Networks (and FI)" -->
<!-- id="199802121415.OAA05142@zephyr.traveller.com" -->
<!-- inreplyto="Truth Machines and Open Networks (and FI)" -->
<title>extropians: Re: Truth Machines and Open Networks (and FI)</title>
<h1>Re: Truth Machines and Open Networks (and FI)</h1>
Craig Presson (<i>dhr@iname.com</i>)<br>
<i>Thu, 12 Feb 1998 08:16:25 -6</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1742">[ date ]</a><a href="index.html#1742">[ thread ]</a><a href="subject.html#1742">[ subject ]</a><a href="author.html#1742">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1743.html">Halperin, Jim: "Amazon.com interview"</a>
<li> <b>Previous message:</b> <a href="1741.html">Lee Daniel Crocker: "Re: Searle"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On 12 Feb 98 at 6:10, Brent Allsop wrote:<br>
<p>
<i>&gt; Great Post Wax &lt;yakwax@yahoo.com&gt;!</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; So would you mind giving up privacy for absolute truth?</i><br>
<i>&gt; </i><br>
<i>&gt;  YES!</i><br>
<i>&gt; </i><br>
<p>
I'd mind, too. I finally ordered (through the alcor.org link to <br>
Amazon -- Amazon's one-click ordering is _dangerous_!) and read _The <br>
First Immortal_ -- now I have to go to the archives and read the <br>
threads I skipped because they had FI spoilers!<br>
<p>
I agree FI is a terrific read. I sucked it down, using all my spare <br>
time out of about 30 calendar hours earlier this week. I even missed <br>
some sleep for it. Found a distinct Heinleinian flavor to it at the <br>
end ...<br>
<p>
IF a truth machine is possible -- and given some of the skilled liars I <br>
have known, I wonder -- it could be used, per Halperin, to put an end <br>
to at least part of politics-as-usual, and to provide assurance of good <br>
faith in contracts (insert digression about how to implement truth <br>
certificates on the Net, since many contracts are made without f2f <br>
contact). However, in the context of the book it felt to me like a Deus <br>
ex Machina -- enter the truth machine, exit the gray-goo and rogue-AI <br>
problems. I'd still prefer a technical solution to the former, and will <br>
have to remain skeptical about the latter. Halperin has it both ways in <br>
FI: his AIs start with no survival imperative or emotions, yet they <br>
evolve. To be sure those traits are never introduced, you'd have to <br>
prove one of two propositions:<br>
<p>
1. That those traits can't evolve from AIs that don't start with them;<br>
2. That a machine can recognize those traits in another machine it is <br>
building, so that it can choose another design approach to avoid <br>
introducing them.<br>
<p>
My gut feeling is that 1 is not true and 2 is difficult. <br>
<p>
Further, I think emotion, or some analogue of it, is tied in to<br>
intelligence at fundamental levels, and that Halperin's<br>
perfectly-Apollonian AIs would have to be able to at least model human<br>
emotion and drives to do what they do (create art and act as judges,<br>
for two instances). (I used to have some references on this, but I<br>
think my Labrador ate them. I apologize for glossing over such an<br>
interesting point).<br>
<p>
Well, the book provides a lot of food for thought. I did catch a couple<br>
of minor technical flaws, but I will be surprised if I am the first to<br>
see them.<br>
<p>
I have to cut this short, being still a member of the involuntary work <br>
force &lt;grin&gt;.<br>
<p>
<p>
-- dhr@iname.com (Freeman Craig Presson)<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1743.html">Halperin, Jim: "Amazon.com interview"</a>
<li> <b>Previous message:</b> <a href="1741.html">Lee Daniel Crocker: "Re: Searle"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
