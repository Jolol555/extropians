<!-- received="Mon Jan 26 05:26:37 1998 MDT" -->
<!-- sent="Mon, 26 Jan 1998 12:30:14 +0000" -->
<!-- name="Charlie Stross" -->
<!-- email="charlie@antipope.org" -->
<!-- subject="Re: (Ir)Relevance of Transhumanity in a Strong AI Future" -->
<!-- id="18167%mrob.uucp@spdcc.com" -->
<!-- inreplyto="0014500017693120000002L002*@MHS" -->
<title>extropians: Re: (Ir)Relevance of Transhumanity in a Strong AI Future</title>
<h1>Re: (Ir)Relevance of Transhumanity in a Strong AI Future</h1>
Charlie Stross (<i>charlie@antipope.org</i>)<br>
<i>Mon, 26 Jan 1998 12:30:14 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1160">[ date ]</a><a href="index.html#1160">[ thread ]</a><a href="subject.html#1160">[ subject ]</a><a href="author.html#1160">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1161.html">mark@unicorn.com: "Re: Gov't Loves Gov't"</a>
<li> <b>Previous message:</b> <a href="1159.html">Charlie Stross: "Re: Gov't loves Gov't"</a>
<li> <b>In reply to:</b> <a href="1087.html">DOUG.BAILEY@ey.com: "(Ir)Relevance of Transhumanity in a Strong AI Future"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1162.html">mark@unicorn.com: "Re: (Ir)Relevance of Transhumanity in a Strong AI Future"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Fri, Jan 23, 1998 at 06:01:15PM -0500, DOUG.BAILEY@ey.com wrote:<br>
<i>&gt; What relevance does transhumanity have in a future where the strong AI</i><br>
<i>&gt; hypothesis turns out to be true?  The intuitive answers appears to be "very</i><br>
<i>&gt; little".  To the extent that we can create artificial minds that are more</i><br>
<i>&gt; intelligent that the human mind, transhumanity would appear to have the same</i><br>
<i>&gt; significance in such a world as trans-raccoonism has today.</i><br>
<p>
What version of AI do you have in mind?<br>
<p>
There's a fascinating essay by Vernor Vinge (Whole Earth Review, Fall 1994<br>
issue) in which he makes a distinction between "weakly superhuman" AI<br>
and "strongly superhuman" AI. <br>
<p>
In a nutshell: weak superhuman AI is qualitatively human-equivalent. It<br>
may run faster than us (cramming a month of thinking into a few minutes)<br>
but it's still not fundamentally better at thinking than a human being<br>
with equivalent information resources and subjective time.<br>
<p>
A strongly superhuman AI is a different kettle of fish: it compares to us<br>
-- or to a weakly superhuman AI -- the way we compare to a dog or a squirrel.<br>
<p>
Vinge postulates that if AI is possible, what we get to start with is a<br>
weakly superhuman AI. However, by adding resources we can enable the <br>
WSAI to do a lot more thinking in a given period of time than we can --<br>
so if it is _possible_ for human-equivalents to give rise to something<br>
qualitatively superior, then the WSAI probably will generate a SSAI<br>
(and do so rather faster than human onlookers expect).<br>
<p>
An intereting point is that Vinge bases his idea of the singularity on<br>
the immediate consequences of an SSAI emerging within our light cone. We<br>
simply _can't_ guess at such an entity's motivations or interests. It<br>
might be an omnibenevolent Tipler-style god-wannabee, or it might be a<br>
Terminator-style kill-em-all Skynet; if you ask me, both <br>
contingencies are about equal (low) probabilities.<br>
<p>
One possibility that occurs to me is that there's an obvious trade-off<br>
between expansion in space and expansion in time. If you want to get lots of<br>
thinking done (but a finite amount -- this isn't a rehash of the old debate<br>
over closed/open universes) do you prefer to expand into a large spherical<br>
region of space, or to stay tightly focussed, run slower, and occupy a long<br>
tubular slice of spacetime? Hmm...<br>
<p>
<p>
<p>
-- Charlie<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1161.html">mark@unicorn.com: "Re: Gov't Loves Gov't"</a>
<li> <b>Previous message:</b> <a href="1159.html">Charlie Stross: "Re: Gov't loves Gov't"</a>
<li> <b>In reply to:</b> <a href="1087.html">DOUG.BAILEY@ey.com: "(Ir)Relevance of Transhumanity in a Strong AI Future"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1162.html">mark@unicorn.com: "Re: (Ir)Relevance of Transhumanity in a Strong AI Future"</a>
<!-- reply="end" -->
</ul>
