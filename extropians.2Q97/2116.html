<!-- received="Wed Jun 25 11:53:08 1997 MDT" -->
<!-- sent="Wed, 25 Jun 1997 13:14:51 -0400" -->
<!-- name="Michael Lorrey" -->
<!-- email="retroman@tpk.net" -->
<!-- subject="Re: Deep Blue colmun in TIME, May 19th" -->
<!-- id="9705258672.AA867265675@ccmgate.platinum.com" -->
<!-- inreplyto="Deep Blue colmun in TIME, May 19th" -->
<title>extropians: Re: Deep Blue colmun in TIME, May 19th</title>
<h1>Re: Deep Blue colmun in TIME, May 19th</h1>
Michael Lorrey (<i>retroman@tpk.net</i>)<br>
<i>Wed, 25 Jun 1997 13:14:51 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2116">[ date ]</a><a href="index.html#2116">[ thread ]</a><a href="subject.html#2116">[ subject ]</a><a href="author.html#2116">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2117.html">Tony Hollick: "Re:  Richard M. Bissell, jr."</a>
<li> <b>Previous message:</b> <a href="2115.html">Rick Knight: "Deep Blue Column in TIME"</a>
<li> <b>Maybe in reply to:</b> <a href="2055.html">Michael Lorrey: "Deep Blue colmun in TIME, May 19th"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Mark Crosby wrote:<br>
<i>&gt; </i><br>
<i>&gt; Michael Lorrey wrote:</i><br>
<i>&gt; &lt;David Gelernter may be a professor of computer science at Yale, but</i><br>
<i>&gt; any informed individual can see that he is not only misinfomed about</i><br>
<i>&gt; his own field, but hopelessly behind the times with respect to our</i><br>
<i>&gt; knowledge of neurophysiology if his column in the May 19th issue is</i><br>
<i>&gt; any indication of his knowledge of progress in artificial intelligence</i><br>
<i>&gt; and brain/machine interface.&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; I doubt very much if Gelernter writes a "column" for TIME! </i><br>
<p>
It wasn't an article, because it was filled with personal opinion and<br>
analysis, that which is normally seen in a column or editorial. He isn't<br>
an editor so he must be a columnists. Maybe an irregular one, maybe a<br>
highly irregular one, but thats the shoe that fits.<br>
<p>
You should<br>
<i>&gt; do a little more research about someone before you flame them based on</i><br>
<i>&gt; a heavily-edited, popular-magazine sidebar.  As Damien Broderick</i><br>
<i>&gt; pointed out, Gelernter is the author of _Mirror Worlds: Or the Day</i><br>
<i>&gt; Software Puts the Universe Into a Shoebox: How It Will Happen and What</i><br>
<i>&gt; it Will Mean_ (1992), one of the premier visions of cyberspace, and</i><br>
<i>&gt; he’s also the developer of one of the first parallel processing</i><br>
<i>&gt; languages, LINDA.  Gelernter’s also the author of _The Muse in the</i><br>
<i>&gt; Machine_.</i><br>
<p>
Then why the hell did he wax so spiritual about the impossibility? Does<br>
this mean that his fiction and futuristic projections are merely cons<br>
targeted at a market?<br>
<p>
<i>&gt; </i><br>
<i>&gt; A lot of serious AI and cybernetics researchers have doubts about</i><br>
<i>&gt; whether human-level intelligence can be represented by anything</i><br>
<i>&gt; resembling a traditional von Neumann computer. These doubts don’t</i><br>
<i>&gt; necessarily imply that they’re worried about a soul or something.</i><br>
<i>&gt; </i><br>
<i>&gt; Mike, throughout your essay you equate AI with brain-machine</i><br>
<i>&gt; interfacing, two very different subjects.</i><br>
<i>&gt; </i><br>
<i>&gt; &lt;Any computer professional has heard of Moore's Law ... This is the</i><br>
<i>&gt; sort of technological progress that has the inflation and productivity</i><br>
<i>&gt; estimators at the Bureau of Labor Statistics in apopleptic fits.&gt;</i><br>
<p>
Its merely a minor digression that further shows how even the "experts"<br>
have trouble acknowledging where things are really going due to their<br>
vested opinions.<br>
<p>
<i>&gt; </i><br>
<i>&gt; So what does this have to do with the price of tea in China? (Are you</i><br>
<i>&gt; suggesting us incompetent BLS bureaucrats would actually get</i><br>
<i>&gt; apoplectic over our work? I thought we’re all supposed to be parasitic</i><br>
<i>&gt; drones!  (-;)  I don’t doubt that microprocessors are becoming more</i><br>
<i>&gt; and more ubiquitous, but, as I noted in our discussion on this earlier</i><br>
<i>&gt; this year, the biggest chunk of the average person’s expenditures</i><br>
<i>&gt; still go for such mundane necessities as food and shelter.</i><br>
<p>
And what does this have to do with the price of tea, either? Obviously<br>
since computers are making everything else more efficiently run and<br>
made, this would have a synergistic effect on many ohter products, a<br>
portion of whose cost is in the technology used to make, store,<br>
transport, and sell it. Cheaper or more efficient technology at all of<br>
these levels leads to lower product costs across the board, and<br>
therefore, to lower inflation. Its no mistake that this same bunch<br>
refuse to recognise the validity of dynamic scoring in tax reduction,<br>
entitlement adjustments, etc.<br>
<i>&gt; </i><br>
<i>&gt; &lt;Gelernter is right in saying it is silly to ascribe intelligence to</i><br>
<i>&gt; Deep Blue, as that computer has as much processing power as say, a</i><br>
<i>&gt; housefly or a maybe even a mouse.&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; Deep Blue was not just a ‘computer’ - there was a muse in the machine,</i><br>
<i>&gt; namely the team of people who programmed it with a knowledge of chess.</i><br>
<p>
Obviously, its a right piece of magic to make a computer with the<br>
processing power of a mouse to beat the Chess Champion of the World.<br>
Imagine what they could do with a computer the size of a human brain.<br>
<p>
<p>
<i>&gt; &lt;While he has the right to claim some supernatural source of our</i><br>
<i>&gt; being, so far he cannot prove that the human mind is nothing more than</i><br>
<i>&gt; a biochemical computer with a neural net architecture. Given this, we</i><br>
<i>&gt; can duplicate that architecture in other forms, even silicon.&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; And you cannot yet prove that the human mind *is* nothing more than a</i><br>
<i>&gt; biochemical computer with a neural net architecture.  So your "given</i><br>
<i>&gt; this" assumption is hardly justified. There are serious scientists who</i><br>
<i>&gt; don’t believe in "supernatural sources of our being" yet still have</i><br>
<i>&gt; doubts about whether traditional computer architectures, and even</i><br>
<i>&gt; neural nets, are adequate to duplicate what the human mind does.  You</i><br>
<i>&gt; might check the Principia Cybernetica web for starters.</i><br>
<i>&gt; </i><br>
<i>&gt; &lt; Associative thinking is, in fact, largely how we think most of the</i><br>
<i>&gt; time, but this can be duplicated by any relational database.&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; Then why is it that hundreds of AI researchers working for decades</i><br>
<i>&gt; have yet to build anything close to a human associative memory.  Why</i><br>
<i>&gt; is it that cybernetic systems (real-time process control systems) and</i><br>
<i>&gt; even management decision support systems shun relational DBMS for</i><br>
<i>&gt; object-oriented or multidimensional approaches?</i><br>
<p>
Merely more elaborate approaches to the same concept do not negate the<br>
validity of the concept. If you look at the previous Deep Blue machine,<br>
you will see that it had half of the processing power of the one that<br>
beat Kasparov. Obviously, a human level artificial database will need to<br>
associate aural, visual, tonsorial as well as memories of all sorts of<br>
other modes of perception in many different ways.  This is known. What<br>
is lacking is a machine with enough oomph to do the job. In the interim,<br>
we may be stifled to research with entities like Vinge's DON.MAC AI<br>
Mailman kernel, needing hours of processing time to simulate a few<br>
moments of intelligence. The breakthrough is a machine that can simulate<br>
on a one to one ratio with processing time, or greater.<br>
<i>&gt; </i><br>
<i>&gt; &lt;The claim that thinking is dictated by emotion is utterly false. At</i><br>
<i>&gt; best our thinking is muddied or even given greater color by our</i><br>
<i>&gt; emotions. Studies have shown that heightened emotions tend to give</i><br>
<i>&gt; greater reinforcement to certain memories, especially experiences</i><br>
<i>&gt; heavily laden with adrenaline, but this behavior can also be easily</i><br>
<i>&gt; accounted for in programming if needed.&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; If there’s no emotion associated with it, you’re unlikely to remember</i><br>
<i>&gt; it.  Again, you’re setting up a straw-man:  Most cognitive scientists</i><br>
<i>&gt; don’t claim that thinking is *dictated* by emotion, but they do</i><br>
<i>&gt; believe that the ‘emotional’ organs of the brain are an essential part</i><br>
<i>&gt; of the long-term memory system.</i><br>
<p>
Because the level of emotion, especially adrenaline linked, amplifies<br>
the clarity of the memory, though distortion of a very clear memory into<br>
false memories or merely false details (as I already stated and you cut<br>
out of the reply) is possible. This can also be programmed into a<br>
machine as a function of the priority of the memory based on the amount<br>
of data input per time period (i.e. the amount of "action") at the time<br>
of the experience.<br>
<i>&gt; </i><br>
<i>&gt; &lt;He goes on, in the end, to state that even if computers are ever</i><br>
<i>&gt; capable of "simulating human thought", they will never be fully human.</i><br>
<i>&gt; "The gap between human and surrogate is permanent and will never be</i><br>
<i>&gt; closed." This is also false. There have been in the last year,</i><br>
<i>&gt; breakthroughs in electronic interfaces between computer chips and</i><br>
<i>&gt; human neurons.&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; You’re mixing apples and oranges again.  Gelernter is talking about AI</i><br>
<i>&gt; - simulating a human-level mind on a computer - a completely different</i><br>
<i>&gt; subject from man-machine interfaces.  Gelernter is talking about the</i><br>
<i>&gt; simulation vs. fabrication problem in systems science, what I call the</i><br>
<i>&gt; issue of whether you can *program* a synthetic intelligence or whether</i><br>
<i>&gt; you have to *grow* one. (Hardly a distinction that the average TIME</i><br>
<i>&gt; reader would be aware of.)</i><br>
<i>&gt; </i><br>
<p>
if you can run the software of the human mind on a computer, and<br>
recognise it as a human intelligence, then there is no reason not to<br>
recognise a human level AI on the same computer as truly aware and<br>
intelligent. If you cannot tell the difference, then there is no<br>
difference. On the issue of whether you can grow an AI on a computer by<br>
itself I say that you cannot raise a human baby to be anything but a<br>
stunted wolf child if it is not raised with a mother or father or both,<br>
so how do you expect to do so with an intelligence thats never existed<br>
before. It will need a nurturing mind in close communion with it to make<br>
it "human" level.<br>
<p>
<i>&gt; Actually, I like your cyborg scenario and think it’s probably the most</i><br>
<i>&gt; likely path to becoming post-human; but, it’s has nothing to do with</i><br>
<i>&gt; the ‘hard’ questions of AI because there’s still a ‘muse in the</i><br>
<i>&gt; machine’; i.e., an ‘organically-grown’ human mind.</i><br>
<i>&gt; </i><br>
<i>&gt; Don’t get me wrong: I think machine intelligence is possible - we</i><br>
<i>&gt; already have it, with some systems that are much more intelligent in</i><br>
<i>&gt; particular domains than an un-augmented you or I could ever be - but</i><br>
<i>&gt; true AI or SI or A-Life, with creativity and individual will and</i><br>
<i>&gt; responsibility requires a lot more than just the raw processing power</i><br>
<i>&gt; implied by your Moore’s law reference.</i><br>
<p>
Sorry, I see it as partly good programming, a super powerful processing<br>
system, and experiences for it to learn how to use that programming.<br>
<p>
<i>&gt; </i><br>
<i>&gt; Processing power is only a small part of the story - growing the right</i><br>
<i>&gt; kind of network is far more important.  Those oriented toward</i><br>
<i>&gt; mechanical engineering always seem to assume that the software will</i><br>
<i>&gt; just automagically emerge if you build the hardware properly.  I was</i><br>
<i>&gt; disappointed that _Nanosystems_ totally ignored software issues</i><br>
<i>&gt; (actually, Drexler assumes backward-chaining, broadcast instructions</i><br>
<i>&gt; from a pre-existing higher level - again, a muse in the machine!)</i><br>
<i>&gt; </i><br>
<i>&gt; &lt;Charges of racism and heresy will fly. Children and less</i><br>
<i>&gt; sophisticated adults will call each other "chip-lover" and</i><br>
<i>&gt; "bio-bigot".&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; But (reread Bruce Sterling’s _Schismatrix_), "bio-bigots" don’t</i><br>
<i>&gt; necessarily imply religious types who shun *any* change to their</i><br>
<i>&gt; ’God-given’ body - it could also refer to those who prefer genetic</i><br>
<i>&gt; engineering and other bio-approaches to augmentation.</i><br>
<i>&gt; </i><br>
<i>&gt; &lt;To remove the augmentations will hobble these personalities as much</i><br>
<i>&gt; as a lobotomy would do so to you or I. These transhumans will lead the</i><br>
<i>&gt; way into this future whether neo-Luddites like Gelernter like it or</i><br>
<i>&gt; not.&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; Again, what does this have to do with the price of tea in China?</i><br>
<i>&gt; Augmentations are not AI, and that’s what Gelernter was talking about.</i><br>
<i>&gt;  If you’re going to label anyone who has concerns about whether</i><br>
<i>&gt; traditional computing techniques can be used to implement AI as a</i><br>
<i>&gt; neo-Luddite, well ...</i><br>
<p>
I found his spiritual pose to be in line with many of the other most<br>
narrowminded statements made in the past 150 years by intelligent people<br>
who should have known better. Namely:<br>
<p>
"Why would anyone need more than 640K RAM?" - Bill Gates<br>
"There is a world market for approximately 2 computers" (or therabouts)<br>
		- IBM, late 1940s<br>
"Heavier than air flight is a physical impossibility that violates the<br>
laws of physics." - Head of the Smithsonian Institute in the late 19th<br>
century<br>
"The airplane will never sink a capital ship." - The Joint Cheifs of<br>
Staff, 1919, shortly before Jimmy Doolittle sunk a captured German<br>
Battleship and Heavy Cruiser with a squadron of biplanes. (No wonder<br>
they court martialed him)<br>
"The use of rocket propulsion in space violates the laws of physics." <br>
	- The New York Times, in response to Dr. Goddards paper on using<br>
rockets to travel to the moon, early 1920's<br>
"God does not play dice with the universe." - Einstein<br>
<p>
At least Gelernter can brag that he's in good company.<br>
<p>
<pre>
-- 
TANSTAAFL!!!
			Michael Lorrey
------------------------------------------------------------
<a href="mailto:retroman@tpk.net">mailto:retroman@tpk.net</a>		Inventor of the Lorrey Drive
Agent Lorrey@ThePentagon.com
Silo_1013@ThePentagon.com	<a href="http://www.tpk.net/~retroman/">http://www.tpk.net/~retroman/</a>
<p>
Mikey's Animatronic Factory
My Own Nuclear Espionage Agency (MONEA)
MIKEYMAS(tm): The New Internet Holiday
Transhumans of New Hampshire (&gt;HNH)
------------------------------------------------------------
#!/usr/local/bin/perl-0777---export-a-crypto-system-sig-RC4-3-lines-PERL
@k=unpack('C*',pack('H*',shift));for(@t=@s=0..255){$y=($k[$_%@k]+$s[$x=$_
]+$y)%256;&amp;S}$x=$y=0;for(unpack('C*',&lt;&gt;)){$x++;$y=($s[$x%=256]+$y)%256;
&amp;S;print pack(C,$_^=$s[($s[$x]+$s[$y])%256])}sub S{@s[$x,$y]=@s[$y,$x]}
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2117.html">Tony Hollick: "Re:  Richard M. Bissell, jr."</a>
<li> <b>Previous message:</b> <a href="2115.html">Rick Knight: "Deep Blue Column in TIME"</a>
<li> <b>Maybe in reply to:</b> <a href="2055.html">Michael Lorrey: "Deep Blue colmun in TIME, May 19th"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
