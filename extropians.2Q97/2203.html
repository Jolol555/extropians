<!-- received="Fri Jun 27 21:52:39 1997 MDT" -->
<!-- sent="Fri, 27 Jun 1997 20:25:03 -0700 (PDT)" -->
<!-- name="John K Clark" -->
<!-- email="johnkc@well.com" -->
<!-- subject="Uploading" -->
<!-- id="199706280325.UAA24385@well.com" -->
<!-- inreplyto="" -->
<title>extropians: Uploading</title>
<h1>Uploading</h1>
John K Clark (<i>johnkc@well.com</i>)<br>
<i>Fri, 27 Jun 1997 20:25:03 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2203">[ date ]</a><a href="index.html#2203">[ thread ]</a><a href="subject.html#2203">[ subject ]</a><a href="author.html#2203">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2204.html">Tony Hollick: "Re: anti-spam?"</a>
<li> <b>Previous message:</b> <a href="2202.html">Anton Sherwood: "Re: "punishment""</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
-----BEGIN PGP SIGNED MESSAGE-----<br>
<p>
On Fri, 27 Jun 1997 Brent Allsop &lt;allsop@swttools.fc.hp.com&gt; Wrote:<br>
                 <br>
<i>        &gt;It doesn't matter to the abstract intelligences that we currently         </i><br>
<i>        &gt;produce whether the representation is punched on paper tape or stored         </i><br>
<i>        &gt;in high speed volatile transistor arrays. What the fundamental nature         </i><br>
<i>        &gt;of the representation is like is not relevant to the particular        </i><br>
<i>        &gt;information being represented in such a machine. </i><br>
                 <br>
<p>
I agree, except my intelligence has always concluded that all intelligence <br>
is abstract.  What on earth would non abstract intelligence be like? <br>
                 <br>
<p>
<i>        &gt;This is very different than us.          </i><br>
<p>
<p>
I see nothing different in the slightest. You will treat this post much the <br>
same if you read it off a cathode ray tube, a liquid crystal screen, listen <br>
to it with a voice synthesizer or print it on a dead tree.<br>
                 <br>
<p>
<i>        &gt;If you completely rewire an abstract computer so that what once         </i><br>
<i>        &gt;physically represented red now represents blue and visa versa, there         </i><br>
<i>        &gt;would be no difference in behavior of the machine.  </i><br>
<p>
<p>
Why? There would be a difference in behavior, unless you change the machine's <br>
memory of color too.  The relationship of one color to another and how that <br>
connects to objects in the real world would be different.  The machine once <br>
said that red, orange and pink were similar, not now. It once thought red and<br>
black neckties looked great, but now they look ugly and he never wares them. <br>
This rewiring would be just as disconcerting for a intelligent machine as it <br>
would be for me or you.<br>
                 <br>
<p>
<i>        &gt;But if you did the same thing in the optic nerve of a human so that         </i><br>
<i>        &gt;it now used a red quale to represent blue wavelengths of light and         </i><br>
<i>        &gt;visa versa, his favorite color quale would still likely be the same         </i><br>
<i>        &gt;sensation which would now represent a different wavelength of light.          </i><br>
        <br>
<p>
Yes, but why wouldn't the same thing be true of a machine, after all, <br>
a machine can have a favorite color too.<br>
               <br>
<p>
<i>        &gt;What the world would be phenomenally "like" to him would be         </i><br>
<i>        &gt;drastically different after such a change.  </i><br>
<p>
<p>
A red quale in isolation is meaningless, if all light produced a red <br>
sensation then color would signify nothing. The red color only has meaning <br>
if there is contrast, if there are other colors that are not red to compare <br>
it to. If you changed my perception of red and blue today then obviously my <br>
subjective experience would be different as would be my behavior, just like <br>
the machine. If you also changed my memory of those colors or made the switch <br>
the day I was born then my behavior today would be no different, just like <br>
the machine. You say my subjective experience would be different but give no <br>
evidence to support your claim.<br>
               <br>
<p>
<i>        &gt;His answer to what red and blue were like would become inverted.           </i><br>
<p>
<p>
You'll have to take my word for it but I am not a machine, I am a human, <br>
but I can not answer what the sensations of red and blue are and I'm quite <br>
sure you can do no better. I can give examples but no definition, I can point <br>
to red and blue things but that's all. I can't say what red and blue are.<br>
               <br>
<p>
<i>        &gt;we are trapped inside our own little spirit world and can't YET know,         </i><br>
<i>        &gt;other than abstractly, the phenomenal nature of anything beyond our         </i><br>
<i>        &gt;skull.    </i><br>
               <br>
<p>
Knowledge is an abstract quantity. Knowledge of anything is abstract.        <br>
               <br>
<p>
<i>        &gt;Our senses only know abstractly what the world is like. But, this         </i><br>
<i>        &gt;will not always be the case.  </i><br>
<p>
<p>
You don't really think the debate on this issue will EVER come to an end do <br>
you? OK, it's the year 2525 and you have just thought up a bright shiny new <br>
theory explaining exactly what subjective experience is all about.  <br>
How do I know if it's even approximately correct?<br>
<p>
<p>
<i>        &gt;When we finally objectively discover what and why these fundamental         </i><br>
<i>        &gt;qualities of our conscious representation are we will eventually be         </i><br>
<i>        &gt;able to pierce and escape this mortal spiritual veil [...] We will        </i><br>
<i>        &gt;be able to endow machines with the ability to have more than abstract      </i><br>
<i>        &gt;knowledge. "Oh THAT's what salt tastes like!"  they will eventually        </i><br>
<i>        &gt;be able to honestly say after being properly endowed.</i><br>
<p>
<p>
No you're incorrect, that's not what salt tastes like to me at all, it's not  <br>
even close. Prove I'm right. Prove I'm wrong.<br>
        <br>
Brent Allsop is an intelligent fellow but he is not conscious, in fact, <br>
all human beings are conscious, except Brent Allsop. Brent has some pitiful <br>
little thing he calls consciousness, but compared to the glorious subjective <br>
experience every other human has it's the difference between a firefly and <br>
a supernova.  Prove I'm right. Prove I'm wrong.     <br>
<p>
<p>
                                            John K Clark      johnkc@well/com<br>
<p>
-----BEGIN PGP SIGNATURE-----<br>
Version: 2.6.i<br>
<p>
iQCzAgUBM7SCyX03wfSpid95AQG76ATwxmueDDNAg+QSAtEQTiczLvnPN/eU041a<br>
+v2UJ8/HlfsR2U7FvOLdWvbj/rGQ23fTIo9z8caJW5HbS6Dw8dOHWf6x5hP5nsIc<br>
Hio74vzs56DTdpYSe0I1rIkZN7RNxyOohoiBqZD1qMdXUUCykhs/ZxyJHbR9UHyV<br>
i5bM9BwNsTUnZj1WOdWa3uVQLGUOLWBHrCN5AIAMdzqQOSu4W0c=<br>
=jIfa<br>
-----END PGP SIGNATURE-----<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2204.html">Tony Hollick: "Re: anti-spam?"</a>
<li> <b>Previous message:</b> <a href="2202.html">Anton Sherwood: "Re: "punishment""</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
