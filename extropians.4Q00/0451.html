<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Let's hear Eugene's ideas</title>
<meta name="Author" content="Eugene Leitl (eugene.leitl@lrz.uni-muenchen.de)">
<meta name="Subject" content="Re: Let's hear Eugene's ideas">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Let's hear Eugene's ideas</h1>
<!-- received="Fri Oct  6 06:46:02 2000" -->
<!-- isoreceived="20001006124602" -->
<!-- sent="Fri, 6 Oct 2000 04:42:20 -0700 (PDT)" -->
<!-- isosent="20001006114220" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: Let's hear Eugene's ideas" -->
<!-- id="14813.47772.741666.194594@lrz.uni-muenchen.de" -->
<!-- inreplyto="3.0.6.32.20001006143455.008a6850@ariel.its.unimelb.edu.au" -->
<strong>From:</strong> Eugene Leitl (<a href="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Let's%20hear%20Eugene's%20ideas&In-Reply-To=&lt;14813.47772.741666.194594@lrz.uni-muenchen.de&gt;"><em>eugene.leitl@lrz.uni-muenchen.de</em></a>)<br>
<strong>Date:</strong> Fri Oct 06 2000 - 05:42:20 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0452.html">Alex Future Bokov: "Web-calculator for how much each candidate's tax plan would save you."</a>
<li><strong>Previous message:</strong> <a href="0450.html">Technotranscendence: "Re: back off, im gay!"</a>
<li><strong>In reply to:</strong> <a href="0433.html">Damien Broderick: "Re: Let's hear Eugene's ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0453.html">Emlyn: "GA intelligence experiment = us?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#451">[ date ]</a>
<a href="index.html#451">[ thread ]</a>
<a href="subject.html#451">[ subject ]</a>
<a href="author.html#451">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Damien Broderick writes:
<br>
<p><em> &gt; I do see the force of the claim, but it might still be wrong, since
</em><br>
<em> &gt; emergence of effective AI might, after all, depend on ramping up the
</em><br>
<em> &gt; substrate technologies by brute force via Moore's law.
</em><br>
<p>We seem to have essentially two bottlenecks: 1) having a fast enough
<br>
hardware with enough bits to represent stuff and 2) a way to &quot;program&quot;
<br>
that hardware so that it acts intelligently (both are not that
<br>
unrelated, since the shape of the hardware has to support the
<br>
&quot;program&quot; paradigm, and vice versa, due to constraints of
<br>
computational physics).
<br>
<p>So we have to execute a two staged approach: use the currently
<br>
available hardware for a search of how to find a general approach to
<br>
making digital systems act intelligently (using input from
<br>
neuroscience and ALife simulation as further source of constraints),
<br>
before embarking on an a self-enhancement process, where the global
<br>
optimum for the pair hardware-&quot;software&quot; is found.
<br>
<p>As far as I can see, the second part of the bootstrap process can be
<br>
handled by the entities itself, is potentilly fraught with positive
<br>
autofeedback loops, and hence should be something done very, very
<br>
carefully.
<br>
<p>Let's look at biology first, as a major source of constraints. Because
<br>
we know it works, and that way we don't have to sample the space of
<br>
duds, which must be huge. Seen from a distance, we've got an
<br>
anisotropic threedimensional excitation/inhibition medium. Stepping a
<br>
bit closer, we've got an electrochemical spiking network, with a
<br>
decaying connection density with distance. Lots of local connections,
<br>
some mid-range connections, very few long-distance
<br>
connections. Unfortunately, there is no clean separation between hard
<br>
and software. You've got system state, with a biological chronone
<br>
(time quantum) of about 1 ms. No adaptation occurs at this scale yet.
<br>
The adaptive hardware has a number of responses, with characteristic
<br>
times in sec, hour and day range. The adaptive processes are
<br>
potentially extremely complex, because relying on a large number of
<br>
diverse diffusible chemicals, expression of genes, cytoskeletal
<br>
activity, and whatnot. Figuring it all out in detail will be hard. But
<br>
necessary, because without detailed understanding of it all, there
<br>
will be uploads.
<br>
<p>We can try to figure it out bottom up, by imaging a biological system
<br>
at molecular resolution and plugging these results into a numerical
<br>
simulation of a biological system at molecular detail (we can't do
<br>
that yet) or a mesoscale mode (eCell, virtual cell), we can look what
<br>
morphogenesis does in simple critters, and use in vivo recording with
<br>
patch clamping, and what not. We can take a middle approach, imaging
<br>
the critter at nm resolution, plugging painstakingly digitized
<br>
neuroanatomy into the simulation and try to fit experimental and &quot;ab
<br>
initio&quot; (i.e. stuff from the molecular level of theory) into the
<br>
simulation, say at compartmental level. We can also try to culture
<br>
neurons in vitro, and record their realtime activity in realtime,
<br>
using voltage and Ca concentration sensitive dyes (taking a large body
<br>
of data in at a glance) and multielectrode arrays. Partly, this
<br>
approach can also work in vivo. At the higher level, you try to map
<br>
neuronal pathways, corellate activity with stimuli using fMRI and MEG,
<br>
and the like.
<br>
<p>This sounds complex, and it is. The ALife approach say: this is a
<br>
particular long-time adaptation to a particular rich system, and it's
<br>
just too damn complex to figure out, and even if we succeed it will be
<br>
of limited relevancy to us anyway, since the constraints of our
<br>
hardware (whether current or future) will certainly be different. So
<br>
they will select a (hopefully sufficiently rich) starter system
<br>
already capable of information processing, and tweak the parameters
<br>
with evolutionary algorithms, and hope they haven't pulled a
<br>
dud. Iterate untill success.
<br>
<p>In neuronal biology, a part of a system (a patch of an excitable
<br>
membrane, a synapse, a neuron) can make the current state impact a
<br>
remote part of a system. And vice versa. The impact is depending at
<br>
the state of self and other, and how the medium between self and other
<br>
is structured, which is a function of history. A part from it is
<br>
generated straight from neuronal embryomorphogenesis, part shaped
<br>
emergently from electrochemical activity in latter phases (i.e. no
<br>
longer explicitly encoded by the genome, which is already true for
<br>
morphogenesis, since the system has already lot of state and
<br>
&quot;computes&quot;), part of it further shaped in active learning and adaption
<br>
outside of the womb. When an animal enters the world it is anything
<br>
but randomly wired. It comes intensely prebiased, having recapitulated
<br>
all the former successful stages of design evolution already did came
<br>
up with. For lower animals, this is already enough. They're a
<br>
conserve, facit of a large number of former learners.
<br>
<p>If I would be co-evolving critters, I would do the following things. I
<br>
would reserve an input vector and an output vector, one for sensors,
<br>
one for motorics. This is the system's only interface to reality,
<br>
whether real or emulated. I would add an adaptively expanding envelope
<br>
of sufficiently rich machinery to map sensorics to motorics, and
<br>
represent state. Lots of state. Huge gobs of state, and then some. The
<br>
more, the less primitive the critter. Trying to encode abovementioned
<br>
framework, I would use use networks of automata. An automaton has
<br>
state, and means to impact that state upon that of distant ones. Which
<br>
one, depends on being or not being connected to it. I would leave
<br>
slack for emergence of multiple classes of automata, resulting in
<br>
different amounts of state, the exact shape how that state is impacted
<br>
by connected automata, the number of connections, and the shape of how
<br>
these connections are prewired. To reduce the number of mutable bits
<br>
and hence the search space, I would add a genetically encoded
<br>
machinery not only encoding the behaviour of these things (including
<br>
how the new connections are formed and destroyed, and how the impact
<br>
is modulated over time), but also how they are initially
<br>
prepatterned. Stepping back one step, you also have to allow the
<br>
genome to operate on how that pattern-generating machinery is
<br>
generating a pattern, for maximum flexibility. The mutation function
<br>
will also to be part of the mutable parts. It needs to learn what
<br>
parts of the genome need to be modified, when and how often. This is a
<br>
preliminary recipe, likely to have lots arbitrary constraints still
<br>
built-in, and probably needs to be revised and expanded several times.
<br>
<p>This has been relatively abstract, but here the hardware
<br>
level/computational physics-derived abstracts start to kick in. We
<br>
know computronium, a 3d hardware implementation of a reversible
<br>
cellular automaton, is provably (Nanotechnology 9 (1998) pp. 162-176)
<br>
the ultimative non-qubit computer. Luckily, cellular automata can be
<br>
written for current hardware which are only bottlenecked by (burst)
<br>
memory bandwidth and due to the fact that only surface need to be
<br>
communicated to neighbour nodes (assuming, only locally interconnected
<br>
nearest-neighbour nodes arranged on a 3d lattice) for processing,
<br>
within reasons scale O(const) in respect to total size as counted in
<br>
cells, as long as the time necessary to exchange of the surface
<br>
information is negligeable in comparison to the time it takes the CPU
<br>
to process the volume.  This indicates the need go to finer grains and
<br>
higher bandwidth/less latency interconnect, preferring large DSP
<br>
clusters to conventional Beowulfs. In fact, for such hardware it might
<br>
make sense to lift the only-nearest neighbour automaton constraint,
<br>
and allow any node within a simulated volume to instantly send
<br>
information to any other automaton in volume simulated by adjacent
<br>
Beowulf/DSP node. This reduces minimal stimulus response latency,
<br>
exploiting particularities of a given architecture (memory acesses
<br>
within the node have a temporally flat profile, which only works
<br>
because limits of computational physics are still remote).
<br>
<p>It is easy to see that custom architectures based on embedded RAM
<br>
technology can provide a substantial speedup (I have a good paper
<br>
sitting on the hard drive somewhere which I can't find right now) on
<br>
above architectures, eventually resulting in 2 1/2 d CA hardware
<br>
completely covering the surface of an e.g. 300 mm wafer. The best way
<br>
to implement this would be probably asynchonous/clockless analog
<br>
CMOS. You could probably fit a complete cell in ~10 um^2 or less of
<br>
silicon real estate, allowing you to fit ~10 kCells on mm^2, or almost
<br>
a billion cells on a wafer. That's a lot of state, and
<br>
(nearest-neighbours) locally that cells could change state with up to
<br>
0.1 THz rate, provided we can dissipate that much heat from a
<br>
surface. Even the most primitive Langmuir-Blodgett deposited molecular
<br>
implementation of above should give us instant two orders of
<br>
magnitude, by shrinking the size of the cell to about a micron. Of
<br>
course this scales to volume integration, resulting in another three
<br>
to four orders of magnitude. We're talking about 10^9 cells in a cubic
<br>
millimeter, or ~10^12 cells in a sugar cube. You can do much better
<br>
with mechanosynthetically deposited computronium.
<br>
<p>Clearly, this architecture scales over several implementation
<br>
technologies, so it would make sense to do it, even if initially only
<br>
in software. By attempting to evolutionary grow CA circuits running on
<br>
a custom hardware (FPGA accelerated cellular automaton machine)
<br>
controlling the behaviour of a small robot de Garis shows he's
<br>
smart. He's far from guaranteed to succeed, partly because he's alone,
<br>
the rest of AI community thinks he's on crack due to his writing on
<br>
artilects (hence automagically marking his methods as tainted), partly
<br>
because he might be missing one or two critical components, also his
<br>
hardware is comparable quite puny (in his place, I would have ordered
<br>
a rack full of 4-8 MBit MMX-capable SHARC DSPs, gluelessly wired on a
<br>
cubic lattice, and interface that to a PC or a small Beowulf for
<br>
control and I/O). In volume, these things are so cheap that he would
<br>
been able to afford 10^4..10^5 nodes. A module of 10^3 of them with
<br>
collectively pack ~1 GByte of on-die SRAM memory while fitting into
<br>
the volume of a big tower, dissipating ~2..5 kW (this is still
<br>
manageable with lamellar metal heat dissipators in strong
<br>
airflow). Here's your ~billion (~1000^3) or so of byte-sized CA cells
<br>
(a cube of ~100^3, about ~MByte in every node), and probably with a
<br>
refresh rate of ~0.1 kHz, depending on how fast the CPU interconnect
<br>
is. This would have required less development work than his custom
<br>
FPGA box he's currently using, and provided considerably more kick --
<br>
all purpose kick, allowing you to reuse the thing, in case this
<br>
particular approach blows you a raspberry, plus future opportunities
<br>
to profit from large-volume part production (DSPs are commodity and
<br>
will eventually move to embedded RAM and more on-die parallelism as
<br>
well as possibly FPGA, which you just can't do by wiring FPGAs with
<br>
RAM).
<br>
<p>As long as Eliezer doesn't start packing hardware of this calibre and
<br>
beyond (way beyond, actually), and won't apply above or similiar
<br>
technologies with the intent to breed an SI, he's below my radar
<br>
screen ;)
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0452.html">Alex Future Bokov: "Web-calculator for how much each candidate's tax plan would save you."</a>
<li><strong>Previous message:</strong> <a href="0450.html">Technotranscendence: "Re: back off, im gay!"</a>
<li><strong>In reply to:</strong> <a href="0433.html">Damien Broderick: "Re: Let's hear Eugene's ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0453.html">Emlyn: "GA intelligence experiment = us?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#451">[ date ]</a>
<a href="index.html#451">[ thread ]</a>
<a href="subject.html#451">[ subject ]</a>
<a href="author.html#451">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:15 MDT</em>
</em>
</small>
</body>
</html>
