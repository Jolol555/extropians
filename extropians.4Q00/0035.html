<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Why would AI want to be friendly?</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Why would AI want to be friendly?">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Why would AI want to be friendly?</h1>
<!-- received="Sun Oct  1 16:43:15 2000" -->
<!-- isoreceived="20001001224315" -->
<!-- sent="Sun, 01 Oct 2000 15:45:02 -0700" -->
<!-- isosent="20001001224502" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="39D7BE6E.36FBC4F2@objectent.com" -->
<!-- inreplyto="20001001.151131.-620717.1.shabrika@juno.com" -->
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;39D7BE6E.36FBC4F2@objectent.com&gt;"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 01 2000 - 16:45:02 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0036.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</a>
<li><strong>Previous message:</strong> <a href="0034.html">Michael M. Butler: "Re: Eugene's nuclear threat"</a>
<li><strong>In reply to:</strong> <a href="0026.html">Barbara Lamar: "Re: Why would AI want to be friendly?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0078.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#35">[ date ]</a>
<a href="index.html#35">[ thread ]</a>
<a href="subject.html#35">[ subject ]</a>
<a href="author.html#35">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Barbara Lamar wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; On Sun, 1 Oct 2000 02:10:17 -0700 (PDT) Eugene Leitl
</em><br>
<em>&gt; &lt;<a href="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;39D7BE6E.36FBC4F2@objectent.com&gt;">eugene.leitl@lrz.uni-muenchen.de</a>&gt; writes:
</em><br>
<em>&gt; &gt; Eliezer S. Yudkowsky writes:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Friendly is not observer-invariant, goddamit. If you want to impose
</em><br>
<em>&gt; &gt; your particular concept on the rest of us via (thankfully
</em><br>
<em>&gt; &gt; hypothetical) AI proxy, then please call a spade a spade. Call it
</em><br>
<em>&gt; &gt; Eliezer-Friendly, or something.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I brought this same point up towards the beginning of this thread,
</em><br>
<em>&gt; addressed specifically to Eliezer, and there was no response.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Two days ago I posted a URL for a recent paper by a couple of people who
</em><br>
<em>&gt; are currently working in the AI field--&quot;Cooperative Coevolution: An
</em><br>
<em>&gt; Architecture for Evolving Coadapted Subcomponents&quot;  by Mitchell A. Potter
</em><br>
<em>&gt; (Navy Center for Applied Research in Artificial Intelligence)  and
</em><br>
<em>&gt; Kenneth A. DeJong (Computer Science Dept. George Mason University).
</em><br>
<em>&gt; Published in *Evolutionary Computation* 8(1) (2000)
</em><br>
<p><p><p><em>&gt; Again there was no response.  As an outsider to the AI field, I would
</em><br>
<em>&gt; nevertheless like to have a better understanding of it.  Has anyone else
</em><br>
<em>&gt; read Potter's and DeJong's paper?
</em><br>
<p>I have not finished it but I believe it is very applicable both to the
<br>
creation/evolution of an SI and to human/AI coevolution.  While I
<br>
believe it is a very important work and highly germane to this dicussion
<br>
(and others) I am letting its implications and repurcussions bounce
<br>
around a little before I say much more.
<br>
<p>&nbsp;
<br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1. What difference would cooperative coevolution make with respect to the
</em><br>
<em>&gt; relationship between humans and highly evolved AI?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 2.  Are the capabilities of AI severely limited in the absence of
</em><br>
<em>&gt; algorithms for cooperative coevolution?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 3.   Potter's and DeJong's  EA's call for destroying unsuccessful
</em><br>
<em>&gt; &quot;species&quot; and maintaining &quot;genetic&quot; diversity by introducing new species
</em><br>
<em>&gt; whenever stagnation is detected in the system.  Stagnation is detected by
</em><br>
<em>&gt; monitoring the quality of interspeices collaborations via checking each
</em><br>
<em>&gt; collaboration for the improvement it provides in the functioning of the
</em><br>
<em>&gt; &quot;ecosystem&quot; as a whole.  Is this methodology  troubling with respect to
</em><br>
<em>&gt; possible future interaction between humans and AI?  Why or why not?  What
</em><br>
<em>&gt; are some other methods of maintaining diversity?
</em><br>
<em>&gt;
</em><br>
<p>It is quite troublesome and probably unnecessary since species can be
<br>
modified inline to have expanded/corrected abilities, including
<br>
ourselves.  Not many seem to yet realize that the rules have changed
<br>
this radically.  It is not only an AI that can self-modify.  Evolution
<br>
does not require destruction of sentient entities.  Without this
<br>
possibility our hopes for immortality are empty.
<br>
<p>A 10 billion dollar question is what is the &quot;ecosystem&quot;?  What are we
<br>
attempting to create?  If we don't know that can we judge whether we are
<br>
getting closer or further away?   
<br>
<p>&nbsp;
<br>
<em>&gt; 4.  Do the &quot;species&quot; in this research correspond more closely to
</em><br>
<em>&gt; biological genes or to biological species?
</em><br>
<em>&gt;
</em><br>
<p>Not sure that question is important.  Both, Neither.
<br>
<p><p>- samantha
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0036.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</a>
<li><strong>Previous message:</strong> <a href="0034.html">Michael M. Butler: "Re: Eugene's nuclear threat"</a>
<li><strong>In reply to:</strong> <a href="0026.html">Barbara Lamar: "Re: Why would AI want to be friendly?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0078.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#35">[ date ]</a>
<a href="index.html#35">[ thread ]</a>
<a href="subject.html#35">[ subject ]</a>
<a href="author.html#35">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:14 MDT</em>
</em>
</small>
</body>
</html>
