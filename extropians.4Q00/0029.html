<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Eugene's nuclear threat</title>
<meta name="Author" content="Eugene Leitl (eugene.leitl@lrz.uni-muenchen.de)">
<meta name="Subject" content="Re: Eugene's nuclear threat">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Eugene's nuclear threat</h1>
<!-- received="Sun Oct  1 14:12:19 2000" -->
<!-- isoreceived="20001001201219" -->
<!-- sent="Sun, 1 Oct 2000 12:08:24 -0700 (PDT)" -->
<!-- isosent="20001001190824" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: Eugene's nuclear threat" -->
<!-- id="14807.35752.73071.306036@lrz.uni-muenchen.de" -->
<!-- inreplyto="39D76EA0.C7D3497@pobox.com" -->
<strong>From:</strong> Eugene Leitl (<a href="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Eugene's%20nuclear%20threat&In-Reply-To=&lt;14807.35752.73071.306036@lrz.uni-muenchen.de&gt;"><em>eugene.leitl@lrz.uni-muenchen.de</em></a>)<br>
<strong>Date:</strong> Sun Oct 01 2000 - 13:08:24 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0030.html">Samantha Atkins: "Re: Attacks (was Re: Why would AI want to be friendly?)"</a>
<li><strong>Previous message:</strong> <a href="0028.html">Dan Fabulich: "Good or bad? was: Eugene's nuclear threat"</a>
<li><strong>In reply to:</strong> <a href="0013.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0032.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0032.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#29">[ date ]</a>
<a href="index.html#29">[ thread ]</a>
<a href="subject.html#29">[ subject ]</a>
<a href="author.html#29">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Eliezer S. Yudkowsky writes:
<br>
<p><em> &gt; Eugene, if you make a mistake, you've got a planet full of people who were
</em><br>
<em> &gt; wiped out by grey goo.  As for the scenario where millions of lightyears of
</em><br>
<p>Whoa! Wait, have you forgotten SOP says we're to nuke preemptively
<br>
anybody who peaks out on the Armageddon scale? _Credible_ grey goo
<br>
developers certainly qualify (there goes that warhead).
<br>
<p>Right now we're in the happy position to be able to discuss crisis
<br>
management strategies several decades before the End-Of-The-World
<br>
technology candidates step up upon the carpet. Let's not waste this
<br>
opportunity in pure rhetorics and mental wankery, and instead look for
<br>
possible solutions, if any. Thankfully, we're not the one in charge,
<br>
so we're not hampered by all kinds of nasty constraints and selective
<br>
reality perceptions. Nothing is yet at stake if we fail here. So let's 
<br>
keep an open mind. We might be able to produce something interesting
<br>
here, something which has a validity scope outside this list.
<br>
<p><em> &gt; people get wiped out - if it were possible for one little planet to make that
</em><br>
<em> &gt; kind of mistake, and there are that many aliens in range, someone *else* would
</em><br>
<p>We don't know whether someone already did. The wavefront may well be
<br>
on the way, but there is no way us knowing that in advance, as long as
<br>
we're not in nucleus' lightcone. By the time our expansion wavefront
<br>
hits a world, it may be already rife with intelligent life. Have you
<br>
ever seen the totoro &quot;Wings of Honneamise&quot;, or imagined a Vinge's
<br>
universe without Zones? Picture the wavefront piranhas breaking over
<br>
an unverse like this. Whoosh.
<br>
<p><em> &gt; have already done it to *us*.  It's just the Earth that's at stake.
</em><br>
&nbsp;
<br>
About 6 billion eggs about to hatch in a single, not too large,
<br>
basket. Odds still do not seem to bulge a single micron.
<br>
<p>What options do we truly have in face of Armageddon technology
<br>
candidates? 
<br>
<p>We can distribute the eggs over many baskets. This makes you pretty
<br>
immune from gray goo and bioweapon extinction events, while still
<br>
making you vulnerable to SI and &lt;some unspecified new physics threat,
<br>
like something which makes you generate desktop GRB, or supercritical
<br>
singularities near deep gravity wells&gt;. These are excellent odds, so
<br>
getting as many people into sustainable habitats outside of Earth
<br>
surface should be a priority. This boils down to cheap launches to LEO
<br>
and portable sustainable ecologies and macroscopic autonomous
<br>
autoreplicators. Unfortunately, collectively we give very little
<br>
priority to this particular alternative reality branch. This one is a
<br>
real bummer, and might make us look mightily stupid (not to mention
<br>
very dead) in the long run. So let's start weaving these baskets.
<br>
<p>What else can we do? We can look out for preventing egg breaking
<br>
events, and we can look out for ways to fortify the eggs, as many of
<br>
them as we can as strong as we can.
<br>
<p><em> &gt; And above all, if you make a mistake - you'll have made a mistake that
</em><br>
<em> &gt; involved deliberate murder.
</em><br>
&nbsp;
<br>
I'm sufficiently not in denial of realpolitik to be cool with that. As
<br>
long you're really sure you know what you're doing (including not
<br>
acting in haste and getting second and third and fourth opinions on
<br>
the matter, while walking the narrow line of implicitly deciding by
<br>
not deciding in time) and are equally clear on the consequences of the
<br>
negative (not doing) and you're the one in the driver's seat, you have
<br>
to decide either way round. Errors will, regrettably, occur, but
<br>
that's strictly unavoidable.
<br>
<p>If you haven't yet noticed, that's how the world works. (Though, in
<br>
most cases, further overlaid by multiple layers of incompetence and
<br>
generic confusion. A certain modicum of fatalism *does* help).
<br>
<p><em> &gt; According to your theories, we have a world where everyone is free to murder
</em><br>
<em> &gt; anyone who works on a technology that someone doesn't like.  According to you,
</em><br>
<p>Of course everybody is free to do anything they like, the question is
<br>
rather whether they can.
<br>
<p><em> &gt; the people destroying GM crops are not doing anything immoral; if they have a
</em><br>
<em> &gt; flaw, it's that they aren't bombing Monsanto's corporate headquarters.  If you
</em><br>
<p>Huh? I don't subscribe to the killer tomato theory. As long as you
<br>
don't start messing with modified rhinoviruses in biological weapons
<br>
lab, the amount of damage you can possibly produce even in the worst
<br>
case is way below the Armageddon scale.
<br>
<p><em> &gt; work on an upload project, then anyone who distrusts uploads and thinks AI is
</em><br>
<em> &gt; safer has a perfect right to stop the uploading project by killing as many
</em><br>
<em> &gt; people as necessary.  *I* would have the right to kill *you*, this instant,
</em><br>
<p>Of course, the question is rather whether I'm sufficiently deterred by 
<br>
the threat and whether compound security is adequate. Btw, I worked in 
<br>
an animal research facility. I figured out the odds were worth the risks.
<br>
<p><em> &gt; because you might slow down AI in favor of nanotechnology.
</em><br>
&nbsp;
<br>
Runaway AI is of course worse than the grey goo scenario.
<br>
<p><em> &gt; Do you really think that kind of world, that systemic process for dealing with
</em><br>
<em> &gt; new technologies, would yield better results than the nonviolent version?
</em><br>
&nbsp;
<br>
I don't know, but I prefer to keep an open mind. I'm not trying to
<br>
exclude certain solutions a priori, just because they're unpopular. I
<br>
will drop them if they're stupid, but so far I don't see why they're
<br>
stupid. 
<br>
<p><em> &gt; Even Calvin and Hobbes know better than that.
</em><br>
&nbsp;
<br>
Great, an imaginary tiger and a cartoon kid decide on world
<br>
policy. This only works in a cartoon.
<br>
<p><em> &gt; C:  &quot;I don't believe in ethics any more. As far as I am concerned, the
</em><br>
<em> &gt;      end justifies the means. Get what you can while the getting's good –
</em><br>
<em> &gt;      that's what I say. It's a dog-eat-dog world, so I'll do whatever I
</em><br>
<em> &gt;      have to do, and let others argue about whether it's right or not.&quot;
</em><br>
<em> &gt; 
</em><br>
<em> &gt; &lt;Hobbes pushes Calvin into a mud hole.&gt;
</em><br>
<em> &gt; 
</em><br>
<em> &gt; C:  &quot;Why did you do THAT?&quot;
</em><br>
<em> &gt; H:  &quot;You were in my way.  Now you're not.  The end justifies the means.&quot;
</em><br>
<em> &gt; C:  &quot;I didn't mean for everyone, you dolt! Just ME!
</em><br>
<p>Where did I say that I'm exempt from being extreme sanctioned? But,
<br>
then, if I ever wear a hat, it's lily white.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0030.html">Samantha Atkins: "Re: Attacks (was Re: Why would AI want to be friendly?)"</a>
<li><strong>Previous message:</strong> <a href="0028.html">Dan Fabulich: "Good or bad? was: Eugene's nuclear threat"</a>
<li><strong>In reply to:</strong> <a href="0013.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0032.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0032.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#29">[ date ]</a>
<a href="index.html#29">[ thread ]</a>
<a href="subject.html#29">[ subject ]</a>
<a href="author.html#29">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:14 MDT</em>
</em>
</small>
</body>
</html>
