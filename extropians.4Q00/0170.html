<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Let's hear Eugene's ideas</title>
<meta name="Author" content="Eugene Leitl (eugene.leitl@lrz.uni-muenchen.de)">
<meta name="Subject" content="Re: Let's hear Eugene's ideas">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Let's hear Eugene's ideas</h1>
<!-- received="Tue Oct  3 07:00:19 2000" -->
<!-- isoreceived="20001003130019" -->
<!-- sent="Tue, 3 Oct 2000 04:56:32 -0700 (PDT)" -->
<!-- isosent="20001003115632" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: Let's hear Eugene's ideas" -->
<!-- id="14809.51568.267668.982002@lrz.uni-muenchen.de" -->
<!-- inreplyto="39D96F5B.FF5743BB@posthuman.com" -->
<strong>From:</strong> Eugene Leitl (<a href="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Let's%20hear%20Eugene's%20ideas&In-Reply-To=&lt;14809.51568.267668.982002@lrz.uni-muenchen.de&gt;"><em>eugene.leitl@lrz.uni-muenchen.de</em></a>)<br>
<strong>Date:</strong> Tue Oct 03 2000 - 05:56:32 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0171.html">Ross A. Finlayson: "Re: Capitalists and concentration camps"</a>
<li><strong>Previous message:</strong> <a href="0169.html">Dehede011@aol.com: "Re: Capitalists and concentration camps"</a>
<li><strong>In reply to:</strong> <a href="0159.html">Brian Atkins: "Re: Let's hear Eugene's ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0177.html">Eliezer S. Yudkowsky: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Reply:</strong> <a href="0177.html">Eliezer S. Yudkowsky: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Reply:</strong> <a href="0230.html">J. R. Molloy: "We're All Intelligence Pushers On This Bus"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#170">[ date ]</a>
<a href="index.html#170">[ thread ]</a>
<a href="subject.html#170">[ subject ]</a>
<a href="author.html#170">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Brian Atkins writes:
<br>
<em> &gt; <a href="mailto:hal@finney.org?Subject=Re:%20Let's%20hear%20Eugene's%20ideas&In-Reply-To=&lt;14809.51568.267668.982002@lrz.uni-muenchen.de&gt;">hal@finney.org</a> wrote:
</em><br>
<em> &gt; &gt;
</em><br>
<em> &gt; &gt; That's not clear.  First, it could easily take longer than 20 years to
</em><br>
<em> &gt; &gt; get superhuman AI, for several reasons:
</em><br>
<em> &gt; &gt; 
</em><br>
<em> &gt; &gt;  - We may not have nanotech in 20 years
</em><br>
<p>I would be indeed very surprised if we don't see (primitive,
<br>
self-assembling) molecular circuitry in 20 years. Possibly less.
<br>
<p><em> &gt; &gt;  - We may hit Moore's Wall before then as computer speeds turn out to be
</em><br>
<em> &gt; &gt;    on an S curve just like every other technology before them
</em><br>
<p>We have to have 2d molecular circuitry if we want to keep the
<br>
integration density linear on the log plot beyond 2014, or so (I don't
<br>
have the data for a back of the envelope, so roll your own breakdown
<br>
point). Very soon after we will have to have volume integrated
<br>
molecular circuitry (probably introduced piecemeal by layers, before
<br>
going molecular crystal). After that, you can only scale up
<br>
volume. Sooner or later you'll run out of atoms, and if no new physics
<br>
is then ready to pick up the ball, Moore will have ran out of
<br>
steam. No surprise there.
<br>
<p>Of course, integration density does not automatically translate in
<br>
front-end performance. There's considerable architectural slack
<br>
present in semiconductor photolitho, exploitable in on-die
<br>
architectures (embedded RAM, ultrabroad SIMD in a register
<br>
parallelism), shrinking system grain size (boosting good die yield),
<br>
evolvable hardware (FPGA &amp; Co), and synergies thereof. Of course,
<br>
currently this is blocked by the state of art in software.
<br>
<p><em> &gt; &gt;  - Software may continue to improve as it has in the past (i.e. not very
</em><br>
<em> &gt; &gt;    fast)
</em><br>
<p>There is considerable doubt that mainstream technologies do improve at
<br>
all. I would have rather written this text on a modern 512-CPU Lisp
<br>
machine (Symbolics, where art thou?) or on a consumer version of CM-10
<br>
than on an x86 Linux box, which has a slower response time than my
<br>
vintage 1986 and 1988 computers. Bleh.
<br>
<p>Though there are validated methodologies how low-defect software can
<br>
be written, this does not scale to high complexity. (Of course, only a
<br>
tiny fraction of the industry are using them at all). I do not see
<br>
anything beyond evolutionary algorithms which is going to deliver
<br>
adaptive, robust, complex systems. As long as we don't try to go near
<br>
human scale, we should be more or less safe.
<br>
<p><em> &gt; &gt;  - AI researchers have a track record of over-optimism
</em><br>
<em> &gt; 
</em><br>
<em> &gt; Perhaps so, in which case Eugene has nothing to worry about. These things
</em><br>
<p>I wish I could *know* this. Too much is at stake.
<br>
<p><em> &gt; above are not what we want to discuss on this thread. We want to hear what
</em><br>
<em> &gt; you propose to do to have a good outcome in a world where the above things
</em><br>
<em> &gt; turn out to be wrong.
</em><br>
&nbsp;
<br>
Though this is directed to hal, I'll make this post a portemanteau.
<br>
<p>Unsurprisingly, I have no silver bullet to offer. Complex reality
<br>
demands solutions which are less than simple, and frequently
<br>
ambiguous.
<br>
<p>To recapitulate, we have a case where memetical evolution driving
<br>
culture and technology has overwhelmed genetic adaptability. We have
<br>
primates with a conserved firmware compelling them to pull a tail
<br>
before checking whether there is a tiger attached to other end of
<br>
it. This made sense once, where explorative behaviour, though risky to
<br>
the individual, provided a potential payoff more than compensating the
<br>
genetically related group as a whole for a few dead hominids along the
<br>
way. This wouldn't matter if we didn't have some (very few, specific,
<br>
easily identifyable) technologies which can amplify microscopic
<br>
decisions to macroscale. Arguably, this already applies to weapons of
<br>
mass destruction, though overhead necessary to soak a landscape in VX
<br>
or delivering a thermonuclear warhead to a city can hardly be called
<br>
microscopic. Though we're no means through the arms race bottleneck
<br>
(Cuba crisis being a damn close shave), we might be through the worst
<br>
of it. The emerging Armageddon-scale technologies are different.
<br>
<p>This doesn't mean we're doomed. 
<br>
<p>This does mean that we have to try to compensate for the firmware
<br>
which makes us overly frisky during the risky passage. There is an
<br>
established methodology associated with managing high risk
<br>
projects. Let's use it. Because our firmware also does prevent us from
<br>
succumbing to external pressures voluntarily, existing ways of dealing
<br>
with deviants need to be applied. This will be an opressive, nasty,
<br>
violent and responsible thing to do. Because the new threats are
<br>
low-profile, there is clearly an early point of diminishing returns,
<br>
where external enforcing authority breaks more than it accomplishes,
<br>
and incites to rebellious behaviour, since pissing people
<br>
off. Clearly, we do not want to go there.
<br>
<p>We can't contain even most of it, but we can reduce the number of
<br>
potential apocalyptic nuclei while we're passing the tight spot.
<br>
<p>(This meta-level action envelope is translatable into a set of
<br>
immediately applicable rules, but I'm no expert in this, and no one is
<br>
paying me to go through the whole thing anyway, so, hopefully, the
<br>
professionals in the emerging fields will come up with self-regulation
<br>
rules, early kudos go to Foresight).
<br>
<p><em> &gt; &gt; Secondly, I suspect that in this time frame we are going to see
</em><br>
<em> &gt; &gt; increased awareness of the dangers of future technology, with Joy's
</em><br>
<em> &gt; &gt; trumpet blast just the beginning.  Joy included &quot;robotics&quot; in his troika
</em><br>
<em> &gt; &gt; of technological terrors (I guess calling it &quot;AI&quot; wouldn't have let him
</em><br>
<em> &gt; &gt; keep to the magic three letters).  If we do see an Index of of Forbidden
</em><br>
<em> &gt; &gt; Technology, it is entirely possible that AI research will be included.
</em><br>
<em> &gt; 
</em><br>
<em> &gt; Don't you think this better happen soon, otherwise the governments will
</em><br>
<em> &gt; end up trying to regulate something that already is in widespread use? There
</em><br>
<em> &gt; are going to be &quot;intelligent&quot; programs out there soon- in fact you can already
</em><br>
<em> &gt; see commercials in the last year touting &quot;intelligent&quot; software packages.
</em><br>
<p>This is what marketing understand under intelligence. The only robust, 
<br>
adaptive all-purpose intelligence -- traces of it -- can be found in
<br>
academic ALife research labs. This technology is right now absolutely
<br>
innocuous, and it will take decades to reach the marketplace as is.
<br>
<p><em> &gt; Do you really think it is likely our government would outlaw this area of
</em><br>
<em> &gt; software development once it becomes a huge market? Extremely unlikely...
</em><br>
<p>As long as we don't see anything approaching the threat threshold, the
<br>
field should not be regulated. AI is already ailing as is, we don't
<br>
have to have Turing pigs breathing down your neck as you code.
<br>
&nbsp;
<br>
<em> &gt; &gt; Third, realistically the AI scenario will take time to unfold.  As I
</em><br>
<em> &gt; &gt; have argued repeatedly, self-improvement can't really take off until
</em><br>
<em> &gt; &gt; we can build super-human intelligence on our own (because IQ 100 is
</em><br>
<em> &gt; &gt; self-evidently not smart enough to figure out how to do AI, or else
</em><br>
<em> &gt; &gt; we'd have had it years ago).  So the climb to human equivalence will
</em><br>
<em> &gt; &gt; continue to be slow and frustrating.  Progress will be incremental,
</em><br>
<em> &gt; &gt; with a gradual expansion of capability.
</em><br>
<em> &gt; 
</em><br>
<em> &gt; Up to a point...
</em><br>
<p>It is not necessary to be extremely intelligent to create
<br>
intelligence, co-evolution is dirt stupid, yet it came up with
<br>
us. Using the same principles, we, barely intelligent primates, can
<br>
push it way further. As long as it doesn't end us all, we *should*
<br>
push it further. I don't know about you, but I'm tired of chipping
<br>
flint while sitting in front of the cave, it's cold, wet, drafty, and
<br>
the lice bite unmercifully.
<br>
&nbsp;
<br>
<em> &gt; &gt; I see the improved AI being put to work immediately because of the many
</em><br>
<em> &gt; &gt; commercial opportunities, so the public will generally be well aware of
</em><br>
<em> &gt; &gt; the state of the art.  The many difficult ethical and practical dilemmas
</em><br>
<em> &gt; 
</em><br>
<em> &gt; Public.. well aware of state of the art... bwhahahaha. No I don't think
</em><br>
<em> &gt; so. At any rate, I guess you are talking about a different scenario here;
</em><br>
<em> &gt; one without Turing Police preventing development?
</em><br>
<p>As soon as the field will start producing results, you can expect
<br>
people to perk up.
<br>
<p><em> &gt; &gt; that appear when you have intelligent machines will become part of the
</em><br>
<em> &gt; &gt; public dialogue long before any super-human AI could appear on the scene.
</em><br>
<em> &gt; &gt; 
</em><br>
<em> &gt; &gt; Therefore I don't think that super-intelligent AI will catch society by
</em><br>
<em> &gt; &gt; surprise, but will appear in a social milieu which is well aware of the
</em><br>
<em> &gt; &gt; possibility, the potential, and the peril.  If society is more concerned
</em><br>
<em> &gt; &gt; about the dangers than the opportunities, then we might well see Turing
</em><br>
<em> &gt; &gt; Police enforcing restrictions on AI research.
</em><br>
<em> &gt; 
</em><br>
<em> &gt; Well I'd love to see how that would work. On one hand you want to allow
</em><br>
<em> &gt; some research in order to get improved smart software packages, but on
</em><br>
<em> &gt; the other hand you want to prevent the &quot;bad&quot; software development that
</em><br>
<p>Intelligence is not only a software problem. We're both hardware and
<br>
software bottlenecked. As long as your available hardware performance
<br>
(number of stored bits, speed with which these bits can be tweaked) is
<br>
below a certain threshold, even provably optimal software is not going
<br>
to go beyond human scale.
<br>
<p>Even if the grains are small, we still have to secure the networks
<br>
against worms, since the Net as a whole is certainly adequate.
<br>
<p><em> &gt; might lead to a real general intelligence? Is the government going to sit
</em><br>
<em> &gt; and watch every line of code that every hacker on the planet types in?
</em><br>
<p>How much hardware will the average hacker have by 2020? How much of it
<br>
will be reconfigurable? What will the state of evolvable hardware by
<br>
2020?
<br>
<p><em> &gt; In an era of super-strong encryption and electronic privacy (we hope)?
</em><br>
<p>We hope.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0171.html">Ross A. Finlayson: "Re: Capitalists and concentration camps"</a>
<li><strong>Previous message:</strong> <a href="0169.html">Dehede011@aol.com: "Re: Capitalists and concentration camps"</a>
<li><strong>In reply to:</strong> <a href="0159.html">Brian Atkins: "Re: Let's hear Eugene's ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0177.html">Eliezer S. Yudkowsky: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Reply:</strong> <a href="0177.html">Eliezer S. Yudkowsky: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Reply:</strong> <a href="0230.html">J. R. Molloy: "We're All Intelligence Pushers On This Bus"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#170">[ date ]</a>
<a href="index.html#170">[ thread ]</a>
<a href="subject.html#170">[ subject ]</a>
<a href="author.html#170">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:14 MDT</em>
</em>
</small>
</body>
</html>
