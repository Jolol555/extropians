<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Eugene's nuclear threat</title>
<meta name="Author" content="Eugene Leitl (eugene.leitl@lrz.uni-muenchen.de)">
<meta name="Subject" content="Eugene's nuclear threat">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Eugene's nuclear threat</h1>
<!-- received="Sun Oct  1 06:16:18 2000" -->
<!-- isoreceived="20001001121618" -->
<!-- sent="Sun, 1 Oct 2000 04:14:04 -0700 (PDT)" -->
<!-- isosent="20001001111404" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="Eugene's nuclear threat" -->
<!-- id="14807.7292.928179.472937@lrz.uni-muenchen.de" -->
<!-- inreplyto="39D4C854.C6C30841@pobox.com" -->
<strong>From:</strong> Eugene Leitl (<a href="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Eugene's%20nuclear%20threat&In-Reply-To=&lt;14807.7292.928179.472937@lrz.uni-muenchen.de&gt;"><em>eugene.leitl@lrz.uni-muenchen.de</em></a>)<br>
<strong>Date:</strong> Sun Oct 01 2000 - 05:14:04 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0005.html">Eugene Leitl: "Re: Back off!  Im gay!"</a>
<li><strong>Previous message:</strong> <a href="0003.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0000.html">Darin Sunley: "Re: Eugene's nuclear threat"</a>
<li><strong>Maybe reply:</strong> <a href="0000.html">Darin Sunley: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0013.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</a>
<li><strong>Maybe reply:</strong> <a href="0020.html">Spudboy100@aol.com: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0025.html">Samantha Atkins: "Re: Eugene's nuclear threat"</a>
<li><strong>Maybe reply:</strong> <a href="0038.html">Spudboy100@aol.com: "Re: Eugene's nuclear threat"</a>
<li><strong>Maybe reply:</strong> <a href="0039.html">Spudboy100@aol.com: "Re: Eugene's nuclear threat"</a>
<li><strong>Maybe reply:</strong> <a href="0099.html">hal@finney.org: "Re: Eugene's nuclear threat"</a>
<li><strong>Maybe reply:</strong> <a href="0615.html">Robert J. Bradbury: "Re: Eugene's nuclear threat"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4">[ date ]</a>
<a href="index.html#4">[ thread ]</a>
<a href="subject.html#4">[ subject ]</a>
<a href="author.html#4">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Eliezer S. Yudkowsky writes:
<br>
<p><em> &gt; Are you so very, very sure that you know better than the people running the
</em><br>
<em> &gt; facilities?  If the people with enough understanding and experience to have
</em><br>
<p>No. 
<br>
<p>But if I make a mistake, we've got a bunch of dead researchers who
<br>
wouldn't listen. If they make a mistake, we've got millions of
<br>
lightyears of planets full of dead people. (Aliens are also people, of
<br>
course). Dunno, these odds sound good to me.
<br>
<p>On a less galactic scale, there is this crazy molecular biologist down
<br>
the street, who subscribes to the &quot;Green Planet, Death to the People&quot;
<br>
church. (Man, these people are sure not Friendly). Funded by fellow
<br>
billionaire church members, he has managed to engineer a
<br>
long-symptomless-latency high-infectivity high-delayed-mortality
<br>
bioweapon, consisting from a dozen of diverse virus families. You know
<br>
he has made successful tests on primates and people (bums snatched off
<br>
the street), and intends to start releasing the stuff in all major
<br>
airports and subways as well in stratospheric bursts (properly
<br>
packaged). All numerical epidemic models they've ran predict &gt;99%
<br>
infection and &gt;95% mortality rate. In other words, the threat is
<br>
rather believable. Because they're rather paranoid, they've got a
<br>
device to reliably (they're very good engineers) self-destruct the
<br>
entire facility, triggerable from a room you just managed to
<br>
penetrate. (Well, I've seen an old James Bond movie yesterday). 
<br>
<p>Would you press the big red button, instantly killing all people on
<br>
property and safely destroying all virus cultures and information on
<br>
how make them?
<br>
<p><em> &gt; created an AI tell you firmly that they have devoted considerable time and
</em><br>
<em> &gt; effort to Friendliness and they believe the chances are as good as they'll
</em><br>
<em> &gt; ever get, would you really nuke them?  Are you so much smarter than they are,
</em><br>
<em> &gt; you who would not have written the code?
</em><br>
&nbsp;
<br>
I'm just smart enough to know no one can be that smart to predict what
<br>
a superhuman Power is going to do. At the same time, Turing, Goedel &amp;
<br>
footnotes to them say you can't, and game theory and evolution theory
<br>
say it won't be a smart thing to try, since offering some constraints
<br>
on behaviour of Powers, which don't look too pretty if you happen to
<br>
be at the human receiving end of it.
<br>
<p>Thankfully, people are not yet smart/stupid enough to try to make a
<br>
Power in a major, concerted effort, so we're limited to the equivalent
<br>
of an industrial accident. Such as your AI suddenly exploding into
<br>
your face, having unexpectedly slided off into evolutionary
<br>
regime. Because 1) you currently don't have any resources worth
<br>
speaking of 2) you seem so far to make every effort to keep it from
<br>
going darwinian, I so far sleep rather safely.
<br>
<p>(Just to be on the safe side, don't publish your realtime WGS 84
<br>
coordinates, will you? ;)
<br>
<p><em> &gt; Thankfully, you will never have authority over nuclear weapons, but even so,
</em><br>
<em> &gt; just saying those words can make life less pleasant for all of us.  You should
</em><br>
<em> &gt; know better.
</em><br>
&nbsp;
<br>
Thankfully, you will never have authority over enough resources for a
<br>
SI project likely to succeed, but even so, just saying those words can
<br>
make life less pleasant for all of us. You should know better.
<br>
<p>Seriously, who's playing the architect of humankind's future destiny
<br>
here? You think you're smart enough for that?
<br>
<p>Instead of trying to persuade people to pull over into a high enough
<br>
fitness regime by dangling enough of juicy carrots in front of their
<br>
noses before embarking on a project to end all projects, or ending up
<br>
there spontaneously, you say &quot;people no good, I'm also only human, but
<br>
I know what is good for the rest of them, so I'll just go ahead, and
<br>
will do it, the faster, the better&quot;. That sounds smart, for sure.
<br>
<p><em> &gt; me&gt; I'm not. I don't like where the logics of it all is leading us. There
</em><br>
<em> &gt; me&gt; must be a more constructive way out.
</em><br>
<em> &gt; 
</em><br>
<em> &gt; There is.  One group creates one mind; one mind creates the Singularity.  That
</em><br>
<p>That sounds disturbingly terminal. 
<br>
<p>Kinda &lt;godwin&gt;Ein Volk Ein Reich Ein Fuhrer&lt;/godwin&gt;.
<br>
<p><em> &gt; much is determined by the dynamics of technology and intelligence; it is not a
</em><br>
<em> &gt; policy decision, and there is no way that I or anyone else can alter it.  At
</em><br>
<p>&quot;We're in a room full of people. There's a hand grenade on the
<br>
table. I'm going to go and pull out the pin. There is no way that I or
<br>
anyone else can alter it.&quot;
<br>
<p>Uh, don't think so. All I have to do is to prevent someone from
<br>
pulling the pin long enough (even if it involves braining them with a
<br>
heavy blunt instrument), until I can evacuate the room. Afterwards,
<br>
the thing may or may not go off, it will be relatively irrelevant.
<br>
<p><em> &gt; some point, you just have to trust someone, and try to minimize coercion or
</em><br>
<em> &gt; regulations or the use of nuclear weapons, on the grounds that having the
</em><br>
<em> &gt; experience and wisdom to create an AI is a better qualification than being
</em><br>
<em> &gt; able to use force.  If the situation were iterated, if it were any kind of
</em><br>
<p>Why has being smart something to do with being reliable? The opposite, 
<br>
if anything. 
<br>
<p>Now we're all sons of bitches. (Of course no one said exactly this
<br>
statement at Trinity, it's an urban legend).
<br>
<p><em> &gt; social interaction, then there would be a rationale for voting and laws -
</em><br>
<em> &gt; democracy is the only known means by which humans can live together.  But if
</em><br>
<em> &gt; it's a single decision, made only once, in an engineering problem, then the
</em><br>
<em> &gt; best available solution is to trust the engineer who makes it - the more
</em><br>
<em> &gt; politics you involve in the problem, the more force and coercion, the smaller
</em><br>
<em> &gt; the chance of a good outcome.
</em><br>
&nbsp;
<br>
I agree to that, but not if that engineer's decision is going to be
<br>
amplified to a space region a couple of hundren million light years in
<br>
diameter. That sounds rather monstrously irreversible, and hence would
<br>
seem to require substantial consensus.
<br>
<p><em> &gt; I'm not just saying this because I think I'll be on the research team.  I'm
</em><br>
<em> &gt; willing to trust people outside myself.  I'd trust the benevolence of Eric
</em><br>
<em> &gt; Drexler, or Mitchell Porter - or Dan Fabulich or Darin Sunley, for that
</em><br>
<em> &gt; matter.  I'm willing to trust the good intentions of any AI programmer unless
</em><br>
<p>Trust in benevolence of single people I hardly know to decide
<br>
something which has a global impact? No, thanks.
<br>
<p><em> &gt; they specifically demonstrate untrustworthiness, and I'm willing to trust the
</em><br>
<em> &gt; engineering competence of any *successful* AI team that demonstrates a basic
</em><br>
<em> &gt; concern with Friendly AI.
</em><br>
&nbsp;
<br>
One non sequitur after another. Your irrational insistence on
<br>
&quot;Friendly AI&quot; at all costs in face of grave objections does not make
<br>
you extremely trustworthy, but you're surely aware of that.
<br>
<p><em> &gt; The idea that &quot;nobody except 'me' can possibly be trusted&quot; is very natural,
</em><br>
<em> &gt; but it isn't realistic, and it can lead to nothing except for endless
</em><br>
<p>Hardly, since most people don't wind up pushing shopping carts down
<br>
the streets for a living. 
<br>
<p>You have to trust other people. It's natural.
<br>
<p><em> &gt; infighting.  I know a lot of important things about Friendly AI, but I also
</em><br>
<em> &gt; know that not everything I know should be necessary for success - my knowledge
</em><br>
<em> &gt; indicates that it should be possible to succeed with a limited subset of my
</em><br>
<em> &gt; knowledge.  And scientists and engineers are, by and large, benevolent - they
</em><br>
<p>Unless you manage to communicate your insights into words and concepts 
<br>
understandable by other people, you're just another Smart Guy with a
<br>
Stupid Idea.
<br>
<p><em> &gt; may express that benevolence in unsafe ways, but I'm willing to trust to good
</em><br>
<em> &gt; intentions.  After all, it's not like I have a choice.
</em><br>
<p>I don't trust anybody (me included), if the stakes are high
<br>
enough. 
<br>
<p>And please tell us, why you think not having a choice.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0005.html">Eugene Leitl: "Re: Back off!  Im gay!"</a>
<li><strong>Previous message:</strong> <a href="0003.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0000.html">Darin Sunley: "Re: Eugene's nuclear threat"</a>
<li><strong>Maybe reply:</strong> <a href="0000.html">Darin Sunley: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0013.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</a>
<li><strong>Maybe reply:</strong> <a href="0020.html">Spudboy100@aol.com: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0025.html">Samantha Atkins: "Re: Eugene's nuclear threat"</a>
<li><strong>Maybe reply:</strong> <a href="0038.html">Spudboy100@aol.com: "Re: Eugene's nuclear threat"</a>
<li><strong>Maybe reply:</strong> <a href="0039.html">Spudboy100@aol.com: "Re: Eugene's nuclear threat"</a>
<li><strong>Maybe reply:</strong> <a href="0099.html">hal@finney.org: "Re: Eugene's nuclear threat"</a>
<li><strong>Maybe reply:</strong> <a href="0615.html">Robert J. Bradbury: "Re: Eugene's nuclear threat"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4">[ date ]</a>
<a href="index.html#4">[ thread ]</a>
<a href="subject.html#4">[ subject ]</a>
<a href="author.html#4">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:14 MDT</em>
</em>
</small>
</body>
</html>
