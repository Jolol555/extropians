<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Eugene's nuclear threat</title>
<meta name="Author" content="hal@finney.org (hal@finney.org)">
<meta name="Subject" content="Re: Eugene's nuclear threat">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Eugene's nuclear threat</h1>
<!-- received="Mon Oct  2 11:44:49 2000" -->
<!-- isoreceived="20001002174449" -->
<!-- sent="Mon, 2 Oct 2000 10:40:04 -0700" -->
<!-- isosent="20001002174004" -->
<!-- name="hal@finney.org" -->
<!-- email="hal@finney.org" -->
<!-- subject="Re: Eugene's nuclear threat" -->
<!-- id="200010021740.KAA03079@finney.org" -->
<!-- inreplyto="Eugene's nuclear threat" -->
<strong>From:</strong> <a href="mailto:hal@finney.org?Subject=Re:%20Eugene's%20nuclear%20threat&In-Reply-To=&lt;200010021740.KAA03079@finney.org&gt;"><em>hal@finney.org</em></a><br>
<strong>Date:</strong> Mon Oct 02 2000 - 11:40:04 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0100.html">J. R. Molloy: "Re: Eugene's nuclear threat"</a>
<li><strong>Previous message:</strong> <a href="0098.html">David Lubkin: "Re: Violence in schools (Was Re: Back off!  Im gay!)"</a>
<li><strong>Maybe in reply to:</strong> <a href="0004.html">Eugene Leitl: "Eugene's nuclear threat"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0105.html">J. R. Molloy: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0105.html">J. R. Molloy: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0122.html">Samantha Atkins: "Re: Eugene's nuclear threat"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#99">[ date ]</a>
<a href="index.html#99">[ thread ]</a>
<a href="subject.html#99">[ subject ]</a>
<a href="author.html#99">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Eugene writes:
<br>
<em>&gt; What can be possibly more poisonous than the end of the world as we
</em><br>
<em>&gt; know it(tm) and death of all or most human beings?
</em><br>
<p>You need to make several assumptions to get to this from the starting
<br>
point of someone trying to develop a &quot;wild&quot; self-enhancing AI through
<br>
evolutionary techniques:
<br>
<p>&nbsp;- that the process will work at all to get any kind of AI
<br>
&nbsp;- that the AI won't hit a bottleneck somewhere on its self-improvement
<br>
&nbsp;&nbsp;&nbsp;trajectory
<br>
&nbsp;- that the super-intelligent AI will want to wipe out humans
<br>
&nbsp;- that it will succeed
<br>
<p>and also, I should add,
<br>
<p>&nbsp;- that this will be a bad thing.
<br>
<p>Without belaboring this last point, I do think that there is a
<br>
difference between seeing humanity wiped out by mindless gray goo,
<br>
versus having the human race supplanted by an expanding, curious,
<br>
exploratory new super-intelligent life form.  If we step back a bit
<br>
from our humanist chauvinism (and forgetting that we ourselves might be
<br>
targets of the malicious AI), we generally do justify the destruction of
<br>
less intellectually advanced life forms in favor of more advanced ones.
<br>
People do this every day when they eat.  From a sufficiently removed
<br>
perspective, replacing the human race with an intelligence vastly more
<br>
aware, more perceptive, more intelligent and more conscious may not be
<br>
entirely evil.  I don't say it should happen, but it is something to
<br>
consider in evaluating the morality of this outcome.
<br>
<p>As far as the first points, I have argued before that the concept of
<br>
self enhancement is far from certain as a path to super-intelligent AI.
<br>
To even get started we must reach near-human level intelligence on our
<br>
own efforts, a goal which has thwarted decades of effort.  Then, once
<br>
this monumental achievement is reached, is an ordinary human being with
<br>
IQ 100 smart enough to contribute materially to an AI project which has
<br>
reached human-level intelligence?  It's pretty questionable, considering
<br>
how complex the algorithms are likely to be.
<br>
<p>More intelligent brains will be even more complex.  It seems very
<br>
plausible to me that we could hit a point where the complexity grows
<br>
faster than the intelligence, so that the smarter the brain got, the
<br>
harder time it would have seeing how to improve itself.  At that point
<br>
the trajectory to super-AI would fizzle out.
<br>
<p>Even if it does work, it is a leap to assume that an evolved AI would
<br>
want to wipe out the human race.  Maybe the thing can be made &quot;friendly&quot;
<br>
in Eliezer's sense while still using an evolutionary paradigm.  Even if
<br>
it is less amiable, the AI might find that humans are useful tools in
<br>
accomplishing its ends.  See, for example, Greg Bear's society in Moving
<br>
Mars, where shadowy super-intelligences run Earth behind the scenes of
<br>
a seemingly diverse and pluralistic society of physically and mentally
<br>
enhanced humans.
<br>
<p>Then you have to assume that the AI will be capable of carrying out
<br>
its threat despite the physical handicaps it faces.  Yes, it has
<br>
the advantage of intelligence, but there is plenty of historical and
<br>
evolutionary evidence that this is not definitive.  Super-intelligence is
<br>
not omnipotence.
<br>
<p>You have to be awfully sure of yourself to contemplate taking the kinds
<br>
of final actions you have described in the face of a threat with so
<br>
many uncertainties.  Our visions of the future can be so intense and
<br>
detailed that we forget how many other possibilities there are.  We need
<br>
to remember our own limitations and weaknesses in foreseeing the future
<br>
before taking precipitate action based on distant extrapolations.
<br>
<p>Hal
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0100.html">J. R. Molloy: "Re: Eugene's nuclear threat"</a>
<li><strong>Previous message:</strong> <a href="0098.html">David Lubkin: "Re: Violence in schools (Was Re: Back off!  Im gay!)"</a>
<li><strong>Maybe in reply to:</strong> <a href="0004.html">Eugene Leitl: "Eugene's nuclear threat"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0105.html">J. R. Molloy: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0105.html">J. R. Molloy: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0122.html">Samantha Atkins: "Re: Eugene's nuclear threat"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#99">[ date ]</a>
<a href="index.html#99">[ thread ]</a>
<a href="subject.html#99">[ subject ]</a>
<a href="author.html#99">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:14 MDT</em>
</em>
</small>
</body>
</html>
