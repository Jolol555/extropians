<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Sentience</title>
<meta name="Author" content="Dan Fabulich (daniel.fabulich@yale.edu)">
<meta name="Subject" content="Re: Sentience">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Sentience</h1>
<!-- received="Sun Dec 17 20:34:47 2000" -->
<!-- isoreceived="20001218033447" -->
<!-- sent="Sun, 17 Dec 2000 22:33:47 -0500 (EST)" -->
<!-- isosent="20001218033347" -->
<!-- name="Dan Fabulich" -->
<!-- email="daniel.fabulich@yale.edu" -->
<!-- subject="Re: Sentience" -->
<!-- id="Pine.GSO.4.10.10012172113540.8605-100000@morpheus.cis.yale.edu" -->
<!-- inreplyto="001201c066ca$35a1d880$52b7403e@i6x7m6" -->
<strong>From:</strong> Dan Fabulich (<a href="mailto:daniel.fabulich@yale.edu?Subject=Re:%20Sentience&In-Reply-To=&lt;Pine.GSO.4.10.10012172113540.8605-100000@morpheus.cis.yale.edu&gt;"><em>daniel.fabulich@yale.edu</em></a>)<br>
<strong>Date:</strong> Sun Dec 17 2000 - 20:33:47 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4196.html">J. R. Molloy: "Re: Immortality"</a>
<li><strong>Previous message:</strong> <a href="4194.html">Damien Broderick: "Re: Hugos"</a>
<li><strong>In reply to:</strong> <a href="4153.html">Steve Nichols: "Re: Sentience"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4237.html">Steve Nichols: "Re: Sentience"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4195">[ date ]</a>
<a href="index.html#4195">[ thread ]</a>
<a href="subject.html#4195">[ subject ]</a>
<a href="author.html#4195">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Steve Nichols wrote:
<br>
<p><em>&gt; Date: Thu, 14 Dec 2000 12:56:40 -0500
</em><br>
<em>&gt; From: &quot;John Clark&quot; &lt;<a href="mailto:jonkc@worldnet.att.net?Subject=Re:%20Sentience&In-Reply-To=&lt;Pine.GSO.4.10.10012172113540.8605-100000@morpheus.cis.yale.edu&gt;">jonkc@worldnet.att.net</a>&gt;
</em><br>
<em>&gt; Subject: Re: Immortality
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;A thermostats makes its decision
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ... excuse me, but thermostats are just switches that kick
</em><br>
<em>&gt; in at a given threshold, the whole point is that it can't &quot;make
</em><br>
<em>&gt; its decision&quot; ....
</em><br>
<em>&gt; 
</em><br>
<p>A bald assertion.  I get the sense that you're not aware of the bigger
<br>
picture John Clark has in mind (and with which I largely agree).
<br>
<p>John and I actually DO think that the human brain is (analogous to) an
<br>
oversized Turing machine.  We think that it's programmed, and that we
<br>
make &quot;decisions&quot; the same way that Deep Blue &quot;decides&quot; to move its
<br>
bishop.
<br>
<p>&quot;But,&quot; you reply, &quot;Deep Blue was programmed to do that!&quot; and we reply:
<br>
SO ARE WE.  We just have a very complex program, given to us by
<br>
millions of years of evolution.  We &quot;change our own logic&quot; the same
<br>
way Deep Blue alters its play style, the same way a thermostat adjusts
<br>
the gas on your heater.  We're all programmed to do that.  Some of us
<br>
have a more complex program than others.
<br>
<p><em>&gt; &gt;based on its internal state and
</em><br>
<em>&gt; 
</em><br>
<em>&gt; A digital switch does not have an &quot;internal state&quot;
</em><br>
<p>Yes, it does.  It has an &quot;desired&quot; temperature, which it must
<br>
&quot;remember&quot; in order to work properly.  The &quot;desired&quot; temperature isn't
<br>
a property of anything else in the thermostat-heater-room system, so
<br>
it must be an internal state of the thermostat.
<br>
<p><em>&gt; &gt;the external state of the environment, people do the same thing, both
</em><br>
<em>&gt; &gt;&quot; readjust their logic in response to the environment&quot; as you say.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; No, people are not preset &quot; say to put on a hat if temperature drops below
</em><br>
<em>&gt; zero degrees or whatever. We can readjust our logic, switches cannot.
</em><br>
<p>These differ only in complexity, not in kind.  Deep Blue is a Turing
<br>
machine: it differs only in complexity from a simple adder.  Our
<br>
brains are just a finite arrangement of simple finite-state machines
<br>
which do simple things, like thermostats.
<br>
<p><em>&gt;     &gt; Turing machines are neither conscious,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;How do you know?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, they would fail the Turing test for starters.
</em><br>
<p>&lt;blink blink&gt; ALL Turing machines???  This is to say that we'll never
<br>
have a computer that will pass the Turing test, that we can never
<br>
write a program so complex that it could trick people into believing
<br>
that it acted like us.  Were you thinking about this when you said
<br>
that?
<br>
<p><em>&gt; You have rather foolishly missed the whole point that ANALOG is the
</em><br>
<em>&gt; abbreviation of &quot;analogous to infinite-state&quot; ... since true infinity
</em><br>
<em>&gt; is a conjecture ......
</em><br>
<p>But that's just the point.  Very-many-state is still finite-state, is
<br>
still predictable, is still programmed, in an important sense.  It is
<br>
difficult to predict the behavior of complex very-many-state hardware,
<br>
but obviously possible in principle.
<br>
<p><em>&gt; 
</em><br>
<em>&gt;     &gt; Turing machines are hard-wired and cannot evince PHASIC
</em><br>
<em>&gt;     &gt;TRANSIENT behaviour  ... correct me if I am wrong?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Ok, you are wrong. A Turing machine is not hard wired, its program
</em><br>
<em>&gt; can put it into any internal configuration, and the only behavior
</em><br>
<em>&gt; a Turing machine can't produce is pure randomness, that is, produce
</em><br>
<em>&gt; an effect without a cause.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Are you actually claiming the Turing machines exhibit phasic
</em><br>
<em>&gt; transients? Cite evidence please, you are utterly lost on this.
</em><br>
<p>There's no evidence to cite; this is a purely philosophical problem.
<br>
I can obviously get a Turing machine to exhibit periodic behavior.  If
<br>
I hooked a Turing machine up to a set of motors hooked up to some
<br>
eyes, I could program it to go into REM.  If you had some other
<br>
requirements, you could tack those on to a kind of Turing Test for
<br>
Phasic Transient Behavior.  The point is that an adequately complex
<br>
Turing machine could pass that test.
<br>
<p><em>&gt; &gt;Seems to me all you've done is conjure up a black box, call it the
</em><br>
<em>&gt; &gt;phantom median eye and say consciousness comes from there.
</em><br>
<em>&gt; &gt;Not very helpful.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; No, the conceptual &quot;black box&quot; which was a problem before MVT
</em><br>
<em>&gt; now has a complete description, in actual terms and as an evolutionary
</em><br>
<em>&gt; narrative, as the phantom media eye. I have done away with mere
</em><br>
<em>&gt; &quot;black box&quot; conjecture!
</em><br>
<p>How good of you to say so.  But just saying THAT doesn't convince
<br>
anyone.
<br>
<p>Look, you're overlooking the very simple point that in order to make
<br>
any kind of induction, you need to first NOTICE a correlation.  You
<br>
can't justify any scientific inference to &quot;consciousness&quot; unless you
<br>
can observe a few cases where the phantom median eye and consciousness
<br>
go together.  You have to *observe* that, say, wherever you find
<br>
consciousness, the pineal eye is less developed, and then posit that
<br>
wherever the pineal eyes is underdeveloped, you'll find consciousness.
<br>
<p>But you can't observe any such cases, thanks to the problem of other
<br>
minds.  I know I have feelings, and that I smile, cry, scream, etc. as
<br>
I have them.  But all I see you do is smile, cry, scream, etc.  How
<br>
can I tell whether you're having feelings, or whether you're just
<br>
going through the motions?  How do I know you're actually conscious,
<br>
rather than just passing the Turing Test?
<br>
<p>Maybe you have something simpler in mind.  Maybe you're just positing
<br>
MVT as a theory to explain how and when things can pass the Turing
<br>
Test.  But you fail on THOSE grounds, too: you provide no *mechanism*
<br>
by which the phantom pineal eye causes people to be conscious, or to
<br>
act conscious, or, well, anything.  You only claim that consciousness
<br>
DOES happen, and you tell us WHEN, but you don't explain HOW.
<br>
<p>That's a black box.
<br>
<p><em>&gt;     &gt;I don't want to get bogged down in individual cases because of the
</em><br>
<em>&gt;     &gt;problem  of solipsism.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You're going to have to get bogged down in it because if you have a complete
</em><br>
<em>&gt; theory of consciousness you should be able to prove that solipsism is untrue
</em><br>
<em>&gt; but you can't and I don't believe such a theory is even possible.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Solipsism is simply one (admissible) viewpoint. It isn't &quot;refutable&quot; since
</em><br>
<em>&gt; I do not deny that people have this thought. Infinite-state capability
</em><br>
<em>&gt; allows
</em><br>
<em>&gt; *any possible thoughts* including the solipsistic ones. Likewise you cannot
</em><br>
<em>&gt; prove it &quot;correct.&quot;
</em><br>
<p>Another miscommunication.  The solipsist you DO have to deal with is
<br>
someone who's a realist about matter but worries about the problem of
<br>
other minds; you really MUST claim to have dealt with it if you have
<br>
a scientific theory of consciousness.  You should be able to say:
<br>
&quot;Look Clark, you can measure my phantom median eye, so you know that I
<br>
MUST be conscious.&quot;  Or whatever.
<br>
<p>But you offer nothing like that.  That's why you're offering a black box.
<br>
<p><em>&gt; Then (whether you believe that I exist independently, or am just
</em><br>
<em>&gt; another facet of *your* consciousness - the solipsistic claim) by
</em><br>
<em>&gt; accepting that at least your dreams exist, you allow a conscious
</em><br>
<em>&gt; phenomenon, and defeat your previous claim that consciousness
</em><br>
<em>&gt; doesn't occur.
</em><br>
<p>I *know* he didn't say that.  He knows that HE'S conscious, but he's
<br>
worried about YOU (and everyone else).
<br>
<p><em>&gt; Do you still deny &quot;consciousness&quot;? I argue the idealist stance that in
</em><br>
<em>&gt; fact the world can be said to exist in consciousness, and not that
</em><br>
<em>&gt; consciousness (phantom pineal eye) is located in space.
</em><br>
<p>Idealism is good and well.  (Not my bag, but, hey.)  But the question
<br>
you STILL need to ask yourself, even as an idealist, is: are those
<br>
other people in the world conscious?  It doesn't matter whether the
<br>
world is in consciousness or not to answer this question; the answer
<br>
could turn out to be &quot;yes&quot; or &quot;no&quot; whether or not we accept idealism.
<br>
<p>-Dan 
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-unless you love someone-
<br>
&nbsp;&nbsp;&nbsp;&nbsp;-nothing else makes any sense-
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e.e. cummings
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4196.html">J. R. Molloy: "Re: Immortality"</a>
<li><strong>Previous message:</strong> <a href="4194.html">Damien Broderick: "Re: Hugos"</a>
<li><strong>In reply to:</strong> <a href="4153.html">Steve Nichols: "Re: Sentience"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4237.html">Steve Nichols: "Re: Sentience"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4195">[ date ]</a>
<a href="index.html#4195">[ thread ]</a>
<a href="subject.html#4195">[ subject ]</a>
<a href="author.html#4195">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:37 MDT</em>
</em>
</small>
</body>
</html>
