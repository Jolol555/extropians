<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Why would AI want to be friendly?</title>
<meta name="Author" content="hal@finney.org (hal@finney.org)">
<meta name="Subject" content="Re: Why would AI want to be friendly?">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Why would AI want to be friendly?</h1>
<!-- received="Sun Oct  1 09:57:00 2000" -->
<!-- isoreceived="20001001155700" -->
<!-- sent="Sun, 1 Oct 2000 08:53:20 -0700" -->
<!-- isosent="20001001155320" -->
<!-- name="hal@finney.org" -->
<!-- email="hal@finney.org" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="200010011553.IAA18838@finney.org" -->
<!-- inreplyto="Why would AI want to be friendly?" -->
<strong>From:</strong> <a href="mailto:hal@finney.org?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;200010011553.IAA18838@finney.org&gt;"><em>hal@finney.org</em></a><br>
<strong>Date:</strong> Sun Oct 01 2000 - 09:53:20 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0011.html">Harvey Newstrom: "RE: Capitalists and concentration camps"</a>
<li><strong>Previous message:</strong> <a href="0009.html">Mike Lorrey: "Re: Capitalists and concentration camps"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0012.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10">[ date ]</a>
<a href="index.html#10">[ thread ]</a>
<a href="subject.html#10">[ subject ]</a>
<a href="author.html#10">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Subgoals doesn't necessarily make sense when applied to an AI.  Subgoals
<br>
are fundamentally heuristics, to help it achieve higher level goals.
<br>
We work with subgoals, because we aren't smart enough to consider the high
<br>
level goals directly.  We want to eat and have things, so we earn money.
<br>
We want to earn money, so we go to work.  We want to go to work, so we
<br>
get in the car.  We want to get in the car, so we walk to the front door.
<br>
Etc.  Breaking problems down in this way allows beings of our limited
<br>
intelligence to make progress in the world.
<br>
<p>Whether AIs use subgoals or not is not important.  What matters is that
<br>
they are trying to maximize their top-level goal.  We can idealize the AI
<br>
behavior as saying, if I make a change to the world state to turn it from
<br>
A to B, which of A or B ranks higher in my top level goal?  It needs to
<br>
be able to answer this question in order to choose what to do.  Subgoals
<br>
may be a necessary heuristic in order to deal with the multiplicity of
<br>
possible actions, or it may turn out that some other mechanism is used.
<br>
But ideally, whatever the internal workings, the actual behavior of the
<br>
AI will be the same as if it did it the brute-force way, considering
<br>
all possible actions and choosing the one that maximizes its goal.
<br>
<p><em>&gt;From this perspective we can see that &quot;make people happy&quot; is far too
</em><br>
vague to be suitable as a top level goal.  We need an algorithm we can
<br>
build into the machine which, given a potential state of the world,
<br>
returns a ranking for how desirable that state is.  The AI's job is then
<br>
to do its best to change the world so as to maximize that state.
<br>
<p>It's also clear from this that there needs to be only one top level goal.
<br>
If you have more than one, the AI would have built in contradictions.
<br>
The top level goal can have a composite structure (maximize A as long as
<br>
B doesn't fall below a certain minimim), but there needs to be just one.
<br>
<p>Hal
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0011.html">Harvey Newstrom: "RE: Capitalists and concentration camps"</a>
<li><strong>Previous message:</strong> <a href="0009.html">Mike Lorrey: "Re: Capitalists and concentration camps"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0012.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10">[ date ]</a>
<a href="index.html#10">[ thread ]</a>
<a href="subject.html#10">[ subject ]</a>
<a href="author.html#10">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:14 MDT</em>
</em>
</small>
</body>
</html>
