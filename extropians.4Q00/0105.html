<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=Windows-1252">
<title>extropians: Re: Eugene's nuclear threat</title>
<meta name="Author" content="J. R. Molloy (jr@shasta.com)">
<meta name="Subject" content="Re: Eugene's nuclear threat">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Eugene's nuclear threat</h1>
<!-- received="Mon Oct  2 12:49:17 2000" -->
<!-- isoreceived="20001002184917" -->
<!-- sent="Mon, 2 Oct 2000 11:49:18 -0700" -->
<!-- isosent="20001002184918" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: Eugene's nuclear threat" -->
<!-- id="008d01c02ca1$86976740$e6bc473f@jrmolloy" -->
<!-- charset="Windows-1252" -->
<!-- inreplyto="200010021740.KAA03079@finney.org" -->
<strong>From:</strong> J. R. Molloy (<a href="mailto:jr@shasta.com?Subject=Re:%20Eugene's%20nuclear%20threat&In-Reply-To=&lt;008d01c02ca1$86976740$e6bc473f@jrmolloy&gt;"><em>jr@shasta.com</em></a>)<br>
<strong>Date:</strong> Mon Oct 02 2000 - 12:49:18 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0106.html">J. R. Molloy: "Re: MacLeod's Cassini Division"</a>
<li><strong>Previous message:</strong> <a href="0104.html">CountZero: "Re: MacLeod's Cassini Division"</a>
<li><strong>In reply to:</strong> <a href="0099.html">hal@finney.org: "Re: Eugene's nuclear threat"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0110.html">J. R. Molloy: "Peering into the ozone hole"</a>
<li><strong>Reply:</strong> <a href="0110.html">J. R. Molloy: "Peering into the ozone hole"</a>
<li><strong>Reply:</strong> <a href="0112.html">Dan Fabulich: "Intelligence increase was: Eugene's nuclear threat"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#105">[ date ]</a>
<a href="index.html#105">[ thread ]</a>
<a href="subject.html#105">[ subject ]</a>
<a href="author.html#105">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Hal Finney pointed out,
<br>
<p><em>&gt; You need to make several assumptions to get to this from the starting
</em><br>
<em>&gt; point of someone trying to develop a &quot;wild&quot; self-enhancing AI through
</em><br>
<em>&gt; evolutionary techniques:
</em><br>
<p>Even better than making assumptions would be to run some actual experiments to
<br>
see how wild self-enhancing AI might in fact evolve.
<br>
<p><em>&gt; Without belaboring this last point, I do think that there is a
</em><br>
<em>&gt; difference between seeing humanity wiped out by mindless gray goo,
</em><br>
<em>&gt; versus having the human race supplanted by an expanding, curious,
</em><br>
<em>&gt; exploratory new super-intelligent life form.  If we step back a bit
</em><br>
<em>&gt; from our humanist chauvinism (and forgetting that we ourselves might be
</em><br>
<em>&gt; targets of the malicious AI),
</em><br>
<p>Even worse, we might be targets of the Friendly AI.
<br>
(What's that say about us!)
<br>
<p><p><em>&gt; ...we generally do justify the destruction of
</em><br>
<em>&gt; less intellectually advanced life forms in favor of more advanced ones.
</em><br>
<em>&gt; People do this every day when they eat.  From a sufficiently removed
</em><br>
<em>&gt; perspective, replacing the human race with an intelligence vastly more
</em><br>
<em>&gt; aware, more perceptive, more intelligent and more conscious may not be
</em><br>
<em>&gt; entirely evil.  I don't say it should happen, but it is something to
</em><br>
<em>&gt; consider in evaluating the morality of this outcome.
</em><br>
<p>The &quot;morality&quot; is this outcome will be determined (as it always is) by the
<br>
winners. The losers don't get to write history, the winners do (and in their own
<br>
language).
<br>
<p><em>&gt; As far as the first points, I have argued before that the concept of
</em><br>
<em>&gt; self enhancement is far from certain as a path to super-intelligent AI.
</em><br>
<p>Nothing is &quot;certain&quot; except for the Uncertainty Principle. Nevertheless, without
<br>
self enhancement, we will have less chance of understanding the issues at stake.
<br>
<p><em>&gt; To even get started we must reach near-human level intelligence on our
</em><br>
<em>&gt; own efforts, a goal which has thwarted decades of effort.
</em><br>
<p>It was *centuries* after Sci-Fi and da Vinci postulated flying machines that
<br>
humans went to the Moon. Thwarted effort seems to spur people on to overcome the
<br>
obstacles, for some reason. Perhaps it's the challenge of the thing. Airplanes
<br>
weren't good for much in 1910... but sixty years (six decades) later, men walked
<br>
on the Moon.
<br>
<p>&nbsp;&nbsp;Then, once
<br>
<em>&gt; this monumental achievement is reached, is an ordinary human being with
</em><br>
<em>&gt; IQ 100 smart enough to contribute materially to an AI project which has
</em><br>
<em>&gt; reached human-level intelligence?  It's pretty questionable, considering
</em><br>
<em>&gt; how complex the algorithms are likely to be.
</em><br>
<p>An ordinary human being with IQ 100 could contribute quite a lot to an AI
<br>
project. For starters, let's see if we could double this IQ via enhancement
<br>
technology. (Hey, I'd volunteer.)
<br>
<p><em>&gt; More intelligent brains will be even more complex.  It seems very
</em><br>
<em>&gt; plausible to me that we could hit a point where the complexity grows
</em><br>
<em>&gt; faster than the intelligence, so that the smarter the brain got, the
</em><br>
<em>&gt; harder time it would have seeing how to improve itself.  At that point
</em><br>
<em>&gt; the trajectory to super-AI would fizzle out.
</em><br>
<p>That doesn't make sense, because if the brain gets smarter, then it has
<br>
amplified capability to see how to improve itself. Conversely, the dumber brains
<br>
get (uh-oh, here comes an anti-drug ad), the difficult it would be for them to
<br>
see how to improve themselves. Right?
<br>
<p><em>&gt;
</em><br>
<em>&gt; Even if it does work, it is a leap to assume that an evolved AI would
</em><br>
<em>&gt; want to wipe out the human race.
</em><br>
<p>I don't know about that. Coincidentally, the smartest people also seem to have
<br>
the most problems relating to the human race at large. If an evolved AI
<br>
succinctly and compellingly explained its reasons for wiping out the human race,
<br>
perhaps we'd treat it like unabomber Ted Kacynski, who has his share of
<br>
endorsements among the Green Party.
<br>
<p><em>&gt;Even if
</em><br>
<em>&gt; it is less amiable, the AI might find that humans are useful tools in
</em><br>
<em>&gt; accomplishing its ends.
</em><br>
<p>Interesting that many people think of AI in the singular. Imagine treating
<br>
natural intelligence the same way:
<br>
<p>*The* biological intelligence (BI) or *the* native intelligence (NI) sounds kind
<br>
of cryptic doesn't it? After all, &quot;the&quot; human intelligence resides in billions
<br>
of craniums. What reason do we have to suppose that AI might not  reside in
<br>
millions of desktop or laboratory machines? I tend to think of them as AIs, Mind
<br>
Children, Robo sapiens, Spiritual Machines, and Artilects, rather than as *the*
<br>
AI.
<br>
<p><em>&gt;  See, for example, Greg Bear's society in Moving
</em><br>
<em>&gt; Mars, where shadowy super-intelligences run Earth behind the scenes of
</em><br>
<em>&gt; a seemingly diverse and pluralistic society of physically and mentally
</em><br>
<em>&gt; enhanced humans.
</em><br>
<p>Shades of _The Matrix_, and deja vu all over again.
<br>
<p><em>&gt; Then you have to assume that the AI will be capable of carrying out
</em><br>
<em>&gt; its threat despite the physical handicaps it faces.  Yes, it has
</em><br>
<em>&gt; the advantage of intelligence, but there is plenty of historical and
</em><br>
<em>&gt; evolutionary evidence that this is not definitive.  Super-intelligence is
</em><br>
<em>&gt; not omnipotence.
</em><br>
<p>Right on. Stephen Hawking is only as menacing as his electric wheelchair can
<br>
make him.
<br>
<p>--J. R.
<br>
<p><p><p>&quot;Sushi... how come they don't cook that stuff?&quot;
<br>
--Alfredo Benzene
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0106.html">J. R. Molloy: "Re: MacLeod's Cassini Division"</a>
<li><strong>Previous message:</strong> <a href="0104.html">CountZero: "Re: MacLeod's Cassini Division"</a>
<li><strong>In reply to:</strong> <a href="0099.html">hal@finney.org: "Re: Eugene's nuclear threat"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0110.html">J. R. Molloy: "Peering into the ozone hole"</a>
<li><strong>Reply:</strong> <a href="0110.html">J. R. Molloy: "Peering into the ozone hole"</a>
<li><strong>Reply:</strong> <a href="0112.html">Dan Fabulich: "Intelligence increase was: Eugene's nuclear threat"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#105">[ date ]</a>
<a href="index.html#105">[ thread ]</a>
<a href="subject.html#105">[ subject ]</a>
<a href="author.html#105">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:14 MDT</em>
</em>
</small>
</body>
</html>
