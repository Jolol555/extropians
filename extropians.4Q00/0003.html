<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Why would AI want to be friendly?</title>
<meta name="Author" content="Eugene Leitl (eugene.leitl@lrz.uni-muenchen.de)">
<meta name="Subject" content="Re: Why would AI want to be friendly?">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Why would AI want to be friendly?</h1>
<!-- received="Sun Oct  1 04:13:22 2000" -->
<!-- isoreceived="20001001101322" -->
<!-- sent="Sun, 1 Oct 2000 02:10:17 -0700 (PDT)" -->
<!-- isosent="20001001091017" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="14806.65401.396602.347547@lrz.uni-muenchen.de" -->
<!-- inreplyto="39D695EB.AC725FD1@pobox.com" -->
<strong>From:</strong> Eugene Leitl (<a href="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;14806.65401.396602.347547@lrz.uni-muenchen.de&gt;"><em>eugene.leitl@lrz.uni-muenchen.de</em></a>)<br>
<strong>Date:</strong> Sun Oct 01 2000 - 03:10:17 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0004.html">Eugene Leitl: "Eugene's nuclear threat"</a>
<li><strong>Previous message:</strong> <a href="0002.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0053.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</a>
<li><strong>Reply:</strong> <a href="0053.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3">[ date ]</a>
<a href="index.html#3">[ thread ]</a>
<a href="subject.html#3">[ subject ]</a>
<a href="author.html#3">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Eliezer S. Yudkowsky writes:
<br>
<p><em> &gt; You need curiosity in order to think, learn, and discover, and you need to
</em><br>
<em> &gt; think, learn, and discover in order to be more generally efficient at
</em><br>
<em> &gt; manipulating reality, and being more generally efficient at manipulating
</em><br>
<em> &gt; reality means you can be more efficiently Friendly.
</em><br>
&nbsp;
<br>
Being more efficient at manipulating reality also means the exact
<br>
opposite of above. A totally impotent devil is not much of a devil.
<br>
<p>And please stop using that &quot;Friendly&quot; thing, because it means
<br>
different things to different people. (The child molester thinks he's
<br>
being very friendly to the child, the child probably sees it in a
<br>
somewhat different light. The Nazis were obviously only trying to
<br>
cleanse the world of an ancient evil. Is not whipping a pain pig
<br>
Friendly? Koran is Good/Evil, The Bible is Good/Evil, The Kapital is
<br>
Good/Evil, &quot;The Road to Serfdom&quot; is Good/Evil, parents telling a child
<br>
not to do A are being Friendly/Unfriendly &lt;insert 10^8 similiar
<br>
examples&gt;).
<br>
<p>Friendly is not observer-invariant, goddamit. If you want to impose
<br>
your particular concept on the rest of us via (thankfully
<br>
hypothetical) AI proxy, then please call a spade a spade. Call it
<br>
Eliezer-Friendly, or something.
<br>
&nbsp;
<br>
<em> &gt; &quot;I'm not rightly sure what kind of thinking could lead to this confusion.&quot; 
</em><br>
<em> &gt; Goals and subgoals are thoughts, not source code.  Subgoals, if they exist at
</em><br>
<p>That's not very helpful conceptry. Have you defined both terms, as you 
<br>
use them? Since you don't seem to use them as most people.
<br>
<p>I would rather use established concepts as &quot;system state&quot; and &quot;system
<br>
Hamiltonian&quot; when dealing with this. Have you read Ashby's classical
<br>
&quot;Design for a Brain&quot;? You can get it from Dover for cheap. It's old
<br>
(1952, 1960), but very modern.
<br>
<p>To me (and most of AI people), a goal is a reasoning target. You
<br>
attempt to reach it in a series of steps, using a progress
<br>
metric. Because you frequently get blocked, or trapped in local
<br>
minima, you have to keep score of past milestones, and backtrack, if
<br>
you haven't been able to make progress for a while. In reality, this
<br>
is being done by an inference engine, sequential or parallel. It tries
<br>
to find a traversible path from A (say, a set of axioms) to B (Is blah
<br>
blah blah true, i.e. can it be inferred from A?), both being
<br>
specifyable in some formal language.
<br>
<p>To me, this is a rather obsolete (and remarkably sterile) view of
<br>
navigation in (potentially very rugged) energy landscapes. Not that I
<br>
assume you're trapped in any such old AI trash, but it is sure
<br>
politically smart to avoid using words like &quot;goals&quot; in context of new
<br>
AI (if there is such a thing). People might misunderstand.
<br>
<p><em> &gt; all, exist as regularities in plans - that is, certain states of the Universe
</em><br>
<em> &gt; are reliably useful in achieving the real, desired states.  Since &quot;subgoals&quot;
</em><br>
<em> &gt; have no independent desirability - they are only useful way-stations along the
</em><br>
<em> &gt; road to the final state - they can be hierarchical, or networked, or strange
</em><br>
<em> &gt; loops, or arranged in whatever order you like; or rather, whatever order best
</em><br>
<em> &gt; mirrors reality.
</em><br>
<p>Huh. Whatever. And, of course, it is clearly obvious what is best
<br>
mirroring reality and what not, without being given a chance of being
<br>
evaluated by reality, since that wouldn't truncate the
<br>
particular-metric-Unfriendly behaviour space region.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0004.html">Eugene Leitl: "Eugene's nuclear threat"</a>
<li><strong>Previous message:</strong> <a href="0002.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0053.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</a>
<li><strong>Reply:</strong> <a href="0053.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3">[ date ]</a>
<a href="index.html#3">[ thread ]</a>
<a href="subject.html#3">[ subject ]</a>
<a href="author.html#3">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:14 MDT</em>
</em>
</small>
</body>
</html>
