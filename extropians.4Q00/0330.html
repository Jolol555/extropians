<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Eliezer S. Yudkowsky' whopper</title>
<meta name="Author" content="Eugene Leitl (eugene.leitl@lrz.uni-muenchen.de)">
<meta name="Subject" content="Re: Eliezer S. Yudkowsky' whopper">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Eliezer S. Yudkowsky' whopper</h1>
<!-- received="Wed Oct  4 14:27:23 2000" -->
<!-- isoreceived="20001004202723" -->
<!-- sent="Wed, 4 Oct 2000 11:13:45 -0700 (PDT)" -->
<!-- isosent="20001004181345" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: Eliezer S. Yudkowsky' whopper" -->
<!-- id="14811.29529.569643.549323@lrz.uni-muenchen.de" -->
<!-- inreplyto="LC2-LFD29jmCVpwfXbf0000082f@hotmail.com" -->
<strong>From:</strong> Eugene Leitl (<a href="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Eliezer%20S.%20Yudkowsky'%20whopper&In-Reply-To=&lt;14811.29529.569643.549323@lrz.uni-muenchen.de&gt;"><em>eugene.leitl@lrz.uni-muenchen.de</em></a>)<br>
<strong>Date:</strong> Wed Oct 04 2000 - 12:13:45 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0331.html">Amara Graps: "Re: Does the state-vector collapse?"</a>
<li><strong>Previous message:</strong> <a href="0329.html">J. R. Molloy: "Re: Intelligence increase"</a>
<li><strong>In reply to:</strong> <a href="0300.html">Andrew Lias: "Re: Eliezer S. Yudkowsky' whopper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0454.html">Emlyn: "Re: Eliezer S. Yudkowsky' whopper"</a>
<li><strong>Reply:</strong> <a href="0454.html">Emlyn: "Re: Eliezer S. Yudkowsky' whopper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#330">[ date ]</a>
<a href="index.html#330">[ thread ]</a>
<a href="subject.html#330">[ subject ]</a>
<a href="author.html#330">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Andrew Lias writes:
<br>
<em> &gt; [very true stuff snipped]
</em><br>
<em> &gt; The simple fact is that the future is unknown and, beyond a very limited 
</em><br>
<em> &gt; range, unknowable.  Making predictions is a risky business at best.  The 
</em><br>
<em> &gt; strong AI community has repeatedly embarrassed itself by making 
</em><br>
<em> &gt; over-confident estimates about when strong AI would show up.  As a 
</em><br>
<em> &gt; consequence, a lot of people outside of the AI community consider the whole 
</em><br>
<em> &gt; venture to be a joke, regardless of the merits of any particular line of 
</em><br>
<em> &gt; research.
</em><br>
&nbsp;
<br>
A have a (somewhat miffed-sounding, since written from the view of
<br>
the Lisp community) view on that from:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.naggum.no/worse-is-better.html">http://www.naggum.no/worse-is-better.html</a>
<br>
<p>(       [...]
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Part of the problem stems from our very dear friends in the artificial
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;intelligence (AI) business. AI has a number of good approaches to
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;formalizing human knowledge and problem solving behavior. However, AI
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;does not provide a panacea in any area of its applicability. Some
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;early promoters of AI to the commercial world raised expectation
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;levels too high. These expectations had to do with the effectiveness
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and deliverability of expert-system-based applications.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;When these expectations were not met, some looked for scapegoats,
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;which frequently were the Lisp companies, particularly when it came to
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;deliverability. Of course, if the AI companies had any notion about
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;what the market would eventually expect from delivered AI software,
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;they never shared it with any Lisp companies I know about. I believe
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the attitude of the AI companies was that the Lisp companies will do
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;what they need to survive, so why share customer lists and information
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with them? [...]
<br>
)
<br>
&nbsp;
<br>
<em> &gt; I think that we all agree that we are approaching a point where an explosion 
</em><br>
<em> &gt; of intelligence will become a viable possibility.  I think, however, that 
</em><br>
<em> &gt; it's rather foolish to assert that it will happen before a given time.  It 
</em><br>
<em> &gt; *might* happen by 2020.  It also *might* happen in 2520.  It also *might* 
</em><br>
<em> &gt; happen tomorrow, if some obscure line of research that nobody is aware of 
</em><br>
<em> &gt; hits the jackpot.  We simply don't know what factors may advance or retard 
</em><br>
<em> &gt; the date.
</em><br>
&nbsp;
<br>
This view is too harsh. We can surely assign probabilities, assuming
<br>
nothing happens to the global civilization (major war, global climate
<br>
flip-flopping (just read a paper on corellation of species extinctions
<br>
with volcanic activity, anthropogenic gases may also act as
<br>
precipitant), asteroidal impact, the triffids, terminal boredom) it's
<br>
extremely unlikely to happen tomorrow, probability rapidly peaking
<br>
after that, having a maximum somewhat by slightly after 2020, but
<br>
*distinctly* before 2520. I would be indeed genuinely surprised if it
<br>
didn't happen before 2050 (you can call me in my home for the elderly
<br>
if this turns out wrong, or drop the notice by the dewar).
<br>
<p><em> &gt; My own concern is that there is a reasonable possibility that we'll hit the 
</em><br>
<em> &gt; Singularity in my lifetime and that it may take a form that will exclude my 
</em><br>
<em> &gt; interests.  My primary interest is that the human race doesn't fall into (or 
</em><br>
<p>Ditto here.
<br>
<p><em> &gt; get dragged into) a non-continuitive extinction (i.e., an extinction where 
</em><br>
<em> &gt; we simply cease, rather than transcending to something more intelligent and 
</em><br>
<em> &gt; capable).  My primary concern is that the only thing that we can control, 
</em><br>
<em> &gt; when it comes to such a singularity is the initial conditions.  I can think 
</em><br>
<p>Question is, whether initial conditions have an impact on early growth
<br>
kinetics. I think they do. If they don't, the whole question is moot,
<br>
anyway, and all we can do is lean back, and watch the pretty pyrotechnics.
<br>
<p><em> &gt; of all to many scenarios where I, personally (to say nothing of the species 
</em><br>
<em> &gt; as a whole), either get left behind or destroyed.  Unfortunately, there's a 
</em><br>
<em> &gt; whole bunch of those.
</em><br>
<em> &gt; 
</em><br>
<em> &gt; It is my hope that we will be able to see *just* far enough ahead that we 
</em><br>
<em> &gt; don't just blunder into the damned thing (ask me about my monkey scenario! 
</em><br>
<em> &gt; ;-).  One thing that seems certain to me is that there seems to be a lot of 
</em><br>
<em> &gt; unfounded speculations regarding the morality and rationality of 
</em><br>
<em> &gt; post-organic hyperintelligence.  It seems that the only same position to 
</em><br>
<p>Guilty as charged. However, evolutionary biology is equally applicable 
<br>
to sentients and nonsentients.
<br>
<p><em> &gt; hold in an arena that harbors such beings it to *be* such a being (and even 
</em><br>
<em> &gt; that might be presumptive -- we are assuming that amplified intelligence is 
</em><br>
<em> &gt; a good thing to have; it's possible that hyperintelligences are prone to 
</em><br>
<em> &gt; fatal or adverse mental states that only manifest beyond a certain level of 
</em><br>
<em> &gt; complexity; after all, we do know that certain pathological mental states, 
</em><br>
<em> &gt; such as a desire for self-martrydom, only show up at human levels of 
</em><br>
<em> &gt; intelligence).
</em><br>
&nbsp;
<br>
Yeah, but we have to decide today, in face of limited data. We have to
<br>
work with current models, however faulty. The longer we wait, the
<br>
harder will it be to change the course.
<br>
&nbsp;
<br>
<em> &gt; Frankly, the notion that we are approaching a Singularity scares the hell 
</em><br>
<em> &gt; out of me.  If I thought that there were some viable way to prevent it or 
</em><br>
<em> &gt; even to safely delay it, I'd probably lobby to do so.  I'm not convinced 
</em><br>
<em> &gt; that there are any such options.  As such, my personal goal is to be an 
</em><br>
<em> &gt; early adopter and hope that self-amplification isn't the mental equivilent 
</em><br>
<em> &gt; of jumping off of a cliff.
</em><br>
<p>Sure, but would be the alternative? (Assuming, it happens at all) we
<br>
can't prevent it, we can (at best) delay it. My statistical (assuming
<br>
current numbers) life expenctancy will be exceeded in about 60
<br>
years. If the Singularity turns out malignant, I can only die once. It
<br>
can be a bit premature, it can be quite nasty, but it will be brief.
<br>
If it's softer, it gives me a genuine chance to achieve immortality
<br>
the Woody Allen way.
<br>
<p>Dunno, sounds good to me.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0331.html">Amara Graps: "Re: Does the state-vector collapse?"</a>
<li><strong>Previous message:</strong> <a href="0329.html">J. R. Molloy: "Re: Intelligence increase"</a>
<li><strong>In reply to:</strong> <a href="0300.html">Andrew Lias: "Re: Eliezer S. Yudkowsky' whopper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0454.html">Emlyn: "Re: Eliezer S. Yudkowsky' whopper"</a>
<li><strong>Reply:</strong> <a href="0454.html">Emlyn: "Re: Eliezer S. Yudkowsky' whopper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#330">[ date ]</a>
<a href="index.html#330">[ thread ]</a>
<a href="subject.html#330">[ subject ]</a>
<a href="author.html#330">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:15 MDT</em>
</em>
</small>
</body>
</html>
