<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=Windows-1252">
<title>extropians: Re: &quot;Cybernetic Totalism?&quot;</title>
<meta name="Author" content="Bryan Moss (bryan.moss@btinternet.com)">
<meta name="Subject" content="Re: &quot;Cybernetic Totalism?&quot;">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: &quot;Cybernetic Totalism?&quot;</h1>
<!-- received="Thu Oct 12 11:31:19 2000" -->
<!-- isoreceived="20001012173119" -->
<!-- sent="Thu, 12 Oct 2000 18:21:31 +0100" -->
<!-- isosent="20001012172131" -->
<!-- name="Bryan Moss" -->
<!-- email="bryan.moss@btinternet.com" -->
<!-- subject="Re: &quot;Cybernetic Totalism?&quot;" -->
<!-- id="000f01c03471$06159380$8b14073e@mossbtinternet.com" -->
<!-- charset="Windows-1252" -->
<!-- inreplyto="b49zokbtlpr.fsf@sans04.nada.kth.se" -->
<strong>From:</strong> Bryan Moss (<a href="mailto:bryan.moss@btinternet.com?Subject=Re:%20&quot;Cybernetic%20Totalism?&quot;&In-Reply-To=&lt;000f01c03471$06159380$8b14073e@mossbtinternet.com&gt;"><em>bryan.moss@btinternet.com</em></a>)<br>
<strong>Date:</strong> Thu Oct 12 2000 - 11:21:31 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1030.html">GBurch1@aol.com: "Re: Good reading material - books on global history"</a>
<li><strong>Previous message:</strong> <a href="1028.html">John Clark: "Re: Future baby nukes (boffin-sized?)"</a>
<li><strong>In reply to:</strong> <a href="0946.html">Anders Sandberg: "Re: &quot;Cybernetic Totalism?&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1035.html">Michael S. Lorrey: "Re: &quot;Cybernetic Totalism?&quot;"</a>
<li><strong>Reply:</strong> <a href="1035.html">Michael S. Lorrey: "Re: &quot;Cybernetic Totalism?&quot;"</a>
<li><strong>Reply:</strong> <a href="1110.html">xgl: "Re: &quot;Cybernetic Totalism?&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1029">[ date ]</a>
<a href="index.html#1029">[ thread ]</a>
<a href="subject.html#1029">[ subject ]</a>
<a href="author.html#1029">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Anders Sandberg wrote:
<br>
<p><em>&gt; <a href="http://www.edge.org/3rd_culture/lanier/lanier_index.html">http://www.edge.org/3rd_culture/lanier/lanier_index.html</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; I found some of the points rather tonic, but the overall
</em><br>
<em>&gt; impression wasn't that high.
</em><br>
<em>&gt;
</em><br>
<em>&gt; What he is aiming at is disproving or undermining the
</em><br>
<em>&gt; *unconsidered* ideology of technological eschatology. To
</em><br>
<em>&gt; some extent I agree with him - there is an awful amount of
</em><br>
<em>&gt; badly thought out fluff on this [...]
</em><br>
<p>This is my interpretation of his argument:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Autonomy has no technological benefit.  That is, we
<br>
cannot *use* autonomous devices.  But the approaches we are
<br>
currently taking towards the creation of autonomous devices
<br>
are technological.  For example, *evolving* a brain is not
<br>
science--it provides no understanding--and, if the goal is
<br>
human equivalency, it's of no technological benefit either.
<br>
(Imagine an automobile manufacturer that has not only fully
<br>
automated their production and design process but only
<br>
produces passenger-less self-driving vehicles.)  Given that
<br>
AI has no apparent merit either as science or technology
<br>
there must be another reason for adopting it as a goal, and
<br>
that reason is the quasi-religious &quot;Cybernetic Totalism&quot;.
<br>
<p>In many ways I think he's right.  Moravec and de Garis seem
<br>
to see themselves as Agents of Evolution; but the
<br>
inevitability of &quot;mind children&quot; is, in Lanier's terms, an
<br>
&quot;intellectual mistake&quot;.  These are, after all, *mind*
<br>
children.  Of course, if we're going to have children they
<br>
might as well be mind children; in essence it is that our
<br>
children--biological or otherwise--are *not* inevitable that
<br>
defines our current situation.  We will soon be able to
<br>
choose what we preserve of ourselves, rather than accepting
<br>
our genes as our lineage.  I think Lanier's mistake, like so
<br>
many critics of technology, is the failure to recognise that
<br>
technology does not create new problems it merely magnifies
<br>
existing ones.  In the case of AI it's that old favourite
<br>
&quot;what are we and what are we doing here?&quot;  You can't
<br>
question the purpose of fully autonomous systems without
<br>
also questioning the purpose of our own society.
<br>
<p>The other area Lanier attacks is AI as interface. I agree
<br>
with him wholeheartedly here, 'intelligent software'
<br>
isn't as smart an idea as it sounds.  Unfortunately current
<br>
measures of interface efficiency give absolute efficiency to
<br>
an automated process; as Lanier points out, however,
<br>
automating a process isn't always in the best interest of
<br>
the user.  I am unaware of a way to measure the
<br>
effectiveness of automation.  I'm also dubious of the merits
<br>
of 'agents' in user interfaces (it would be simple to test
<br>
the idea that another human presents the best interface,
<br>
just get a skilled computer operator to act as the agent).
<br>
The point of Lanier's (and my) digression is that interface
<br>
is another often-used justification for the goal of AI that
<br>
is also questionable.  I also agree with Lanier's argument
<br>
that the promise of AI causes today's lacklustre software.
<br>
(It's not just a matter of annoying paperclips either;
<br>
people are sequential and difficult to navigate, computers
<br>
are increasingly adopting these traits at the expense of
<br>
exploiting the spatial skills of their users.  Design
<br>
principles are routinely ignored because programmers
<br>
anthropomorphise their programs.  The slow uptake of object
<br>
oriented code and distributed processing may also have its
<br>
roots in the anthropomorphic beginnings of computing,
<br>
although complexity is probably the main culprit.)
<br>
<p>It may also be that Lanier is using AI's questionable
<br>
application as a user interface to challenge the idea that
<br>
AI could become integral to society rather than simply be
<br>
used to automate facets of society into a kind of
<br>
disconnectedness (as with my example of the automobile
<br>
manufacturer).  If we want AI to form a part of society and
<br>
do not simply accept AI as our mind children and &quot;hand over
<br>
the reigns&quot; we have to find a niche in society that involves
<br>
interaction rather than automated isolation.  By questioning
<br>
this niche Lanier adds merit to his argument.
<br>
<p><em>&gt; [...] we should see how we can polish up transhumanist
</em><br>
<em>&gt; thinking in order not to fall into the traps he describes.
</em><br>
<p>I think Lanier makes some good points that are difficult to
<br>
find in what is essentially a very confused essay.  The main
<br>
thing we should take away from this is the questionable
<br>
nature of AI as a goal, not because it is necessarily a bad
<br>
goal but because, for me, it illuminates a bigger problem.
<br>
After all, what is society but a fully autonomous system?
<br>
And what external purpose does that system serve?  For me
<br>
Lanier's essay was an affirmation of my own doubts about
<br>
transhumanism.  Without a purpose we cannot architect our
<br>
future, we need to discover the precise things we wish to
<br>
preserve about ourselves and our society and only then can
<br>
we go forward.  In my mind it is not enough to say &quot;I want
<br>
to live forever&quot;; &quot;I&quot; is simply shorthand, I want to know
<br>
what it is about me that I should preserve and why I should
<br>
preserve it.  I think these problems run deep enough that
<br>
we'll need more than polish.
<br>
<p>BM
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1030.html">GBurch1@aol.com: "Re: Good reading material - books on global history"</a>
<li><strong>Previous message:</strong> <a href="1028.html">John Clark: "Re: Future baby nukes (boffin-sized?)"</a>
<li><strong>In reply to:</strong> <a href="0946.html">Anders Sandberg: "Re: &quot;Cybernetic Totalism?&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1035.html">Michael S. Lorrey: "Re: &quot;Cybernetic Totalism?&quot;"</a>
<li><strong>Reply:</strong> <a href="1035.html">Michael S. Lorrey: "Re: &quot;Cybernetic Totalism?&quot;"</a>
<li><strong>Reply:</strong> <a href="1110.html">xgl: "Re: &quot;Cybernetic Totalism?&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1029">[ date ]</a>
<a href="index.html#1029">[ thread ]</a>
<a href="subject.html#1029">[ subject ]</a>
<a href="author.html#1029">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:16 MDT</em>
</em>
</small>
</body>
</html>
