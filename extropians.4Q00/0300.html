<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Eliezer S. Yudkowsky' whopper</title>
<meta name="Author" content="Andrew Lias (anrwlias@hotmail.com)">
<meta name="Subject" content="Re: Eliezer S. Yudkowsky' whopper">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Eliezer S. Yudkowsky' whopper</h1>
<!-- received="Wed Oct  4 11:03:11 2000" -->
<!-- isoreceived="20001004170311" -->
<!-- sent="Wed, 04 Oct 2000 17:02:59 GMT" -->
<!-- isosent="20001004170259" -->
<!-- name="Andrew Lias" -->
<!-- email="anrwlias@hotmail.com" -->
<!-- subject="Re: Eliezer S. Yudkowsky' whopper" -->
<!-- id="LC2-LFD29jmCVpwfXbf0000082f@hotmail.com" -->
<!-- inreplyto="Eliezer S. Yudkowsky' whopper" -->
<strong>From:</strong> Andrew Lias (<a href="mailto:anrwlias@hotmail.com?Subject=Re:%20Eliezer%20S.%20Yudkowsky'%20whopper&In-Reply-To=&lt;LC2-LFD29jmCVpwfXbf0000082f@hotmail.com&gt;"><em>anrwlias@hotmail.com</em></a>)<br>
<strong>Date:</strong> Wed Oct 04 2000 - 11:02:59 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0301.html">Michael S. Lorrey: "Re: &quot;the species' immune system&quot;"</a>
<li><strong>Previous message:</strong> <a href="0299.html">Dehede011@aol.com: "Re: Capitalists and coercion"</a>
<li><strong>Maybe in reply to:</strong> <a href="0060.html">J. R. Molloy: "Eliezer S. Yudkowsky' whopper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0330.html">Eugene Leitl: "Re: Eliezer S. Yudkowsky' whopper"</a>
<li><strong>Reply:</strong> <a href="0330.html">Eugene Leitl: "Re: Eliezer S. Yudkowsky' whopper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#300">[ date ]</a>
<a href="index.html#300">[ thread ]</a>
<a href="subject.html#300">[ subject ]</a>
<a href="author.html#300">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
<a href="mailto:Spudboy100@aol.com?Subject=Re:%20Eliezer%20S.%20Yudkowsky'%20whopper&In-Reply-To=&lt;LC2-LFD29jmCVpwfXbf0000082f@hotmail.com&gt;">Spudboy100@aol.com</a> write:
<br>
<p><em>&gt;I think that technological predicitions for the last 20 years have erred on
</em><br>
<em>&gt;the side over-optimism, in the same sense that we'd have colonies in space 
</em><br>
<em>&gt;by
</em><br>
<em>&gt;the year 2000 and moving sidewalks. Following that conjecture the 
</em><br>
<em>&gt;scientific
</em><br>
<em>&gt;advancements will continue, but never as quickly as desired by the
</em><br>
futurists/transhumanists/extropians.
<br>
<p>I'll agree that these sorts of predictions will almost, inevitably, be 
<br>
wrong, but I also think that it's just as dangerous to predict that these 
<br>
accounts are overly optimistic as it is to declare that they are overly 
<br>
pesimisstic.
<br>
<p>Technological development is a funny thing.  Some things that seem like a 
<br>
slam dunk (e.g., space colonies) prove to be inviable in the predicted 
<br>
timeframe (either because technology doesn't advance fast enough, or because 
<br>
of other issues that retard the tech).  On the other hand, some technologies 
<br>
spring up and bite us long before we ever expect them.
<br>
<p>Personal computers are a great example of the latter.  Every science fiction 
<br>
writer in the golden age was envisioning moon colonies (if not actual 
<br>
cities) by 2000, but none of them pictured computers as being anything less 
<br>
than gigantic.
<br>
<p>The simple fact is that the future is unknown and, beyond a very limited 
<br>
range, unknowable.  Making predictions is a risky business at best.  The 
<br>
strong AI community has repeatedly embarrassed itself by making 
<br>
over-confident estimates about when strong AI would show up.  As a 
<br>
consequence, a lot of people outside of the AI community consider the whole 
<br>
venture to be a joke, regardless of the merits of any particular line of 
<br>
research.
<br>
<p>I think that we all agree that we are approaching a point where an explosion 
<br>
of intelligence will become a viable possibility.  I think, however, that 
<br>
it's rather foolish to assert that it will happen before a given time.  It 
<br>
*might* happen by 2020.  It also *might* happen in 2520.  It also *might* 
<br>
happen tomorrow, if some obscure line of research that nobody is aware of 
<br>
hits the jackpot.  We simply don't know what factors may advance or retard 
<br>
the date.
<br>
<p>My own concern is that there is a reasonable possibility that we'll hit the 
<br>
Singularity in my lifetime and that it may take a form that will exclude my 
<br>
interests.  My primary interest is that the human race doesn't fall into (or 
<br>
get dragged into) a non-continuitive extinction (i.e., an extinction where 
<br>
we simply cease, rather than transcending to something more intelligent and 
<br>
capable).  My primary concern is that the only thing that we can control, 
<br>
when it comes to such a singularity is the initial conditions.  I can think 
<br>
of all to many scenarios where I, personally (to say nothing of the species 
<br>
as a whole), either get left behind or destroyed.  Unfortunately, there's a 
<br>
whole bunch of those.
<br>
<p>It is my hope that we will be able to see *just* far enough ahead that we 
<br>
don't just blunder into the damned thing (ask me about my monkey scenario! 
<br>
;-).  One thing that seems certain to me is that there seems to be a lot of 
<br>
unfounded speculations regarding the morality and rationality of 
<br>
post-organic hyperintelligence.  It seems that the only same position to 
<br>
hold in an arena that harbors such beings it to *be* such a being (and even 
<br>
that might be presumptive -- we are assuming that amplified intelligence is 
<br>
a good thing to have; it's possible that hyperintelligences are prone to 
<br>
fatal or adverse mental states that only manifest beyond a certain level of 
<br>
complexity; after all, we do know that certain pathological mental states, 
<br>
such as a desire for self-martrydom, only show up at human levels of 
<br>
intelligence).
<br>
<p>Frankly, the notion that we are approaching a Singularity scares the hell 
<br>
out of me.  If I thought that there were some viable way to prevent it or 
<br>
even to safely delay it, I'd probably lobby to do so.  I'm not convinced 
<br>
that there are any such options.  As such, my personal goal is to be an 
<br>
early adopter and hope that self-amplification isn't the mental equivilent 
<br>
of jumping off of a cliff.
<br>
_________________________________________________________________________
<br>
Get Your Private, Free E-mail from MSN Hotmail at <a href="http://www.hotmail.com">http://www.hotmail.com</a>.
<br>
<p>Share information about yourself, create your own public profile at 
<br>
<a href="http://profiles.msn.com">http://profiles.msn.com</a>.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0301.html">Michael S. Lorrey: "Re: &quot;the species' immune system&quot;"</a>
<li><strong>Previous message:</strong> <a href="0299.html">Dehede011@aol.com: "Re: Capitalists and coercion"</a>
<li><strong>Maybe in reply to:</strong> <a href="0060.html">J. R. Molloy: "Eliezer S. Yudkowsky' whopper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0330.html">Eugene Leitl: "Re: Eliezer S. Yudkowsky' whopper"</a>
<li><strong>Reply:</strong> <a href="0330.html">Eugene Leitl: "Re: Eliezer S. Yudkowsky' whopper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#300">[ date ]</a>
<a href="index.html#300">[ thread ]</a>
<a href="subject.html#300">[ subject ]</a>
<a href="author.html#300">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:15 MDT</em>
</em>
</small>
</body>
</html>
