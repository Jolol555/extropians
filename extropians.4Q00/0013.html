<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Eugene's nuclear threat</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Eugene's nuclear threat">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Eugene's nuclear threat</h1>
<!-- received="Sun Oct  1 11:07:10 2000" -->
<!-- isoreceived="20001001170710" -->
<!-- sent="Sun, 01 Oct 2000 13:04:32 -0400" -->
<!-- isosent="20001001170432" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Eugene's nuclear threat" -->
<!-- id="39D76EA0.C7D3497@pobox.com" -->
<!-- inreplyto="14807.7292.928179.472937@lrz.uni-muenchen.de" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Eugene's%20nuclear%20threat&In-Reply-To=&lt;39D76EA0.C7D3497@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 01 2000 - 11:04:32 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0014.html">Eliezer S. Yudkowsky: "Re: Why wouldn't friendly AI leave fundies in the dust?"</a>
<li><strong>Previous message:</strong> <a href="0012.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<li><strong>In reply to:</strong> <a href="0004.html">Eugene Leitl: "Eugene's nuclear threat"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0015.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0015.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0029.html">Eugene Leitl: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0089.html">J. R. Molloy: "Re: Eugene's nuclear threat"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13">[ date ]</a>
<a href="index.html#13">[ thread ]</a>
<a href="subject.html#13">[ subject ]</a>
<a href="author.html#13">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Eugene Leitl wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; But if I make a mistake, we've got a bunch of dead researchers who
</em><br>
<em>&gt; wouldn't listen. If they make a mistake, we've got millions of
</em><br>
<em>&gt; lightyears of planets full of dead people. (Aliens are also people, of
</em><br>
<em>&gt; course). Dunno, these odds sound good to me.
</em><br>
<p>Eugene, if you make a mistake, you've got a planet full of people who were
<br>
wiped out by grey goo.  As for the scenario where millions of lightyears of
<br>
people get wiped out - if it were possible for one little planet to make that
<br>
kind of mistake, and there are that many aliens in range, someone *else* would
<br>
have already done it to *us*.  It's just the Earth that's at stake.
<br>
<p>And above all, if you make a mistake - you'll have made a mistake that
<br>
involved deliberate murder.
<br>
<p>According to your theories, we have a world where everyone is free to murder
<br>
anyone who works on a technology that someone doesn't like.  According to you,
<br>
the people destroying GM crops are not doing anything immoral; if they have a
<br>
flaw, it's that they aren't bombing Monsanto's corporate headquarters.  If you
<br>
work on an upload project, then anyone who distrusts uploads and thinks AI is
<br>
safer has a perfect right to stop the uploading project by killing as many
<br>
people as necessary.  *I* would have the right to kill *you*, this instant,
<br>
because you might slow down AI in favor of nanotechnology.
<br>
<p>Do you really think that kind of world, that systemic process for dealing with
<br>
new technologies, would yield better results than the nonviolent version?
<br>
<p>Even Calvin and Hobbes know better than that.
<br>
<p>C:  &quot;I don't believe in ethics any more. As far as I am concerned, the
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;end justifies the means. Get what you can while the getting's good –
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that's what I say. It's a dog-eat-dog world, so I'll do whatever I
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;have to do, and let others argue about whether it's right or not.&quot;
<br>
<p>&lt;Hobbes pushes Calvin into a mud hole.&gt;
<br>
<p>C:  &quot;Why did you do THAT?&quot;
<br>
H:  &quot;You were in my way.  Now you're not.  The end justifies the means.&quot;
<br>
C:  &quot;I didn't mean for everyone, you dolt! Just ME!
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0014.html">Eliezer S. Yudkowsky: "Re: Why wouldn't friendly AI leave fundies in the dust?"</a>
<li><strong>Previous message:</strong> <a href="0012.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<li><strong>In reply to:</strong> <a href="0004.html">Eugene Leitl: "Eugene's nuclear threat"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0015.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0015.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0029.html">Eugene Leitl: "Re: Eugene's nuclear threat"</a>
<li><strong>Reply:</strong> <a href="0089.html">J. R. Molloy: "Re: Eugene's nuclear threat"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13">[ date ]</a>
<a href="index.html#13">[ thread ]</a>
<a href="subject.html#13">[ subject ]</a>
<a href="author.html#13">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:14 MDT</em>
</em>
</small>
</body>
</html>
