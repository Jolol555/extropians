<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Let's hear Eugene's ideas</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Let's hear Eugene's ideas">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Let's hear Eugene's ideas</h1>
<!-- received="Thu Oct  5 04:19:46 2000" -->
<!-- isoreceived="20001005101946" -->
<!-- sent="Thu, 05 Oct 2000 03:21:26 -0700" -->
<!-- isosent="20001005102126" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Let's hear Eugene's ideas" -->
<!-- id="39DC5626.9A09D95C@objectent.com" -->
<!-- inreplyto="39D96F5B.FF5743BB@posthuman.com" -->
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Let's%20hear%20Eugene's%20ideas&In-Reply-To=&lt;39DC5626.9A09D95C@objectent.com&gt;"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Thu Oct 05 2000 - 04:21:26 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0372.html">Samantha Atkins: "Re: Eugene's nuclear threat"</a>
<li><strong>Previous message:</strong> <a href="0370.html">Eugene Leitl: "controversial risk assessment of strangelet generator"</a>
<li><strong>In reply to:</strong> <a href="0159.html">Brian Atkins: "Re: Let's hear Eugene's ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0433.html">Damien Broderick: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Reply:</strong> <a href="0433.html">Damien Broderick: "Re: Let's hear Eugene's ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#371">[ date ]</a>
<a href="index.html#371">[ thread ]</a>
<a href="subject.html#371">[ subject ]</a>
<a href="author.html#371">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Brian Atkins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; <a href="mailto:hal@finney.org?Subject=Re:%20Let's%20hear%20Eugene's%20ideas&In-Reply-To=&lt;39DC5626.9A09D95C@objectent.com&gt;">hal@finney.org</a> wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Brian Atkins writes:
</em><br>
<em>&gt; &gt; &gt; Well as I said to Eugene- look around at the reality of the next 20 years
</em><br>
<em>&gt; &gt; &gt; (max). There are likely to be no Turing Police tracking down and containing
</em><br>
<em>&gt; &gt; &gt; all these AIs that all the hackers and scientists out there will dream up.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; That's not clear.  First, it could easily take longer than 20 years to
</em><br>
<em>&gt; &gt; get superhuman AI, for several reasons:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;  - We may not have nanotech in 20 years
</em><br>
<em>&gt; &gt;  - We may hit Moore's Wall before then as computer speeds turn out to be
</em><br>
<em>&gt; &gt;    on an S curve just like every other technology before them
</em><br>
<em>&gt; &gt;  - Software may continue to improve as it has in the past (i.e. not very
</em><br>
<em>&gt; &gt;    fast)
</em><br>
<em>&gt; &gt;  - AI researchers have a track record of over-optimism
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Perhaps so, in which case Eugene has nothing to worry about. These things
</em><br>
<em>&gt; above are not what we want to discuss on this thread. We want to hear what
</em><br>
<em>&gt; you propose to do to have a good outcome in a world where the above things
</em><br>
<em>&gt; turn out to be wrong.
</em><br>
<em>&gt; 
</em><br>
<p>I was not aware that it was your business to define or delimit what we
<br>
discuss in this or any other thread.  It is actually quite important
<br>
that we understand the relative steepness of the ramp toward Singularity
<br>
because our what we spend time and energy on to insure the best outcomes
<br>
we can changes with our conclusions.  
<br>
<p><em>&gt; &gt;
</em><br>
<em>&gt; &gt; Secondly, I suspect that in this time frame we are going to see
</em><br>
<em>&gt; &gt; increased awareness of the dangers of future technology, with Joy's
</em><br>
<em>&gt; &gt; trumpet blast just the beginning.  Joy included &quot;robotics&quot; in his troika
</em><br>
<em>&gt; &gt; of technological terrors (I guess calling it &quot;AI&quot; wouldn't have let him
</em><br>
<em>&gt; &gt; keep to the magic three letters).  If we do see an Index of of Forbidden
</em><br>
<em>&gt; &gt; Technology, it is entirely possible that AI research will be included.
</em><br>
<em>&gt;
</em><br>
<p>That is extremely unlikely.  What AI we use is very delimited and quite
<br>
crucial to the problem domains it is deployed for.  We need greater
<br>
strategic intelligence including from AI.  AI is a very many splendored
<br>
thing.  We sould not throw out everything to do with AI just because we
<br>
are afraid of SI.  That would be dumber that teaching creationism. 
<br>
<p>&nbsp;
<br>
<em>&gt; Don't you think this better happen soon, otherwise the governments will
</em><br>
<em>&gt; end up trying to regulate something that already is in widespread use? There
</em><br>
<em>&gt; are going to be &quot;intelligent&quot; programs out there soon- in fact you can already
</em><br>
<em>&gt; see commercials in the last year touting &quot;intelligent&quot; software packages.
</em><br>
<em>&gt; Do you really think it is likely our government would outlaw this area of
</em><br>
<em>&gt; software development once it becomes a huge market? Extremely unlikely...
</em><br>
<em>&gt;
</em><br>
<p>There are intelligent programs out there today and have been for quite
<br>
some years now.  But most &quot;intelligent&quot; software packages aren't and
<br>
insult the intelligence of their buyers.  
<br>
&nbsp;
<br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Third, realistically the AI scenario will take time to unfold.  As I
</em><br>
<em>&gt; &gt; have argued repeatedly, self-improvement can't really take off until
</em><br>
<em>&gt; &gt; we can build super-human intelligence on our own (because IQ 100 is
</em><br>
<em>&gt; &gt; self-evidently not smart enough to figure out how to do AI, or else
</em><br>
<em>&gt; &gt; we'd have had it years ago).  So the climb to human equivalence will
</em><br>
<em>&gt; &gt; continue to be slow and frustrating.  Progress will be incremental,
</em><br>
<em>&gt; &gt; with a gradual expansion of capability.
</em><br>
<em>&gt;
</em><br>
<p>No cutting edge technology comes from the middle of the Bell Curve.  I
<br>
don't see the point of this argument.  
<br>
<p>&nbsp;
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt; that appear when you have intelligent machines will become part of the
</em><br>
<em>&gt; &gt; public dialogue long before any super-human AI could appear on the scene.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Therefore I don't think that super-intelligent AI will catch society by
</em><br>
<em>&gt; &gt; surprise, but will appear in a social milieu which is well aware of the
</em><br>
<em>&gt; &gt; possibility, the potential, and the peril.  If society is more concerned
</em><br>
<em>&gt; &gt; about the dangers than the opportunities, then we might well see Turing
</em><br>
<em>&gt; &gt; Police enforcing restrictions on AI research.
</em><br>
<em>&gt;
</em><br>
<p>I agree.  Along the way we overcome the first level fears of loss of
<br>
jobs and income and the benefits going only to the richest corporations
<br>
and government.  Show me AI tools that work with me as intelligent
<br>
assistants in producing what is important to me and a lot of fear and
<br>
uncertainty begins to melt away. 
<br>
&nbsp;
<br>
<em>&gt; Well I'd love to see how that would work. On one hand you want to allow
</em><br>
<em>&gt; some research in order to get improved smart software packages, but on
</em><br>
<em>&gt; the other hand you want to prevent the &quot;bad&quot; software development that
</em><br>
<em>&gt; might lead to a real general intelligence? Is the government going to sit
</em><br>
<em>&gt; and watch every line of code that every hacker on the planet types in?
</em><br>
<em>&gt; In an era of super-strong encryption and electronic privacy (we hope)?
</em><br>
<p><p>Eventually, AI will become more and more general and self-organizing. 
<br>
Code, generally must go in that direction if we are to reap the benefits
<br>
of computerization.  We cannot indefinitely write code like we do
<br>
today.  Nor can we have only programs that are &quot;mindless&quot;.  But the cure
<br>
will lead over time to a self-organizing, self-optimizing software
<br>
environment and likely to increasing self-awareness and general
<br>
intelligence.  If there has been a strong sybiotic relationship and
<br>
building of trust along the way then this eventually is far less likely
<br>
to be deadly than otherwise.  On the other hand, building an SI in the
<br>
basement and springing it on the world one fine morning is likely to
<br>
generate maximum reaction, fear, chance of really bad outcome and
<br>
general discord.   Assuming you could get there from here, of course.
<br>
<p>- samantha
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0372.html">Samantha Atkins: "Re: Eugene's nuclear threat"</a>
<li><strong>Previous message:</strong> <a href="0370.html">Eugene Leitl: "controversial risk assessment of strangelet generator"</a>
<li><strong>In reply to:</strong> <a href="0159.html">Brian Atkins: "Re: Let's hear Eugene's ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0433.html">Damien Broderick: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Reply:</strong> <a href="0433.html">Damien Broderick: "Re: Let's hear Eugene's ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#371">[ date ]</a>
<a href="index.html#371">[ thread ]</a>
<a href="subject.html#371">[ subject ]</a>
<a href="author.html#371">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:15 MDT</em>
</em>
</small>
</body>
</html>
