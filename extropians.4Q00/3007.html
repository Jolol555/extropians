<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: The mathematics of effective perfection</title>
<meta name="Author" content="hal@finney.org (hal@finney.org)">
<meta name="Subject" content="Re: The mathematics of effective perfection">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: The mathematics of effective perfection</h1>
<!-- received="Wed Nov 29 10:57:00 2000" -->
<!-- isoreceived="20001129175700" -->
<!-- sent="Wed, 29 Nov 2000 09:49:19 -0800" -->
<!-- isosent="20001129174919" -->
<!-- name="hal@finney.org" -->
<!-- email="hal@finney.org" -->
<!-- subject="Re: The mathematics of effective perfection" -->
<!-- id="200011291749.JAA10451@finney.org" -->
<!-- inreplyto="The mathematics of effective perfection" -->
<strong>From:</strong> <a href="mailto:hal@finney.org?Subject=Re:%20The%20mathematics%20of%20effective%20perfection&In-Reply-To=&lt;200011291749.JAA10451@finney.org&gt;"><em>hal@finney.org</em></a><br>
<strong>Date:</strong> Wed Nov 29 2000 - 10:49:19 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3008.html">Eugene.Leitl@lrz.uni-muenchen.de: "Re: Free riding on Gnutella white paper"</a>
<li><strong>Previous message:</strong> <a href="3006.html">Eugene.Leitl@lrz.uni-muenchen.de: "Re: THE PLANT: It Failed"</a>
<li><strong>Maybe in reply to:</strong> <a href="2940.html">Eliezer S. Yudkowsky: "The mathematics of effective perfection"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3007">[ date ]</a>
<a href="index.html#3007">[ thread ]</a>
<a href="subject.html#3007">[ subject ]</a>
<a href="author.html#3007">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
John Clark writes, quoting Eliezer:
<br>
<em>&gt;     &gt;Nonetheless, if a  transhuman can have &quot;effectively perfect&quot; self-knowledge
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't see how. Tomorrow  you might find a proof of the Goldbach Conjecture
</em><br>
<em>&gt; and prove it true, or tomorrow you might find a counterexample and prove it false,
</em><br>
<em>&gt; or it might be Turing unprovable, meaning it's true so you'll never find a counterexample
</em><br>
<em>&gt; to prove it wrong but a finite proof does not exist so you'll never find a way to
</em><br>
<em>&gt; prove it's correct. You might not  find a proof or a counterexample, not in
</em><br>
<em>&gt; a year, not in a million years,  not in 10^9^9^9 years, not ever. You won't
</em><br>
<em>&gt; even know your task is hopeless so you might just keep plugging away at the
</em><br>
<em>&gt; problem for eternity and make absolutely zero progress.  We don't know even
</em><br>
<em>&gt; approximately how this might turn out because we can't assign meaningful
</em><br>
<em>&gt; probabilities to the various possible outcomes, we don't know and can't know
</em><br>
<em>&gt; what if anything our mind will come up with. I just don't know what I'm going to
</em><br>
<em>&gt; do tomorrow because I don't understand myself very well, and the same would
</em><br>
<em>&gt; be true if I was a chimp or a human or a Transhuman.
</em><br>
<p>So in your view, for an entity to have perfect self-knowledge requires
<br>
it to be able to answer any question about its future behavior, even a
<br>
question which would require infinite computing power to answer?
<br>
<p>I don't think this is a reasonable requirement.  We wouldn't expect
<br>
an entity to be able to answer questions on other topics which require
<br>
infinite computations, would we?  I don't see why you view this as an
<br>
imperfection.  An entity could have perfect knowledge of the rules of
<br>
the Game of Life, say, without being able to predict whether an arbitrary
<br>
pattern will stabilize or not.
<br>
<p>Maybe it's just semantics, and for you, perfect knowledge means
<br>
omega-point levels of omniscience?
<br>
<p>To me, the interesting question is whether an entity can have perfect
<br>
self-knowledge in the sense that it has an accurate and complete model
<br>
of its own mind.  It can answer any question about the present state of
<br>
its own mind accurately (modulo trickery along the lines of &quot;John Clark
<br>
cannot consistently assert this sentence,&quot; which shed little light on
<br>
the fundamental questions IMO).  We don't have perfect (or probably even
<br>
good) knowledge of ourselves in this sense.  Will future intelligences
<br>
be similarly restricted?  I don't see why they should.
<br>
<p>Hal
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3008.html">Eugene.Leitl@lrz.uni-muenchen.de: "Re: Free riding on Gnutella white paper"</a>
<li><strong>Previous message:</strong> <a href="3006.html">Eugene.Leitl@lrz.uni-muenchen.de: "Re: THE PLANT: It Failed"</a>
<li><strong>Maybe in reply to:</strong> <a href="2940.html">Eliezer S. Yudkowsky: "The mathematics of effective perfection"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3007">[ date ]</a>
<a href="index.html#3007">[ thread ]</a>
<a href="subject.html#3007">[ subject ]</a>
<a href="author.html#3007">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:32 MDT</em>
</em>
</small>
</body>
</html>
