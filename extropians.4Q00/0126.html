<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Let's hear Eugene's ideas</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: Let's hear Eugene's ideas">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Let's hear Eugene's ideas</h1>
<!-- received="Mon Oct  2 18:41:30 2000" -->
<!-- isoreceived="20001003004130" -->
<!-- sent="Mon, 02 Oct 2000 20:42:43 -0400" -->
<!-- isosent="20001003004243" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: Let's hear Eugene's ideas" -->
<!-- id="39D92B83.4013B4D@posthuman.com" -->
<!-- inreplyto="00100210044801.00680@tachyon" -->
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20Let's%20hear%20Eugene's%20ideas&In-Reply-To=&lt;39D92B83.4013B4D@posthuman.com&gt;"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Mon Oct 02 2000 - 18:42:43 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0127.html">J. R. Molloy: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Previous message:</strong> <a href="0125.html">J. R. Molloy: "friendly AI"</a>
<li><strong>In reply to:</strong> <a href="0117.html">James Rogers: "Re: Let's hear Eugene's ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0128.html">J. R. Molloy: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Reply:</strong> <a href="0128.html">J. R. Molloy: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Reply:</strong> <a href="0200.html">James Rogers: "Re: Let's hear Eugene's ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#126">[ date ]</a>
<a href="index.html#126">[ thread ]</a>
<a href="subject.html#126">[ subject ]</a>
<a href="author.html#126">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
James Rogers wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; On Mon, 02 Oct 2000, Eliezer Yudkowsky wrote:
</em><br>
<em>&gt; &gt; Looking over Eugene's posts, I begin to become confused.  As far as I can
</em><br>
<em>&gt; &gt; tell, Eugene thinks that seed AI (both evolutionary and nonevolutionary),
</em><br>
<em>&gt; &gt; nanotechnology, and uploading will all inevitably end in disaster.  I could be
</em><br>
<em>&gt; &gt; wrong about Eugene's opinion on uploading, but as I recall Eugene said to
</em><br>
<em>&gt; &gt; Molloy that the rapid self-enhancement loop means that one-mind-wins all even
</em><br>
<em>&gt; &gt; in a multi-AI scenario, and presumably this statement applies to uploading as
</em><br>
<em>&gt; &gt; well.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; While I don't speak for Eugene, I think I understand what his primary
</em><br>
<em>&gt; concern is.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;From most of what I have seen thrown about, most singularity/AI
</em><br>
<em>&gt; development plans appear to have extremely half-assed deployment schemes,
</em><br>
<em>&gt; to the point of being grossly negligent.  Most of the researchers seem
</em><br>
<em>&gt; largely concerned with development rather than working on the problems of
</em><br>
<em>&gt; deployment.  The pervasive &quot;we'll just flip the switch and the universe
</em><br>
<em>&gt; will change as we know it&quot; attitude is unnecessarily careless and arguably
</em><br>
<em>&gt; not even a necessary outcome of such research, but if the attitude persists
</em><br>
<em>&gt; all sorts of untold damage may happen as a result.  As with all
</em><br>
<em>&gt; potentially nasty new technologies, you have to run it like a military
</em><br>
<em>&gt; operation, having a vast number of contingency plans at your disposal in
</em><br>
<em>&gt; case things do go wrong.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I personally believe that a controlled and reasonably safe deployment
</em><br>
<em>&gt; scheme is possible, and certainly preferable.  And contrary to what some
</em><br>
<em>&gt; people will argue, I have not seen an argument that has convinced me that
</em><br>
<em>&gt; controlled growth of the AI is not feasible; it has to obey the same laws
</em><br>
<em>&gt; of physics and mathematics as everyone else.  If our contingency
</em><br>
<em>&gt; technologies are not adequate at the time AI is created, put hard resource
</em><br>
<em>&gt; constraints on the AI until contingencies are in place.  A constrained AI
</em><br>
<em>&gt; is still extraordinarly useful, even if operating at below its potential.
</em><br>
<em>&gt; The very fact that a demonstrable AI technology exists (coupled with the
</em><br>
<em>&gt; early technology development capabilities of said AI) should allow one to
</em><br>
<em>&gt; directly access and/or leverage enough financial resources to start
</em><br>
<em>&gt; working on a comprehensive program of getting people off of our orbiting
</em><br>
<em>&gt; rock and hopefully outside of our local neighborhood.  I would prefer to
</em><br>
<em>&gt; be observing from a very safe distance before unleashing an unconstrained
</em><br>
<em>&gt; AI upon the planet.
</em><br>
<p>Well as I said to Eugene- look around at the reality of the next 20 years
<br>
(max). There are likely to be no Turing Police tracking down and containing
<br>
all these AIs that all the hackers and scientists out there will dream up.
<br>
And secondly, this whole &quot;get into space&quot; idea is also completely unlikely
<br>
to happen within the said timeperiod. Do you have any /realistic/ ideas?
<br>
<p>For the record, as Eliezer described, SIAI does not plan for the kind of
<br>
half-assed deployment scheme you indirectly attribute.
<br>
<pre>
-- 
Brian Atkins
Director, Singularity Institute for Artificial Intelligence
<a href="http://www.singinst.org/">http://www.singinst.org/</a>
</pre>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0127.html">J. R. Molloy: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Previous message:</strong> <a href="0125.html">J. R. Molloy: "friendly AI"</a>
<li><strong>In reply to:</strong> <a href="0117.html">James Rogers: "Re: Let's hear Eugene's ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0128.html">J. R. Molloy: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Reply:</strong> <a href="0128.html">J. R. Molloy: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Reply:</strong> <a href="0200.html">James Rogers: "Re: Let's hear Eugene's ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#126">[ date ]</a>
<a href="index.html#126">[ thread ]</a>
<a href="subject.html#126">[ subject ]</a>
<a href="author.html#126">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:14 MDT</em>
</em>
</small>
</body>
</html>
