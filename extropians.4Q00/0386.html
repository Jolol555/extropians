<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Eliezer S. Yudkowsky' whopper</title>
<meta name="Author" content="Andrew Lias (anrwlias@hotmail.com)">
<meta name="Subject" content="Re: Eliezer S. Yudkowsky' whopper">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Eliezer S. Yudkowsky' whopper</h1>
<!-- received="Thu Oct  5 09:08:14 2000" -->
<!-- isoreceived="20001005150814" -->
<!-- sent="Thu, 05 Oct 2000 15:08:05 GMT" -->
<!-- isosent="20001005150805" -->
<!-- name="Andrew Lias" -->
<!-- email="anrwlias@hotmail.com" -->
<!-- subject="Re: Eliezer S. Yudkowsky' whopper" -->
<!-- id="F171NVPkMlIxoyNW3FA0000c8f1@hotmail.com" -->
<!-- inreplyto="Eliezer S. Yudkowsky' whopper" -->
<strong>From:</strong> Andrew Lias (<a href="mailto:anrwlias@hotmail.com?Subject=Re:%20Eliezer%20S.%20Yudkowsky'%20whopper&In-Reply-To=&lt;F171NVPkMlIxoyNW3FA0000c8f1@hotmail.com&gt;"><em>anrwlias@hotmail.com</em></a>)<br>
<strong>Date:</strong> Thu Oct 05 2000 - 09:08:05 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0387.html">Bryan Moss: "Re: Capitalists and coercion"</a>
<li><strong>Previous message:</strong> <a href="0385.html">Eugene Leitl: "Re: Intelligence increase"</a>
<li><strong>Maybe in reply to:</strong> <a href="0060.html">J. R. Molloy: "Eliezer S. Yudkowsky' whopper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0423.html">Spudboy100@aol.com: "Re: Eliezer S. Yudkowsky' whopper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#386">[ date ]</a>
<a href="index.html#386">[ thread ]</a>
<a href="subject.html#386">[ subject ]</a>
<a href="author.html#386">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Eugene Leitl &lt;<a href="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Eliezer%20S.%20Yudkowsky'%20whopper&In-Reply-To=&lt;F171NVPkMlIxoyNW3FA0000c8f1@hotmail.com&gt;">eugene.leitl@lrz.uni-muenchen.de</a>&gt; writes:
<br>
<em>&gt;Andrew Lias writes:
</em><br>
<p><em>&gt;  &gt; Making predictions is a risky business at best.  The
</em><br>
<em>&gt;  &gt; strong AI community has repeatedly embarrassed itself by making
</em><br>
<em>&gt;  &gt; over-confident estimates about when strong AI would show up.
</em><br>
[...]
<br>
<em>&gt;A have a (somewhat miffed-sounding, since written from the view of
</em><br>
<em>&gt;the Lisp community) view on that from:
</em><br>
<em>&gt;      <a href="http://www.naggum.no/worse-is-better.html">http://www.naggum.no/worse-is-better.html</a>
</em><br>
[...]
<br>
Great example!
<br>
<p><p><em>&gt;  &gt; I think that we all agree that we are approaching a point where an 
</em><br>
<em>&gt;explosion
</em><br>
<em>&gt;  &gt; of intelligence will become a viable possibility.  I think, however, 
</em><br>
<em>&gt;that
</em><br>
<em>&gt;  &gt; it's rather foolish to assert that it will happen before a given time.  
</em><br>
<em>&gt;It
</em><br>
<em>&gt;  &gt; *might* happen by 2020.  It also *might* happen in 2520.  It also 
</em><br>
<em>&gt;*might*
</em><br>
<em>&gt;  &gt; happen tomorrow, if some obscure line of research that nobody is aware 
</em><br>
<em>&gt;of
</em><br>
<em>&gt;  &gt; hits the jackpot.  We simply don't know what factors may advance or 
</em><br>
<em>&gt;retard
</em><br>
<em>&gt;  &gt; the date.
</em><br>
<em>&gt;
</em><br>
<em>&gt;This view is too harsh. We can surely assign probabilities, assuming
</em><br>
<em>&gt;nothing happens to the global civilization (major war, global climate
</em><br>
<em>&gt;flip-flopping (just read a paper on corellation of species extinctions
</em><br>
<em>&gt;with volcanic activity, anthropogenic gases may also act as
</em><br>
<em>&gt;precipitant), asteroidal impact, the triffids, terminal boredom) it's
</em><br>
<em>&gt;extremely unlikely to happen tomorrow, probability rapidly peaking
</em><br>
<em>&gt;after that, having a maximum somewhat by slightly after 2020, but
</em><br>
<em>&gt;*distinctly* before 2520.
</em><br>
<p>Oh, I quite agree.  I deliberatly bracketed the 2020 guess with a 
<br>
hyper-pessimistic and a hyper-optimistic set of guesses.  I would be just as 
<br>
surprised as you would be if it took that long (or if it happened tomorrow). 
<br>
&nbsp;&nbsp;On the other hand, we really don't know what factors are going to effect 
<br>
the date.  As that Grand Old Man of Extropianism, Vernor Vinge, stated, if 
<br>
the complexity of the brain is five or six orders of magnitude beyond our 
<br>
current estimates, it may be such a complicated task to duplicate 
<br>
intelligence in other substrates that, even though its feasible *in 
<br>
principle*, it may be impossible in practice.  Now, I want to emphasize that 
<br>
(just like Vinge), I doubt that we will run into such a roadblock, but we 
<br>
shouldn't dismiss the possibility out of hand.
<br>
<p>My major concern with predictions is one of credibility.  As noted, I 
<br>
consider the Singularity to be a real possibility and one that is (whatever 
<br>
else you think of it) consequential to us Homo saps.  It is definitely 
<br>
something that we don't want to just blunder into.  Foresight is our most 
<br>
precious commodity when it comes to this issue.  But is we, who have been 
<br>
giving it the most serious thought, hinge our warnings on a particular date, 
<br>
we run the same risk that evangelical rapturites run when they make their 
<br>
predictions -- the date comes and goes and we have egg on our face.  Any 
<br>
predictions we make after that are going to be viewed with suspicious, as 
<br>
will all of our other ideas.  Let's face it, we're far enough on the fringe 
<br>
as it is.  Given that I *do* think that we have important things to say, we 
<br>
really should give due consideration to our reputations, else we're going to 
<br>
be ignored at a point in time when it may be critically important to be 
<br>
heard.  Even vague guesses (e.g., &quot;Sometime in the 21st Century&quot;) may come 
<br>
to bite us.  At best, I think that we should simply say that we think that 
<br>
we are approaching a crucial event in human history and that, although no 
<br>
one knows when it will happen, it may well happen within our lifetimes.
<br>
<p><em>&gt;I would be indeed genuinely surprised if it
</em><br>
<em>&gt;didn't happen before 2050 (you can call me in my home for the elderly
</em><br>
<em>&gt;if this turns out wrong, or drop the notice by the dewar).
</em><br>
<p>To be honest, I would share your surprise.  Don't think that I don't have my 
<br>
own private guesstimates.  I'm just not going to publicly stake my 
<br>
reputation on them. :-)
<br>
<p><em>&gt;  &gt; My primary concern is that the only thing that we can control,
</em><br>
<em>&gt;  &gt; when it comes to such a singularity is the initial conditions.  I can 
</em><br>
<em>&gt;think
</em><br>
<em>&gt;
</em><br>
<em>&gt;Question is, whether initial conditions have an impact on early growth
</em><br>
<em>&gt;kinetics. I think they do. If they don't, the whole question is moot,
</em><br>
<em>&gt;anyway, and all we can do is lean back, and watch the pretty pyrotechnics.
</em><br>
<p>Oh, sure.  I would be a fool to deny this.  Perhaps a more precise statement 
<br>
would be that *if* there's anything we can control with respect to the final 
<br>
manifestation of the Singularity, it's going to be in setting the initial 
<br>
conditions.  It's one of those cases where we may as well try because we 
<br>
don't have anything to lose for the effort and potentially much to gain.
<br>
<p><em>&gt;  &gt; It is my hope that we will be able to see *just* far enough ahead that 
</em><br>
<em>&gt;we
</em><br>
<em>&gt;  &gt; don't just blunder into the damned thing (ask me about my monkey 
</em><br>
<em>&gt;scenario!
</em><br>
<em>&gt;  &gt; ;-).  One thing that seems certain to me is that there seems to be a 
</em><br>
<em>&gt;lot of
</em><br>
<em>&gt;  &gt; unfounded speculations regarding the morality and rationality of
</em><br>
<em>&gt;  &gt; post-organic hyperintelligence.  It seems that the only same position 
</em><br>
<em>&gt;to
</em><br>
<em>&gt;
</em><br>
<em>&gt;Guilty as charged. However, evolutionary biology is equally applicable
</em><br>
<em>&gt;to sentients and nonsentients.
</em><br>
<p>I would agree.  Of course, as J.S.B. Haldane (roughly) said (using &quot;Nature&quot; 
<br>
to mean evolutionary forces) Nature is more clever than you.  Evolutionary 
<br>
biology has great explanatory value when it comes to accounting for the 
<br>
current state of affairs, but it's predictive capacity (while undeniably 
<br>
existant) is far more limited in scope.  This is certainly true when dealing 
<br>
with hyperintelligent entities who will have much more direct control over 
<br>
their upgrade paths than any other creature in the history of biology has 
<br>
ever had (including ourselves).  As such, I expect them to follow rational 
<br>
rules of development (which is NOT to be confused with them being rational 
<br>
beings -- as noted, I think that this is an unfounded assumption), but that 
<br>
the predictive scope of those rules (or our capacity to derive all the 
<br>
relevant rules) may be limited.  Which isn't to say that we shouldn't try.  
<br>
Indeed, I think that making a concerted effort to develop predictive models 
<br>
is the most responsible thing that we can do with respect to the whole 
<br>
thing.
<br>
<p><em>&gt;  &gt; hold in an arena that harbors such beings it to *be* such a being (and 
</em><br>
<em>&gt;even
</em><br>
<em>&gt;  &gt; that might be presumptive -- we are assuming that amplified 
</em><br>
<em>&gt;intelligence is
</em><br>
<em>&gt;  &gt; a good thing to have; it's possible that hyperintelligences are prone 
</em><br>
<em>&gt;to
</em><br>
<em>&gt;  &gt; fatal or adverse mental states that only manifest beyond a certain 
</em><br>
<em>&gt;level of
</em><br>
<em>&gt;  &gt; complexity; after all, we do know that certain pathological mental 
</em><br>
<em>&gt;states,
</em><br>
<em>&gt;  &gt; such as a desire for self-martrydom, only show up at human levels of
</em><br>
<em>&gt;  &gt; intelligence).
</em><br>
<em>&gt;
</em><br>
<em>&gt;Yeah, but we have to decide today, in face of limited data. We have to
</em><br>
<em>&gt;work with current models, however faulty. The longer we wait, the
</em><br>
<em>&gt;harder will it be to change the course.
</em><br>
<p>Absolutely!  I am not advocating that we should throw our hands up in the 
<br>
air our of a fatal sense of frustration.  However, in order to make a good 
<br>
start, we really do need to examine our assumptions as closely as possible.  
<br>
In my thoroughly unhumble opinion, too many in this group as still working 
<br>
from positions based on unfounded assumptions.  I realize that dynamic 
<br>
optimism goes hand in hand with Extropianism (which is one reason I hesitate 
<br>
to call myself a party Extropian), but there's a point where optimism 
<br>
becomes Polyannistic.  To use a mythological metaphore, Epimetheus was 
<br>
certainly more optimistic than Prometheus, but Prometheus was more often 
<br>
right.  When it comes to foresight, an overly optimistic view can blind us.
<br>
<p><em>&gt;  &gt; Frankly, the notion that we are approaching a Singularity scares the 
</em><br>
<em>&gt;hell
</em><br>
<em>&gt;  &gt; out of me.  If I thought that there were some viable way to prevent it 
</em><br>
<em>&gt;or
</em><br>
<em>&gt;  &gt; even to safely delay it, I'd probably lobby to do so.  I'm not 
</em><br>
<em>&gt;convinced
</em><br>
<em>&gt;  &gt; that there are any such options.  As such, my personal goal is to be an
</em><br>
<em>&gt;  &gt; early adopter and hope that self-amplification isn't the mental 
</em><br>
<em>&gt;equivilent
</em><br>
<em>&gt;  &gt; of jumping off of a cliff.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Sure, but would be the alternative?
</em><br>
<p>Well, that's my point!  There isn't one.  At least not one that I can see.  
<br>
As such, I'd rather deliberately move towards the Singularity, however 
<br>
cautiously, rather than stumbling into it blindly.  I'm certainly not 
<br>
advocating an osterich attitude.  It's coming.  It has the potential to be a 
<br>
very, very, very bad thing.  It also has the potential to be a very, very, 
<br>
very good thing.  We must do what we can to influence events such that they 
<br>
favor the latter case and not the former.  Among the things we need to 
<br>
consider is whether or not it is possible, or even advisable, to allow 
<br>
(non-upload) AIs to precede us and, if they precede us, whether its 
<br>
possible, or even advisable, to try to place controls on them.  If not, what 
<br>
can we do to predispose to them treat the rest of us well.  Similar 
<br>
questions arise over the question of intelligence amplification and uploads. 
<br>
&nbsp;&nbsp;&nbsp;Nor am I suggesting that you folks have been lax -- I've subscribed 
<br>
because there are important and interesting points being debated.  My major 
<br>
concern is that a lot of the current discussion does seem to hinge on some 
<br>
basic assumptions that seem a bit dubious to me.  One in particular that 
<br>
sticks out is that a lot of folks around here seem to equate increased 
<br>
intelligence with increased rationality.  At best, I think that it can be 
<br>
argued that more intelligence gives one the potential to be more rational, 
<br>
but I think that it can be argued, just as reasonably, that more 
<br>
intelligence also grants the capacity for greater irrationality.  As such, 
<br>
simply trusting an SI with the keys to the future, under the presumption 
<br>
that it will do take the most rational course of action, seems to be a naive 
<br>
presumption from where I'm standing.
<br>
<p><em>&gt;(Assuming, it happens at all) we
</em><br>
<em>&gt;can't prevent it, we can (at best) delay it. My statistical (assuming
</em><br>
<em>&gt;current numbers) life expenctancy will be exceeded in about 60
</em><br>
<em>&gt;years. If the Singularity turns out malignant, I can only die once.
</em><br>
<p>Unless you get a really malevolent entity out of the process.  The converse 
<br>
of a technological Heaven is a technological Hell.
<br>
<p><em>&gt;It
</em><br>
<em>&gt;can be a bit premature, it can be quite nasty, but it will be brief.
</em><br>
<em>&gt;If it's softer, it gives me a genuine chance to achieve immortality
</em><br>
<em>&gt;the Woody Allen way.
</em><br>
<p>I'm with you, there.  I do *hope* that the optimistic projections will 
<br>
obtain.  When people ask me what my lifes ambition is, I am tempted to say 
<br>
that I want to become a god.  I don't because I know how such a statement 
<br>
would shock most people -- it never the less is what I am hoping will be the 
<br>
case.
<br>
<p><em>&gt;Dunno, sounds good to me.
</em><br>
<p>All I can say, for certain, about the Singularity is that it is looming 
<br>
before us.  I don't think that any of us, as of yet, have a clear idea of 
<br>
what lies beyond it.  Doing what we can to develop some plausible idea of 
<br>
how it can fall out (plausible meaning, here, well-modeled) and how we can 
<br>
effect the fall out must be, I think, our first duty.
<br>
<p>We have the unenviable task of being children who must decide how we become 
<br>
adults (lest the decision is wrenched from our hands by circumstance).  It 
<br>
is an exciting thing, but we can not afford to underestimate the dangers 
<br>
involved.
<br>
<p><p><p>_________________________________________________________________________
<br>
Get Your Private, Free E-mail from MSN Hotmail at <a href="http://www.hotmail.com">http://www.hotmail.com</a>.
<br>
<p>Share information about yourself, create your own public profile at 
<br>
<a href="http://profiles.msn.com">http://profiles.msn.com</a>.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0387.html">Bryan Moss: "Re: Capitalists and coercion"</a>
<li><strong>Previous message:</strong> <a href="0385.html">Eugene Leitl: "Re: Intelligence increase"</a>
<li><strong>Maybe in reply to:</strong> <a href="0060.html">J. R. Molloy: "Eliezer S. Yudkowsky' whopper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0423.html">Spudboy100@aol.com: "Re: Eliezer S. Yudkowsky' whopper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#386">[ date ]</a>
<a href="index.html#386">[ thread ]</a>
<a href="subject.html#386">[ subject ]</a>
<a href="author.html#386">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:15 MDT</em>
</em>
</small>
</body>
</html>
