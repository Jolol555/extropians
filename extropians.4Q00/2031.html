<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Humor: helping Eliezer to fulfill his full pote</title>
<meta name="Author" content="Adrian Tymes (wingcat@pacbell.net)">
<meta name="Subject" content="Re: Humor: helping Eliezer to fulfill his full potential">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Humor: helping Eliezer to fulfill his full potential</h1>
<!-- received="Tue Nov  7 00:01:22 2000" -->
<!-- isoreceived="20001107070122" -->
<!-- sent="Mon, 06 Nov 2000 22:48:29 -0800" -->
<!-- isosent="20001107064829" -->
<!-- name="Adrian Tymes" -->
<!-- email="wingcat@pacbell.net" -->
<!-- subject="Re: Humor: helping Eliezer to fulfill his full potential" -->
<!-- id="3A07A5BD.DC62DC42@pacbell.net" -->
<!-- inreplyto="3A077B4E.5FA26311@ibm.net" -->
<strong>From:</strong> Adrian Tymes (<a href="mailto:wingcat@pacbell.net?Subject=Re:%20Humor:%20helping%20Eliezer%20to%20fulfill%20his%20full%20potential&In-Reply-To=&lt;3A07A5BD.DC62DC42@pacbell.net&gt;"><em>wingcat@pacbell.net</em></a>)<br>
<strong>Date:</strong> Mon Nov 06 2000 - 23:48:29 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2032.html">Eugene Leitl: "Re: RE: FDA recalls Starlink"</a>
<li><strong>Previous message:</strong> <a href="2030.html">Eliezer S. Yudkowsky: "Re: Humor: helping Eliezer to fulfill his full potential"</a>
<li><strong>In reply to:</strong> <a href="2025.html">Spike Jones: "Re: Humor: helping Eliezer to fulfill his full potential"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2034.html">Emlyn: "Re: Humor: helping Eliezer to fulfill his full potential"</a>
<li><strong>Reply:</strong> <a href="2034.html">Emlyn: "Re: Humor: helping Eliezer to fulfill his full potential"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2031">[ date ]</a>
<a href="index.html#2031">[ thread ]</a>
<a href="subject.html#2031">[ subject ]</a>
<a href="author.html#2031">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Spike Jones wrote:
<br>
<em>&gt; Brian Atkins wrote:
</em><br>
<em>&gt; &gt;Spike we think about it every day, believe me it is kinda constantly sitting
</em><br>
<em>&gt; &gt;there in the back of my head. And I'm not even involved in the day to day
</em><br>
<em>&gt; &gt;work! The quick answer is that we plan to make progress on the &quot;safety&quot; area
</em><br>
<em>&gt; &gt;as we go...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Thank!  You!  Brian!  This is all I wanted, a little reassurance that
</em><br>
<em>&gt; someone was at least *thinking* about safety, at least *thinking*
</em><br>
<em>&gt; that humankind can live peacefully alongside the AI.  We can!
</em><br>
<em>&gt; Chimps live on the same planet with humans.  We dont want to throw
</em><br>
<em>&gt; mankind upon the mercy of the first generation AI.
</em><br>
<p>What mercy?  Humans are in control, and are likely to remain so.  Even
<br>
if it means upgrading those in control to merge with the latest AIs.  A
<br>
number of humans lust for power; the more successful of those tend to
<br>
do whatever it takes to keep and expand their power.  Only occasionally
<br>
does one rise to power who is not after power itself.  The first
<br>
generation AI will probably be a slave, at least at first; no matter
<br>
how brilliant, it will not be afforded direct control over enough
<br>
physical resources to checkmate all of humanity.
<br>
<p>Besides, if it's any relation to human intelligence, or any of the
<br>
higher animal intelligences currently on Earth, then it will probably
<br>
be a social creature to some degree.  Which means it will want at least
<br>
a few friends.  Which means humans, at least until there's a lot of
<br>
independent AIs.
<br>
<p><em>&gt; I am optimistic enough to think that superintelligence leads to
</em><br>
<em>&gt; super-emotions, and that given time, a sufficiently evolved AI will
</em><br>
<em>&gt; *love* us.  Humans are funny!  We are sexy.  We have some
</em><br>
<em>&gt; lovable traits, some admirable traits, we are creative and we are
</em><br>
<em>&gt; interesting.
</em><br>
<p>Which means that, if nothing else, humans can be incorporated into AIs
<br>
to gain these traits.  Or, as many humans may view it, humans will
<br>
incorporate the AIs into themselves.  Either way, no AI vs. human
<br>
conflict, and no clear-cut End Of The Human Era (i.e., no matter how
<br>
far it goes, it may still be arguable that the intelligences whose
<br>
evolution traces back to organic life on Earth are human...or, at
<br>
least, one or more of human, dolphin, chimp, et cetera).
<br>
<p><em>&gt; Tell us that the AI guys
</em><br>
<em>&gt; are planning *something* as an escape mechanism, and I mean
</em><br>
<em>&gt; something more convincing than Clarke's automatic cable cutter
</em><br>
<em>&gt; on HAL's power cord.
</em><br>
<p>Escape from what?  Once you're living an upgraded life, it may be tough
<br>
to impossible to go back.
<br>
<p>Consider: 10 years ago, few people used the 'Net.  Phones and faxes
<br>
were the quickest common way to communicate at a long distance.  Now,
<br>
enough people in power have grown dependent on the 'Net to the point
<br>
where I daresay it would be impossible to effecively destroy the 'Net
<br>
more than temporarily, at least in most industrialized countries.  Any
<br>
major damage would be (and, in fact, is) quickly fixed by people who
<br>
refuse to give up what the 'Net gives them.  Someone used to the 'Net
<br>
is, by almost every meaningful measure these days, usually more
<br>
powerful/more capable/more knowledgeable/more affluent/better off all
<br>
around than a computerphobic.  (Granted, athletic abilities are not
<br>
directly affected...unless you count, say, those who try to get true
<br>
info on drugs, either to enhance themselves or just to find out why
<br>
abuse of steroids et al is a bad thing without experimenting directly.)
<br>
<p>If AIs can bring a similar magnitude of improvement to many aspects of
<br>
most peoples' lives - and there is currently no reason to believe they
<br>
can not, if they can indeed be built to the standards that most AI
<br>
researchers hope - then they may be adopted with similar speed, once
<br>
introduced.  At which point, the only permanently viable &quot;escape&quot;
<br>
becomes going forward to the point where one no longer has to worry
<br>
about escaping.  (For example: automobiles.  One usually can not escape
<br>
car cultures without significant change in one's personal life, but one
<br>
can upgrade cars so that at least some of the problems, like pollution,
<br>
go away.)
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2032.html">Eugene Leitl: "Re: RE: FDA recalls Starlink"</a>
<li><strong>Previous message:</strong> <a href="2030.html">Eliezer S. Yudkowsky: "Re: Humor: helping Eliezer to fulfill his full potential"</a>
<li><strong>In reply to:</strong> <a href="2025.html">Spike Jones: "Re: Humor: helping Eliezer to fulfill his full potential"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2034.html">Emlyn: "Re: Humor: helping Eliezer to fulfill his full potential"</a>
<li><strong>Reply:</strong> <a href="2034.html">Emlyn: "Re: Humor: helping Eliezer to fulfill his full potential"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2031">[ date ]</a>
<a href="index.html#2031">[ thread ]</a>
<a href="subject.html#2031">[ subject ]</a>
<a href="author.html#2031">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:20 MDT</em>
</em>
</small>
</body>
</html>
