<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Why would AI want to be friendly?</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Why would AI want to be friendly?">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Why would AI want to be friendly?</h1>
<!-- received="Mon Oct  2 12:18:32 2000" -->
<!-- isoreceived="20001002181832" -->
<!-- sent="Mon, 02 Oct 2000 11:20:25 -0700" -->
<!-- isosent="20001002182025" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="39D8D1E9.3D7C32B6@objectent.com" -->
<!-- inreplyto="14808.26774.871582.993363@lrz.uni-muenchen.de" -->
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;39D8D1E9.3D7C32B6@objectent.com&gt;"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Mon Oct 02 2000 - 12:20:25 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0104.html">CountZero: "Re: MacLeod's Cassini Division"</a>
<li><strong>Previous message:</strong> <a href="0102.html">J. R. Molloy: "Re: Violence in schools (Was Re: Back off!  Im gay!)"</a>
<li><strong>In reply to:</strong> <a href="0078.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0130.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#103">[ date ]</a>
<a href="index.html#103">[ thread ]</a>
<a href="subject.html#103">[ subject ]</a>
<a href="author.html#103">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Eugene Leitl wrote:
<br>
<em>&gt; 
</em><br>
<p><em>&gt;  &gt; Some questions I have include:
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; 1. What difference would cooperative coevolution make with respect to the
</em><br>
<em>&gt;  &gt; relationship between humans and highly evolved AI?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The AI can't derive any benefits from a transaction with the human
</em><br>
<em>&gt; player(s). Apart from eating you, of course, since it has good use for
</em><br>
<em>&gt; all the atoms in your body.
</em><br>
<p><p>If this is truly the case then every human being should work their butt
<br>
off to insure that such a creation is never built.  Hopefully we can
<br>
come up with better scenarios that allow us to have our SI and not be
<br>
eaten too.  But if the above is the end of the discussion then what sort
<br>
of person would actually try to build this?  Wouldn't such a person be
<br>
doing technology as a sacred Holy Grail even if it would for certain
<br>
destroy all humanity?  Sorry, but this vision certainly won't get me out
<br>
of bed every morning.
<br>
<p>We are assuming a perfect AI with not only switching capacity 10^6
<br>
better than us but an equal or greater degree of parellism (or something
<br>
as good or better) happens pretty much overnight and it thus has zero
<br>
use for humans (as if &quot;use for&quot; is the end and be-all all interactions
<br>
anyway).  We assume in our technological fever dream that it will almost
<br>
at once think oh so much faster and better than us little meatballs that
<br>
it will see us as less than nothing and dispose of us as a lite snack. 
<br>
We assume ethics is bunk and only survival and growth will drive this
<br>
super-intelligence.  And we will largely cheer it on and help build it. 
<br>
Then we wonder why there are technophobes in the world.
<br>
If I believed this is the best we can possibly come up with then I would
<br>
be a technophobe.  
<br>
<p><em>&gt; 
</em><br>
<p><em>&gt; 
</em><br>
<em>&gt; In comparison to mechanosynthesis-based life we're extremely
</em><br>
<em>&gt; time-space-energy inefficient. Our relative fitness is so low we don't
</em><br>
<em>&gt; have the chance of a snowball in hell.
</em><br>
<p>Sure.  If you assume Darwinian evolution and a rather single-minded
<br>
survival of the fittest model is the end and be-all of what intelligent
<br>
and super-intelligent beings can come up with.  I am not willing to
<br>
believe that as of yet.
<br>
<p><p>- samantha
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0104.html">CountZero: "Re: MacLeod's Cassini Division"</a>
<li><strong>Previous message:</strong> <a href="0102.html">J. R. Molloy: "Re: Violence in schools (Was Re: Back off!  Im gay!)"</a>
<li><strong>In reply to:</strong> <a href="0078.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0130.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#103">[ date ]</a>
<a href="index.html#103">[ thread ]</a>
<a href="subject.html#103">[ subject ]</a>
<a href="author.html#103">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:14 MDT</em>
</em>
</small>
</body>
</html>
