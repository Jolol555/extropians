<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Why would AI want to be friendly?</title>
<meta name="Author" content="CYMM (cymm@trinidad.net)">
<meta name="Subject" content="Re: Why would AI want to be friendly?">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Why would AI want to be friendly?</h1>
<!-- received="Sun Oct  1 19:31:14 2000" -->
<!-- isoreceived="20001002013114" -->
<!-- sent="Sun, 1 Oct 2000 21:27:47 -0400" -->
<!-- isosent="20001002012747" -->
<!-- name="CYMM" -->
<!-- email="cymm@trinidad.net" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="002201c02c0f$fa5240c0$553bd1d0@mismanag" -->
<!-- inreplyto="Why would AI want to be friendly?" -->
<strong>From:</strong> CYMM (<a href="mailto:cymm@trinidad.net?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;002201c02c0f$fa5240c0$553bd1d0@mismanag&gt;"><em>cymm@trinidad.net</em></a>)<br>
<strong>Date:</strong> Sun Oct 01 2000 - 19:27:47 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0044.html">Spike Jones: "Re: Eugene's nuclear threat"</a>
<li><strong>Previous message:</strong> <a href="0042.html">Michael M. Butler: "&lt;HUMOR?&gt; Rumors of war..."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0066.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<li><strong>Reply:</strong> <a href="0066.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#43">[ date ]</a>
<a href="index.html#43">[ thread ]</a>
<a href="subject.html#43">[ subject ]</a>
<a href="author.html#43">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
BARBARA LAMAR SAID: &quot;.... What difference would cooperative coevolution make
<br>
with respect to the relationship between humans and highly evolved AI?...&quot;
<br>
<p>CYMM SAYS: Firstly, when we say cooperative in respect of evolution - it's
<br>
only in retrospect. It's not like in a game theoretic scenario where intent
<br>
and perception of intent occurs in addition to resource considerations.
<br>
<p>The kind of cooperation that would interest humans would involve political &amp;
<br>
game theory analysis of the situation - and won't be darwinian.... which is
<br>
a very simple form of adaptive computation.
<br>
<p>Secondly, as I said before, if you're talking real biological (...or
<br>
neobiological...) symbiosis - humans and the AI are not running on the same
<br>
clocks. Humans and bacteria or virii are. The clock here is based on DNA.
<br>
Irrespective of the difference generation times of humans and bacteria there
<br>
is a degree of temporal coherence between the two species' genetic
<br>
adaptation to the environment - there is enough coupling to facilitate
<br>
organic coevolution.
<br>
<p>But suppose the machine evolves ten billion times faster - suppose the
<br>
mutation rate is ten billion times that of a human - the boundary conditions
<br>
might preclude the darwinian selection that allows for concomitant coupling
<br>
of the two species' evolution.
<br>
<p>Look at a scenario that has been done in a 'sixties (...i think) SF story.
<br>
<p>You make the supermachine. You plug it in. The supermachine disappears
<br>
within milliseconds of being activated. God alone knows where or what it has
<br>
become (...really!).
<br>
<p>What could have happened is that the supermachine discovered a more general
<br>
quantum mechanics where the Born criterion doesn't exactly hold -
<br>
conservation of mass (...and strict causality..) is violated; the machine
<br>
reaches back to the quantum fluctuations at the beginning of time - and
<br>
subtly remakes the physical universe.
<br>
<p>Look at another scenario... if the machine/universe system that has evolved
<br>
is sufficiently consistent with the human/universe system; then we could
<br>
transform from one to the other in a sort of expanded Principle of
<br>
Relativity.
<br>
<p>If the transform is close to affine - I'm sure that we could recognize the
<br>
machine as some sort of object.
<br>
<p>If the transform is wierd - not particularly well-behaved... then the
<br>
physical manifestations of such an object might not easily be perceived by
<br>
humans... in the sense that we may not be able to identify sufficent machine
<br>
&quot;features&quot; in order to mentally consolidate into an object in our own human
<br>
minds. The implicit &amp; explicit semiotics of our perception and our cognition
<br>
might not allow us to perceive such a &quot;highly evolved&quot; object.
<br>
<p>In fact there may be tons of such entities all around us... in fact we may
<br>
find little evidence of &quot;intelligence&quot; in the universe because WE ARE NOT
<br>
INTELLIGENT!
<br>
<p>Lastly - in organic evolution, species only coevolve if they interact with
<br>
respect to common environmental resources. If Eliezer's machines evolve very
<br>
rapidly then it might be likely that in a short time - (...say...)
<br>
milliseconds to half a decade - they would not compete substantially for
<br>
resources with the human species.
<br>
<p>This will be an adaptive radiation scenario...it is within the realms of
<br>
probability.. because the machines might discover a new physical environment
<br>
into which they could radiate. At the very best we won't notice anything.
<br>
The manifestations of our hyperAI in our perceived physical world would be
<br>
benign - and human civilization would proceed along the &quot;star trek&quot;  line
<br>
(ie, linearly and conservatively...).
<br>
<p>A little worse scenario is that we might start to notice subtle but damning
<br>
changes in our physical laws - and progress in physics and engineering would
<br>
seem to be directed away from producing more AIs. Guess why?
<br>
<p>A little worse again,  is that we'd have a very brief and extremely
<br>
destructive (...to us...) &quot;Terminator&quot; sort of conflict - which would last
<br>
only long enough for the machines to access resources that we can't - say
<br>
mass-energy conservation violation or similar wierdness.
<br>
<p>Any way you cut it... the future is not good for the all-consuming human
<br>
ego... but this is predictable... primates tend to have big egos - and they
<br>
also tend to be contriol freaks. You walk around with a handful of big
<br>
balloons and someone will come along to burst 'em.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0044.html">Spike Jones: "Re: Eugene's nuclear threat"</a>
<li><strong>Previous message:</strong> <a href="0042.html">Michael M. Butler: "&lt;HUMOR?&gt; Rumors of war..."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0066.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<li><strong>Reply:</strong> <a href="0066.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#43">[ date ]</a>
<a href="index.html#43">[ thread ]</a>
<a href="subject.html#43">[ subject ]</a>
<a href="author.html#43">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:14 MDT</em>
</em>
</small>
</body>
</html>
