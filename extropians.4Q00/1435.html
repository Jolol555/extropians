<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Human Fallibility and Dangerous Technologies</title>
<meta name="Author" content="James J. Hughes (jhughes@changesurfer.com)">
<meta name="Subject" content="Human Fallibility and Dangerous Technologies">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Human Fallibility and Dangerous Technologies</h1>
<!-- received="Sun Oct 22 12:55:44 2000" -->
<!-- isoreceived="20001022185544" -->
<!-- sent="Sun, 22 Oct 2000 14:25:04 -0400" -->
<!-- isosent="20001022182504" -->
<!-- name="James J. Hughes" -->
<!-- email="jhughes@changesurfer.com" -->
<!-- subject="Human Fallibility and Dangerous Technologies" -->
<!-- id="000d01c03c5a$186cca80$4445b6d1@oemcomputer" -->
<strong>From:</strong> James J. Hughes (<a href="mailto:jhughes@changesurfer.com?Subject=Re:%20Human%20Fallibility%20and%20Dangerous%20Technologies&In-Reply-To=&lt;000d01c03c5a$186cca80$4445b6d1@oemcomputer&gt;"><em>jhughes@changesurfer.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 22 2000 - 12:25:04 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1436.html">James J. Hughes: "RE: extropians-digest V5 #291"</a>
<li><strong>Previous message:</strong> <a href="1434.html">Eliezer S. Yudkowsky: "Re: META: number of postings"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1435">[ date ]</a>
<a href="index.html#1435">[ thread ]</a>
<a href="subject.html#1435">[ subject ]</a>
<a href="author.html#1435">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
S U M M A R Y
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lethal Arrogance:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Human Fallibility and Dangerous Technologies
<br>
&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lloyd J. Dumas, Professor of Political Economy
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;University of Texas (Dallas)
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(New York: St. Martin's Press, 1999)
<br>
<p><p>&nbsp;&nbsp;Lethal Arrogance: Human Fallibility and Dangerous Technologies is a 
<br>
warning about technological hubris, about the unthinking assumption 
<br>
that we can always control the technologies we create --- no matter 
<br>
how powerful, no matter how dangerous --- and permanently avoid 
<br>
disaster. We are humans, not gods. We may be extraordinarily capable, 
<br>
but we are not perfect. The point of the warning is to focus 
<br>
attention on the need to take the fact of our fallibility much more 
<br>
seriously than we currently do in making social and technological 
<br>
choices.
<br>
<p>&nbsp;&nbsp;Lethal Arrogance is not a back-to-nature, anti-technology tract. It 
<br>
is both a wake-up call and a guide to making wiser decisions about 
<br>
the technological directions we follow, decisions that profoundly 
<br>
effect the shape of the world in which we live.
<br>
<p>&nbsp;&nbsp;The book is written simply enough to be accessible to the educated 
<br>
layperson, while at the same time accurately enough to be credible to 
<br>
technical experts. It approaches the problem of human and technical 
<br>
fallibility from many different directions, combining the insights of 
<br>
various fields of science and engineering with those of psychology, 
<br>
sociology, social psychology, medicine, mathematics, architecture, 
<br>
political science and security studies.
<br>
<p>&nbsp;&nbsp;The argument is built carefully, step-by-step. It is supported by 
<br>
reference to studies, survey and other data, and the analyses and 
<br>
experience of experts, as well as many illustrative reports drawn 
<br>
from newspapers and magazines, and long forgotten archives. All of 
<br>
this is woven into a pattern that reveals the critical importance of 
<br>
abandoning those technologies that are so dangerous that fallible 
<br>
human beings cannot operate them indefinitely without triggering 
<br>
disaster. In the end, Lethal Arrogance is a clear and strong argument 
<br>
for abolishing nuclear, chemical and biological weapons of mass 
<br>
destruction, as well as phasing out nuclear power and highly toxic 
<br>
chemical technologies.
<br>
<p>&nbsp;&nbsp;After a brief prologue relating actual 20th century disasters to 
<br>
the potential for 21st century technological nightmares, the first 
<br>
chapter examines the positive and negative effects of technology on 
<br>
human wellbeing, defines &quot;dangerous technologies&quot; and shows that they 
<br>
have spread around the globe.
<br>
<p>&nbsp;&nbsp;The next four chapters deal with the implications of human and 
<br>
technical failure in dangerous technological systems, including those 
<br>
involving weapons of mass destruction. The first chapter in this 
<br>
section (Chapter 2) focuses on the actual and potential threat of 
<br>
nuclear, chemical and biological terrorism (as well as terrorism 
<br>
involving other dangerous technologies). Chapter 3 looks at accidents 
<br>
with nuclear, chemical and biological weapons as well as other 
<br>
dangerous technologies that do not involve weapons. Chapter 4 deals 
<br>
with the problem of safeguarding inventories of dangerous technology 
<br>
products, the materials used to make them and hazardous waste 
<br>
generated during their manufacture. Then, in this section's final 
<br>
chapter, the possibility that human and technical failure might 
<br>
someday trigger a full scale accidental nuclear war is considered.
<br>
<p>&nbsp;&nbsp;Chapters 6-8 considers the many dimensions of human fallibility. 
<br>
First drug abuse, alcoholism and mental illness, then stress, 
<br>
isolation and boredom in work and life are connected to the problem 
<br>
of human error in dangerous technological systems. Finally, the 
<br>
relationship of bureaucracy, &quot;groupthink&quot; and even group psychosis 
<br>
(as in cult behavior) to human-induced technological disaster is 
<br>
explored.
<br>
<p>&nbsp;&nbsp;Chapters 9 and 10 focus on the failure of technical systems in 
<br>
general and computers as a particularly important special case. The 
<br>
meaning of technological risk and the problematic nature of risk 
<br>
analysis are discussed in Chapter 11.
<br>
<p>&nbsp;&nbsp;Chapter 12, the last chapter, considers what can and must be done 
<br>
if we are to permanently avoid disaster.
<br>
<p><p>-----------------------
<br>
J. Hughes                          &quot;On Saturday, my teachers, me, and all 
<br>
Changesurfer Radio            my friends went to Never Never Land. 
<br>
www.changesurfer.com               It was a short trip.&quot;
<br>
<a href="mailto:jhughes@changesurfer.com?Subject=Re:%20Human%20Fallibility%20and%20Dangerous%20Technologies&In-Reply-To=&lt;000d01c03c5a$186cca80$4445b6d1@oemcomputer&gt;">jhughes@changesurfer.com</a>        Tristan Bock-Hughes, 3
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1436.html">James J. Hughes: "RE: extropians-digest V5 #291"</a>
<li><strong>Previous message:</strong> <a href="1434.html">Eliezer S. Yudkowsky: "Re: META: number of postings"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1435">[ date ]</a>
<a href="index.html#1435">[ thread ]</a>
<a href="subject.html#1435">[ subject ]</a>
<a href="author.html#1435">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:18 MDT</em>
</em>
</small>
</body>
</html>
