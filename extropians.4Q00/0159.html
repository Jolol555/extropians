<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Let's hear Eugene's ideas</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: Let's hear Eugene's ideas">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Let's hear Eugene's ideas</h1>
<!-- received="Mon Oct  2 23:31:00 2000" -->
<!-- isoreceived="20001003053100" -->
<!-- sent="Tue, 03 Oct 2000 01:32:11 -0400" -->
<!-- isosent="20001003053211" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: Let's hear Eugene's ideas" -->
<!-- id="39D96F5B.FF5743BB@posthuman.com" -->
<!-- inreplyto="200010030145.SAA04498@finney.org" -->
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20Let's%20hear%20Eugene's%20ideas&In-Reply-To=&lt;39D96F5B.FF5743BB@posthuman.com&gt;"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Mon Oct 02 2000 - 23:32:11 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0160.html">hal@finney.org: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Previous message:</strong> <a href="0158.html">J. R. Molloy: "Re: Nostaliga? (based on Back off! I'm gay!)"</a>
<li><strong>In reply to:</strong> <a href="0133.html">hal@finney.org: "Re: Let's hear Eugene's ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0170.html">Eugene Leitl: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Reply:</strong> <a href="0170.html">Eugene Leitl: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Reply:</strong> <a href="0371.html">Samantha Atkins: "Re: Let's hear Eugene's ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#159">[ date ]</a>
<a href="index.html#159">[ thread ]</a>
<a href="subject.html#159">[ subject ]</a>
<a href="author.html#159">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
<a href="mailto:hal@finney.org?Subject=Re:%20Let's%20hear%20Eugene's%20ideas&In-Reply-To=&lt;39D96F5B.FF5743BB@posthuman.com&gt;">hal@finney.org</a> wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Brian Atkins writes:
</em><br>
<em>&gt; &gt; Well as I said to Eugene- look around at the reality of the next 20 years
</em><br>
<em>&gt; &gt; (max). There are likely to be no Turing Police tracking down and containing
</em><br>
<em>&gt; &gt; all these AIs that all the hackers and scientists out there will dream up.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; That's not clear.  First, it could easily take longer than 20 years to
</em><br>
<em>&gt; get superhuman AI, for several reasons:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;  - We may not have nanotech in 20 years
</em><br>
<em>&gt;  - We may hit Moore's Wall before then as computer speeds turn out to be
</em><br>
<em>&gt;    on an S curve just like every other technology before them
</em><br>
<em>&gt;  - Software may continue to improve as it has in the past (i.e. not very
</em><br>
<em>&gt;    fast)
</em><br>
<em>&gt;  - AI researchers have a track record of over-optimism
</em><br>
<p>Perhaps so, in which case Eugene has nothing to worry about. These things
<br>
above are not what we want to discuss on this thread. We want to hear what
<br>
you propose to do to have a good outcome in a world where the above things
<br>
turn out to be wrong.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Secondly, I suspect that in this time frame we are going to see
</em><br>
<em>&gt; increased awareness of the dangers of future technology, with Joy's
</em><br>
<em>&gt; trumpet blast just the beginning.  Joy included &quot;robotics&quot; in his troika
</em><br>
<em>&gt; of technological terrors (I guess calling it &quot;AI&quot; wouldn't have let him
</em><br>
<em>&gt; keep to the magic three letters).  If we do see an Index of of Forbidden
</em><br>
<em>&gt; Technology, it is entirely possible that AI research will be included.
</em><br>
<p>Don't you think this better happen soon, otherwise the governments will
<br>
end up trying to regulate something that already is in widespread use? There
<br>
are going to be &quot;intelligent&quot; programs out there soon- in fact you can already
<br>
see commercials in the last year touting &quot;intelligent&quot; software packages.
<br>
Do you really think it is likely our government would outlaw this area of
<br>
software development once it becomes a huge market? Extremely unlikely...
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Third, realistically the AI scenario will take time to unfold.  As I
</em><br>
<em>&gt; have argued repeatedly, self-improvement can't really take off until
</em><br>
<em>&gt; we can build super-human intelligence on our own (because IQ 100 is
</em><br>
<em>&gt; self-evidently not smart enough to figure out how to do AI, or else
</em><br>
<em>&gt; we'd have had it years ago).  So the climb to human equivalence will
</em><br>
<em>&gt; continue to be slow and frustrating.  Progress will be incremental,
</em><br>
<em>&gt; with a gradual expansion of capability.
</em><br>
<p>Up to a point...
<br>
<p><em>&gt; 
</em><br>
<em>&gt; I see the improved AI being put to work immediately because of the many
</em><br>
<em>&gt; commercial opportunities, so the public will generally be well aware of
</em><br>
<em>&gt; the state of the art.  The many difficult ethical and practical dilemmas
</em><br>
<p>Public.. well aware of state of the art... bwhahahaha. No I don't think
<br>
so. At any rate, I guess you are talking about a different scenario here;
<br>
one without Turing Police preventing development?
<br>
<p><em>&gt; that appear when you have intelligent machines will become part of the
</em><br>
<em>&gt; public dialogue long before any super-human AI could appear on the scene.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Therefore I don't think that super-intelligent AI will catch society by
</em><br>
<em>&gt; surprise, but will appear in a social milieu which is well aware of the
</em><br>
<em>&gt; possibility, the potential, and the peril.  If society is more concerned
</em><br>
<em>&gt; about the dangers than the opportunities, then we might well see Turing
</em><br>
<em>&gt; Police enforcing restrictions on AI research.
</em><br>
<p>Well I'd love to see how that would work. On one hand you want to allow
<br>
some research in order to get improved smart software packages, but on
<br>
the other hand you want to prevent the &quot;bad&quot; software development that
<br>
might lead to a real general intelligence? Is the government going to sit
<br>
and watch every line of code that every hacker on the planet types in?
<br>
In an era of super-strong encryption and electronic privacy (we hope)?
<br>
<pre>
-- 
Brian Atkins
Director, Singularity Institute for Artificial Intelligence
<a href="http://www.singinst.org/">http://www.singinst.org/</a>
</pre>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0160.html">hal@finney.org: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Previous message:</strong> <a href="0158.html">J. R. Molloy: "Re: Nostaliga? (based on Back off! I'm gay!)"</a>
<li><strong>In reply to:</strong> <a href="0133.html">hal@finney.org: "Re: Let's hear Eugene's ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0170.html">Eugene Leitl: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Reply:</strong> <a href="0170.html">Eugene Leitl: "Re: Let's hear Eugene's ideas"</a>
<li><strong>Reply:</strong> <a href="0371.html">Samantha Atkins: "Re: Let's hear Eugene's ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#159">[ date ]</a>
<a href="index.html#159">[ thread ]</a>
<a href="subject.html#159">[ subject ]</a>
<a href="author.html#159">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:14 MDT</em>
</em>
</small>
</body>
</html>
