<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Sentience</title>
<meta name="Author" content="Dan Fabulich (daniel.fabulich@yale.edu)">
<meta name="Subject" content="Re: Sentience">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Sentience</h1>
<!-- received="Mon Dec 18 18:31:33 2000" -->
<!-- isoreceived="20001219013133" -->
<!-- sent="Mon, 18 Dec 2000 20:30:28 -0500 (EST)" -->
<!-- isosent="20001219013028" -->
<!-- name="Dan Fabulich" -->
<!-- email="daniel.fabulich@yale.edu" -->
<!-- subject="Re: Sentience" -->
<!-- id="Pine.GSO.4.10.10012181925370.12562-100000@morpheus.cis.yale.edu" -->
<!-- inreplyto="002701c067bf$8ab97bc0$efb1403e@i6x7m6" -->
<strong>From:</strong> Dan Fabulich (<a href="mailto:daniel.fabulich@yale.edu?Subject=Re:%20Sentience&In-Reply-To=&lt;Pine.GSO.4.10.10012181925370.12562-100000@morpheus.cis.yale.edu&gt;"><em>daniel.fabulich@yale.edu</em></a>)<br>
<strong>Date:</strong> Mon Dec 18 2000 - 18:30:28 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4262.html">Michael M. Butler: "Re: HUMOR Re: (fwd) Signs of the Times"</a>
<li><strong>Previous message:</strong> <a href="4260.html">Harvey Newstrom: "Re: Clarification and limited apology."</a>
<li><strong>In reply to:</strong> <a href="4237.html">Steve Nichols: "Re: Sentience"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4287.html">Eliezer S. Yudkowsky: "Re: Sentience"</a>
<li><strong>Reply:</strong> <a href="4287.html">Eliezer S. Yudkowsky: "Re: Sentience"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4261">[ date ]</a>
<a href="index.html#4261">[ thread ]</a>
<a href="subject.html#4261">[ subject ]</a>
<a href="author.html#4261">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Steve Nichols wrote:
<br>
<p><em>&gt; &gt;I had not heard the news that brains were infinite-state.  Last I
</em><br>
<em>&gt; &gt;heard, atoms were all finite-state, so a finite clump of atoms must
</em><br>
<em>&gt; &gt;necessarily be finite-state.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;The brain has very very many possible states, but it is no more
</em><br>
<em>&gt; &gt;infinite state than a hundred billion thermostats would be
</em><br>
<em>&gt; &gt;infinite-state, if you wired them all together in an interesting way.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; @ But the brain can self-organise and forge new neuronal patterns.
</em><br>
<em>&gt; @ Furthermore, it is  a massively parallel distributed system, and
</em><br>
<em>&gt; @ *not* a Turing machine or von Neumann computer. Cite Kohonnen &amp;c.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; @ I doubt that it is ever in the same state twice!
</em><br>
<p>I doubt that as well!  But cite von Neumann, Turing and Church for
<br>
pointing out that a Turing machine can do everything a massively
<br>
parallel distributed system can do given enough time and tape.  We
<br>
use massively parallel systems today because they are faster than
<br>
cheaper than doing these jobs serially, not because there are some
<br>
things that massively parellal systems can do but Turing machines
<br>
can't.
<br>
<p><em>&gt; @ Any thought is possible, there are no boundaries to imagination.
</em><br>
<p>Yes, there are.  Try to think of a REALLY BIG NUMBER.  Now think of a
<br>
number that long with all of its digits randomized.  You can imagine
<br>
it as an opaque term (just like most of us can only imagine a number
<br>
as large as a trillion) but you can't actually hold it in your mind.
<br>
<p>Alternately, try to imagine all of Shakespeare's Hamlet at once.
<br>
<p>Hey.  That's sort of like the memory limitations on a computer.  Hmm.
<br>
<p><em>&gt; @ Apparently there are more possible moves in a game of chess than
</em><br>
<em>&gt; @ there are atoms in the universe, and there are presumably an (infinite)
</em><br>
<em>&gt; @ different possible games of chess.
</em><br>
<p>Ah, no.  Here again you've mistaken really really big with infinite.
<br>
There quite obviously AREN'T an infinite number of games of chess,
<br>
because there are a finite number of legal states that the board can
<br>
be in.
<br>
<p>Actually, you're right if you count games where the players keep
<br>
moving their pieces back and forth in a circle.  They can do that an
<br>
arbitrary number of times.  But I don't think that's quite what you meant.
<br>
<p><em> &gt; This is because we are talking of
</em><br>
<em>&gt; @ dynamic potentials. The thing with MVT is that we recognise a non-
</em><br>
<em>&gt; @ physical component ... this is true infinite-state!
</em><br>
<p>Non-physical components may well be infinite-state.  But non-physical
<br>
components can't interact with physical ones, so they can't be
<br>
measured.
<br>
<p>Why do you bother in making claims about non-physical components?  Why
<br>
not settle for purely physical animal intelligence and be done with
<br>
this stupid field of philosophy?
<br>
<p><em>&gt; &gt;John and I actually DO think that the human brain is (analogous to) an
</em><br>
<em>&gt; &gt;oversized Turing machine.  We think that it's programmed, and that we
</em><br>
<em>&gt; &gt;make &quot;decisions&quot; the same way that Deep Blue &quot;decides&quot; to move its
</em><br>
<em>&gt; &gt;bishop.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How does such (unsubstantiable) conjecture even begin to explain
</em><br>
<em>&gt; consciousness?
</em><br>
<p>I said that so as to make sure you'd know what Clark and I had in
<br>
mind; I didn't expect to convert you just by *saying* that.
<br>
<p><em>&gt; How can any Turing machine, a glorified card-reader, have
</em><br>
<em>&gt; experience?
</em><br>
<p>That's a Really Good Question.  But how can atoms, a glorified billard
<br>
game, have experiences? ;)
<br>
<p><em>&gt; Anyway, big blue is a CPU machine, not even a neural computer, so is
</em><br>
<em>&gt; absolutely nothing like a brain. Are you saying the DNA is the
</em><br>
<em>&gt; &quot;program&quot; or what other medium are you identifying?
</em><br>
<p>The medium is your neurons.  None of them can do anything
<br>
non-physical.  So your brain can't do anything non-physical.  So your
<br>
brain is entirely physical.  Physical stuff happens in lock-step
<br>
(because time happens in very small quanta) so the brain is emulatable
<br>
with a sufficiently complex Turing machine.
<br>
<p><em>&gt; Hang about, Deep Blue is an extremely limited program that just plays
</em><br>
<em>&gt; common chess. It is so dumb it can't even play other chess-like games, let
</em><br>
<em>&gt; alone &quot;learn&quot; ... it cannot even start to cope with natural language.
</em><br>
<p>We AI people have a problem.  People say: &quot;You can't make a computer
<br>
'learn.'&quot;  Then we make them learn.  They respond to things in their
<br>
environment in an interesting and useful way.  And people say: &quot;THAT'S
<br>
not really learning!  THIS is learning!&quot;  So we make them do that, and
<br>
they say &quot;THAT'S not really learning!  THIS is learing!&quot;
<br>
<p>Do you KNOW how long people said that we couldn't make a computer that
<br>
could improve its own playing style?  And to then come back and have
<br>
people say &quot;Hah.  Chess is easy!&quot;  It chills a man's heart, I tell you.
<br>
<p>Improving your game of chess is learning.  It's simple learning, but
<br>
it IS learning.  More to the point, we don't do anything more
<br>
interesting, on a metaphysical level, than what Deep Blue does.
<br>
<p><em>&gt; What about extrapolation abilities
</em><br>
<p>Deep Blue extrapolates.  &quot;Not enough!&quot;  I don't care.
<br>
<p><em>&gt; how can programs alter themselves to deal with entirely new
</em><br>
<em>&gt; circumstances?
</em><br>
<p>Not all PEOPLE are good at that.  But nonetheless, an adequately
<br>
complex Turing program can &quot;deal with&quot; circumstances as new as we can.
<br>
<p><em>&gt; Complexity (more depth) is not any sort of answer. Learning
</em><br>
<em>&gt; abilities are needed.
</em><br>
<p>No.  Deep Blue has everything we need as far as learning chess is
<br>
concerned.  Humans have a few general algorithms with lots of entropy
<br>
thrown in.  We'll find them, if it's worth it, or we'll build our own
<br>
from scratch.  And we'll program them.  Just like Deep Blue.
<br>
<p><em>&gt; &gt; Yes, it does.  It has an &quot;desired&quot; temperature, which it must
</em><br>
<em>&gt; &gt; &quot;remember&quot; in order to work properly.  The &quot;desired&quot; temperature isn't
</em><br>
<em>&gt; &gt; a property of anything else in the thermostat-heater-room system, so
</em><br>
<em>&gt; &gt; it must be an internal state of the thermostat.
</em><br>
<p><em>&gt; But precisely my point is it has not control over any &quot;memory&quot; of
</em><br>
<em>&gt; the temperature setting, this is made EXTERNALLY and is not
</em><br>
<em>&gt; within the thermostat's remit.
</em><br>
<p>The same thing happens to us.  We don't have free will.  Nothing is
<br>
within our remit.
<br>
<p><em>&gt; So you are completely wrong in
</em><br>
<em>&gt; anthromorphising that the thermostat &quot;desires&quot; a setting. A digital
</em><br>
<em>&gt; switch is On or OFF, there is no intermediate or internal state, either
</em><br>
<em>&gt; current goes through it or not. It cannot override its programming.
</em><br>
<p>Neither can you.  Your programming is more complex, but that's all.
<br>
You act according to your desires and beliefs.  You cannot change your
<br>
desires very much, and you can hardly change your desire to desire
<br>
differnt things at all.  You cannot change your beliefs either.  You
<br>
can decide what to say, but you cannot decide, for example, to drop
<br>
your view about MVT and agree with me.  You have to be convinced, and
<br>
you're not in control of when you get convinced.
<br>
<p>You're not in control of when you're happy.  You do things to make
<br>
yourself happy because you desire happiness because that's part of the
<br>
program.  When you do things, you have only the slightest control over
<br>
whether they make you happy or not, and considerably less control over
<br>
your desire to feel happy.  You're not in control of these things.
<br>
<p>A 128-bit thermostat knows the current setting on the heater, along
<br>
with the desired temperature down to almost 40 decimal places.  It has
<br>
128 on-off switches stringed together.  It knows 128 on-off facts.
<br>
You know on the order of several hundred billion of them.  But not
<br>
more than that.  Not infinite, by a long shot.
<br>
<p><em>&gt; &gt; &gt;     &gt; Turing machines are neither conscious,
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt;How do you know?
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Well, they would fail the Turing test for starters.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; &lt;blink blink&gt; ALL Turing machines???  This is to say that we'll never
</em><br>
<em>&gt; &gt; have a computer that will pass the Turing test, that we can never
</em><br>
<em>&gt; &gt; write a program so complex that it could trick people into believing
</em><br>
<em>&gt; &gt; that it acted like us.  Were you thinking about this when you said
</em><br>
<em>&gt; &gt; that?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Even passing the Turing test does not suggest consciousness, just AI.
</em><br>
<p>And what suggests consciousness besides intelligence?  What test do
<br>
YOU have for consciousness besides intelligence?  Surely not phasic
<br>
transients?  I can make my Turing machine flick its eyes about.
<br>
<p><em>&gt; The very fact that every aspect of Turing machines actions can be
</em><br>
<em>&gt; predicted, I would say, might even preclude them from consciousness.
</em><br>
<p>No more than you are precluded from consciousness.  Your atoms have no
<br>
free will.  You are composed entirely of atoms.  You have no more free
<br>
will than they do.  An adequately informed and powerful computer could
<br>
calculate your every move in a controlled environment.
<br>
<p><em>&gt; Yes, but as I try to point out, what MVT brings to table is *absent* or
</em><br>
<em>&gt; non-physical, phantom components that complete the circuit!
</em><br>
<p>Right, I'd forgotten that.  But non-physical components are no more
<br>
explanatory than invoking the soul unless you have an emprical test
<br>
for them.  How can you show that you're measuring something
<br>
non-physical, rather than just physical brain activity?
<br>
<p><em>&gt; A cathode ray tube is infinite-state in that it is fully variable, scalar,
</em><br>
<em>&gt; but because it operates within boundaries it is ANALOG. This stuff
</em><br>
<em>&gt; is fairly rudimentary solid-state physics, you have no grounds to be
</em><br>
<em>&gt; obfusticating here.
</em><br>
<p>Analog is NOT infinite-state.  It is very-many-state.  Really big
<br>
number.  So big that you can approximate that they're infinite in all
<br>
the equations and get correct results.  But they are NOT
<br>
infinite-state.  The analog to infinity is only *really big*.  You
<br>
seem to keep forgetting that.
<br>
<p>A hundred billion is a really big number.  Close enough to infinity
<br>
for most purposes.  But not these purposes.  Because it implies that
<br>
if I hook a hundred billion thermostats together, or give my Turing
<br>
machine a hundred billion feet of tape, my machine could do whatever
<br>
your brain and your analog equipment can do.
<br>
<p>That's the difference between infinite-state and awfully-many-state.
<br>
It's the difference between transcedent and awe-inspiring.
<br>
<p><em>&gt; No, phasic transients occur simply because a circuit is undergoing
</em><br>
<em>&gt; transformation from finite-state (lock step) to self-organising ....
</em><br>
<em>&gt; and they happen after the removal of an external clock (whether
</em><br>
<em>&gt; electronic or organic pineal eye). Aren't Turing machines always
</em><br>
<em>&gt; lock-stepped?
</em><br>
<p>Why, yes.  Along with ATOMS.
<br>
<p>I've not seen data that shows that phasic transients occur on account
<br>
of a circuit going from finite state to very-many-state, but I'll take
<br>
your word for it, because it hardly matters.  Very-many-state is not
<br>
infinite state, so a Very-large Turing machine can do the job nicely.
<br>
<p><em>&gt; &gt;Look, you're overlooking the very simple point that in order to make
</em><br>
<em>&gt; &gt;any kind of induction, you need to first NOTICE a correlation.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Absolutely. The main body of MVT concerns comparative brain
</em><br>
<em>&gt; anatomy and behavioural difference between E-2 and E-1 animals.
</em><br>
<p>Ah.  That's good news.  But you just got through telling me that
<br>
intelligence is not consciousness, around line 150 or so.  So while I
<br>
really do hope you have an excellent theory of intelligence on your
<br>
hands, a perfect theory of intelligence is, unfortunately, a
<br>
non-explanation of consciousness.
<br>
<p><em>&gt; You obviously haven't read anything of MVT. All mammals and birds
</em><br>
<em>&gt; are E-1, and have REM. No cold-blooded animals have REM. The
</em><br>
<em>&gt; pineal eye atrophied across all species during the reptilian/mammalian
</em><br>
<em>&gt; boundary, and during the emergence of endothermy (internal or warm-
</em><br>
<em>&gt; blooded strategies). There is a clear experimental correlation between
</em><br>
<em>&gt; absence of pineal input (after pinealectomy, or when pineal eye has
</em><br>
<em>&gt; been covered by metal foil and subject reptiles compared with a control
</em><br>
<em>&gt; group) and intelligent behaviour ... *awareness* ... I don't really like to
</em><br>
<em>&gt; use the &quot;C&quot; word!
</em><br>
<p>Actually, I knew that tidbit.  But then you're observing intelligence,
<br>
not consciousness.
<br>
<p>The problem of other minds asks: &quot;I know I'm conscious and
<br>
intelligent, and I know you act intelligently.  But are you conscious,
<br>
or do you just act that way?&quot;  This is a problem which your theory
<br>
doesn't solve.  Keep your pants on, a solid theory of animal
<br>
intelligence will still win you the Nobel, but it won't help you solve
<br>
the mind-body problem.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt;But you can't observe any such cases, thanks to the problem of other
</em><br>
<em>&gt; &gt;minds.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, as I mention above, there are about 130 years of records of such
</em><br>
<em>&gt; experiments.  Other minds is an artificial lingoistic problem, it
</em><br>
<em>&gt; doesn't stop  consciousness (sentience) happening, just gives philosophers
</em><br>
<em>&gt; something to argue amongst themselves about.
</em><br>
<p>Yeeeeessss.  But so is the mind-body problem.  Why do you even CLAIM
<br>
to have solved it?  Why not sit pretty with a theory of animal
<br>
intelligence and let the philosophers do their jobs on consciousness,
<br>
for which you have no physical explanation?
<br>
<p><em>&gt; Newborn infants seem to have empathic abilities, plus abilities
</em><br>
<em>&gt; to monitor and judge emotions in others.
</em><br>
<p>So does a lie detector.  Another input device to attach to a complex
<br>
Turing machine.
<br>
<p><em>&gt; It is a fair bet that if someone
</em><br>
<em>&gt; is screaming in pain, particularly if they have correlating signs such
</em><br>
<em>&gt; as a red-hot poker sticking up their arse, that they are actually feeling
</em><br>
<em>&gt; pain, in much the same way you would. I really fail to see the problem here,
</em><br>
<em>&gt; other than that you cannot be the other person so have to rely on reports.
</em><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;^^^^^^^^^^^
<br>
<p>That IS the problem!  The problem is that I have no proof that brain
<br>
activity implies actual feelings (in anyone but me), only with the
<br>
behavior.  This is a rather boring philosophical problem.  I recommend
<br>
that you ignore it, that you never think on it again, and sell the
<br>
world your theory of animal intelligence.  Most importantly, never
<br>
again use the word consciousness.  You have no proof of that.  Nor any
<br>
reason to care!  So skip it.
<br>
<p><em>&gt; &gt;Maybe you have something simpler in mind.  Maybe you're just positing
</em><br>
<em>&gt; &gt;MVT as a theory to explain how and when things can pass the Turing
</em><br>
<em>&gt; &gt;Test.  But you fail on THOSE grounds, too: you provide no *mechanism*
</em><br>
<em>&gt; &gt;by which the phantom pineal eye causes people to be conscious, or to
</em><br>
<em>&gt; &gt;act conscious, or, well, anything.  You only claim that consciousness
</em><br>
<em>&gt; &gt;DOES happen, and you tell us WHEN, but you don't explain HOW.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; On the contrary, MVT takes Melzack's neuromatrix theory of self (and
</em><br>
<em>&gt; gateway theory of pain) a step further, and does explain how experiences
</em><br>
<em>&gt; are identified by the brain as being self-originating (neurosignatures &amp;c)
</em><br>
<em>&gt; or not. The deep structures of the brain evolved concurrently in early
</em><br>
<em>&gt; evolution with their main sensory information supplier, the median or
</em><br>
<em>&gt; *primal* eye (not &quot;third eye&quot; ... it predates lateral eyes). The brain
</em><br>
<em>&gt; expects
</em><br>
<em>&gt; information from the median eye ... and when it doesn't come from external
</em><br>
<em>&gt; light, generates phantom information instead.
</em><br>
<p>All physical.  Why does THAT correlate with consciousness?  Why not
<br>
settle for intelligence?
<br>
<p><em>&gt; NO. Fodorian modules and central executive theories, supervenience
</em><br>
<em>&gt; and all the other philosophical lingoistic drivel are not physiology
</em><br>
<em>&gt; based, nor do they give a clear account of the *evolution* of
</em><br>
<em>&gt; consciousness.
</em><br>
<p>But neither do you.  Consciousness IS philosophical drivel.  Let us
<br>
keep it.  We've reserved &quot;intelligence&quot; just for people like you, so
<br>
you have something to prove and talk about scientifically and so you
<br>
guys can never run us philosophers out of a job.  Why bother attacking
<br>
philosophy on this point?
<br>
<p><em>&gt; Experimental evidence can only observe behaviour .... would that be
</em><br>
<em>&gt; acceptable to you? If so, MVT has it in abundance. However, if as I
</em><br>
<em>&gt; suspect you are not happy with circumstantial evidence (correlation
</em><br>
<em>&gt; between REM and dream mentation, even in humans, cannot be
</em><br>
<em>&gt; absolutely proven since it relies on reports of the dreamer) then
</em><br>
<em>&gt; YOU have a problem, because you can never accept any account of
</em><br>
<em>&gt; consciousness, MVT or not.
</em><br>
<p>Why YES.  I think we're finally on the same page!
<br>
<p><em>&gt; No-one has come up with a better idea than MVT, which explains both
</em><br>
<em>&gt; walking and sleeping consciousness (24/7). Your Turing machine idea
</em><br>
<em>&gt; doesn't even reach first base, it only models intelligence.
</em><br>
<p>But what do YOU get besides intelligence?  Besides intelligent
<br>
behavior?  Hell, what do you NEED more than than that, besides
<br>
philosophical drivel?
<br>
<p><em>&gt; Perhaps I should develop new and irresistible hypnotic applications
</em><br>
<em>&gt; from MVT and enforce belief in it ... would this satisfy you?
</em><br>
<p>It would certainly get you that Nobel!
<br>
<p>-Dan
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-unless you love someone-
<br>
&nbsp;&nbsp;&nbsp;&nbsp;-nothing else makes any sense-
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e.e. cummings
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4262.html">Michael M. Butler: "Re: HUMOR Re: (fwd) Signs of the Times"</a>
<li><strong>Previous message:</strong> <a href="4260.html">Harvey Newstrom: "Re: Clarification and limited apology."</a>
<li><strong>In reply to:</strong> <a href="4237.html">Steve Nichols: "Re: Sentience"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4287.html">Eliezer S. Yudkowsky: "Re: Sentience"</a>
<li><strong>Reply:</strong> <a href="4287.html">Eliezer S. Yudkowsky: "Re: Sentience"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4261">[ date ]</a>
<a href="index.html#4261">[ thread ]</a>
<a href="subject.html#4261">[ subject ]</a>
<a href="author.html#4261">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:38 MDT</em>
</em>
</small>
</body>
</html>
