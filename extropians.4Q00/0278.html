<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Let's hear Eugene's ideas</title>
<meta name="Author" content="Eugene Leitl (eugene.leitl@lrz.uni-muenchen.de)">
<meta name="Subject" content="Re: Let's hear Eugene's ideas">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Let's hear Eugene's ideas</h1>
<!-- received="Wed Oct  4 04:22:52 2000" -->
<!-- isoreceived="20001004102252" -->
<!-- sent="Wed, 4 Oct 2000 02:10:31 -0700 (PDT)" -->
<!-- isosent="20001004091031" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: Let's hear Eugene's ideas" -->
<!-- id="14810.62471.347918.819222@lrz.uni-muenchen.de" -->
<!-- inreplyto="39D9EB7C.F5109023@pobox.com" -->
<strong>From:</strong> Eugene Leitl (<a href="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Let's%20hear%20Eugene's%20ideas&In-Reply-To=&lt;14810.62471.347918.819222@lrz.uni-muenchen.de&gt;"><em>eugene.leitl@lrz.uni-muenchen.de</em></a>)<br>
<strong>Date:</strong> Wed Oct 04 2000 - 03:10:31 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0279.html">Technotranscendence: "AT&amp;T vows no censorship on new network"</a>
<li><strong>Previous message:</strong> <a href="0277.html">Samantha Atkins: "Re: Attacks (was Re: Why would AI want to be friendly?)"</a>
<li><strong>In reply to:</strong> <a href="0177.html">Eliezer S. Yudkowsky: "Re: Let's hear Eugene's ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0230.html">J. R. Molloy: "We're All Intelligence Pushers On This Bus"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#278">[ date ]</a>
<a href="index.html#278">[ thread ]</a>
<a href="subject.html#278">[ subject ]</a>
<a href="author.html#278">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Eliezer S. Yudkowsky writes:
<br>
<p><em> &gt; 
</em><br>
<em> &gt; As far as I can see, this totally fails to address my original question.  I
</em><br>
<p>That's quite true, because I didn't yet have time to address the
<br>
original question. Spend too many hours with mail, already.
<br>
<p><em> &gt; just want to know what you would consider a 'win' scenario.  When
</em><br>
<em> &gt; nanotechnology, AI, and uploading are all outlawed, how exactly do you 'pass
</em><br>
<em> &gt; the tight spot' and where are you after you've passed it?
</em><br>
<p>I'm not interested in outlawing nanotechnology, AI or uploading. I'm
<br>
sorry you misinterpreted what I've said. 
<br>
<p>I'm interested in *temporarily* imposing enforcible regulations on: 1)
<br>
engineering biological pathogens suitable for warfare and terrorism 2)
<br>
potentially forthcoming devices based on potentially forthcoming new
<br>
physics which could potentially destroy the Earth or this spacetime
<br>
(fat chance, this is more to keep the list complete, than anything),
<br>
3) free-environment (this also includes space, since you want the
<br>
solar system pristine and not full of nasty grey goo which can eat
<br>
through a hull or suit in minutes) capable self-rep machine-phase
<br>
mechanosynthetic systems and 4) AI of about and beyond human
<br>
capacity. Insect, rodent and even chimp grade AI is ok, and indeed
<br>
welcome, but they must be restricted in their computational resources
<br>
available to them and be not allowed to fool with their own works, as
<br>
well as by people by tamper-proof packaging. Maybe not an
<br>
electromagnetic gun, but a thermite shell. This might result in
<br>
restrictions in amount of computational power available to individuals
<br>
outside of restricted research installations. Once again, this is
<br>
temporarily, and adaptively negotiable, since a threat needs to be
<br>
reevaluated in face of new information. Potentially, we're getting out
<br>
panties in a bunch over nothing, but there's no way to tell a priori.
<br>
<p>I do not want to impose restrictions on the upload *technology*, quite
<br>
the opposite. (If fact I would make it along with developing the
<br>
computronium substrate the the top research priority if I had any
<br>
political power). The only restrictions which should hold is which
<br>
*people* to upload *first*. We need to make sure that these people
<br>
make self-amplification and self-enhancement a very, very low
<br>
priority. These means that, similiar to astronauts, these people must
<br>
be carefully selected, since they're going to help us with uploading
<br>
the rest. The strategy should be to haul over as many people as we can
<br>
as quickly as we can with the very best fidelity we can. Preferably
<br>
for free, which means that the costs must fall several orders of
<br>
magnitude, because the first uploads will be *expensive*.
<br>
<p>Obviously, during the conversion process the uploader rights need to
<br>
be restricted to prevent a runaway resulting in instant Singularity,
<br>
until the conversion is complete. (If this is not safely
<br>
implementable, we must drop this requirement. These restriction things
<br>
tend to backfire nastily, so we must trim them to the barest
<br>
essentials).
<br>
<p>After the exodus is complete but for those recalcitrant curmugeons who
<br>
absolutely, positively don't want to and deal with all negotiators
<br>
using firearms as arguments, all restrictions are lifted, and off we
<br>
go. No one knows what will happen next, but I do definitely do not
<br>
want to be in the shoes of those who are left behind, because through
<br>
the decision to stay they've rendered themselves extremely powerless
<br>
and vulnerable. For instance, if the (already anticipable) need arises
<br>
to disassemble the Earth, they're then left absolutely at mercy of
<br>
diassemblers. Depending on their ethics and state of development and
<br>
whether others will intervene (my current self certainly would want
<br>
to), they may or may not upload them by force, or consider them just
<br>
as unremarkable part of the planetary surface resource. (Well, okay,
<br>
maybe I am evil).
<br>
<p>My current self would want to team up with a group of competent
<br>
like-minded, leave Earth surface as soon as possible (unless the means
<br>
of travel are unavailable, or impose a critical delay), and head for
<br>
the Moon or any suitable nearby space rock. I would not be interested
<br>
in exponential self-amplification, but certainly in self-improvement,
<br>
anticipating impending need to compete with other streamlined people
<br>
and beings. Clearly, my current self can't speak for my future self no
<br>
more than my 1 year old can speak for the adult me. Your mileage may
<br>
vary.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0279.html">Technotranscendence: "AT&amp;T vows no censorship on new network"</a>
<li><strong>Previous message:</strong> <a href="0277.html">Samantha Atkins: "Re: Attacks (was Re: Why would AI want to be friendly?)"</a>
<li><strong>In reply to:</strong> <a href="0177.html">Eliezer S. Yudkowsky: "Re: Let's hear Eugene's ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0230.html">J. R. Molloy: "We're All Intelligence Pushers On This Bus"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#278">[ date ]</a>
<a href="index.html#278">[ thread ]</a>
<a href="subject.html#278">[ subject ]</a>
<a href="author.html#278">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:15 MDT</em>
</em>
</small>
</body>
</html>
