<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Humor: helping Eliezer to fulfill his full pote</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: Humor: helping Eliezer to fulfill his full potential">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Humor: helping Eliezer to fulfill his full potential</h1>
<!-- received="Mon Nov  6 07:32:54 2000" -->
<!-- isoreceived="20001106143254" -->
<!-- sent="Mon, 06 Nov 2000 09:24:07 -0500" -->
<!-- isosent="20001106142407" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: Humor: helping Eliezer to fulfill his full potential" -->
<!-- id="3A06BF07.F84D8594@posthuman.com" -->
<!-- inreplyto="Humor: helping Eliezer to fulfill his full potential" -->
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20Humor:%20helping%20Eliezer%20to%20fulfill%20his%20full%20potential&In-Reply-To=&lt;3A06BF07.F84D8594@posthuman.com&gt;"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Mon Nov 06 2000 - 07:24:07 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1988.html">Brian D Williams: "Optimal Eating"</a>
<li><strong>Previous message:</strong> <a href="1986.html">Brian Atkins: "Re: Are extropians becoming mainstream?"</a>
<li><strong>Maybe in reply to:</strong> <a href="1954.html">John  M Grigg: "Humor: helping Eliezer to fulfill his full potential"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1991.html">Ken Clements: "Re: Humor: helping Eliezer to fulfill his full potential"</a>
<li><strong>Reply:</strong> <a href="1991.html">Ken Clements: "Re: Humor: helping Eliezer to fulfill his full potential"</a>
<li><strong>Reply:</strong> <a href="1996.html">Max More: "AI safeguards [Was: Re: Humor: helping Eliezer to fulfill his full potential]"</a>
<li><strong>Reply:</strong> <a href="2025.html">Spike Jones: "Re: Humor: helping Eliezer to fulfill his full potential"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1987">[ date ]</a>
<a href="index.html#1987">[ thread ]</a>
<a href="subject.html#1987">[ subject ]</a>
<a href="author.html#1987">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Spike we think about it every day, believe me it is kinda constantly sitting
<br>
there in the back of my head. And I'm not even involved in the day to day
<br>
work! The quick answer is that we plan to make progress on the &quot;safety&quot; area
<br>
as we go... right now this is all in the conceptual (or pre-conceptual?)
<br>
phase, and you will likely see at least one kind of equally conceptual
<br>
&quot;safety&quot; paper produced relatively soon. As we begin to get a firmer grasp
<br>
on the technology we will be able to reduce the risks further and further,
<br>
and produce more substantive documents describing this. It is not currently
<br>
in the plans to have an &quot;off switch&quot;, although that might become more possible
<br>
in the future? Our main intention is to reduce all possible risks before
<br>
hitting the &quot;on switch&quot;, kinda like how Foresight wants to reduce all the
<br>
risks of nanotech before the public gets ahold of it.. except that they have
<br>
no real control over how things turn out, and we do since we at least have
<br>
perfect control over the initial conditions and can run simulations to see
<br>
how things might progress from there.
<br>
<p>P.S. we aren't the only ones out there... recently we have come across a
<br>
competing AI project that we rate as having a significant (greater than zero)
<br>
chance of &quot;waking up&quot;... and it is set for completion circa 2003 at the latest.
<br>
I really think it would be good if there was an equivalent of Foresight for
<br>
the AI area... Foresight for now is still so focused on nanotech they don't
<br>
see the chance to expand into a more general Foresight organization covering
<br>
all of Bill Joy's worries.
<br>
<p>Spike Jones wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Brian Atkins wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Mainly just in case Eugene or den Otter show up? :-) Don't get me started
</em><br>
<em>&gt; &gt; on our bunker and small arsenal of weapons...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Do allow me to make one semi-serious point among the mirth
</em><br>
<em>&gt; and jocularity.  Whenever the nanotech enthusiasts speak publicly,
</em><br>
<em>&gt; they point out the dangers of the technology, and spend some time
</em><br>
<em>&gt; describing possible safety measures, such as Ralph's broadcast
</em><br>
<em>&gt; architecture.  Every time I hear Drexler or Merkle speak, they
</em><br>
<em>&gt; mention safeguards.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What is Singularity Institute's counterpart to that?  Are yall thinking
</em><br>
<em>&gt; safety?  Is there some kind of technology which would allow yall
</em><br>
<em>&gt; to hit the off switch if something goes awry?  You know Im a big
</em><br>
<em>&gt; fan of the HWoMBeP and all of yall, but if yall are rushing headlong
</em><br>
<em>&gt; into unknown and dangerous technology without appropriate safety
</em><br>
<em>&gt; measures (if such a term even applies) then I understand some of
</em><br>
<em>&gt; the criticism yall are getting.  spike
</em><br>
<pre>
-- 
Brian Atkins
Director, Singularity Institute for Artificial Intelligence
<a href="http://www.singinst.org/">http://www.singinst.org/</a>
</pre>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1988.html">Brian D Williams: "Optimal Eating"</a>
<li><strong>Previous message:</strong> <a href="1986.html">Brian Atkins: "Re: Are extropians becoming mainstream?"</a>
<li><strong>Maybe in reply to:</strong> <a href="1954.html">John  M Grigg: "Humor: helping Eliezer to fulfill his full potential"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1991.html">Ken Clements: "Re: Humor: helping Eliezer to fulfill his full potential"</a>
<li><strong>Reply:</strong> <a href="1991.html">Ken Clements: "Re: Humor: helping Eliezer to fulfill his full potential"</a>
<li><strong>Reply:</strong> <a href="1996.html">Max More: "AI safeguards [Was: Re: Humor: helping Eliezer to fulfill his full potential]"</a>
<li><strong>Reply:</strong> <a href="2025.html">Spike Jones: "Re: Humor: helping Eliezer to fulfill his full potential"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1987">[ date ]</a>
<a href="index.html#1987">[ thread ]</a>
<a href="subject.html#1987">[ subject ]</a>
<a href="author.html#1987">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:19 MDT</em>
</em>
</small>
</body>
</html>
