<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Spoiler Review of A.I.</title>
<meta name="Author" content="Robin Hanson (rhanson@gmu.edu)">
<meta name="Subject" content="Re: Spoiler Review of A.I.">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Spoiler Review of A.I.</h1>
<!-- received="Mon Jul  2 10:48:17 2001" -->
<!-- isoreceived="20010702164817" -->
<!-- sent="Mon, 02 Jul 2001 12:48:07 -0400" -->
<!-- isosent="20010702164807" -->
<!-- name="Robin Hanson" -->
<!-- email="rhanson@gmu.edu" -->
<!-- subject="Re: Spoiler Review of A.I." -->
<!-- id="4.2.0.58.20010702114030.00c55100@mail.gmu.edu" -->
<!-- inreplyto="3B3F8010.74BE8C51@pobox.com" -->
<strong>From:</strong> Robin Hanson (<a href="mailto:rhanson@gmu.edu?Subject=Re:%20Spoiler%20Review%20of%20A.I.&In-Reply-To=&lt;4.2.0.58.20010702114030.00c55100@mail.gmu.edu&gt;"><em>rhanson@gmu.edu</em></a>)<br>
<strong>Date:</strong> Mon Jul 02 2001 - 10:48:07 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0095.html">Robin Hanson: "Re: My AI Review  - spoilers further down..."</a>
<li><strong>Previous message:</strong> <a href="0093.html">Spudboy100@aol.com: "David Gelernter &amp; Scopeware"</a>
<li><strong>In reply to:</strong> <a href="0022.html">Eliezer S. Yudkowsky: "Spoiler Review of A.I."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0141.html">Spike Jones: "Re: Spoiler Review of A.I."</a>
<li><strong>Reply:</strong> <a href="0141.html">Spike Jones: "Re: Spoiler Review of A.I."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#94">[ date ]</a>
<a href="index.html#94">[ thread ]</a>
<a href="subject.html#94">[ subject ]</a>
<a href="author.html#94">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
** This Contains Spoilers!! **
<br>
<p><p><p><p><p><p><p><p><p>Eliezer Yudkowsky wrote:
<br>
<em>&gt;The very first thing that struck me about A.I. was the rather extreme
</em><br>
<em>&gt;stupidity of the AI *researchers*. ... David is beta software.  His
</em><br>
<em>&gt;emotional responses ... show a binary, all-or-nothing quality.
</em><br>
<em>&gt;Then the AI researchers had the bright idea of putting this beta software
</em><br>
<em>&gt;into a human body, adding in those emotions calculated to produce maximal
</em><br>
<em>&gt;emotional attachment on the part of humans, and giving it as a human
</em><br>
<em>&gt;surrogate to a mother already in an emotionally unstable state because her
</em><br>
<em>&gt;own child has been in medical cryonic suspension for five years.
</em><br>
<em>&gt;...  David realizes that his mother will someday die, ...  His mother,
</em><br>
<em>&gt;... feels enormous emotional stress at the thought of returning David to be
</em><br>
<em>&gt;incinerated.  Nobody thought of this back when they were building a
</em><br>
<em>&gt;loving, lovable, naturally immortal, allegedly disposable child?
</em><br>
<p>I found the stupidity level to be plausible, and not extreme.  David's
<br>
father was a company employee, not a random person, and at some point
<br>
you'd have to test the system with a real mother.  Most of what you
<br>
see as thoughtless, I see as insensitive - they just didn't care.
<br>
David feels bad that his mom might die?  Who cares what David feels.
<br>
Mom would feel awful giving up this product?  That's the ideal product
<br>
- one customers couldn't think of living without.
<br>
<p>What you see as binary capabilities, I see as crude context-dependent
<br>
choices.  When you don't see the subtleties of a situation, your
<br>
actions will have more variance, and seem more all or nothing compared
<br>
to appropriate responses.
<br>
<p><em>&gt;... David nearly kills himself competing with his revived brother, by
</em><br>
<em>&gt;attempting to eat; the second catastrophe occurs when David nearly drowns
</em><br>
<em>&gt;his brother.  ... the AI researchers should have thought of it.
</em><br>
<p>You can't think of everything - that's what beta tests are for.  David
<br>
killing himself wasn't a huge potential loss, though killing his brother is,
<br>
but I think we can see that as an unlikely risk that the company was
<br>
well willing to take given the huge potential profits awaiting.  Stories
<br>
all the time focus on telling about the consequences of unlikely events.
<br>
<p>I did find it implausible, but not crazy, that robots would be that much
<br>
denser than humans.  David should have floated - androids are designed
<br>
to fit into human physical slots, and density is one obvious parameter
<br>
to fit.
<br>
<p><em>&gt;... Again, someone at the mecha corporation was being damn stupid
</em><br>
<em>&gt;and deserves to be sued into bankruptcy.
</em><br>
<p>But these were company folks who signed everything asked of them.
<br>
Should no one be allowed to agree to be a beta-tester?
<br>
<p>What was perhaps less plausible was that the company didn't have
<br>
people constantly watching the test ready to take over in the event
<br>
of some disaster.  But that would be a lot more expensive - maybe they
<br>
instead choose ten families to test David in at the same time.
<br>
<p>Also less plausible was not having tracking devices in all robots.
<br>
But storytellers are really finding it hard to adapt to the brave
<br>
new world where all characters can always know where all the other
<br>
characters are and talk to them at any time.
<br>
<p><em>&gt;group of androids who are scavenging spare parts from a dump.  ...
</em><br>
<em>&gt;Why do these nonemotional androids want to survive?  ...
</em><br>
<p>I think it is that these androids are actually a lot more
<br>
sophisticated that most humans around them give them credit for.
<br>
All that blather about David being the first robot to have emotions
<br>
is just marketing hype - until David, robots just weren't very good
<br>
at expressing emotions in a way to induce sympathy from humans.
<br>
But a robot like Gigolo Joe just couldn't do what he did without
<br>
having a lot of things that function as emotions internally.
<br>
<p><em>&gt;And the crowd rises and boos the ringmaster off the stage - &quot;Mecha does
</em><br>
<em>&gt;not plead for its life!&quot; - but their decision is correct only by
</em><br>
<em>&gt;coincidence.
</em><br>
<p>Yes, that's the point.  Whether humans treat you nicely has little
<br>
to do with your absolute moral worth - it is mostly about whether
<br>
you can put on a good enough emotional display.
<br>
<p><em>&gt;... surely an advanced AI knows what 'fiction' is, and an AI boy
</em><br>
<em>&gt;knows that bedtime stories aren't true.
</em><br>
<p>Surely if its makers want the AI to suffer the sort of confusions
<br>
that real boys do, they might be able to construct such an AI.
<br>
<p><em>&gt;... Gigolo Joe's speech about how
</em><br>
<em>&gt;humans resent robots because they know that, in the end, robots will be
</em><br>
<em>&gt;all that's left.  Where did *that* come from?
</em><br>
<p>It makes more sense if you assume that Gigolo Joe is a lot more than
<br>
a chatbot and simple sex droid.  He is fully capable as an abstract
<br>
reasoner.  He lacks more in his ability to display and evoke emotions.
<br>
And some of these lackings are probably by design - just as the slightly
<br>
unreal flesh tone is probably by design - so humans can be &quot;racist&quot;.
<br>
<p><em>&gt;...  If there are that many Davids, why are they all designed to
</em><br>
<em>&gt;have the human emotion of wanting to be unique?
</em><br>
<p>I thought it was because human boys have that emotion.
<br>
<p><em>&gt;...  For that matter, what possessed the idiots in Marketing to
</em><br>
<em>&gt;produce a batch of identical AIs all named David, instead of giving them
</em><br>
<em>&gt;individual faces and individual voices and maybe some quirks of
</em><br>
<em>&gt;personality?  ...
</em><br>
<p>The boy bodies on the rack did have different faces I think.
<br>
<p><em>&gt;Finally, after David realizes that he is not unique, he deliberately
</em><br>
<em>&gt;topples off a window ledge into the ocean.  Uh... why?  How is that a
</em><br>
<em>&gt;means to the end of getting his mother to love him?  ...
</em><br>
<p>David doesn't seem to fully realize that he is not a human boy.
<br>
He seems to be designed to think that he is human and do what
<br>
a human would do.
<br>
<p><em>&gt;pouting gibberish about yada-yada space-time yada-yada pathways yada-yada
</em><br>
<em>&gt;DNA yada-yada only one day yada-yada.  ..
</em><br>
<p>Yeah, that was the obvious dumb plot device.  I won't defend it.
<br>
<p><em>&gt;The Successors could easily have given David a full deck of emotions, or
</em><br>
<em>&gt;could easily have created an immortal virtual Monica that was real to the
</em><br>
<em>&gt;limit of David's limited perceptions.  Why didn't they?
</em><br>
<p>Yeah, that bugged me too.
<br>
<p><em>&gt;I know there is a certain style of filmmaking that holds that the viewer
</em><br>
<em>&gt;should be allowed to pick their own ending, and I hate that style with a
</em><br>
<em>&gt;fiery passion.
</em><br>
<p>Like Hal, I didn't mind it.
<br>
<p>Were any of the other fathers out there bugged by David's not imprinting on
<br>
his father?  David loved only his mother; the guy was &quot;Henry&quot;.  David didn't
<br>
care for or miss Henry at all.  I suppose we could write this off as
<br>
postulating a big swing back to strong gender roles, but even so it seemed
<br>
implausibly extreme.
<br>
<p>One other thing that bugged me was that there was no
<br>
<p>Robin Hanson  <a href="mailto:rhanson@gmu.edu?Subject=Re:%20Spoiler%20Review%20of%20A.I.&In-Reply-To=&lt;4.2.0.58.20010702114030.00c55100@mail.gmu.edu&gt;">rhanson@gmu.edu</a>  <a href="http://hanson.gmu.edu">http://hanson.gmu.edu</a>
<br>
Asst. Prof. Economics, George Mason University
<br>
MSN 1D3, Carow Hall, Fairfax VA 22030-4444
<br>
703-993-2326  FAX: 703-993-2323
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0095.html">Robin Hanson: "Re: My AI Review  - spoilers further down..."</a>
<li><strong>Previous message:</strong> <a href="0093.html">Spudboy100@aol.com: "David Gelernter &amp; Scopeware"</a>
<li><strong>In reply to:</strong> <a href="0022.html">Eliezer S. Yudkowsky: "Spoiler Review of A.I."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0141.html">Spike Jones: "Re: Spoiler Review of A.I."</a>
<li><strong>Reply:</strong> <a href="0141.html">Spike Jones: "Re: Spoiler Review of A.I."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#94">[ date ]</a>
<a href="index.html#94">[ thread ]</a>
<a href="subject.html#94">[ subject ]</a>
<a href="author.html#94">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Fri Oct 12 2001 - 14:39:41 MDT</em>
</em>
</small>
</body>
</html>
