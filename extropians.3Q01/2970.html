<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Sysops and enlightened despots</title>
<meta name="Author" content="Anders Sandberg (asa@nada.kth.se)">
<meta name="Subject" content="Re: Sysops and enlightened despots">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Sysops and enlightened despots</h1>
<!-- received="Fri Aug  3 09:37:43 2001" -->
<!-- isoreceived="20010803153743" -->
<!-- sent="Fri, 3 Aug 2001 17:37:31 +0200" -->
<!-- isosent="20010803153731" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: Sysops and enlightened despots" -->
<!-- id="20010803173731.A15573@akira.nada.kth.se" -->
<!-- inreplyto="3B6A4CD0.CF16D0FF@posthuman.com" -->
<strong>From:</strong> Anders Sandberg (<a href="mailto:asa@nada.kth.se?Subject=Re:%20Sysops%20and%20enlightened%20despots&In-Reply-To=&lt;20010803173731.A15573@akira.nada.kth.se&gt;"><em>asa@nada.kth.se</em></a>)<br>
<strong>Date:</strong> Fri Aug 03 2001 - 09:37:31 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2971.html">Reason: "RE: Stem Cell Debate --Banned in the USA ---&gt; libertarian societies as  unicorn"</a>
<li><strong>Previous message:</strong> <a href="2969.html">Sabine Atkins: "LISTS: Extropian dating mailing list now available"</a>
<li><strong>In reply to:</strong> <a href="2953.html">Brian Atkins: "Re: Stem Cell Debate --Banned in the USA ---&gt; libertarian societies as  unicorn"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2974.html">Reason: "RE: Sysops and enlightened despots"</a>
<li><strong>Reply:</strong> <a href="2974.html">Reason: "RE: Sysops and enlightened despots"</a>
<li><strong>Reply:</strong> <a href="2994.html">Brian Atkins: "Re: Sysops and enlightened despots"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2970">[ date ]</a>
<a href="index.html#2970">[ thread ]</a>
<a href="subject.html#2970">[ subject ]</a>
<a href="author.html#2970">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
On Fri, Aug 03, 2001 at 03:03:44AM -0400, Brian Atkins wrote:
<br>
<em>&gt; Reason wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I have yet to see a better solution to the issue. At some point the matter
</em><br>
<em>&gt; (as in atoms) must fall under someone's control, and personally I don't
</em><br>
<em>&gt; relish the idea of having to constantly protect myself from everyone else
</em><br>
<em>&gt; who can't be trusted with nanotech and AI. All it takes is one Blight to
</em><br>
<em>&gt; wipe us out. That kind of threat does not go away as humans progress to
</em><br>
<em>&gt; transhumanity, rather it increases in likelihood. What is the stable state
</em><br>
<em>&gt; if not Sysop or total death? There may be some other possibilities, can
</em><br>
<em>&gt; you name some?
</em><br>
<p>Stable states are dead states. I would say another possibility would be an
<br>
eternally growing selforganised critical state - sure, disasters happen,
<br>
but countermeasures also emerge. The whole is constantly evolving and
<br>
changing.
<br>
<p>Having (post)human development constrained to a small part of the available
<br>
technology/culture/whatever space in order to ensure safety is going to run
<br>
into a Gödel-like trap. There are likely undecidable threats out there,
<br>
things that cannot be determined to be dangerous or not using any finite
<br>
computational capability. Hence the only way of ensuring security is to
<br>
limit ourselves to a finite space - which goes counter to a lot of the core
<br>
transhumanist ideas. Or the sysop would have to allow undecidable risks and
<br>
similar hard-to-detect threats. One category of threats to worry about are
<br>
of course threats the sysop would itself run into while looking for threats
<br>
- they could wipe us out exactly because (post)humanity had not been
<br>
allowed the necessary dispersal and freedom that might otherwise have
<br>
saved at least some. 
<br>
<p>This is essentially the same problem as any enlightened despot scheme (and
<br>
there is of course the huge range of ethical problems with such schemes
<br>
too), put in a fresh sf setting. Enlightened despots make bad rulers
<br>
because they cannot exist: they need accurate information about the
<br>
preferences of everybody, which is not possible for any human ruler. The
<br>
version 2.0 scenario assuming the omniscient AI runs into the same problem
<br>
anyway: it would need to handle an amount of information of the same order
<br>
of magnitude as the information processing in the entire society. Hence it
<br>
would itself be a sizeable fraction of society information-wise (and itself
<br>
a source of plenty of input in need of analysis). Given such technology, as
<br>
soon as any other system in society becomes more complex the ruler AI would
<br>
have to become more complex to keep ahead. Again the outcome is that either
<br>
growth must be limited or the society is going to end up embedded within an
<br>
ever more complex system that spends most of its resources monitoring
<br>
itself. 
<br>
<p>The idea that we need someone to protect ourselves from ourselves in this
<br>
way really hinges on the idea that certain technologies are instantly and
<br>
throughly devastating, and assumes that the only way they can be handled is
<br>
if as few beings as possible get their manipulators on them. Both of these
<br>
assumptions are debatable, and I think it is dangerous to blithely say that
<br>
the sysop scenario is the only realistic alternative to global death. It
<br>
forecloses further analysis of the assumptions and alternatives by
<br>
suggesting that the question is really settled and there is only one way.
<br>
It also paints a simplistic picture of choices that could easily be used to
<br>
attack transhumanism, either as strongly hegemonic (&quot;They want to make a
<br>
computer to rule the world! Gosh, I only thought those claims they are
<br>
actually thinking like movie mad scientists were just insults.&quot;) or as
<br>
totally out of touch with reality (&quot;Why build a super-AI (as if such stuff
<br>
could exist in the first place) when we can just follow Bill Joy and
<br>
relinquish dangerous technology?!&quot;).
<br>
<p>A lot of this was hashed through in the article about nanarchy in Extropy,
<br>
I think. Nothing new under the Dyson.
<br>
<p>Existential risks are a problem, but I'm worried people are overly
<br>
simplistic when solving them. My own sketch above about a self-organised
<br>
critical state is complex, messy and hard to analyse in detail because it
<br>
is evolving. That is a memetic handicap, since solutions that are easy to
<br>
state and grasp sound so much better - relinquishment of technology or
<br>
sysops fit in wonderfully with the memetic receptors people have. But
<br>
complex problems seldom have simple neat solutions. Bill Joy is wrong
<br>
because his solution cannot practically work, the sysop is simple to state
<br>
but hides all the enormous complexity inside the system. 
<br>
<p><pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
<a href="mailto:asa@nada.kth.se?Subject=Re:%20Sysops%20and%20enlightened%20despots&In-Reply-To=&lt;20010803173731.A15573@akira.nada.kth.se&gt;">asa@nada.kth.se</a>                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2971.html">Reason: "RE: Stem Cell Debate --Banned in the USA ---&gt; libertarian societies as  unicorn"</a>
<li><strong>Previous message:</strong> <a href="2969.html">Sabine Atkins: "LISTS: Extropian dating mailing list now available"</a>
<li><strong>In reply to:</strong> <a href="2953.html">Brian Atkins: "Re: Stem Cell Debate --Banned in the USA ---&gt; libertarian societies as  unicorn"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2974.html">Reason: "RE: Sysops and enlightened despots"</a>
<li><strong>Reply:</strong> <a href="2974.html">Reason: "RE: Sysops and enlightened despots"</a>
<li><strong>Reply:</strong> <a href="2994.html">Brian Atkins: "Re: Sysops and enlightened despots"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2970">[ date ]</a>
<a href="index.html#2970">[ thread ]</a>
<a href="subject.html#2970">[ subject ]</a>
<a href="author.html#2970">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Fri Oct 12 2001 - 14:40:01 MDT</em>
</em>
</small>
</body>
</html>
