<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: GAC</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: GAC">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: GAC</h1>
<!-- received="Mon Jul  2 10:46:28 2001" -->
<!-- isoreceived="20010702164628" -->
<!-- sent="Mon, 02 Jul 2001 12:46:28 -0400" -->
<!-- isosent="20010702164628" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: GAC" -->
<!-- id="3B40A564.82D9970C@pobox.com" -->
<!-- inreplyto="021401c1030e$3bb61a20$94159818@jessicap.vtr.net" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20GAC&In-Reply-To=&lt;3B40A564.82D9970C@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Jul 02 2001 - 10:46:28 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0093.html">Spudboy100@aol.com: "David Gelernter &amp; Scopeware"</a>
<li><strong>Previous message:</strong> <a href="0091.html">hal@finney.org: "Re: Movie Review - AI Artificial Intelligence"</a>
<li><strong>In reply to:</strong> <a href="0085.html">Chris & Jessie McKinstry: "GAC"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0108.html">Amara D. Angelica: "RE: GAC"</a>
<li><strong>Reply:</strong> <a href="0108.html">Amara D. Angelica: "RE: GAC"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#92">[ date ]</a>
<a href="index.html#92">[ thread ]</a>
<a href="subject.html#92">[ subject ]</a>
<a href="author.html#92">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Chris &amp; Jessie McKinstry wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I just joined this list after Amara Angelica from KurzweilAI pointed out
</em><br>
<em>&gt; that there was some talk about GAC in this group. I've looked at some of the
</em><br>
<em>&gt; posts and would like to make some comments:
</em><br>
<p>Hello, Chris, welcome on board.  If my comment was one of the ones you
<br>
read, well, I hope you weren't too offended, but I stand by all of it. 
<br>
&quot;Basically friendly toward people, unremittingly harsh toward ideas&quot; is my
<br>
motto.
<br>
<p><em>&gt; 1 - GAC is a black box. I have made no disclosures on what it uses for
</em><br>
<em>&gt; pattern matching (not that it should matter if you place any value in the
</em><br>
<em>&gt; Turing Test which bars knowledge of the internals of a system.) But, I am
</em><br>
<em>&gt; doing experiments now with SOMs and SRNs.
</em><br>
<p>(SOMs and SRNs:  Self-Organizing Maps and Simple Recurrent Networks.)
<br>
<p>The fact that GAC is a black box is not encouraging, at least to me.  In
<br>
the continuing fight between the group that believes consciousness to be
<br>
fundamentally a simple algorithm, and the group that thinks you have to
<br>
build and build and build before the AI has enough internal complexity to
<br>
represent the single thought &quot;Hello, world&quot;, I'm with the latter group. 
<br>
In fact, I would identify my stance with Tooby-and-Cosmides and modern
<br>
functional neuroanatomy rather than any of the traditional AI sources. 
<br>
Anyway, if GAC is a black box, it says to me that you think a simple
<br>
algorithm lies at the core; if you're playing with SOMs and SRNs, it says
<br>
to me that the internal functionality complexity of GAC is almost nil.
<br>
<p><em>&gt; 2 - The primary purpose of GAC is to build a fitness test for humanness in a
</em><br>
<em>&gt; binary response domain. This will in the future allow GAC to babysit a truly
</em><br>
<em>&gt; evolving artificial consciousness, rewarding and punishing it as needed at
</em><br>
<em>&gt; machine speeds.
</em><br>
<p>That certainly isn't what it says on your Website.  On your Website, it
<br>
says things along the lines of:  GAC!  The new revolution in AI!  The
<br>
first step towards true artificial consciousness!  We're teaching it what
<br>
it means to be human!
<br>
<p>I judge GAC according to those claims.  (I have no objection to someone
<br>
claiming a revolution in AI, of course; it's a legitimate claim that can
<br>
be legitimately argued.  It's just that in this case, I think the claim
<br>
turns out to be totally, totally wrong.)
<br>
<p><em>&gt; 2.5 - The key to evolving anything is the fitness test. If I want to evolve
</em><br>
<em>&gt; a picture of the Mona Lisa, then I need a fitness test for the Mona Lisa. A
</em><br>
<em>&gt; good fitness test for the Mona Lisa would be a copy of an image of the Mona
</em><br>
<em>&gt; Lisa. To rate the quality of an evolving picture, I would just need to
</em><br>
<em>&gt; compare pixels. The next best thing to using an image would be   random
</em><br>
<em>&gt; sample of pixels; the larger the sample, the better will be the evolved
</em><br>
<em>&gt; copy. Right now, GAC is a 50,000+ term fitness test for humanness. At each
</em><br>
<em>&gt; one of those points GAC knows what it should expect it were testing an
</em><br>
<em>&gt; average human, because for each one of those points GAC has made at least 20
</em><br>
<em>&gt; measurements of real people.
</em><br>
<p>I disagree with this whole approach to AI, of course, since GAC is mostly
<br>
a useful fitness test for Cyc, and I think that Cyc is also on the wrong
<br>
tack.  In fact, I might even go farther than that, and state that, like a
<br>
static image of the Mona Lisa, GAC is chiefly useful for evolving another
<br>
copy of GAC; you can't use it to evolve Leonardo da Vinci or even good
<br>
pictures in general.  Regardless, my chief objection is that you are not
<br>
selling a &quot;fitness test&quot;, you are selling a &quot;generic artificial
<br>
consciousness&quot;. 
<br>
<p><em>&gt; 3 - Any contradictions in GAC are real contradictions in us. It can't
</em><br>
<em>&gt; believe anything that hasn't been confirmed by at least 20 people.
</em><br>
<p>Okay.  You do realize that two groups of 20 people can quite often have
<br>
inter-person disagreements that would never be represented within a single
<br>
mind?  There are rather more than 20 people who would confirm the
<br>
statements &quot;Christianity is the one true religion&quot;, &quot;Judaism is the one
<br>
true religion&quot;, &quot;Hinduism is the one true religion&quot;, and &quot;Zoroastrianism
<br>
is the one true religion&quot;, but very few people who would believe all of
<br>
these things simultaneously.  But, what the heck, I'm being pedantic; I
<br>
agree with (3) as stated.
<br>
<p><em>&gt; 4 - GAC is science. Over 8 million actual measurements of human consensus
</em><br>
<em>&gt; have been made. There are at least two other projects that claim to be
</em><br>
<em>&gt; collecting human consensus information - CYC and Open Mind - neither has
</em><br>
<em>&gt; actually done the science to verify that what is in their databases is
</em><br>
<em>&gt; actually consensus human fact. It's all hearsay until the each item is
</em><br>
<em>&gt; presented to at least 20 people (central limit theorem.)
</em><br>
<p>4.1:  I'm not fond of Cyc either.  But Cyc isn't claiming to collect human
<br>
consensus information; rather, they are claiming to collect the
<br>
commonsense knowledge that one human might be expected to have.  I think
<br>
Cyc has nothing but a bunch of suggestively named LISP predicates.  If
<br>
they *were* collecting knowledge, however, what would be relevant would
<br>
not be whether the knowledge was true, or whether it was consensual, but
<br>
whether it duplicated the relevant functional complexity of the
<br>
commonsense knowledge possessed by a single human mind.
<br>
<p>4.2:  Performing lots and lots of actual measurements does not make it
<br>
science.  To make it science, you need to come up with a central
<br>
hypothesis about AI or human cognition, use the hypothesis to make a
<br>
prediction, and use those lots and lots of measurements to test that
<br>
prediction.  Analogously, I would also note that until GAC can use its
<br>
pixels to predict new pixels, it is not &quot;AI&quot; in even the smallest sense;
<br>
it remains a frozen picture, possibly useful as a fitness test for some
<br>
other AI (I disagree), but not intelligent in itself; as unthinking as the
<br>
binay JPEG data of the Mona Lisa.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0093.html">Spudboy100@aol.com: "David Gelernter &amp; Scopeware"</a>
<li><strong>Previous message:</strong> <a href="0091.html">hal@finney.org: "Re: Movie Review - AI Artificial Intelligence"</a>
<li><strong>In reply to:</strong> <a href="0085.html">Chris & Jessie McKinstry: "GAC"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0108.html">Amara D. Angelica: "RE: GAC"</a>
<li><strong>Reply:</strong> <a href="0108.html">Amara D. Angelica: "RE: GAC"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#92">[ date ]</a>
<a href="index.html#92">[ thread ]</a>
<a href="subject.html#92">[ subject ]</a>
<a href="author.html#92">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Fri Oct 12 2001 - 14:39:41 MDT</em>
</em>
</small>
</body>
</html>
