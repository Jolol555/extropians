<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Hawking on AI dominance</title>
<meta name="Author" content="J. R. Molloy (jr@shasta.com)">
<meta name="Subject" content="Re: Hawking on AI dominance">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Hawking on AI dominance</h1>
<!-- received="Mon Sep 10 13:33:04 2001" -->
<!-- isoreceived="20010910193304" -->
<!-- sent="Mon, 10 Sep 2001 12:33:00 -0700" -->
<!-- isosent="20010910193300" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: Hawking on AI dominance" -->
<!-- id="04cc01c13a2f$6a5c6ac0$be5c2a42@jrmolloy" -->
<!-- inreplyto="009101c13987$20721480$8ec9868b@rjblackford" -->
<strong>From:</strong> J. R. Molloy (<a href="mailto:jr@shasta.com?Subject=Re:%20Hawking%20on%20AI%20dominance&In-Reply-To=&lt;04cc01c13a2f$6a5c6ac0$be5c2a42@jrmolloy&gt;"><em>jr@shasta.com</em></a>)<br>
<strong>Date:</strong> Mon Sep 10 2001 - 13:33:00 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6014.html">Brian Phillips: "Re: Singularity: Can't happen here"</a>
<li><strong>Previous message:</strong> <a href="6012.html">Samantha Atkins: "Re: Singularity: can't happen here"</a>
<li><strong>In reply to:</strong> <a href="5963.html">Russell Blackford: "Re: Hawking on AI dominance"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6022.html">Party of Citizens: "Robo-Mathematician"</a>
<li><strong>Reply:</strong> <a href="6022.html">Party of Citizens: "Robo-Mathematician"</a>
<li><strong>Reply:</strong> <a href="6023.html">Party of Citizens: "Pope Roboticus I"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6013">[ date ]</a>
<a href="index.html#6013">[ thread ]</a>
<a href="subject.html#6013">[ subject ]</a>
<a href="author.html#6013">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
From: &quot;Russell Blackford&quot; &lt;<a href="mailto:RussellBlackford@bigpond.com?Subject=Re:%20Hawking%20on%20AI%20dominance&In-Reply-To=&lt;04cc01c13a2f$6a5c6ac0$be5c2a42@jrmolloy&gt;">RussellBlackford@bigpond.com</a>&gt;
<br>
<em>&gt; You seem to be putting a position
</em><br>
<em>&gt; that the only, or the overriding, value is the ability to solve problems.
</em><br>
<em>&gt; But isn't this a value judgment?
</em><br>
<p>Define intelligence as the ability to solve problems and answer questions, and
<br>
think of this as a definition, instead of a value judgment, and the positional
<br>
disparity dissolves. BTW, I stole this definition from Dr. Francis Heylighen.
<br>
It could be the &quot;value&quot; of AI, I suppose, if one wants to conflate &quot;strong AI&quot;
<br>
with value. But I don't see the &quot;value&quot; of &quot;strong AI&quot; at issue in this
<br>
discussion. Hawking apparently envisions &quot;strong AI&quot; as potentially
<br>
tryrannical and dangerous (the myth of the monstrous machine), while Kurzweil
<br>
invokes &quot;human values&quot; as a remedy (another version of Eliezer's &quot;Friendly AI&quot;
<br>
scenario).
<br>
<p>To review, Hawking's position is:
<br>
``We must develop as quickly as possible technologies that make possible a
<br>
direct connection between brain and computer, so that artificial brains
<br>
contribute to human intelligence rather than opposing it,'' and we should do
<br>
this &quot;if we want biological systems to remain superior to electronic ones.''
<br>
<p>Ray Kurzweil disagreed, stating:
<br>
<em>&gt; I don't agree with Hawking that &quot;strong
</em><br>
<em>&gt; AI&quot; is a fate to be avoided. I do believe that we have the ability to shape
</em><br>
<em>&gt; this destiny to reflect our human values, if only we could achieve a
</em><br>
<em>&gt; consensus on what those are.
</em><br>
<p>I agree with Kurzweil that &quot;strong AI&quot; is _not_ to be avoided. What's more, I
<br>
think &quot;strong AI&quot; is to be diligently sought and enthusiastically welcomed.
<br>
Nevertheless, I disagree with Kurzweil's invocation of &quot;human values&quot; because
<br>
it seems to me that subjective values of any kind contradict and impede the
<br>
emergence of &quot;strong AI.&quot;
<br>
(&quot;strong AI&quot; = human-competitive AI, which can quickly evolve to artificial
<br>
superintelligence)
<br>
<p><em>&gt; I place a great value on
</em><br>
<em>&gt; intelligence/problem solving ability as well, but I also think there's a lot
</em><br>
<em>&gt; of truth in the dictum that I can't quite remember sufficiently well to
</em><br>
<em>&gt; quote accurately from Hume - the one about reason being the slave of the
</em><br>
<em>&gt; passions. The passions, in turn, doubtless have a biological basis. I'm
</em><br>
<em>&gt; finding it very hard imagining an intelligence totally devoid of &quot;passions&quot;
</em><br>
<em>&gt; or why it would be a good thing. I'm not even as confident as you that such
</em><br>
<em>&gt; an intelligence would have greater problem-solving power. What would
</em><br>
<em>&gt; *motivate* it to solve problems if it had no values at all?
</em><br>
<p>Hume was a philosopher, right? &gt;poof&lt; ...there goes his credibility. ©¿©¬
<br>
Just kidding. Actually I think that making reason a slave rather than a master
<br>
explains much of human misery and sorrow. Although human intelligence has its
<br>
origin in biology, it does not follow that intelligence must remain tethered
<br>
to it. Hence, we have the transhumanist movement.
<br>
Nothing _intrinsic_ motivates AI to do anything at all, which is precisely why
<br>
Hawking's notion of AI &quot;opposing&quot; human intelligence is preposterous. By
<br>
itself, without input from an external operant, AI does nothing It just sits
<br>
there like a perfectly enlightened master, like a supercomputer on idle, like
<br>
a programmable calculator awaiting the next factor. &quot;Sitting quietly, doing
<br>
nothing, Spring comes, and the grass grows by itself,&quot; as Li Po might put it.
<br>
Such an intelligence (some prototypes of which have been demonstrated) has
<br>
greater (than human) problem-solving power because its reasoning is based on a
<br>
more accurate model of reality -- one that is not weighted by the biases and
<br>
distortions of human values.
<br>
<p><em>&gt; Shouldn't it at
</em><br>
<em>&gt; least be motivated by curiosity?
</em><br>
<p>I don't see any reason to embellish AI with anthropomorphic features.
<br>
Heuristic algorithms have commonly functioned as &quot;curiosity&quot; in intelligent
<br>
agents such as web crawlers and gophers to retrieve information. Beyond the
<br>
acquisition of knowledge, &quot;curiosity&quot; does not compute, and in fact would be
<br>
counterproductive in AI.
<br>
<p><em>&gt; Moreover, why should we give up our values
</em><br>
<em>&gt; which are based wholly or partly on our biological interests?
</em><br>
<p>What do you mean &quot;we,&quot; meat puppet? ©¿©¬
<br>
OK, I'm kidding again... hope you see the value of humor.
<br>
We should give up our values because they are fake, phony, and fraudulent.
<br>
People say they are fighting over &quot;values&quot; when they go to war (which involves
<br>
killing, looting, raping, etc.), but what they're really fighting over is
<br>
territory, resources, and &quot;biological interests.&quot; These &quot;values&quot; are what
<br>
eventuate in global suicide via total war. As Eliezer figured out years ago,
<br>
the human race stands at a crossroads now: One road leads to punctuated
<br>
evolution and a quantum leap to a biological phase transition, aka the
<br>
singularity. The other road leads to dystopian nightmare police states and
<br>
&quot;Outlaw School&quot; totalitarian political systems.
<br>
AI (&quot;strong AI&quot; -- see note below) is the technology that makes the phase
<br>
transition beyond biological interests possible. By some definitions, A-life
<br>
is still a form of &quot;biology&quot; but I concede to those who prefer the term
<br>
&quot;vitology.&quot;
<br>
<p><em>&gt; Also, don't
</em><br>
<em>&gt; you think, given that a lot of problem solving uses hypothetico-deductive
</em><br>
<em>&gt; reasoning, not purely deductive reasoning, that it might be very hard
</em><br>
<em>&gt; developing a system capable of conjectures and yet with no values?
</em><br>
<p>The real problem is that we have an over-abundance of conjectures already, so
<br>
no, I don't see any difficulty here. To clarify this point, differentiate
<br>
between solving problems via reasoning versus *discovering* solutions via
<br>
pattern recognition. Discovering solutions by recognizing patterns is a
<br>
sub-set of the more inclusive term &quot;problem solving.&quot; We don't need to develop
<br>
a system capable of conjecture because we already have such a system. It's
<br>
called the human brain, and it has managed to fill the world with conjectures.
<br>
As I envision it, &quot;strong AI,&quot; just like any other intelligence, does not
<br>
function in isolation, but rather in relation to others. The difficulty for
<br>
humans in this new society of intelligent entities will be *accepting* the
<br>
solutions provided by machines that are more intelligent than the most intelli
<br>
gent humans. Do you think the Pope will be able to accept solutions offered by
<br>
machine intelligence which contradict the dogma of the church? Remember
<br>
Galileo?
<br>
<p><em>&gt; It seems
</em><br>
<em>&gt; to me that even being conscious would give the system something analogous to
</em><br>
<em>&gt; biological interests. If the system isn't conscious, why should we value it
</em><br>
<em>&gt; except as a useful tool to our ends, based on *our* values? Etc?
</em><br>
<p>Well, since I don't believe in &quot;consciousness,&quot; perhaps we can let the &quot;strong
<br>
AI&quot; decide for itself how to handle this kind of question when the occasion
<br>
arises. As for why we should &quot;value&quot; the AI system, I think you've answered
<br>
that yourself when you refer to it as &quot;a useful tool.&quot; Initially, that's
<br>
exactly why developers try to increase the intelligence of systems, because
<br>
that makes them more useful. This is based on the performance capabilities of
<br>
the machine intelligence under consideration. For example, a system which
<br>
yields a ten million dollar profit is twice as valuable as one which nets a
<br>
mere five million.
<br>
<p><em>&gt; I, in my turn, suspect that you may have answers to these questions, but I
</em><br>
<em>&gt; haven't seen anything convincing from you so far. Do you want to spell it
</em><br>
<em>&gt; out?
</em><br>
<p>It's not my intention to &quot;convince&quot; you of anything, but you're right, it
<br>
makes me happy to spell it out for you.
<br>
<p><a href="http://www.ecs.soton.ac.uk/~phl/ctit/ho1/node1.html">http://www.ecs.soton.ac.uk/~phl/ctit/ho1/node1.html</a>
<br>
The Strong AI view says: the brain is a complex information processing
<br>
machine, but &quot;consciousness&quot; and understanding are by products of the
<br>
complicated symbol manipulation when we process information. We will
<br>
eventually be able to model this process and reproduce it.
<br>
The Weak AI view says: the brain is something more than an information
<br>
processing machine and although we will be able to model some of its
<br>
functionality, it will never be possible to model all the properties of the
<br>
brain.
<br>
<p>©¿©¬
<br>
<p>Stay hungry,
<br>
<p>--J. R.
<br>
<p>Useless hypotheses, etc.:
<br>
&nbsp;consciousness, phlogiston, philosophy, vitalism, mind, free will, qualia,
<br>
analog computing, cultural relativism, GAC, Cyc, Eliza, cryonics, individual
<br>
uniqueness, ego, human values
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Everything that can happen has already happened, not just once,
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;but an infinite number of times, and will continue to do so forever.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Everything that can happen = more than anyone can imagine.)
<br>
<p>We won't move into a better future until we debunk religiosity, the most
<br>
regressive force now operating in society.
<br>
<a href="http://groups.yahoo.com/group/Virtropy/message/2949">http://groups.yahoo.com/group/Virtropy/message/2949</a>
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6014.html">Brian Phillips: "Re: Singularity: Can't happen here"</a>
<li><strong>Previous message:</strong> <a href="6012.html">Samantha Atkins: "Re: Singularity: can't happen here"</a>
<li><strong>In reply to:</strong> <a href="5963.html">Russell Blackford: "Re: Hawking on AI dominance"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6022.html">Party of Citizens: "Robo-Mathematician"</a>
<li><strong>Reply:</strong> <a href="6022.html">Party of Citizens: "Robo-Mathematician"</a>
<li><strong>Reply:</strong> <a href="6023.html">Party of Citizens: "Pope Roboticus I"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6013">[ date ]</a>
<a href="index.html#6013">[ thread ]</a>
<a href="subject.html#6013">[ subject ]</a>
<a href="author.html#6013">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Fri Oct 12 2001 - 14:40:27 MDT</em>
</em>
</small>
</body>
</html>
