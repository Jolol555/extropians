<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Hawking on AI dominance</title>
<meta name="Author" content="Mike Lorrey (mlorrey@datamann.com)">
<meta name="Subject" content="Re: Hawking on AI dominance">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Hawking on AI dominance</h1>
<!-- received="Sun Sep  9 18:42:05 2001" -->
<!-- isoreceived="20010910004205" -->
<!-- sent="Sun, 09 Sep 2001 20:45:39 -0400" -->
<!-- isosent="20010910004539" -->
<!-- name="Mike Lorrey" -->
<!-- email="mlorrey@datamann.com" -->
<!-- subject="Re: Hawking on AI dominance" -->
<!-- id="3B9C0D33.ADD756B5@datamann.com" -->
<!-- inreplyto="009101c13987$20721480$8ec9868b@rjblackford" -->
<strong>From:</strong> Mike Lorrey (<a href="mailto:mlorrey@datamann.com?Subject=Re:%20Hawking%20on%20AI%20dominance&In-Reply-To=&lt;3B9C0D33.ADD756B5@datamann.com&gt;"><em>mlorrey@datamann.com</em></a>)<br>
<strong>Date:</strong> Sun Sep 09 2001 - 18:45:39 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5965.html">Damien Broderick: "Re: Singularity: can't happen here"</a>
<li><strong>Previous message:</strong> <a href="5963.html">Russell Blackford: "Re: Hawking on AI dominance"</a>
<li><strong>In reply to:</strong> <a href="5963.html">Russell Blackford: "Re: Hawking on AI dominance"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5974.html">Eliezer S. Yudkowsky: "Re: Hawking on AI dominance"</a>
<li><strong>Reply:</strong> <a href="5974.html">Eliezer S. Yudkowsky: "Re: Hawking on AI dominance"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5964">[ date ]</a>
<a href="index.html#5964">[ thread ]</a>
<a href="subject.html#5964">[ subject ]</a>
<a href="author.html#5964">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Russell Blackford wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; JR said
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;From an evolutionary psychology standpoint, human values evolve from
</em><br>
<em>&gt; human needs, which are tied to biological needs.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Which is a pretty good answer the question I asked. My own answer would
</em><br>
<em>&gt; probably be fairly similar, though I'd distinguish (as you might, too)
</em><br>
<em>&gt; various kinds of needs. For example, my needs as a particular biological
</em><br>
<em>&gt; organism might differ from what was needed by my ancestors for inclusive
</em><br>
<em>&gt; fitness in the evolutionary environment.
</em><br>
<p>Yes, this is a good statement, and one can clearly see by layers of
<br>
abstraction that biological needs are dictated by the environment, which
<br>
are dictated by the laws of nature. 
<br>
<p>On that basis, in order for technological intelligence to develop and
<br>
thrive in this universe, it must behave/fit in a particular niche of
<br>
evolved behavior, at least part of which is constructed of those pesky
<br>
'values'.
<br>
<p>This is obviously highly qualified by issues of anthropic principles and
<br>
Fermi Paradoxes: the lack of any alternatives, so far as we can tell,
<br>
demonstrates the need for those factors by which the intelligences 
<br>
known as 'humans' developed. 
<br>
<p>Until presented with any alternatives, we have to act from this default
<br>
state, and trying to start from scratch is more likely to result in
<br>
failure to achieve AI than not. Therefore, any AI we develop will likely
<br>
act and behave mighty human-like, with at least *some* human values. As
<br>
an intelligence with higher abilities to self-modify, whether it retains
<br>
those values will obviously depend upon whether human values are as
<br>
objective as some think, as well as whether we actually hard-wire them
<br>
in to build &quot;A Friendly AI&quot;.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; JR added:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;I suspect you have your own opinions about where values come from.
</em><br>
<em>&gt; Regardless
</em><br>
<em>&gt; of where they come from, human values are not necessary for pure
</em><br>
<em>&gt; intelligence,
</em><br>
<em>&gt; and my conjecture is that they interfere with pure intelligence (which is
</em><br>
<em>&gt; the
</em><br>
<em>&gt; ability to solve problems autonomously).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; JR, I'm just trying to get a handle on your thinking. I *think* I now see
</em><br>
<em>&gt; why you say an AI with values would be (in a sense) &quot;weak AI&quot;. Actually, I
</em><br>
<em>&gt; assumed you had something like this in mind but wasn't sure.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You are, of course, redefining the terms &quot;strong AI&quot; and &quot;weak AI&quot;, but I
</em><br>
<em>&gt; realise you are doing so deliberately for rhetorical effect, so that's okay.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Can I take this a bit further, however? You seem to be putting a position
</em><br>
<em>&gt; that the only, or the overriding, value is the ability to solve problems.
</em><br>
<em>&gt; But isn't this a value judgment? I'm not trying to be cute, just trying to
</em><br>
<em>&gt; see how you get to this point. 
</em><br>
<p>Yes, as others have pointed out in other issues, this is called
<br>
'stealing the assumption'.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5965.html">Damien Broderick: "Re: Singularity: can't happen here"</a>
<li><strong>Previous message:</strong> <a href="5963.html">Russell Blackford: "Re: Hawking on AI dominance"</a>
<li><strong>In reply to:</strong> <a href="5963.html">Russell Blackford: "Re: Hawking on AI dominance"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5974.html">Eliezer S. Yudkowsky: "Re: Hawking on AI dominance"</a>
<li><strong>Reply:</strong> <a href="5974.html">Eliezer S. Yudkowsky: "Re: Hawking on AI dominance"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5964">[ date ]</a>
<a href="index.html#5964">[ thread ]</a>
<a href="subject.html#5964">[ subject ]</a>
<a href="author.html#5964">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Fri Oct 12 2001 - 14:40:27 MDT</em>
</em>
</small>
</body>
</html>
