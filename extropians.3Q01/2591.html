<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=Windows-1252">
<title>extropians: FW: Singularity vs. Ronald Mcdonald</title>
<meta name="Author" content="Ben Goertzel (ben@webmind.com)">
<meta name="Subject" content="FW: Singularity vs. Ronald Mcdonald">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>FW: Singularity vs. Ronald Mcdonald</h1>
<!-- received="Mon Jul 30 08:56:09 2001" -->
<!-- isoreceived="20010730145609" -->
<!-- sent="Mon, 30 Jul 2001 10:44:14 -0400" -->
<!-- isosent="20010730144414" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@webmind.com" -->
<!-- subject="FW: Singularity vs. Ronald Mcdonald" -->
<!-- id="NDBBIBGFAPPPBODIPJMMIEKJFNAA.ben@webmind.com" -->
<!-- charset="Windows-1252" -->
<!-- inreplyto="Singularity vs. Ronald Mcdonald" -->
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@webmind.com?Subject=FW:%20Singularity%20vs.%20Ronald%20Mcdonald&In-Reply-To=&lt;NDBBIBGFAPPPBODIPJMMIEKJFNAA.ben@webmind.com&gt;"><em>ben@webmind.com</em></a>)<br>
<strong>Date:</strong> Mon Jul 30 2001 - 08:44:14 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2592.html">Harvey Newstrom: "What is the singularity?"</a>
<li><strong>Previous message:</strong> <a href="2590.html">Mike Lorrey: "Re: Property and the Law, and is it a priority?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2698.html">Stirling Westrup: "Re: FW: Singularity vs. Ronald Mcdonald"</a>
<li><strong>Reply:</strong> <a href="2698.html">Stirling Westrup: "Re: FW: Singularity vs. Ronald Mcdonald"</a>
<li><strong>Maybe reply:</strong> <a href="2738.html">Stirling Westrup: "Re: FW: Singularity vs. Ronald Mcdonald"</a>
<li><strong>Maybe reply:</strong> <a href="2740.html">Stirling Westrup: "Re: FW: Singularity vs. Ronald Mcdonald"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2591">[ date ]</a>
<a href="index.html#2591">[ thread ]</a>
<a href="subject.html#2591">[ subject ]</a>
<a href="author.html#2591">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
<em>&gt; Date: Sun, 29 Jul 2001 11:27:57 EST
</em><br>
<em>&gt; From: &quot;Stirling Westrup&quot; &lt;<a href="mailto:sti@cam.org?Subject=FW:%20Singularity%20vs.%20Ronald%20Mcdonald&In-Reply-To=&lt;NDBBIBGFAPPPBODIPJMMIEKJFNAA.ben@webmind.com&gt;">sti@cam.org</a>&gt;
</em><br>
<em>&gt; Subject: Re: Big Bang is Bunk
</em><br>
<em>&gt;
</em><br>
<em>&gt; Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; The Singularity is defined to occur as soon as any
</em><br>
greater-than-human
<br>
<em>&gt; &gt; intelligence comes into existence - big or small.  It has to be
</em><br>
genuine,
<br>
<em>&gt; &gt; hardware transhumanity, not just humans put together in
</em><br>
<em>&gt; interesting shapes,
</em><br>
<em>&gt; &gt; but that's it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; By who's definition?
</em><br>
<p>Not mine.
<br>
<p>Counterargument 1)
<br>
<p>A transhuman intelligence could conceivably come about *without* having
<br>
far-reaching effects for humanity as a whole.
<br>
<p>An example of how this could come about is given in Stanislaw Lem's
<br>
story
<br>
about the AI &quot;Honest Annie,&quot; which as soon as it became
<br>
superintelligent,
<br>
cut off its communications with humans
<br>
<p><p>Counterargument 2)
<br>
<p>It's not inconceivable either that we could create superb
<br>
nanotechnology,
<br>
molecular assemblers, etc., but discover that for complex reasons of
<br>
physical law and complexity science, there is NO WAY TO MAKE AN
<br>
INTELLIGENCE
<br>
SIGNIFICANTLY GREATER THAN HUMAN INTELLIGENCE.  I think this is
<br>
immensely
<br>
unlikely but it's not impossible.  In this case we could reshape the
<br>
solar
<br>
system into a perfect image of Ronald McDonald's butt, create free food
<br>
for
<br>
all and infinite molecularly-induced orgasmic bliss on command, travel
<br>
between universes on quark-powered Razor scooters, and do all sorts of
<br>
other
<br>
funky-cool Singularity-ish stuff, but NOT create superhuman
<br>
intelligence...
<br>
<p><p><p>I happen to agree with Eli that the creation of transhuman intelligence
<br>
is
<br>
both the most likely path to a Singularity, and very likely to lead to a
<br>
Singularity if it occurs.  But these statements can't reasonably be
<br>
posited
<br>
with 100% certainty, and hence they're not the *definition* of the
<br>
technological Singularity.
<br>
<p>-- Ben G
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2592.html">Harvey Newstrom: "What is the singularity?"</a>
<li><strong>Previous message:</strong> <a href="2590.html">Mike Lorrey: "Re: Property and the Law, and is it a priority?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2698.html">Stirling Westrup: "Re: FW: Singularity vs. Ronald Mcdonald"</a>
<li><strong>Reply:</strong> <a href="2698.html">Stirling Westrup: "Re: FW: Singularity vs. Ronald Mcdonald"</a>
<li><strong>Maybe reply:</strong> <a href="2738.html">Stirling Westrup: "Re: FW: Singularity vs. Ronald Mcdonald"</a>
<li><strong>Maybe reply:</strong> <a href="2740.html">Stirling Westrup: "Re: FW: Singularity vs. Ronald Mcdonald"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2591">[ date ]</a>
<a href="index.html#2591">[ thread ]</a>
<a href="subject.html#2591">[ subject ]</a>
<a href="author.html#2591">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Fri Oct 12 2001 - 14:39:59 MDT</em>
</em>
</small>
</body>
</html>
