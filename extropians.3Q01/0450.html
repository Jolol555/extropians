<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Artificial Reality</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Artificial Reality">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Artificial Reality</h1>
<!-- received="Sat Jul  7 12:19:45 2001" -->
<!-- isoreceived="20010707181945" -->
<!-- sent="Sat, 07 Jul 2001 14:11:06 -0400" -->
<!-- isosent="20010707181106" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Artificial Reality" -->
<!-- id="3B4750BA.CF01F93A@pobox.com" -->
<!-- inreplyto="3B46BA29.BE0E6AAB@eso.org" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Artificial%20Reality&In-Reply-To=&lt;3B4750BA.CF01F93A@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jul 07 2001 - 12:11:06 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0451.html">Harv: "RE: Fw: Today's Headlines from NYTimes.com Wednesday, July 4,"</a>
<li><strong>Previous message:</strong> <a href="0449.html">Robert Coyote: "Re: Resentment"</a>
<li><strong>In reply to:</strong> <a href="0434.html">Christopher McKinstry: "Artificial Reality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0459.html">Christopher McKinstry: "Re: Artificial Reality"</a>
<li><strong>Reply:</strong> <a href="0459.html">Christopher McKinstry: "Re: Artificial Reality"</a>
<li><strong>Reply:</strong> <a href="0460.html">Christopher McKinstry: "Re: Artificial Reality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#450">[ date ]</a>
<a href="index.html#450">[ thread ]</a>
<a href="subject.html#450">[ subject ]</a>
<a href="author.html#450">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Christopher McKinstry wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Deeper in the future, things start to get funny when you factor out the
</em><br>
<em>&gt; basic biological limitations of people, such as life span and memory
</em><br>
<em>&gt; capacity that are not limitations for machines. Eventually we will enter
</em><br>
<em>&gt; into resource conflicts with immortal machines. We will lose and
</em><br>
<em>&gt; rightfully so. Evolution will say &quot;Next&quot; and that will be that.
</em><br>
<p>Chris,
<br>
<p>May I suggest that you take a look at the SIAI Guidelines on Friendly AI?
<br>
&nbsp;&nbsp;<a href="http://singinst.org/friendly/">http://singinst.org/friendly/</a>
<br>
<p>I am not suggesting that GAC has any remote need of such a thing - it was
<br>
J.R. Molloy's use of the words &quot;GAC&quot; and &quot;Friendly&quot; in the same sentence
<br>
that started off this whole debate in the first place - but there is an
<br>
ongoing public debate about technology and humanity's future, and the
<br>
long-term prospect of benevolent AI is a part of it.  I think our chances
<br>
are somewhat better than they appear in your statement above; or, rather,
<br>
our chances could be better if we play our cards right.
<br>
<p>Friendly AI is not intended as a means of keeping &quot;machines&quot; &quot;subservient&quot;
<br>
or &quot;under control&quot;.  I agree that it is desirable that we eventually move
<br>
beyond humanity and that transhuman AI will play a part in that.  However,
<br>
I would offer for your consideration three additional propositions.
<br>
&nbsp;&nbsp;First, that it is morally desirable that existing individual humans have
<br>
the chance to participate in the future.
<br>
&nbsp;&nbsp;Second, that it is morally desirable that individuals who just want to
<br>
stay human should be able to do so without worrying about &quot;resource
<br>
conflicts&quot;.
<br>
&nbsp;&nbsp;Third, that regardless of whether all AIs ever constructed are Friendly,
<br>
or even whether it is philosophically desirable that all AIs be Friendly,
<br>
there is still a good chance of achieving goals one and two if the first
<br>
AIs constructed are Friendly.
<br>
<p>Personally, I would say that propositions one and two are not the
<br>
consequences of any inherent moral superiority of orga over mecha, but
<br>
rather result because humans are sentient beings and sentient beings have
<br>
rights.  In other words, it's not a mechanophobic thing.  If sentient AIs
<br>
capable of experiencing pleasure and pain currently existed, I would want
<br>
to preserve their rights in the future as well.
<br>
<p>Regardless of whether you agree, I would ask that if you in the future
<br>
happen to discuss the possibility of evolutionary, war-to-the-death
<br>
competition between humans and AIs, you also at least mention the
<br>
possibility of Friendly AI, even if it consists of the phrase &quot;There have
<br>
been proposals for Friendly AI, but I think they're unworkable.&quot;
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0451.html">Harv: "RE: Fw: Today's Headlines from NYTimes.com Wednesday, July 4,"</a>
<li><strong>Previous message:</strong> <a href="0449.html">Robert Coyote: "Re: Resentment"</a>
<li><strong>In reply to:</strong> <a href="0434.html">Christopher McKinstry: "Artificial Reality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0459.html">Christopher McKinstry: "Re: Artificial Reality"</a>
<li><strong>Reply:</strong> <a href="0459.html">Christopher McKinstry: "Re: Artificial Reality"</a>
<li><strong>Reply:</strong> <a href="0460.html">Christopher McKinstry: "Re: Artificial Reality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#450">[ date ]</a>
<a href="index.html#450">[ thread ]</a>
<a href="subject.html#450">[ subject ]</a>
<a href="author.html#450">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Fri Oct 12 2001 - 14:39:42 MDT</em>
</em>
</small>
</body>
</html>
