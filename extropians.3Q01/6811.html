<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: A specific refutation of genocide</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="A specific refutation of genocide">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>A specific refutation of genocide</h1>
<!-- received="Sun Sep 16 21:13:27 2001" -->
<!-- isoreceived="20010917031327" -->
<!-- sent="Sun, 16 Sep 2001 23:13:25 -0400" -->
<!-- isosent="20010917031325" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="A specific refutation of genocide" -->
<!-- id="3BA56A55.8FFE92CB@pobox.com" -->
<!-- inreplyto="Pine.LNX.4.10.10109161130120.26018-100000@server.aeiveos.com" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20A%20specific%20refutation%20of%20genocide&In-Reply-To=&lt;3BA56A55.8FFE92CB@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Sep 16 2001 - 21:13:25 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6812.html">CurtAdams@aol.com: "Re: jane's on al-qaeda"</a>
<li><strong>Previous message:</strong> <a href="6810.html">Damien Broderick: "Re: TERRORISM: Is genocide the logical solution?"</a>
<li><strong>In reply to:</strong> <a href="6758.html">Robert J. Bradbury: "TERRORISM: Is genocide the logical solution?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6839.html">Robert J. Bradbury: "TERRORISM: Is genocide the logical solution?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6811">[ date ]</a>
<a href="index.html#6811">[ thread ]</a>
<a href="subject.html#6811">[ subject ]</a>
<a href="author.html#6811">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
&quot;Robert J. Bradbury&quot; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I know that by raising this issue, I am going to take a large
</em><br>
<em>&gt; amount of flak.
</em><br>
<p>Yes, you will.  Let us hope that you are defeated, and that post-defeat
<br>
you are not quoted as being representative of Extropianism.
<br>
<p>But to explain why genocide is un-Extropian requires more than horror. 
<br>
Unpopularity does not prove falsity.  So I'm going to skip the exercise
<br>
period I was about to take, and focus solely on responding to Robert
<br>
Bradbury's proposal.  I hope that his adherence to the ethical principle
<br>
of not concealing his beliefs does indeed turn out to outweigh the
<br>
specific, obvious, and predictable consequences of his posting those
<br>
beliefs to the Extropian mailing list.
<br>
<p><em>&gt; From my perspective the analysis is relatively simple.  If the
</em><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;^^
<br>
<em>&gt; population of Afganistan, or the people supported by them
</em><br>
<em>&gt; delay the onset of an era of advanced technological capabilities
</em><br>
<em>&gt; by 6 months or more, the value of their lives is negative.
</em><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;^                     ^^
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;net              will have been
<br>
<p>Post-Singularity, some members of humanity may request that their &quot;score&quot;
<br>
be totalled up (i.e., what was the net effect of their pre-Singularity
<br>
lives).  It seems likely that the scores of many individuals will be
<br>
negative, including class-action lawyers, many Congressfolk, and of course
<br>
terrorists.  It may also be that, because of the Taliban, the mean net
<br>
score of &quot;all the individuals in Afghanistan&quot; will be negative.  It may
<br>
even be that if a natural catastrophe were to wipe out the nation of
<br>
Afghanistan, superintelligences post-Singularity would view it as a
<br>
definite net positive in history, the way some historians now view the
<br>
Black Death.
<br>
<p>Nonetheless, it is impermissible for a modern-day human to kill people
<br>
whom she expects to have a negative score post-Singularity.  The action
<br>
will probably not accelerate the Singularity, and the reasoning behind it
<br>
is flawed.
<br>
<p>There are many reasons for this, and I'll probably only be able to touch
<br>
on a few, but I'll try.
<br>
<p>PART I:
<br>
<p>First, let's review the simple reasons - the ones that don't touch upon
<br>
that strangest of ethical heuristics, &quot;the end does not justify the
<br>
means&quot;.
<br>
<p>Within the first-order scenarios, we have the big &quot;IF&quot; in the logic, i.e.
<br>
&quot;IF the Afghani lives turn out to be negative&quot;.  Maybe somewhere in
<br>
Afghanistan is the next Einstein or Newton.  Maybe the Third World
<br>
countries, like Gollum, still have some part yet to play.  Maybe today's
<br>
rogue states will someday act as haven for Luddite-hunted technologists
<br>
instead of terrorists..  There are AI researchers today in Germany and
<br>
Japan.  It's easy to construct scenarios in which the first-order action
<br>
of wiping out a country goes wrong directly.
<br>
<p>Now you could say that these are all possibilities as tenuous and
<br>
constructed as Pascal's Wager, and that equal and opposite scenarios in
<br>
which Afghanistan contains the next Hitler are just as easy to construct. 
<br>
And this is true; uncomfortable, but true.  Still, there are many possible
<br>
outcomes, and if the negative and positive possible outcomes are equally
<br>
balanced, then all that remains is the definite and certain short-term
<br>
cost of twenty-five million deaths.  Stalin killed tens of millions of
<br>
people, allegedly in pursuit of benefits that turned out to be simply
<br>
nonexistent.  So the difficulty of accurately modeling the world does need
<br>
to be considered.  To the extent you really don't know the effects of a
<br>
human's life, the net value converges to the intrinsic valuation of &quot;one&quot;.
<br>
<p>Then there are the second-order reasons, having to do with the reaction of
<br>
other humans to the action of genocide.  Exterminating the Afghani would
<br>
probably send an equal or much greater number of states rogue, and result
<br>
in the rise of hundreds of new terrorist organizations.  Justifiably so! 
<br>
Using nuclear weapons against Afghanistan might easily result in the use
<br>
of nuclear weapons against the United States and would most certainly
<br>
render the United States a pariah among nations.  International law would
<br>
be dead.  The global political situation would be destabilized enormously.
<br>
<p>Within this list, even the proposal of committing genocide has damaged
<br>
your reputation.  Many respondents note that you have probably just
<br>
damaged the Extropian cause and managed to slow down the Singularity a
<br>
little.  Maybe this is a necessary cost of ethically required honesty, but
<br>
it's a cost nonetheless.
<br>
<p>Of course, the second-order reasons, as presented, are strictly
<br>
environmental; they note that people react badly to genocide without
<br>
considering whether or not genocide is unethical and thus whether the
<br>
reactions are rational.  It is sometimes ethical to defy the majority
<br>
opinion, although that doesn't change the real consequences of doing so.
<br>
<p>PART II
<br>
<p>The bad reaction to the proposed genocide of Afghanistan *is* rational,
<br>
and the proposal *is* unethical.
<br>
<p>In Robert Bradbury's worldview, there is a certain future event such that
<br>
the delay of this future event by one additional day results in 150,000
<br>
additional unnecessary deaths.  (A feature shared by my own worldview as
<br>
well.)  Under Robert Bradbury's proposal as I understand it, if the
<br>
predicted result of 25,000,000 deaths is that the event will be
<br>
accelerated by at least six months, this leads to taking the action of
<br>
killing those 25,000,000 people.
<br>
<p>Different worldviews, however, may have different expectations, as well as
<br>
different morals, and even different standards for rationality.  These
<br>
different worldviews may regard Robert Bradbury's actions as undesirable;
<br>
may furthermore decide that certain actions are desirable which Robert
<br>
Bradbury finds to be undesirable.  The key insight is that these different
<br>
worldviews can be treated as being analogous to players in the Prisoner's
<br>
Dilemna.  Refusing to kill even when it seems justified is analogous to
<br>
cooperating, and employing &quot;The ends justify the means&quot; logic is analogous
<br>
to defecting.  Furthermore, and this is the most important point, this
<br>
analogy still holds EVEN IF ONE WORLDVIEW IS RIGHT AND THE OTHER ONES ARE
<br>
WRONG.
<br>
<p>Let's say that we have Robert Bradbury in one corner of the room, and Bill
<br>
Joy in another corner.  Robert Bradbury, because Bill Joy is spreading
<br>
technophobia, decides that it's moral to shoot Bill Joy out of hand.  Bill
<br>
Joy, because Robert Bradbury wrote an article about Matrioshka Brains,
<br>
decides it's okay to shoot Robert Bradbury out of hand.  Bang, bang; they
<br>
both die.  If their negative and positive values cancel out, the net loss
<br>
is two human lives.  If Robert Bradbury's positive score would have been
<br>
greater than Bill Joy's negative score, the game is *very* negative-sum
<br>
and the Singularity has been delayed.
<br>
<p>The reason not to shoot people whom you think deserve it:  If everyone
<br>
shot the people who they thought deserved it, the world would be much
<br>
worse off - from your perspective, and from everyone's perspective, simply
<br>
because so many people would die, once the shooting started.  But
<br>
especially from the Extropian, scientific perspective, because scientists
<br>
and technologists of all stripes would be among the first victims.
<br>
<p>This doesn't change even if you assume that one worldview really is right
<br>
and that all the other ones really are wrong.  If you give all worldviews
<br>
equal credence, if you assume symmetry, then your perspective is analogous
<br>
to that of an altruist watching an iterated Prisoner's Dilemna being
<br>
played out among multiple people whom you all care about equally; you want
<br>
them to play positive-sum games and not negative-sum games so that the
<br>
global positive sum is maximized.  Specifying that one worldview is right
<br>
and the others wrong is analogous to taking the perspective of a single
<br>
player whose goals are wholly personal; that player will still adopt a
<br>
strategy of playing positive-sum games and cooperating, because that is
<br>
how you win in the iterated prisoner's dilemna.
<br>
<p>The analogy here is not between a selfish player and a selfish worldview,
<br>
but between a purely selfish player and a worldview that gives zero
<br>
credence to other worldviews, and between a purely altruistic player and a
<br>
worldview that gives equal credence to all other worldviews.  It should be
<br>
understood that this analogy is STRICTLY MATHEMATICAL and is not in any
<br>
sense a moral analogy between selfishness and daring to defy the majority
<br>
opinion.  (I go along with Reverend Rock on this one:  &quot;An open mind is a
<br>
great place for other people to dump their garbage.&quot;  I may value a
<br>
Luddite's life and volition but I don't value her opinion.)  The point is
<br>
that a selfish player will *still* cooperate.  You don't have to be a
<br>
cultural relativist about the value of technology to refrain from shooting
<br>
Ralph Nader.
<br>
<p>Not all cases of &quot;the end justifies the means&quot; fail.  Not all instances of
<br>
military force go wrong.  The American Revolution involved killing people
<br>
for what seemed like a good reason - and it worked, establishing a lasting
<br>
happiness that went on to vastly outweigh the blood spent.  But the
<br>
twentieth century saw the rise of Stalin and Hitler, who killed millions
<br>
under justifications that never materialized.  It also saw the rise of
<br>
Martin Luther King and Gandhi, who achieved the lasting respect of the
<br>
entire world for their espousal of refraining from violence even in
<br>
contexts where it appears justified.  So today, in the world that World
<br>
War II created, we tend to be more cautious.
<br>
<p>Tutored by the twentieth century, the moral people of today's world have
<br>
agreed among themselves to value all human lives equally.  Some, though
<br>
not all, say that a murderer becomes targetable and can be killed to
<br>
prevent further killing; none say that it is moral to kill arbitrary
<br>
targets in an attempt to optimize the system.  To get along with the
<br>
cooperators of this world, you must agree to adopt cooperation as a
<br>
working theory, even if you don't believe that it reflects the ultimate
<br>
underlying morality.  To do otherwise will be seen as a &quot;defection&quot; by
<br>
exactly those players that you most want to interact with in the future. 
<br>
Including me.  I would interpret what you've done so far as &quot;willingness
<br>
to take flak as a result of playing Devil's Advocate&quot;, rather than &quot;Robert
<br>
Bradbury announces his intention to defect&quot;, but others may not see it
<br>
that way.  (Though I sort of wish you'd asked me or Robin Hanson or Nick
<br>
Bostrom about this controversial idea of yours before posting it directly
<br>
to the Extropians list...)
<br>
<p>POST SCRIPTUM:
<br>
<p>Students of evolutionary psychology will know that organisms are
<br>
adaptation-executors rather than fitness-maximizers; selfishly motivated
<br>
cooperation can turn into an impulse toward genuine altruism, either
<br>
because the latter is cognitively simpler and easier to evolve, or because
<br>
a potential partner will prefer to partner with a genuine altruist rather
<br>
than a &quot;fair-weather friend&quot;.  The mathematical analogy for
<br>
worldview/players is an impulse to *genuinely* value all other worldviews
<br>
equally, not just as a working theory adopted for the sake of
<br>
cooperation.  In small doses, this can counter the innate human tendency
<br>
toward self-overestimation and contribute to meta-rationality.  In large
<br>
doses, it turns into cultural relativism.
<br>
<p>I do *not* like cultural relativism.  Unlike people, worldviews do have
<br>
different inherent values; some are right and some are wrong.  Science,
<br>
for example, can be seen as a set of rules for worldview interaction that
<br>
will, over time, favor correct worldviews over incorrect ones.  This
<br>
involves allowing some initial credence to all worldviews, but the amount
<br>
of credence allowed can rapidly go to zero if the worldview fails to make
<br>
original correct predictions, or one if the new worldview outpredicts a
<br>
current theory.
<br>
<p>There is no ethical imperative to value someone else's worldview.  There
<br>
is an ethical imperative to not take actions that value that other
<br>
person's worldview at absolute zero - for example, suppressing an idea
<br>
that you believe to be incorrect.  (What if someone else, believing your
<br>
own worldview to be definitely incorrect, censored you?)  But that refusal
<br>
to censor is not the same as actually changing your own opinion's based on
<br>
Ralph Nader's.
<br>
<p>In the case of the great iterated non-zero-sum game that is human
<br>
existence, I think that the genuine altruism that has evolved is more
<br>
valid than the game-theoretical selfishly motivated reciprocal altruism
<br>
that is evolution's &quot;motive&quot;.  Modern-day humans have sex with birth
<br>
control, and I think that's sane, so I can engage in altruism that I
<br>
regard as an end in itself.  (Of course the philosophy is more complex
<br>
than this!  But anyway...)
<br>
<p>Still, I do not believe that cultural relativism, the unconditional
<br>
imperative, is more valid than the conditional imperative of agreeing to
<br>
not take action against other worldviews for the sole reason of not
<br>
wanting other worldviews to take action against you.  I think that wanting
<br>
all worldviews to play fair with you is the legitimate reason for playing
<br>
fair yourself, and that there is *not* a moral imperative to actually
<br>
believe those other worldviews, to let them impact your beliefs.  Beliefs
<br>
should be strictly governed by reality.  Meta-rationality (see Robin
<br>
Hanson) is a good reason for taking the beliefs of others into account as
<br>
additional sensory data which may be useful for finding the truth.  The
<br>
human tendency toward self-estimation is a good reason to pay attention to
<br>
the opinions of others and not just your own.  But the need to get along
<br>
with the neighbors is *not* a valid reason to change your beliefs.  Your
<br>
neighbors may be wrong.
<br>
<p>POST POST SCRIPTUM:
<br>
<p>The rest of the Universe doesn't necessarily obey the same rules that hold
<br>
in our own backyard.  The above analysis of ethics is geared to
<br>
human-level intelligence, the human emotional complex, observed human
<br>
history, the rules of human existence, and the adaptational base created
<br>
by the human ancestral environment.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6812.html">CurtAdams@aol.com: "Re: jane's on al-qaeda"</a>
<li><strong>Previous message:</strong> <a href="6810.html">Damien Broderick: "Re: TERRORISM: Is genocide the logical solution?"</a>
<li><strong>In reply to:</strong> <a href="6758.html">Robert J. Bradbury: "TERRORISM: Is genocide the logical solution?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6839.html">Robert J. Bradbury: "TERRORISM: Is genocide the logical solution?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6811">[ date ]</a>
<a href="index.html#6811">[ thread ]</a>
<a href="subject.html#6811">[ subject ]</a>
<a href="author.html#6811">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Fri Oct 12 2001 - 14:40:49 MDT</em>
</em>
</small>
</body>
</html>
