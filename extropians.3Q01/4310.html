<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: The Dazzle Effect:  Staring into the Singularity</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: The Dazzle Effect:  Staring into the Singularity">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: The Dazzle Effect:  Staring into the Singularity</h1>
<!-- received="Thu Aug 16 12:56:58 2001" -->
<!-- isoreceived="20010816185658" -->
<!-- sent="Thu, 16 Aug 2001 14:52:21 -0400" -->
<!-- isosent="20010816185221" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The Dazzle Effect:  Staring into the Singularity" -->
<!-- id="3B7C1665.EA6F6469@pobox.com" -->
<!-- inreplyto="3B7BD5B7.20003@earthlink.net" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20Dazzle%20Effect:%20%20Staring%20into%20the%20Singularity&In-Reply-To=&lt;3B7C1665.EA6F6469@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Aug 16 2001 - 12:52:21 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4311.html">Spudboy100@aol.com: "Re: why immortality?"</a>
<li><strong>Previous message:</strong> <a href="4309.html">Terry W. Colvin: "The First Americans - 9,500-year-old mystery"</a>
<li><strong>In reply to:</strong> <a href="4289.html">Charles Hixson: "The Dazzle Effect:  Staring into the Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4331.html">Charles Hixson: "Re: The Dazzle Effect:  Staring into the Singularity"</a>
<li><strong>Reply:</strong> <a href="4331.html">Charles Hixson: "Re: The Dazzle Effect:  Staring into the Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4310">[ date ]</a>
<a href="index.html#4310">[ thread ]</a>
<a href="subject.html#4310">[ subject ]</a>
<a href="author.html#4310">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Charles Hixson wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Just a few data points.
</em><br>
<em>&gt; 1) Yesterday I reread Staring into the Singularity, and did a bit of
</em><br>
<em>&gt; arithmetic.  Assuming that in March the computer speed was, indeed,
</em><br>
<em>&gt; doubling every 18 mos.  And assuming, as indicated, that each successive
</em><br>
<em>&gt; doubling takes half the time of the previous one, then in March 2004 the
</em><br>
<em>&gt; curve goes vertical.
</em><br>
<p>Say what?  Okay, first of all, that was a metaphor, not a projection.  And
<br>
secondarily, it would be a metaphor that would only start *after* you had
<br>
human-equivalent AI researchers running on the existing silicon.  We don't
<br>
have this now and therefore there is no imaginable reason why the curve
<br>
would go vertical in March 2004.  Today it's just the same 'ol same 'ol
<br>
doubling every eighteen months.
<br>
<p>If you do have human-equivalent AI, the intelligence curve probably goes
<br>
vertical after a couple of weeks, not a couple of years, because
<br>
improvement in software and the absorption of additional hardware should
<br>
be easily sufficient to carry a human-equivalent AI to superintelligence
<br>
and nanotechnology.
<br>
<p><em>&gt; 6) I think it was in C Friendly AI that the projection was made that a
</em><br>
<em>&gt; minimum system needed by a seed AI would be 32 processors running at 2 GHz.
</em><br>
<p>No, that was a number I pulled out of thin air so as to have an example. 
<br>
Moreover, it wasn't an example of a finished seed AI, but rather an AI in
<br>
the process of development.
<br>
<p><em>&gt; 7) So by the end of the year it should be possible to put together a
</em><br>
<em>&gt; system about twice the minimum strength for around $10,000.
</em><br>
<p>The number was specifically chosen to be achievable using present-day
<br>
technologies, rather than say those of five years down the road.  It's a
<br>
minimum strength for development.  Not human equivalence.  If human
<br>
equivalence was that cheap we probably really would have AI by now,
<br>
software problem or no.
<br>
<p><em>&gt; 10) The assertion has been made that the more computing power you have,
</em><br>
<em>&gt; the easier it is to end up with an AI (though not necessarily a friendly
</em><br>
<em>&gt; one).  So by the middle of next year any reasonably profitable business
</em><br>
<em>&gt; should be able to put together enough computing power, if it chooses to.
</em><br>
<p>Any profitable business can already put together enough hardware that they
<br>
could start developing if they knew what they were doing.  This was
<br>
probably true even back in 1995.  But to get AI without knowing what
<br>
you're doing, you probably need substantially more hardware than exists in
<br>
the human mind.
<br>
<p>However, if the Moon were made of computronium instead of green cheese (as
<br>
Eugene Leitl put it), it would probably *not* take all that much work to
<br>
get it to wake up.  At that scale (endless quintillions of brainpower) it
<br>
becomes possible to brute-force an evolutionary algorithm in which the
<br>
individual units are millions of brainpower.  If the Moon were made of
<br>
computronium instead of green cheese I doubt it would take so much as a
<br>
week for me *or* Eugene Leitl to wake it up, if it didn't wake up before
<br>
then due to self-organization of any noise in the circuitry.  If the Moon
<br>
were made of computronium you could probably write one self-replicating
<br>
error-prone program a la Tierra and see it wake up not too long
<br>
afterwards.  A lunar mass is one bloody heck of a lot of computronium.
<br>
<p>Friendliness is an entirely different issue, although if the evolutionary
<br>
process goes through a pattern roughly equivalent to human evolution,
<br>
specifically including imperfectly deceptive intelligent social organisms,
<br>
then there might be humanlike altruists in the resulting civilization. 
<br>
The problem is probably that a computronium substrate makes it easier for
<br>
minds to amalgate, meaning that evolution there could take a quite
<br>
different (and faster) course if it started with a Tierra algorithm; if
<br>
there's a programmed evolutionary pattern using preallocated millions of
<br>
brainpower in individual units, then the evolutionary intelligence curve
<br>
for the population of a trillion such individuals will not remotely
<br>
resemble the growth curve for human intelligence over time - more like a
<br>
sudden, sharp snap from stupid individuals to superintelligent individuals
<br>
without the opportunity to develop emotional sophistication during a long
<br>
intermediate stage of socially interacting non-transhuman general
<br>
intelligences.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4311.html">Spudboy100@aol.com: "Re: why immortality?"</a>
<li><strong>Previous message:</strong> <a href="4309.html">Terry W. Colvin: "The First Americans - 9,500-year-old mystery"</a>
<li><strong>In reply to:</strong> <a href="4289.html">Charles Hixson: "The Dazzle Effect:  Staring into the Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4331.html">Charles Hixson: "Re: The Dazzle Effect:  Staring into the Singularity"</a>
<li><strong>Reply:</strong> <a href="4331.html">Charles Hixson: "Re: The Dazzle Effect:  Staring into the Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4310">[ date ]</a>
<a href="index.html#4310">[ thread ]</a>
<a href="subject.html#4310">[ subject ]</a>
<a href="author.html#4310">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Fri Oct 12 2001 - 14:40:10 MDT</em>
</em>
</small>
</body>
</html>
