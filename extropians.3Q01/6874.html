<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: TERRORISM: Is genocide the logical solution?</title>
<meta name="Author" content="Mark Walker (mdwalker@quickclic.net)">
<meta name="Subject" content="Re: TERRORISM: Is genocide the logical solution?">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: TERRORISM: Is genocide the logical solution?</h1>
<!-- received="Mon Sep 17 09:26:49 2001" -->
<!-- isoreceived="20010917152649" -->
<!-- sent="Mon, 17 Sep 2001 11:24:03 -0700" -->
<!-- isosent="20010917182403" -->
<!-- name="Mark Walker" -->
<!-- email="mdwalker@quickclic.net" -->
<!-- subject="Re: TERRORISM: Is genocide the logical solution?" -->
<!-- id="005101c13fa5$eefe9520$c8c8f418@southmount.com" -->
<!-- inreplyto="Pine.LNX.4.10.10109161130120.26018-100000@server.aeiveos.com" -->
<strong>From:</strong> Mark Walker (<a href="mailto:mdwalker@quickclic.net?Subject=Re:%20TERRORISM:%20Is%20genocide%20the%20logical%20solution?&In-Reply-To=&lt;005101c13fa5$eefe9520$c8c8f418@southmount.com&gt;"><em>mdwalker@quickclic.net</em></a>)<br>
<strong>Date:</strong> Mon Sep 17 2001 - 12:24:03 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6875.html">Harvey Newstrom: "RE: Another new offensive strategies"</a>
<li><strong>Previous message:</strong> <a href="6873.html">John Clark: "Re: Human lives: Do the math (was: Impact on history)"</a>
<li><strong>In reply to:</strong> <a href="6758.html">Robert J. Bradbury: "TERRORISM: Is genocide the logical solution?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6889.html">Eliezer S. Yudkowsky: "Re: TERRORISM: Is genocide the logical solution?"</a>
<li><strong>Reply:</strong> <a href="6889.html">Eliezer S. Yudkowsky: "Re: TERRORISM: Is genocide the logical solution?"</a>
<li><strong>Reply:</strong> <a href="6918.html">Anders Sandberg: "Das Elend des Transhumanismus (Was: Is genocide the logical solution?)"</a>
<li><strong>Reply:</strong> <a href="6977.html">Damien Broderick: "Re: TERRORISM: Is genocide the logical solution?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6874">[ date ]</a>
<a href="index.html#6874">[ thread ]</a>
<a href="subject.html#6874">[ subject ]</a>
<a href="author.html#6874">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Robert has taken a lot of grief for this post including being charged with
<br>
the worst crime: engaging in &quot;pro-entropy&quot; (Anders) activities. With some
<br>
reluctance I feel compelled to defend the logic of what Robert is saying
<br>
because I think he raises an important question, namely: what are we willing
<br>
to sacrifice now in terms of lives to advance the effects of the
<br>
singularity?  I take it that this is the more general form of the question
<br>
that Robert is asking when he writes:
<br>
<p><em>&gt; &gt;From a rational position, *if* the case can be made that the
</em><br>
<em>&gt; Afgani position &amp; politics is likely to result in the diversion
</em><br>
<em>&gt; of resources and delay the development of the technologies we anticipate
</em><br>
<em>&gt; developing by more than 6 months, then a plan of genocide to
</em><br>
<em>&gt; bury the country in rubble seems justified.
</em><br>
<p>Anders made one of the most considered responses. He says Robert's position
<br>
is based on an ethical mistake:
<br>
<p><em>&gt;The
</em><br>
<em>&gt; ethical is most serious: you assume that human lives can have
</em><br>
<em>&gt; negative value, and do an essentially utilitarian calculation not
</em><br>
<em>&gt; of happiness, but simply of utility towards a certain goal. Humans
</em><br>
<em>&gt; not promoting this goal are a vaste of resources and a potential
</em><br>
<em>&gt; threat, so eliminate them.
</em><br>
<p>First off, I don't see where Robert assumes that humans are a vast resource,
<br>
rather, he seems to be asking us to trade off some lives now for many more
<br>
live in the future. Furthermore, Anders claims that Robert has made an
<br>
ethical mistake, but this is a controversial claim in ethics. Robert assumes
<br>
a consequentialist position, i.e., he assumes that the right act here is the
<br>
one that has the best consequence, namely, the one that saves the most lives
<br>
in the end. Anders, as far as I can tell, expresses the deontological view
<br>
which says that some actions are morally obligatory regardless of their
<br>
consequences:
<br>
<p><p><em>&gt; The core idea of transhumanism is human development, so that we
</em><br>
<em>&gt; can extend our potential immensely and become something new. This
</em><br>
<em>&gt; is based on the assumption that human life in whatever form it may
</em><br>
<em>&gt; be (including potential successor beings) is valuable and worth
</em><br>
<em>&gt; something in itself. It must not be destroyed, because that
</em><br>
<em>&gt; destroys what transhumanism strives to preserve and enhance. Even
</em><br>
<em>&gt; if some humans are not helpful in achieving transhumanity doesn't
</em><br>
<em>&gt; mean their existence is worthless, and if they are an active
</em><br>
<em>&gt; hindrance to your (or anybody elses) plans destroying their lives
</em><br>
<em>&gt; is always wrong as long as they do not initiate force against you.
</em><br>
<p>A simple test for figuring out whether you are a deontologist or a
<br>
consequentialist is to ask the following: Is it always wrong to take an
<br>
innocent life? If you are a deontologist about this matter then you must say
<br>
that it is always wrong to take an innocent human life no matter what the
<br>
consequences. A consequentialist, in contrast, will weigh the consequences
<br>
of this action. If by killing one innocent person you could save two lives
<br>
would you kill that person? How about if killing that innocent person saved
<br>
10,000 lives? A consequentialist will say that, &quot;yes, it is morally correct
<br>
to kill the one innocent person&quot;. (Myself I am a consequentialist, it is the
<br>
worse moral position besides all the others. Hopefully, with the singularity
<br>
we will not have to choose between the deontological and the
<br>
consequentialist positions).
<br>
&nbsp;&nbsp;&nbsp;&nbsp;With this distinction we can see that Anders may have been a bit swift
<br>
in pulling the &quot;fascist card&quot;:
<br>
<p><p><em>&gt;
</em><br>
<em>&gt; A transhumanism not based on this core does not deserve the
</em><br>
<em>&gt; humanism part of the name. And a transhumanism that bases itself
</em><br>
<em>&gt; on the utilitarist approach of using humans as means to an end
</em><br>
<em>&gt; rather than ends in themselves becomes the ideological twin of
</em><br>
<em>&gt; fascism, with the sole difference that the singularity takes the
</em><br>
<em>&gt; place of the national community in the ideological structure.
</em><br>
<em>&gt;
</em><br>
To make this charge of an &quot;ideological twin&quot; stick Anders would need to show
<br>
at least (a) that transhumanism is necessarily deontological in structure,
<br>
and (b) that Robert intended the consequence to be weighted is that of an
<br>
ideal, namely, the singularity, rather than the lives that the singularity
<br>
will save. However, (a) is an open question in my mind and, as I've said,
<br>
Robert's discussion seems predicated on the assumption that the singularity
<br>
will save a great number of lives. That is, it is to misunderstand (or not
<br>
read carefully) the form of Robert's argument:  the singularity is the means
<br>
to the end of saving lives, it is not that sacrificing lives are the means
<br>
to the goal of the singularity. Thus Robert writes:
<br>
<p><p><em>&gt; We also know, from calculations that I and Eliezer (independently)
</em><br>
<em>&gt; have done, that the annual cost between where we are now and the
</em><br>
<em>&gt; full manifestation of what we expect is feasible is of the order
</em><br>
<em>&gt; of 50 million lives per year.
</em><br>
<p><p>I guess there is a sense in which I agree with Robert for in effect he is
<br>
saying that if we know that by sacrificing a certain number of lives  today
<br>
we can save many more tomorrow. Notice that Robert did not say that he knows
<br>
that the antecedent is true, this was one of the questions he was raising.
<br>
To repeat, he says:
<br>
<p><em>&gt; &gt;From a rational position, *if* the case can be made that the
</em><br>
<em>&gt; Afgani position &amp; politics is likely to result in the diversion
</em><br>
<em>&gt; of resources and delay the development of the technologies we anticipate
</em><br>
<em>&gt; developing by more than 6 months, then a plan of genocide to
</em><br>
<em>&gt; bury the country
</em><br>
<p>It seems that we are way too ignorant of the consequences of our actions to
<br>
know that the antecedent of this conditional is true, nor are we ever likely
<br>
to know it is true. We don't know whether such actions might result in a
<br>
global war or that in so doing we might be killing a person who actually
<br>
makes a significant breakthrough to real AI. Also, we must admit that we do
<br>
not know that the consequent will obtain either. We do not know whether a
<br>
singularity is possible, even with all our resources devoted to it, and
<br>
whether it will really save lives. (Can we really be sure that a
<br>
postsingularity superintelligence might not think that it is best for
<br>
humanity not to possess technology and not to be uploaded for the same sorts
<br>
of reasons we do not think we need to upload are pet goldfish? Perhaps, the
<br>
superintelligence will reason that humans have the best lives when they are
<br>
simple hunter/gathers. Yikes!)  Obviously, we hope and believe this is the
<br>
case, but I for would not be willing to sacrifice millions of lives given
<br>
our tremendous ignorance about these matters.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;Having said this, I think that Robert might have been able to raise
<br>
these sorts of questions in a less inflammatory way and I wish he had done
<br>
so.
<br>
Mark.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6875.html">Harvey Newstrom: "RE: Another new offensive strategies"</a>
<li><strong>Previous message:</strong> <a href="6873.html">John Clark: "Re: Human lives: Do the math (was: Impact on history)"</a>
<li><strong>In reply to:</strong> <a href="6758.html">Robert J. Bradbury: "TERRORISM: Is genocide the logical solution?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6889.html">Eliezer S. Yudkowsky: "Re: TERRORISM: Is genocide the logical solution?"</a>
<li><strong>Reply:</strong> <a href="6889.html">Eliezer S. Yudkowsky: "Re: TERRORISM: Is genocide the logical solution?"</a>
<li><strong>Reply:</strong> <a href="6918.html">Anders Sandberg: "Das Elend des Transhumanismus (Was: Is genocide the logical solution?)"</a>
<li><strong>Reply:</strong> <a href="6977.html">Damien Broderick: "Re: TERRORISM: Is genocide the logical solution?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6874">[ date ]</a>
<a href="index.html#6874">[ thread ]</a>
<a href="subject.html#6874">[ subject ]</a>
<a href="author.html#6874">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Fri Oct 12 2001 - 14:40:50 MDT</em>
</em>
</small>
</body>
</html>
