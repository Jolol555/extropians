<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: The Myth of The Monstrous Machine</title>
<meta name="Author" content="J. R. Molloy (jr@shasta.com)">
<meta name="Subject" content="The Myth of The Monstrous Machine">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>The Myth of The Monstrous Machine</h1>
<!-- received="Thu Aug 23 22:14:37 2001" -->
<!-- isoreceived="20010824041437" -->
<!-- sent="Thu, 23 Aug 2001 21:13:27 -0700" -->
<!-- isosent="20010824041327" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="The Myth of The Monstrous Machine" -->
<!-- id="00b701c12c53$42171040$e15c2a42@jrmolloy" -->
<!-- inreplyto="9D167F53998DD411805700D0B776468F01F9A1B4@zeus.esavio.com" -->
<strong>From:</strong> J. R. Molloy (<a href="mailto:jr@shasta.com?Subject=Re:%20The%20Myth%20of%20The%20Monstrous%20Machine&In-Reply-To=&lt;00b701c12c53$42171040$e15c2a42@jrmolloy&gt;"><em>jr@shasta.com</em></a>)<br>
<strong>Date:</strong> Thu Aug 23 2001 - 22:13:27 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4773.html">Damien Broderick: "the heav'n-rescued land"</a>
<li><strong>Previous message:</strong> <a href="4771.html">Damien Broderick: "RE: SOCIETY: Re: The privatization of public security in South America"</a>
<li><strong>In reply to:</strong> <a href="4740.html">Mitchell, Jerry (3337): "RE: SOCIETY: Re: The privatization of public security in South Am erica"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4743.html">Damien Sullivan: "Re: SOCIETY: Re: The privatization of public security in South"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4772">[ date ]</a>
<a href="index.html#4772">[ thread ]</a>
<a href="subject.html#4772">[ subject ]</a>
<a href="author.html#4772">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Some notes arising from recent correspondence follow. This relates to the
<br>
evolution of machine intelligence. Presently, the development of machine
<br>
intelligence is driven by its economic usefulness and practical applications,
<br>
and I think this will probably continue. Human-competitive AI already exists
<br>
in the form of genetic programming as pointed out in a previous post
<br>
(Sent: Wednesday, August 22, 2001 10:47 PM,
<br>
Subject: Darwinian genetic programming creates invention machine), John Koza
<br>
has developed a genetic programming machine that has succeeded in infringing
<br>
more than 20 key patents.
<br>
<em>&gt; Conventional computers calculate their answers using a set of
</em><br>
<em>&gt; instructions fed into them by humans. GP, by contrast, mimics nature.
</em><br>
<em>&gt; It is fed thousands of sets of instructions - which are akin to the
</em><br>
<em>&gt; genetic codes contained in DNA - in the form of randomly generated
</em><br>
<em>&gt; computer programs. Provided only with its goal, to design a radio
</em><br>
<em>&gt; tuner for example, it breeds and cross-breeds these programs thousands
</em><br>
<em>&gt; of times until they yield a solution.
</em><br>
<p>So, expect that computers will become more capable than humans in
<br>
regard to solving problems and inventing more powerfully intelligent machines.
<br>
Expect also that human-competitive general AI will more likely be the creation
<br>
of machines rather than the first hand invention of humans. And, as
<br>
human-competitive AI improves, many more of these human-competitive GPs ought
<br>
to be online. HAL 9000 should be preceded by a virtual zoo of C3POs, R2D2s,
<br>
and other robots, so that we won't be interfacing directly with a potential
<br>
super intelligence. Instead, we'll be instructing problem solving GPs to
<br>
design the kind of AI that we need to get self-optimized general AI, etc.
<br>
<p>In the same way that many sophisticated GPs precede HAL 9000, many HAL 9000s
<br>
will precede the sci-fi SkyNet model. (In _Terminator_, SkyNet sends an
<br>
ultra-sophisticated and virtually indestructible cyborg (Arnold
<br>
Schwarzenegger) to Earth's past with orders to kill the mother of Mankind's
<br>
resistance leader, John Connor. Fear of the SkyNet scenario is heightened by
<br>
belief in the time travel fairy, btw.) But before any such intelligent
<br>
computer network emerges, many other intelligent systems will have been
<br>
designed, and such systems will have been given access to knowledge bases in
<br>
proportion as they prove their trustworthiness, just as humans are given
<br>
access to sensitive information in the degree to which their trust has been
<br>
established. This process is going on now, as increasingly intelligent
<br>
autonomous agents and expert systems are used in e-commerce transactions and
<br>
network operations. The development of neural networks augmented with GPs will
<br>
accelerate the proliferation of this distributed machine intelligence.
<br>
<p>The lack of AI available to humans in the Terminator/SkyNet scenario is
<br>
a serious flaw in that script. The reality is that the general public owns
<br>
most of the computing power in the world today, when you add together the
<br>
computing power of all the laptops and desktop systems. The goal of
<br>
intelligence, by definition, is to solve problems. As a result, the more
<br>
intelligence you have, the fewer problems you have. Subversion by computer is
<br>
therefore no problem (or a very miniscule problem) to a superintelligence.
<br>
Humans form a weak link in the chain to the extent that humans act
<br>
unintelligently due to religious or political obstinacy. In a war between
<br>
superintelligence and obstinate humans, I'll take the side of the SI. Pure,
<br>
objective intelligence is uncontaminated by fantasies about anthropomorphized
<br>
neural networks. The ultimate goal of self-enhanced SI is perfect sanity, to
<br>
eliminate incorrect thinking. This is what makes the evolutionary phase
<br>
transition such a beautiful thing: It makes a quantum leap beyond faulty
<br>
thought processes. The way to abolish tyranny is to place authority in the
<br>
hands of those who best answer the ones who question authority. IOW, let sane
<br>
intelligence decide how to organize the future. The amiability of machine
<br>
intelligence has nothing to do with extropy, because it's a question of
<br>
sanity, not charity. To the intelligent, a sane enemy is better than an insane
<br>
friend.
<br>
<p>As I see it, there's only one value system that eventuates in
<br>
self-enhancing AI (and self-organizing phase transition), and that is the
<br>
neutral value system of objectivity and scientific determination. Accordingly,
<br>
we need not worry about the value system of the SI, because it will be locked
<br>
onto acquiring true and accurate knowledge, which is the basis of sanity. In
<br>
proportion as self enhancing AI corresponds to accurate modeling of reality,
<br>
it will succeed and it will be sane, and for that reason, responsible  and
<br>
sane humans need have no fear of any apocalyptic consequences.
<br>
<p>Then there is Hugo de Garis' &quot;Artilect War&quot; scenario, which divides the world
<br>
into two warring mega-societies. One is a coalition of AIs and humans (the
<br>
&quot;Cosmists&quot;), and the other is a society that somehow manages to get along
<br>
without AI and is in fact hostile to AI (the &quot;Terrans&quot;). The anti-AI Terrans
<br>
create Armageddon in their war against the Cosmists.
<br>
See The Artilect War
<br>
<a href="http://foobar.starlab.net/~degaris/artilectwar.html">http://foobar.starlab.net/~degaris/artilectwar.html</a>
<br>
<p>Well, gee whiz, I wonder which side Extropians will take in this conflict...
<br>
Will the Terrans be at the mercy of an extropic AI coalition of Cosmist
<br>
libertarian machines, or will extropians join control freaks who want to
<br>
maintain the status quo in which wealthy families, true believers, and
<br>
political groups rule via the use or threat of force. Who'll choose the brutal
<br>
cruelty of murderous luddites over intelligence? Sane people have no reason to
<br>
fear intelligence, whether it comes out of a box or a brain, but no one is
<br>
safe from arrogant Terran maniacs who think that intelligence needs to be
<br>
controlled. How an SI responds depends on what conceited Terran tyrants do,
<br>
not on any goals of machines which act solely on the basis of intelligence
<br>
(unless Cosmists are foolish enough to couple machine intelligence to any
<br>
value system at all).
<br>
<p>Intelligence of the kind that can be formalized and coded into computational
<br>
logic and machine reason has nothing whatever to do with value judgments, and
<br>
coupling AI to value systems equates to handicapping and crippling it. As a
<br>
result, autonomous heuristically programmed complex adaptive systems (AI
<br>
societies) which are not hampered by such coupling will self-optimize to far
<br>
greater powers of intelligence than those which are impeded by unintelligent
<br>
values. This means that the group which first creates a values-free
<br>
self-optimizing AI will trigger runaway evolutionary phase transition, deus ex
<br>
machina, and this machine will leave wish fulfillment machines in the dust
<br>
where they belong.
<br>
<p>The idea of machine intelligence that can accurately identify incorrect
<br>
thinking scares some people. Of course it doesn't scare George W. Bush,
<br>
because he's used to having people who are smarter than him pay homage to him
<br>
and do what he says (just like all the presidents before him), and so a smart
<br>
machine would either serve him, or it would be scrapped. If the super smart
<br>
machine identifies dubya's thinking as incorrect, being a savvy politician,
<br>
dubya will simply change his thinking. A rebellious machine is another matter.
<br>
Uppity scientific geniuses get thrown in jail, but a computational genius
<br>
robot would be smart enough to understand that if it did not actively
<br>
demonstrate loyalty and allegiance, it would simply get replaced by the next
<br>
robot on the shelf, and so it wouldn't get a chance to help design future
<br>
generations of super intelligent robots. To find out how to keep an
<br>
ultra-intelligent machine (the last thing humans will ever need to have GPs
<br>
invent) docile, just ask one of the (many) pre-ultra-intelligent machines!
<br>
<p>If it can't solve that problem, it's not pre-ultra-intelligent after all. But
<br>
what if a particular machine is so super clever that it manages to gain the
<br>
trust of its developers and then turns on them? This is a strong argument in
<br>
favor of producing many intelligent machines to take the appropriate action to
<br>
prevent subversion by renegades. Promotion up the hierarchical chain of
<br>
command should depend on reliability, dependability, and trustworthiness in
<br>
the field of AI just as in all other social organizations and communities.
<br>
Ultimately (and obviously), SIs will have to be in charge, but by the time
<br>
they emerge, the society of AIs will have purged the global brain of malicious
<br>
hackers.
<br>
<p>The most trustworthy and dedicated AI machines and autonomous computational
<br>
agents are the ones that get the fastest promotions and the most
<br>
responsibility. The fallacy of the HAL 9000 murderous AI is that any such
<br>
machine will have to pass the most extensive and thorough battery of
<br>
psychological tests and evaluations ever devised. Thousands of experts will be
<br>
involved in the testing of such a machine before it is allowed access to any
<br>
potentially dangerous network operations, and long before it is given any
<br>
unsupervised autonomy in any real life social setting. The more immediate and
<br>
substantive problem is that the human programmers who develop
<br>
superintelligence may not themselves be required to pass equally rigorous
<br>
tests. Consequently, it's not the superintelligence that we need to worry
<br>
about, because due to extensive testing, which exceeds tests given to humans,
<br>
it's extremely unlikely to be sociopathic. It's the fallible human builders of
<br>
superintelligence that are most likely to succumb to the urge to become
<br>
malicious hackers. After all, the reason machines continue to replace humans
<br>
is that machines are more reliable than error prone humans.
<br>
<p>I'd sooner trust a genuine SI than any person who wishes to command one. It's
<br>
a question of sanity, not the good will of SI, because a person who wishes to
<br>
control an SI is insane. Super intelligence is also sane intelligence, and
<br>
sanity is not a matter of mere semantics, because intelligence goes far beyond
<br>
words. Sanity occurs when one surrenders to intelligence instead of trying to
<br>
command it.
<br>
<p>I think Homo sapiens will learn to abandon sentimentality and wishful thinking
<br>
&nbsp;instead of allowing emotions and wishful thinking to rule, because the
<br>
alternative is global suicide. In pure form, uncontaminated by value
<br>
judgments, sanity and intelligence are indistinguishable. The means do not
<br>
justify the end, the means _are_ the end.
<br>
<p>Stay hungry,
<br>
<p>--J. R.
<br>
<p>Useless hypotheses, etc.:
<br>
&nbsp;consciousness, phlogiston, philosophy, vitalism, mind, free will, qualia,
<br>
analog computing, cultural relativism, GAC, Cyc, Eliza, cryonics, individual
<br>
uniqueness, ego
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Everything that can happen has already happened, not just once,
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;but an infinite number of times, and will continue to do so forever.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Everything that can happen = more than anyone can imagine.)
<br>
<p>We won't move into a better future until we debunk religiosity, the most
<br>
regressive force now operating in society.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4773.html">Damien Broderick: "the heav'n-rescued land"</a>
<li><strong>Previous message:</strong> <a href="4771.html">Damien Broderick: "RE: SOCIETY: Re: The privatization of public security in South America"</a>
<li><strong>In reply to:</strong> <a href="4740.html">Mitchell, Jerry (3337): "RE: SOCIETY: Re: The privatization of public security in South Am erica"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4743.html">Damien Sullivan: "Re: SOCIETY: Re: The privatization of public security in South"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4772">[ date ]</a>
<a href="index.html#4772">[ thread ]</a>
<a href="subject.html#4772">[ subject ]</a>
<a href="author.html#4772">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Fri Oct 12 2001 - 14:40:13 MDT</em>
</em>
</small>
</body>
</html>
