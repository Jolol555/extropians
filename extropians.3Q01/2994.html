<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Sysops and enlightened despots</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: Sysops and enlightened despots">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Sysops and enlightened despots</h1>
<!-- received="Fri Aug  3 16:54:34 2001" -->
<!-- isoreceived="20010803225434" -->
<!-- sent="Fri, 03 Aug 2001 18:56:28 -0400" -->
<!-- isosent="20010803225628" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: Sysops and enlightened despots" -->
<!-- id="3B6B2C1C.3199A52E@posthuman.com" -->
<!-- inreplyto="20010803173731.A15573@akira.nada.kth.se" -->
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20Sysops%20and%20enlightened%20despots&In-Reply-To=&lt;3B6B2C1C.3199A52E@posthuman.com&gt;"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Fri Aug 03 2001 - 16:56:28 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2995.html">Brian Atkins: "Re: Stem Cell Debate --Banned in the USA ---&gt; libertarian societies as   unicorn"</a>
<li><strong>Previous message:</strong> <a href="2993.html">James Rogers: "Re: Chem: Silicon Explosives now"</a>
<li><strong>In reply to:</strong> <a href="2970.html">Anders Sandberg: "Re: Sysops and enlightened despots"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2971.html">Reason: "RE: Stem Cell Debate --Banned in the USA ---&gt; libertarian societies as  unicorn"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2994">[ date ]</a>
<a href="index.html#2994">[ thread ]</a>
<a href="subject.html#2994">[ subject ]</a>
<a href="author.html#2994">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Anders Sandberg wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; On Fri, Aug 03, 2001 at 03:03:44AM -0400, Brian Atkins wrote:
</em><br>
<em>&gt; &gt; Reason wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I have yet to see a better solution to the issue. At some point the matter
</em><br>
<em>&gt; &gt; (as in atoms) must fall under someone's control, and personally I don't
</em><br>
<em>&gt; &gt; relish the idea of having to constantly protect myself from everyone else
</em><br>
<em>&gt; &gt; who can't be trusted with nanotech and AI. All it takes is one Blight to
</em><br>
<em>&gt; &gt; wipe us out. That kind of threat does not go away as humans progress to
</em><br>
<em>&gt; &gt; transhumanity, rather it increases in likelihood. What is the stable state
</em><br>
<em>&gt; &gt; if not Sysop or total death? There may be some other possibilities, can
</em><br>
<em>&gt; &gt; you name some?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Stable states are dead states. I would say another possibility would be an
</em><br>
<em>&gt; eternally growing selforganised critical state - sure, disasters happen,
</em><br>
<em>&gt; but countermeasures also emerge. The whole is constantly evolving and
</em><br>
<em>&gt; changing.
</em><br>
<p>When I say stable state I do not mean that in terms of anything other
<br>
than providing a certain specific underlying layer of &quot;services&quot; that
<br>
make it impossible to begin to slide towards the death stable state
<br>
(unless of course everyone suddenly decides to do that). Think of this
<br>
as something like the Constitution of the USA. It provides a basis to
<br>
prevent certain things from happening, yet it does not cap or limit what
<br>
grows on top of it.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Having (post)human development constrained to a small part of the available
</em><br>
<em>&gt; technology/culture/whatever space in order to ensure safety is going to run
</em><br>
<em>&gt; into a Gödel-like trap. There are likely undecidable threats out there,
</em><br>
<em>&gt; things that cannot be determined to be dangerous or not using any finite
</em><br>
<em>&gt; computational capability. Hence the only way of ensuring security is to
</em><br>
<p>Can you be more specific about this? It sounds to me like the people who
<br>
claim we will eventually find something uncomputable. How about an example.
<br>
The Sysop can simulate inside of itself anything it runs into, to find out
<br>
what it does. Only if something was &quot;bigger&quot; than the Sysop itself would
<br>
it be unable to do this, at least it seems that way to me.
<br>
<p><em>&gt; limit ourselves to a finite space - which goes counter to a lot of the core
</em><br>
<em>&gt; transhumanist ideas. Or the sysop would have to allow undecidable risks and
</em><br>
<em>&gt; similar hard-to-detect threats. One category of threats to worry about are
</em><br>
<em>&gt; of course threats the sysop would itself run into while looking for threats
</em><br>
<em>&gt; - they could wipe us out exactly because (post)humanity had not been
</em><br>
<em>&gt; allowed the necessary dispersal and freedom that might otherwise have
</em><br>
<em>&gt; saved at least some.
</em><br>
<p>I think this is not making a lot of sense. Remember that the hypothetical
<br>
Sysop is primarily concerned (at least in its spare time :-) with ensuring
<br>
safety for all. If it did happen to run into something it couldn't handle
<br>
while say hitching along in some transhumanists' spaceship, it would most
<br>
certaily WARN everyone it could. Remember the Sysop Scenario only actually
<br>
happens if a FAI decides it is the best way to go. However that doesn't
<br>
preclude it from changing its mind later on if it turns out to be a
<br>
failure or a better way is discovered.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; This is essentially the same problem as any enlightened despot scheme (and
</em><br>
<em>&gt; there is of course the huge range of ethical problems with such schemes
</em><br>
<em>&gt; too), put in a fresh sf setting. Enlightened despots make bad rulers
</em><br>
<em>&gt; because they cannot exist: they need accurate information about the
</em><br>
<em>&gt; preferences of everybody, which is not possible for any human ruler. The
</em><br>
<em>&gt; version 2.0 scenario assuming the omniscient AI runs into the same problem
</em><br>
<em>&gt; anyway: it would need to handle an amount of information of the same order
</em><br>
<em>&gt; of magnitude as the information processing in the entire society. Hence it
</em><br>
<em>&gt; would itself be a sizeable fraction of society information-wise (and itself
</em><br>
<em>&gt; a source of plenty of input in need of analysis). Given such technology, as
</em><br>
<em>&gt; soon as any other system in society becomes more complex the ruler AI would
</em><br>
<em>&gt; have to become more complex to keep ahead. Again the outcome is that either
</em><br>
<em>&gt; growth must be limited or the society is going to end up embedded within an
</em><br>
<em>&gt; ever more complex system that spends most of its resources monitoring
</em><br>
<em>&gt; itself.
</em><br>
<p>I don't think this necessarily is the case. In my view, the Sysop only
<br>
had to be as complex (in terms of intelligence, not capacity) as the
<br>
most complex entity in Sysop Space. Everytime it runs into something
<br>
new it will as you say need to evaluate it, but after that it will
<br>
already have the &quot;recipe&quot; stored for it. What it really comes down
<br>
to is perhaps granularity- can the Sysop distribute copies of itself
<br>
around to different locales as the Space grows? If so, I don't see the
<br>
potential for some kind of complexity explosion. Again it looks to me
<br>
more like a relatively stable underlying cost of future society, just
<br>
like electricity or government is for us today.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; The idea that we need someone to protect ourselves from ourselves in this
</em><br>
<em>&gt; way really hinges on the idea that certain technologies are instantly and
</em><br>
<em>&gt; throughly devastating, and assumes that the only way they can be handled is
</em><br>
<em>&gt; if as few beings as possible get their manipulators on them. Both of these
</em><br>
<p>No actually not. I personally wouldn't like it even if a relatively small
<br>
tech catastrophe only killed off 100k people. Also I reiterate the Sysop
<br>
Scenario does not limit handling these technologies, it only limits using
<br>
them in a certain way against a certain person or group of people, etc.
<br>
<p><em>&gt; assumptions are debatable, and I think it is dangerous to blithely say that
</em><br>
<em>&gt; the sysop scenario is the only realistic alternative to global death. It
</em><br>
<p>I'm waiting to hear other scenarios. Gambling is not one, or at least not
<br>
one that isn't quite a bit more debatable and dangerous.
<br>
<p><em>&gt; forecloses further analysis of the assumptions and alternatives by
</em><br>
<em>&gt; suggesting that the question is really settled and there is only one way.
</em><br>
<p>Just to make it clear I do not hold this viewpoint. I am eager to find an
<br>
even better theoretical solution. So far I don't see any. Just because we
<br>
survived nuclear war (so far) does not mean we will automagically survive
<br>
death when 6 billion individuals possess far greater powers. From my
<br>
perspective anyone advocating a completely &quot;hands off&quot; attitude to the
<br>
future is acting extremely irresponsibly.
<br>
<p><em>&gt; It also paints a simplistic picture of choices that could easily be used to
</em><br>
<em>&gt; attack transhumanism, either as strongly hegemonic (&quot;They want to make a
</em><br>
<em>&gt; computer to rule the world! Gosh, I only thought those claims they are
</em><br>
<em>&gt; actually thinking like movie mad scientists were just insults.&quot;) or as
</em><br>
<em>&gt; totally out of touch with reality (&quot;Why build a super-AI (as if such stuff
</em><br>
<em>&gt; could exist in the first place) when we can just follow Bill Joy and
</em><br>
<em>&gt; relinquish dangerous technology?!&quot;).
</em><br>
<p>Well you'll be glad to know we pretty much wiped out all references to this
<br>
on our website months ago. The new meme is &quot;Transition Guide&quot;. Feedback
<br>
would be great.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; A lot of this was hashed through in the article about nanarchy in Extropy,
</em><br>
<em>&gt; I think. Nothing new under the Dyson.
</em><br>
<p>I don't think I've seen that, does anyone have a quick URL?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Existential risks are a problem, but I'm worried people are overly
</em><br>
<em>&gt; simplistic when solving them. My own sketch above about a self-organised
</em><br>
<em>&gt; critical state is complex, messy and hard to analyse in detail because it
</em><br>
<em>&gt; is evolving. That is a memetic handicap, since solutions that are easy to
</em><br>
<em>&gt; state and grasp sound so much better - relinquishment of technology or
</em><br>
<em>&gt; sysops fit in wonderfully with the memetic receptors people have. But
</em><br>
<em>&gt; complex problems seldom have simple neat solutions. Bill Joy is wrong
</em><br>
<em>&gt; because his solution cannot practically work, the sysop is simple to state
</em><br>
<em>&gt; but hides all the enormous complexity inside the system.
</em><br>
<em>&gt; 
</em><br>
<p>Well Anders, it is not as if we here are simpletons ok? We've thought
<br>
through these issues as much as anyone else around here. We just came
<br>
up with a different and quite possibly superior solution. I think
<br>
attacking is because it sounds simple is not very useful. Sometimes
<br>
elegant ideas are better. Sometimes leaving things to chaos is bad. I
<br>
think the burden of proof is as much on you as on me here, so I at
<br>
this point remain unconvinced to say the least that a &quot;messy&quot; future
<br>
is either less risky or more desirable.
<br>
<pre>
-- 
Brian Atkins
Singularity Institute for Artificial Intelligence
<a href="http://www.singinst.org/">http://www.singinst.org/</a>
</pre>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2995.html">Brian Atkins: "Re: Stem Cell Debate --Banned in the USA ---&gt; libertarian societies as   unicorn"</a>
<li><strong>Previous message:</strong> <a href="2993.html">James Rogers: "Re: Chem: Silicon Explosives now"</a>
<li><strong>In reply to:</strong> <a href="2970.html">Anders Sandberg: "Re: Sysops and enlightened despots"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2971.html">Reason: "RE: Stem Cell Debate --Banned in the USA ---&gt; libertarian societies as  unicorn"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2994">[ date ]</a>
<a href="index.html#2994">[ thread ]</a>
<a href="subject.html#2994">[ subject ]</a>
<a href="author.html#2994">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Fri Oct 12 2001 - 14:40:01 MDT</em>
</em>
</small>
</body>
</html>
