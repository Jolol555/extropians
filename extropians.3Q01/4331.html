<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: The Dazzle Effect:  Staring into the Singularity</title>
<meta name="Author" content="Charles Hixson (charleshixsn@earthlink.net)">
<meta name="Subject" content="Re: The Dazzle Effect:  Staring into the Singularity">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: The Dazzle Effect:  Staring into the Singularity</h1>
<!-- received="Thu Aug 16 16:20:47 2001" -->
<!-- isoreceived="20010816222047" -->
<!-- sent="Thu, 16 Aug 2001 15:20:18 -0700" -->
<!-- isosent="20010816222018" -->
<!-- name="Charles Hixson" -->
<!-- email="charleshixsn@earthlink.net" -->
<!-- subject="Re: The Dazzle Effect:  Staring into the Singularity" -->
<!-- id="3B7C4722.8050505@earthlink.net" -->
<!-- inreplyto="3B7C1665.EA6F6469@pobox.com" -->
<strong>From:</strong> Charles Hixson (<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20The%20Dazzle%20Effect:%20%20Staring%20into%20the%20Singularity&In-Reply-To=&lt;3B7C4722.8050505@earthlink.net&gt;"><em>charleshixsn@earthlink.net</em></a>)<br>
<strong>Date:</strong> Thu Aug 16 2001 - 16:20:18 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4332.html">Robert Coyote: "Re: SOCIETY: The privatization of public security in South America?"</a>
<li><strong>Previous message:</strong> <a href="4330.html">natashavita@earthlink.net: "RE: Mind/Body dualism What's the deal?"</a>
<li><strong>In reply to:</strong> <a href="4310.html">Eliezer S. Yudkowsky: "Re: The Dazzle Effect:  Staring into the Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4340.html">Lee Corbin: "RE: The Dazzle Effect:  Staring into the Singularity"</a>
<li><strong>Reply:</strong> <a href="4340.html">Lee Corbin: "RE: The Dazzle Effect:  Staring into the Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4331">[ date ]</a>
<a href="index.html#4331">[ thread ]</a>
<a href="subject.html#4331">[ subject ]</a>
<a href="author.html#4331">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Eliezer S. Yudkowsky wrote:
<br>
<em>&gt; Charles Hixson wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;Just a few data points.
</em><br>
<em>&gt;&gt;1) Yesterday I reread Staring into the Singularity, and did a bit of
</em><br>
<em>&gt;&gt;arithmetic.  Assuming that in March the computer speed was, indeed,
</em><br>
<em>&gt;&gt;doubling every 18 mos.  And assuming, as indicated, that each successive
</em><br>
<em>&gt;&gt;doubling takes half the time of the previous one, then in March 2004 the
</em><br>
<em>&gt;&gt;curve goes vertical.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Say what?  Okay, first of all, that was a metaphor, not a projection.  And
</em><br>
<em>&gt; secondarily, it would be a metaphor that would only start *after* you had
</em><br>
<em>&gt; human-equivalent AI researchers running on the existing silicon.  We don't
</em><br>
<em>&gt; have this now and therefore there is no imaginable reason why the curve
</em><br>
<em>&gt; would go vertical in March 2004.  Today it's just the same 'ol same 'ol
</em><br>
<em>&gt; doubling every eighteen months.
</em><br>
If you want to say metaphor rather than projection, that's reasonable. 
<br>
I'm the one who threw numbers in.  But though they may not be human 
<br>
equivalent, there are computers involved in nearly every phase of chip 
<br>
design.  So I think the reinforcement effect that you mentioned has 
<br>
already started.  Actually, I think that it started years ago, but it's 
<br>
hard to notice in the early part of the curve.  And as each step of the 
<br>
process becomes automated, the reinforcement effect is stronger.  (And I 
<br>
don't have enough data points to allow me to tell what the -current- 
<br>
rate of doubling is.  But it sure has been getting shorter.
<br>
(Note:  I'm pretty sure that this automating of the manufacturing 
<br>
processes is one of the steps needed for the injection of an intelligent 
<br>
AI to have the effect that you postulated.  And I expect it to be there. 
<br>
&nbsp;&nbsp;It just isn't going to wait for the AI before showing up.)
<br>
<p>A good part of the problem of prediction is that it is quite difficult 
<br>
to know what the shape of the curve is.  And another is guessing how 
<br>
difficult the unsolved problems are.  And trend curves are nortorious 
<br>
liars.  (I remember one that said we would have infinite electrical 
<br>
power generation by around 1984.)  The most that can be hoped is that 
<br>
they will be guideposts.  That said, this curve, whatever it's real 
<br>
shape, has much more reason behind it than most I have seen.  And, of 
<br>
course, the cycles predicted by Moore's law have been shrinking.  There 
<br>
was a time not long ago when I never expected to see a 25GB disk drive. 
<br>
(OK, that's a different, reinforcing, curve.)
<br>
<p><em>&gt; 
</em><br>
<em>&gt; If you do have human-equivalent AI, the intelligence curve probably goes
</em><br>
<em>&gt; vertical after a couple of weeks, not a couple of years, because
</em><br>
<em>&gt; improvement in software and the absorption of additional hardware should
</em><br>
<em>&gt; be easily sufficient to carry a human-equivalent AI to superintelligence
</em><br>
<em>&gt; and nanotechnology.
</em><br>
<em>&gt; 
</em><br>
Understood.  But when I read that section, it seemed to be that a lot of 
<br>
the effects being analysed had already kicked in, though at a slower 
<br>
speed, due to needing to be transduced through human engineers.
<br>
<p><em>&gt; 
</em><br>
<em>&gt;&gt;6) I think it was in C Friendly AI that the projection was made that a
</em><br>
<em>&gt;&gt;minimum system needed by a seed AI would be 32 processors running at 2 GHz.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; No, that was a number I pulled out of thin air so as to have an example. 
</em><br>
<em>&gt; Moreover, it wasn't an example of a finished seed AI, but rather an AI in
</em><br>
<em>&gt; the process of development.
</em><br>
<em>&gt; 
</em><br>
OK, but I was think the developement can be started (though less 
<br>
effectively) one an even lower power computer than that.  How far it can 
<br>
be carried...  well, that may be a different matter.  Clearly a 
<br>
uniprocessor that takes all night to build a kernel is to &quot;weak&quot; for 
<br>
much serious work, but perhaps it could be used to settle some 
<br>
architectural questions, etc.  And if not a seed, could a least be a oocyte.
<br>
<p>Actually, I think that there will eventually turn out to be a lineal 
<br>
descent between the work being done now and what will eventually arise. 
<br>
&nbsp;&nbsp;And this is one of the reasons that it is important that the ground 
<br>
work for friendliness be designed as quickly as possible.  The curve 
<br>
that I postulated was an immensely optomistic/threatening one, but not 
<br>
totally beyond reason.  A quite weak reason, I grant you, but in this 
<br>
case, perhaps one would wish to be prepared early.
<br>
<p><em>&gt; 
</em><br>
<em>&gt;&gt;7) So by the end of the year it should be possible to put together a
</em><br>
<em>&gt;&gt;system about twice the minimum strength for around $10,000.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The number was specifically chosen to be achievable using present-day
</em><br>
<em>&gt; technologies, rather than say those of five years down the road.  It's a
</em><br>
<em>&gt; minimum strength for development.  Not human equivalence.  If human
</em><br>
<em>&gt; equivalence was that cheap we probably really would have AI by now,
</em><br>
<em>&gt; software problem or no.
</em><br>
<em>&gt; 
</em><br>
It's a wild guess, but my guess is that the first fully capable 
<br>
development system that gets a working seed will be able to leverage 
<br>
it's computing power, possibly via something like the SETI, into a fully 
<br>
capable system.  Only the most connected tasks would be done locally, 
<br>
and all weakly connectec or low-priority tasks would be farmed out.  So 
<br>
the total MegaFlops available wouldn't be measured by the speed of the 
<br>
main host.  That would just be for code that was time-sensitive.  This 
<br>
would, of course, drastically decrease the speed of the AI, but would 
<br>
compensate by enabling it to crack tougher problems.
<br>
<em>&gt; 
</em><br>
<em>&gt;&gt;10) The assertion has been made that the more computing power you have,
</em><br>
<em>&gt;&gt;the easier it is to end up with an AI (though not necessarily a friendly
</em><br>
<em>&gt;&gt;one).  So by the middle of next year any reasonably profitable business
</em><br>
<em>&gt;&gt;should be able to put together enough computing power, if it chooses to.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Any profitable business can already put together enough hardware that they
</em><br>
<em>&gt; could start developing if they knew what they were doing.  This was
</em><br>
<em>&gt; probably true even back in 1995.  But to get AI without knowing what
</em><br>
<em>&gt; you're doing, you probably need substantially more hardware than exists in
</em><br>
<em>&gt; the human mind.
</em><br>
<p>But people will know what they are doing.  They are making the system 
<br>
easier to use.  They are enabling it to solve problems better.  They are 
<br>
improving the user interface.  They are consolidating the company 
<br>
resources.  Etc.  I would agree that none of these is sufficient, but 
<br>
some combination might be.  Particularly if the system has to figure out 
<br>
what the answer is to a problem the user is having.  That would use a 
<br>
good deal of what is needed to get an AI going just by itself.  And 
<br>
Boeing is currently working with a computer physical modeling system to 
<br>
determine how planes should be designed to minimize damage to passengers 
<br>
in case of a crash.  Not AI, but a big chunk of it.
<br>
<p>So take the Boeing system, and make it easy for the engineers and their 
<br>
managers to use, and then let it be expanded to handle the projection of 
<br>
desireable business strategies.  It wouldn't take too many cycles of 
<br>
expansion to get something that deserved the label of AI, even if it 
<br>
wasn't at all like what we had been thinking of.  And that's just one 
<br>
company.  IBM is more likely to do it on purpose.
<br>
<em>&gt; 
</em><br>
<em>&gt; However, if the Moon were made of computronium instead of green cheese (as
</em><br>
<em>&gt; Eugene Leitl put it), it would probably *not* take all that much work to
</em><br>
<em>&gt; get it to wake up.  At that scale (endless quintillions of brainpower) it
</em><br>
<p>I'm not really sure that I accept this line of argument.  Bridges are 
<br>
strong, but they don't grow muscles.  You need to start adding in power 
<br>
systems so that they flex if they sense an earthquake.  And control 
<br>
systems.  And sensors.  A big adding machine would probably stay an 
<br>
adding machine if nothing else were done.  Easier, yes.  Inevitable, I 
<br>
don't think so.  It could manage with much less efficient algorithms, 
<br>
but it would still need to be aimed at something near the right goal
<br>
<p><em>&gt; becomes possible to brute-force an evolutionary algorithm in which the
</em><br>
<em>&gt; individual units are millions of brainpower.  If the Moon were made of
</em><br>
<em>&gt; computronium instead of green cheese I doubt it would take so much as a
</em><br>
<em>&gt; week for me *or* Eugene Leitl to wake it up, if it didn't wake up before
</em><br>
<em>&gt; then due to self-organization of any noise in the circuitry.  If the Moon
</em><br>
<em>&gt; were made of computronium you could probably write one self-replicating
</em><br>
<em>&gt; error-prone program a la Tierra and see it wake up not too long
</em><br>
<em>&gt; afterwards.  A lunar mass is one bloody heck of a lot of computronium.
</em><br>
<p>OK.  That way would work.
<br>
<em>&gt; 
</em><br>
<em>&gt; Friendliness is an entirely different issue, although if the evolutionary
</em><br>
<em>&gt; process goes through a pattern roughly equivalent to human evolution,
</em><br>
<p>Actually, I think that if you want to depend on evolution to give you 
<br>
friendliness, then you had better have a development environment 
<br>
substantially different from that of people.  There are an awful lot of 
<br>
extinct spieces not around any more.
<br>
<p><em>&gt; ...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; --              --              --              --              -- 
</em><br>
<em>&gt; Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
</em><br>
<em>&gt; Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
But what really happened was that I just plugged the numbers in, and was 
<br>
shocked.  I hadn't been expecting to get a number less than 2005.  My 
<br>
&quot;projections&quot; have been 2005-2030 ever since I read the Vernor Vinge 
<br>
paper.  To come up with an earlier estimate rather than a later one was 
<br>
a shock.  (That's why I titled it the Dazzle Effect... I was fairly sure 
<br>
that once my &quot;vision&quot; recovered, I'd see something besides blinding lights.)
<br>
<pre>
-- 
Charles Hixson
<p>Copy software legally, the GNU way!
Use GNU software, and legally make and share copies of software.
See <a href="http://www.gnu.org">http://www.gnu.org</a>
     <a href="http://www.redhat.com">http://www.redhat.com</a>
     <a href="http://www.linux-mandrake.com">http://www.linux-mandrake.com</a>
     <a href="http://www.calderasystems.com/">http://www.calderasystems.com/</a>
     <a href="http://www.linuxapps.com/">http://www.linuxapps.com/</a>
</pre>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4332.html">Robert Coyote: "Re: SOCIETY: The privatization of public security in South America?"</a>
<li><strong>Previous message:</strong> <a href="4330.html">natashavita@earthlink.net: "RE: Mind/Body dualism What's the deal?"</a>
<li><strong>In reply to:</strong> <a href="4310.html">Eliezer S. Yudkowsky: "Re: The Dazzle Effect:  Staring into the Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4340.html">Lee Corbin: "RE: The Dazzle Effect:  Staring into the Singularity"</a>
<li><strong>Reply:</strong> <a href="4340.html">Lee Corbin: "RE: The Dazzle Effect:  Staring into the Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4331">[ date ]</a>
<a href="index.html#4331">[ thread ]</a>
<a href="subject.html#4331">[ subject ]</a>
<a href="author.html#4331">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Fri Oct 12 2001 - 14:40:10 MDT</em>
</em>
</small>
</body>
</html>
