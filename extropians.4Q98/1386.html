<!-- received="Thu Nov 19 13:12:29 1998 MST" -->
<!-- sent="Thu, 19 Nov 1998 15:12:37 -0500" -->
<!-- name="Michael Lorrey" -->
<!-- email="retroman@together.net" -->
<!-- subject="Re: distributed AI was Re: computronium prime-oxide" -->
<!-- id="36547BB4.3C0648DD@together.net" -->
<!-- inreplyto="distributed AI was Re: computronium prime-oxide" -->
<!-- version=1.10, linesinbody=39 -->
<html><head><title>extropy: Re: distributed AI was Re: computronium prime-oxide</title>
<meta name=author content="Michael Lorrey">
<link rel=author rev=made href="mailto:retroman@together.net" title ="Michael Lorrey">
</head><body>
<h1>Re: distributed AI was Re: computronium prime-oxide</h1>
Michael Lorrey (<i>retroman@together.net</i>)<br>
<i>Thu, 19 Nov 1998 15:12:37 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1386">[ date ]</a><a href="index.html#1386">[ thread ]</a><a href="subject.html#1386">[ subject ]</a><a href="author.html#1386">[ author ]</a>
<!-- next="start" -->
<li><a href="1387.html">[ Next ]</a><a href="1385.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1384.html">Hal Finney</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Hal Finney wrote:

<p>
<a href="1384.html#1386qlink1">&gt; Timothy Bates, &lt;tbates@bunyip.bhs.mq.edu.au&gt;, writes:</a><br>
<i>&gt; &gt; I asked Daniel Dennett if he was a Turing functionalist: he said yes and</i><br>
<i>&gt; &gt; no ;-)</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; I think he said yes, but that interaction is critical, he views</i><br>
<i>&gt; &gt; intelligence as being embedded in the machines environment as well as in</i><br>
<i>&gt; &gt; its own hardware. For this reason he doubted that a paper tape Turing</i><br>
<i>&gt; &gt; machine could be conscious as the environmental context on which</i><br>
<i>&gt; &gt; consciousness is scaffolded (in his, but not my) view, would be invisible</i><br>
<i>&gt; &gt; to it: a kind of Nyquist limit for implementation speeds.</i><br>
<i>&gt;</i><br>
<i>&gt; I don't understand this view.</i><br>
<i>&gt;</i><br>
<a name="1395qlink1"><i>&gt; Do you think Dennett would predict that a person would instantly become</i><br>
<i>&gt; unconscious if he were put into a complete sensory deprivation booth?</i><br>
<i>&gt; It seems pretty obvious to me that he would continue to be conscious.</i><br>
<i>&gt; There was a fad for sensory deprivation in the 1970s and from what I</i><br>
<i>&gt; understand you sometimes would begin to hallucinate after a while, but</i><br>
<i>&gt; you certainly still remained conscious (and I suspect the hallucination</i><br>
<i>&gt; had much to do with expectations).</a></i><br>
<i>&gt;</i><br>
<i>&gt; In any case, surely the Turing machine could include a modest VR</i><br>
<i>&gt; simulation in addition to the brain simulation, sufficient to provide</i><br>
<i>&gt; virtual environmental interaction.  Or would Dennett claim that</i><br>
<i>&gt; consciousness only occurs when the TM interacts with the "real world"?</i><br>
<i>&gt; That would seem absurd.</i><br>

<p>
<a name="1403qlink2"><a name="1395qlink2">I dunno. Think of a fetus. Do you know of any cases whatsoever where the fetus
was aware and thinking while in the womb, which is a remarkable sensory
deprivation chamber. A human being does not really become aware until at least
a few weeks to a few months after being born. They need time to learn how to
sort out all of the sensory imput into a rational format.</a></a>

<p>
Mike Lorrey
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1387.html">[ Next ]</a><a href="1385.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1384.html">Hal Finney</a>
<!-- nextthread="start" -->
</ul>
</body></html>
