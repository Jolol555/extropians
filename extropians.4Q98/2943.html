<!-- received="Tue Dec 15 09:08:18 1998 MST" -->
<!-- sent="Tue, 15 Dec 1998 10:07:53 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Challenge of Design Complexity" -->
<!-- id="005001be2845$133f90e0$352501c0@mfg130" -->
<!-- inreplyto="3.0.3.32.19981214102317.00c3e2e4@econ.berkeley.edu" -->
<!-- version=1.10, linesinbody=62 -->
<html><head><title>extropy: RE: Challenge of Design Complexity</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Challenge of Design Complexity</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Tue, 15 Dec 1998 10:07:53 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2943">[ date ]</a><a href="index.html#2943">[ thread ]</a><a href="subject.html#2943">[ subject ]</a><a href="author.html#2943">[ author ]</a>
<!-- next="start" -->
<li><a href="2944.html">[ Next ]</a><a href="2942.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2788.html">Robin Hanson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2963.html">Robin Hanson</a>
</ul>
<!-- body="start" -->

<p>
Robin Hanson wrote:
<br>
<a name="2963qlink1"><a href="2788.html#2943qlink1">&gt; Similar issues have been considered.  For example, here is an exerpt from</a><br>
<i>&gt; <a href="http://www.extropy.org/eo/articles/vc.html#hanson">http://www.extropy.org/eo/articles/vc.html#hanson</a></i><br>

<p>
Yes, I've read the Singularity debate.  My argument is a rather different
one that your comment about IQ enhancement, and it leads to different
conclusions.

<p>
If you make an AI that increases its own IQ by 1%, that tells you nothing
about how difficult the next 1% improvement will be.  It could be 10%
harder, or 0.5% harder, or not any harder at all.  The only way to find out
is to try it and see what happens.</a>

<p>
What we can predict is that if you try to repeatedly enhance one narrow
ability, you will eventually reach a point of diminishing returns.  At some
point, you will find that further improvements to ability Y are impractical,
because ability X can not handle the increased complexity of your design.

<p>
This effect is not unique to intelligence enhancement.  Something similar
occurs whenever any narrow field of research pulls ahead of the general rate
of progress.  Eventually it reaches a point where further advances require
improvements in some unrelated technology, and you have to wait for those
improvements to be made.

<p>
<a name="2963qlink2">Now, this does not mean that the whole thing is a game of diminishing
<a name="2993qlink1">returns.  Recent history demonstrates that if you research</a> enough different
<a name="2993qlink2">things, you can create a situation in which the ability of your</a> society to
<a name="2993qlink3">make technological advances increases faster than the difficulty</a> of taking
the next step.  A reasonable extrapolation of the trend would predict a
century or two of steadily-accelerating progress before things begin to
change so fast that an unenhanced human can't cope.</a>

<p>
<a name="2963qlink3">Intelligence enhancement (IE) of any kind would, however,  add a new
dimension to this saga.<a name="2993qlink6">  Roughly speaking, the our rate of progress is
determined by:

<pre>
					    R * P * I
	Progress / unit of time =  -----------
						T * C

</pre>
<p>
Where R represents the resources available to each researcher, P is the
</a>
population of researchers, I is the average intelligence of the researchers,
T is our current level of technological sophistication, and C is a measure
of the time and effort required for researchers to communicate.</a>  Most of the
<a name="2993qlink10"><a name="2963qlink4">increasing rate of change in recent times comes from a slow geometric
increase in both R and P, and a steady drop in C.  Since the changes in R
and C are both due to technology, the whole process tends to feed</a> on itself.

<p>
Meaningful IE would make I increase in roughly the same fashion as R.  Not
only would this dramatically speed up our rate of advance, it would also
increase the rate at which our rate of advance speeds up.</a>


<p>
This post is long enough already, so I'll leave off speculating about the
Singularity itself until next time.


<p>
Billy Brown, MCSE+I
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2944.html">[ Next ]</a><a href="2942.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2788.html">Robin Hanson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2963.html">Robin Hanson</a>
</ul>
</body></html>
