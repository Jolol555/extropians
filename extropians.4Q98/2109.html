<!-- received="Fri Dec  4 15:43:33 1998 MST" -->
<!-- sent="Fri, 04 Dec 1998 17:48:09 -0500" -->
<!-- name="Dan Fabulich" -->
<!-- email="daniel.fabulich@yale.edu" -->
<!-- subject="Re: Singularity: Individual, Borg, Death?" -->
<!-- id="199812042243.RAA06650@pantheon-po03.its.yale.edu" -->
<!-- inreplyto="36685CFF.FE2ABD22@pobox.com" -->
<!-- version=1.10, linesinbody=32 -->
<html><head><title>extropy: Re: Singularity: Individual, Borg, Death?</title>
<meta name=author content="Dan Fabulich">
<link rel=author rev=made href="mailto:daniel.fabulich@yale.edu" title ="Dan Fabulich">
</head><body>
<h1>Re: Singularity: Individual, Borg, Death?</h1>
Dan Fabulich (<i>daniel.fabulich@yale.edu</i>)<br>
<i>Fri, 04 Dec 1998 17:48:09 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2109">[ date ]</a><a href="index.html#2109">[ thread ]</a><a href="subject.html#2109">[ subject ]</a><a href="author.html#2109">[ author ]</a>
<!-- next="start" -->
<li><a href="2110.html">[ Next ]</a>
<b>In reply to:</b> <a href="2108.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="2131qlink1">Eliezer S. Yudkowsky wrote:
<br>
<a href="2108.html#2109qlink1">&gt;You have some people who say, "The choices are all the same; I'll do what</a><br>
<i>&gt;seems best to me."  At a higher level of self-awareness, you have:  "I'll</i><br>
<i>&gt;stick with the evolutionary system I was born in, they're all the same."</i><br>
At a
<br>
<a href="2108.html#2109qlink2">&gt;higher level of self-awareness, you say:  "Which new system you choose</a><br>
depends
<br>
<a href="2108.html#2109qlink3">&gt;on how your current system evaluates that choice."  Choices, systems,</a><br>
<i>&gt;trajectories... but I want to jump out of the system and choose the real</i><br>
answer.</a>

<p>
Will a "superintelligence" be able to "jump out of the system and choose
the real answer?"  Just because you're superintelligent doesn't mean you're
transcendent.

<p>
Similarly, I think there's a strong argument against being able to "jump
out of the system" without resulting in a universe where we can't even make
probabilistic guesses about truth and falsehood.  Logic is an excellent
example of this.  We have a notion that things generally follow logically
from one another, but this depends inherently on certain principles of
non-contradiction, rules of inference, etc.  If you jump too far out of
such a system, you'll have no logic at all.

<p>
You make an apt point when you note that "which new system you choose
depends on how your current system evaluates that choice."  But why do you
think superintelligences wouldn't be bound by such a rule?

<p>
-Dan

<p>
	-GIVE ME IMMORTALITY OR GIVE ME DEATH-
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2110.html">[ Next ]</a>
<b>In reply to:</b> <a href="2108.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
