<!-- received="Wed Dec  9 21:33:07 1998 MST" -->
<!-- sent="Thu, 10 Dec 1998 04:31:41 +0000" -->
<!-- name="Nick Bostrom" -->
<!-- email="bostrom@ndirect.co.uk" -->
<!-- subject="Re: Infinite utility (was: Re: Pascal's Wager)" -->
<!-- id="199812100432.EAA18391@andromeda.hosts.netdirect.net.uk" -->
<!-- inreplyto="3.0.3.32.19981209092648.00cd0ec4@econ.berkeley.edu" -->
<!-- version=1.10, linesinbody=43 -->
<html><head><title>extropy: Re: Infinite utility (was: Re: Pascal's Wager)</title>
<meta name=author content="Nick Bostrom">
<link rel=author rev=made href="mailto:bostrom@ndirect.co.uk" title ="Nick Bostrom">
</head><body>
<h1>Re: Infinite utility (was: Re: Pascal's Wager)</h1>
Nick Bostrom (<i>bostrom@ndirect.co.uk</i>)<br>
<i>Thu, 10 Dec 1998 04:31:41 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2365">[ date ]</a><a href="index.html#2365">[ thread ]</a><a href="subject.html#2365">[ subject ]</a><a href="author.html#2365">[ author ]</a>
<!-- next="start" -->
<li><a href="2366.html">[ Next ]</a><a href="2364.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2330.html">Robin Hanson</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Robin Hanson wrote:

<p>
<a href="2330.html#2365qlink1">&gt; Flipping the argument around, anyone who doesn't choose their actions</a><br>
<i>&gt; soley on how it influences the chance for eternal bliss *does* </i><br>
<i>&gt; discount future benefits.  Thus the vast majority of people do </i><br>
<i>&gt; discount the future.  </i><br>

<p>
Yes, that was what I suggested too. The problem that I was concerned 
with, however, was not for decision theory as such, but for a certain 
ethical theory, utilitarianism (and possibly for one theory of 
personal identity as well). It seems wrong to say that in an 
infinite universe there would be nothing morally wrong with torturing 
innocent people for a little fun (especially considering that our 
universe probably really is infinite, according to recent data). Yet, 
on the standard version of utilitarianism, an action is good if and 
only if it maximizes total expected utility (where every equally 
sentient being counts for equally much). So such torture would be a 
good action on that theory, since it maximizes utility (like all 
other actions). So this version of utilitarianism gives an 
unacceptable result. That problem is that when you decide to give 
everybody's well-being the same weight, then discount factors won't 
remove the infinity.

<p>
One remedy that one might attempt (which I mentioned in my reply to 
Dan) would be to extend standard decision theory in the following 
way. In order to determine whether an action A is right:


<OL>
  <li>  For each action, calculate how likely it would be that it would 
lead to an infinite amount of utility (x%) and how likely it would 
lead to an infinite amount of negative utility (y%). If x - y is 
greater for some other action than for A, then A is wrong. If not, go 
to step 2.

  <li>  Disregard all the outcomes where there is either an infinite 
utility or an infinite negative utility. Calculate the expected 
utility over all remaining outcomes. If some other action has a 
higher expected utility than A, then A is wrong. Otherwise A is 
right.
Nick Bostrom
<a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a>      n.bostrom@lse.ac.uk
Department of Philosophy, Logic and Scientific Method
London School of Economics

</OL>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2366.html">[ Next ]</a><a href="2364.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2330.html">Robin Hanson</a>
<!-- nextthread="start" -->
</ul>
</body></html>
