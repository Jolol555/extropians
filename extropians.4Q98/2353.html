<!-- received="Wed Dec  9 14:14:55 1998 MST" -->
<!-- sent="Wed, 9 Dec 1998 16:03:57 -0500" -->
<!-- name="Doug Bailey" -->
<!-- email="Doug.Bailey@ey.com" -->
<!-- subject="Emergent Properties of SIs (was Re: Singularity: AI Morality)" -->
<!-- id="199812092114.QAA21487@gateway2.ey.com" -->
<!-- inreplyto="Singularity: AI Morality)" -->
<!-- version=1.10, linesinbody=32 -->
<html><head><title>extropy: Emergent Properties of SIs (was Re: Singularity: AI Morality)</title>
<meta name=author content="Doug Bailey">
<link rel=author rev=made href="mailto:Doug.Bailey@ey.com" title ="Doug Bailey">
</head><body>
<h1>Emergent Properties of SIs (was Re: Singularity: AI Morality)</h1>
Doug Bailey (<i>Doug.Bailey@ey.com</i>)<br>
<i>Wed, 9 Dec 1998 16:03:57 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2353">[ date ]</a><a href="index.html#2353">[ thread ]</a><a href="subject.html#2353">[ subject ]</a><a href="author.html#2353">[ author ]</a>
<!-- next="start" -->
<li><a href="2354.html">[ Next ]</a><a href="2352.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2343.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eliezer Yudkowsky wrote:

<p>
<a href="2343.html#2353qlink1">&gt; As for pulling this trick on genuine SIs:</a><br>
<i>&gt; </i><br>
<i>&gt; This would ENSURE that at least one of the SIs went nuts, </i><br>
<i>&gt; broke out of your little sandbox, and stomped on the planet!  </i><br>
<i>&gt; This multiplies the risk factor by a hundred times for no </i><br>
<i>&gt; conceivable benefit!  I would rather have three million</i><br>
<i>&gt; lines of Asimov Laws written in COBOL than run evolutionary </i><br>
<i>&gt; simulations!  No matter how badly you screw up ONE mind, </i><br>
<i>&gt; there's a good chance it will shake it off and go sane!</i><br>

<p>
The greatest potential for benefit and/or cost to our minds 
as a result of the emergence of SI (whether weak or strong) are
the properties that will emerge from intelligent conscious systems
far more complex than our own minds.  The only two possibilities
I can think of off the top of my head are Strong superintelligence
and some form of superconsciousness.  However, there most probably are
emergent properties that are utterly beyond our minds to predict 
or grasp.

<p>
While it is a conundrum to speculate in such a manner, it is hard
to substantiate that intelligence or consciousness would have
been predictable properties of the increasing complexity of
self-replicating systems.  The conundrum is that nothing would have
been able to attempt such a prediction prior to the emergence of
these properties without tainting the predictive process.

<p>
Doug Bailey
<br>
doug.bailey@ey.com
<br>
nanotech@cwix.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2354.html">[ Next ]</a><a href="2352.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2343.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
