<!-- received="Thu Oct  1 14:38:21 1998 MST" -->
<!-- sent="Thu, 01 Oct 1998 15:45:16 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Economic Growth Assuming Machine Intelligence" -->
<!-- id="3613E9DB.6D4DB9B3@pobox.com" -->
<!-- inreplyto="Economic Growth Assuming Machine Intelligence" -->
<!-- version=1.10, linesinbody=45 -->
<html><head><title>extropy: Re: Economic Growth Assuming Machine Intelligence</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Economic Growth Assuming Machine Intelligence</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Thu, 01 Oct 1998 15:45:16 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#11">[ date ]</a><a href="index.html#11">[ thread ]</a><a href="subject.html#11">[ subject ]</a><a href="author.html#11">[ author ]</a>
<!-- next="start" -->
<li><a href="0012.html">[ Next ]</a><b>In reply to:</b> <a href="0001.html">morris/arla johnson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0012.html">Robin Hanson</a>
</ul>
<!-- body="start" -->

<p>
Robin Hanson wrote:
<br>
<i>&gt; </i><br>
<a href="0001.html#0011qlink1">&gt; A simple neo-classical growth model examines the implications</a><br>
<i>&gt; of machine intelligence, machines which could substitute for,</i><br>
<i>&gt; rather than complement, human labor.<a name="0012qlink1">  Steady state growth rates</i><br>
<i>&gt; could easily rise by an order of magnitude or more.  But with</i><br>
<i>&gt; steady growth, wages and per-intelligence consumption could fall</i><br>
<i>&gt; as fast as computer prices do.</i><br>

<p>
I shall leave aside my personal objections, since I think you may find
Gubrud's arguments more convincing;</a> they use essentially the same set of
assumptions.  While the main focus is on nanotech, it also deals with the
effects of advanced artificial intelligence.

<p>
"Nanotechnology and International Security"
<a href="http://squid.umd.edu/~gubrud/">http://squid.umd.edu/~gubrud/</a>

<p>
(From the abstract:)
<br>
"Whereas the perfection of nuclear explosives established a strategic
stalemate, advanced molecular manufacturing based on self-replicating systems,
or any military production system fully automated by advanced artificial
intelligence, would lead to instability in a confrontation between rough
equals. Rivals would feel pressured to preempt, if possible, in initiating a
full-scale military buildup, and certainly not to be caught behind. As the
rearmament reached high levels, close contact between forces at sea and in
space would give an advantage to the first to strike."

<p>
In the event that you've read it already, which seems probable, I would just
like to say this:  The flip side of rapid progress is incredibly destructive
wars.  And if you think the currency meltdown is causing global
destabilization, the differential equations you blithely toss around would
shatter national economies like glass.  Since I don't believe a Weak
Singularity is probable, I can say dispassionately that the Weak Singularity
your paper models would probably end in the violent death of a significant
fraction of mankind.  The average human might be freed from the "stresses" of
the working life and the necessity of making a living, but not to engage in a
life of leisure.  Judging by the Middle Ages, those who do not work for a
living generally occupy themselves with wars of conquest.
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0012.html">[ Next ]</a><b>In reply to:</b> <a href="0001.html">morris/arla johnson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0012.html">Robin Hanson</a>
</ul>
</body></html>
