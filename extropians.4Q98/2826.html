<!-- received="Mon Dec 14 14:28:12 1998 MST" -->
<!-- sent="Mon, 14 Dec 1998 15:30:02 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Singularity: AI Morality" -->
<!-- id="36758350.642E3900@pobox.com" -->
<!-- inreplyto="Singularity: AI Morality" -->
<!-- version=1.10, linesinbody=31 -->
<html><head><title>extropy: Re: Singularity: AI Morality</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Singularity: AI Morality</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Mon, 14 Dec 1998 15:30:02 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2826">[ date ]</a><a href="index.html#2826">[ thread ]</a><a href="subject.html#2826">[ subject ]</a><a href="author.html#2826">[ author ]</a>
<!-- next="start" -->
<li><a href="2827.html">[ Next ]</a><a href="2825.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2784.html">Robin Hanson</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Robin Hanson wrote:
<br>
<i>&gt; </i><br>
<i>&gt; Billy Brown wrote:</i><br>
<i>&gt; </i><br>
<a href="2784.html#2826qlink1">&gt; &gt;If you want to implement a sentient AI, there is no obvious reason to do</a><br>
<i>&gt; &gt;things this way.  It would make more sense to implement as many mechanisms</i><br>
<i>&gt; &gt;as you like for suggesting possible goals, then have a single system for</i><br>
<i>&gt; &gt;selecting which ones to pursue.</i><br>
<i>&gt; </i><br>
<i>&gt; There is no obvious reason to do things the way humans do, but there is</i><br>
<i>&gt; no obvious reason not to either.  I think it is fair to say that few things</i><br>
<i>&gt; are obvious about high level AI organization and design.</i><br>

<p>
I would disagree.  Few things are obviously true, but many things are
obviously false - the 8-circuit design, to continue stamping madly on the
greasy spot where there used to lie a dead horse.

<p>
<a name="3322qlink1">I would say that humans have many conflicting goal systems, overlapping where
they shouldn't and unlinked where they should.  I would demand an extremely
good reason before giving an AI more than one goal system.  Multiple
intuitions, yes; multiple goals and multiple intentions; but not more than one
goal system - and absolutely not an silicoendocrine system to duplicate the
stresses we experience as the result of conflict.  Even if there are good
evolutionary reasons!  It's just too much of a risk.</a>
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2827.html">[ Next ]</a><a href="2825.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2784.html">Robin Hanson</a>
<!-- nextthread="start" -->
</ul>
</body></html>
