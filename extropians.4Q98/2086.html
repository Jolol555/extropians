<!-- received="Fri Dec  4 02:39:44 1998 MST" -->
<!-- sent="Fri, 4 Dec 1998 19:39:37 +1000 (GMT+1000)" -->
<!-- name="Alejandro Dubrovsky" -->
<!-- email="s335984@student.uq.edu.au" -->
<!-- subject="Re: Singularity: Individual, Borg, Death?" -->
<!-- id="Pine.OSF.4.05.9812041857040.14738-100000@student.uq.edu.au" -->
<!-- inreplyto="3666B5EB.39DB37FD@pobox.com" -->
<!-- version=1.10, linesinbody=71 -->
<html><head><title>extropy: Re: Singularity: Individual, Borg, Death?</title>
<meta name=author content="Alejandro Dubrovsky">
<link rel=author rev=made href="mailto:s335984@student.uq.edu.au" title ="Alejandro Dubrovsky">
</head><body>
<h1>Re: Singularity: Individual, Borg, Death?</h1>
Alejandro Dubrovsky (<i>s335984@student.uq.edu.au</i>)<br>
<i>Fri, 4 Dec 1998 19:39:37 +1000 (GMT+1000)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2086">[ date ]</a><a href="index.html#2086">[ thread ]</a><a href="subject.html#2086">[ subject ]</a><a href="author.html#2086">[ author ]</a>
<!-- next="start" -->
<li><a href="2087.html">[ Next ]</a><a href="2085.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2050.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2093.html">Nick Bostrom</a>
</ul>
<!-- body="start" -->

<p>
On Thu, 3 Dec 1998, Eliezer S. Yudkowsky wrote:

<p>
<a href="2050.html#2086qlink1">&gt; Okay.  First, the actual Interim logic:</a><br>
<i>&gt; </i><br>
<i>&gt; Step 1:  Start with no supergoals, no initial values.</i><br>
<i>&gt;   Result:  Blank slate.</i><br>
<i>&gt; Step 2:  Establish a goal with nonzero value.</i><br>
<i>&gt;   Either all goals have zero value (~P) or at least one goal has nonzero value</i><br>
<i>&gt; (P).  Assign a probability of Unknown to P and 1-Unknown to ~P.  Observe that</i><br>
<i>&gt; ~P (all goals have zero value) cancels out of any choice in which it is</i><br>
<i>&gt; present; regardless of the value of Unknown, the grand total contribution to</i><br>
<i>&gt; any choice of ~P is zero.  All choices can be made as if Unknown=1, that is,</i><br>
<i>&gt; as if it were certain that "at least one goal has nonzero value".  (This</i><br>
<i>&gt; doesn't _prove_ P, only that we can _act_ as if P.)</i><br>
<i>&gt;   Result:  A goal with nonzero renormalized value and Unknown content.</i><br>
<i>&gt; Step 3:  Link Unknown supergoal to specified subgoal.</i><br>
<i>&gt;   This uses two statements about the physical world which are too complex to</i><br>
<i>&gt; be justified here, but are nevertheless very probable:  First, that</i><br>
<i>&gt; superintelligence is the best way to determine Unknown values; and second,</i><br>
<i>&gt; that superintelligence will attempt to assign correct goal values and choose</i><br>
<i>&gt; using assigned values.</i><br>
<i>&gt;   Result:  "Superintelligence" is subgoal with positive value.</i><br>

<p>
<a name="2099qlink1">I'm assuming that what you are doing is trying to maximize the value of
the system.  I don't see, though how you can assume that the goals' values
are positive.  ie "Either life has meaning or it doesn't, but i don't see
any way of knowing if the discovery of the meaning of life is good or bad"</a>

<p>
 &gt; 
<br>
<i>&gt; Or to summarize:  "Either life has meaning, or it doesn't.  I can act as if it</i><br>
<a href="2050.html#2086qlink2">&gt; does - or at least, the alternative doesn't influence choices.  Now I'm not</a><br>
<i>&gt; dumb enough to think I have the vaguest idea what it's all for, but I think</i><br>
<i>&gt; that a superintelligence could figure it out - or at least, I don't see any</i><br>
<i>&gt; way to figure it out without superintelligence.  Likewise, I think that a</i><br>
<i>&gt; superintelligence would do what's right - or at least, I don't see anything</i><br>
<i>&gt; else for a superintelligence to do."</i><br>
<i>&gt; </i><br>
<i>&gt; --</i><br>
<i>&gt; </i><br>
<i>&gt; There are some unspoken grounding assumptions here about the nature of goals,</i><br>
<i>&gt; but they are not part of what I think of as the "Singularity logic".</i><br>
<i>&gt; </i><br>
<a name="2099qlink2"><i>&gt; Logical Assumptions:</i><br>
<i>&gt; LA1.  Questions of morality have real answers - that is, unique,</i><br>
<i>&gt; observer-independent answers external from our opinions.</i><br>
<i>&gt; Justification:  If ~LA1, I can do whatever I want and there will be no true</i><br>
<i>&gt; reason why I am wrong; and what I want is to behave as if LA1 - thus making my</i><br>
<i>&gt; behavior rational regardless of the probability assigned to LA1.</i><br>

<p>
I disagree.  If there are multiple, observer-dependent answers, then
there's still a morality system that affects you and you could still be in
the wrong, and this situation would fall into ~LA1.</a>
<a name="2099qlink3">And even if LA1, i don't see how BEHAVING as if LA1 is more rational than
if behaving as if ~LA1.  As in John Clark's email about Pascal's wager,
the rational way to behave if LA1 might be to behave as if ~LA1, depending
the nature of the real moral answers.</a>

<p>
[expansions snipped - definitions in agreement with mine]

<p>
<a name="2099qlink4">My rejection of LA1 makes me unfit (by your conclusion, with which i
mostly agree with) to argue rationally about morality, and i suppose my
claim (like many others') is that you cannot argue rationally about
morality since LA1 seems very weak.</a>
<br>
I'm not sure if i'm clear or even if i understand your arguments correctly
so explain/criticise/dismiss at your leisure.  I don't get easily offended
either so flame if need be.
<br>
chau
<br>
Alejandro Dubrovsky
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2087.html">[ Next ]</a><a href="2085.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2050.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2093.html">Nick Bostrom</a>
</ul>
</body></html>
