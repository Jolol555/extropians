<!-- received="Wed Dec  9 08:08:59 1998 MST" -->
<!-- sent="Wed, 9 Dec 1998 09:08:37 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Singularity: AI Morality" -->
<!-- id="001201be2385$ce32a280$352501c0@mfg130" -->
<!-- inreplyto="199812082302.XAA30159@andromeda.hosts.netdirect.net.uk" -->
<!-- version=1.10, linesinbody=38 -->
<html><head><title>extropy: RE: Singularity: AI Morality</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Singularity: AI Morality</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Wed, 9 Dec 1998 09:08:37 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2321">[ date ]</a><a href="index.html#2321">[ thread ]</a><a href="subject.html#2321">[ subject ]</a><a href="author.html#2321">[ author ]</a>
<!-- next="start" -->
<li><a href="2322.html">[ Next ]</a><a href="2320.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2287.html">Nick Bostrom</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2331.html">Robin Hanson</a>
</ul>
<!-- body="start" -->

<p>
<a name="2327qlink1">Nick Bostrom wrote:
<br>
<a href="2287.html#2321qlink1">&gt; I think the trick is not to use coersive measures, but rather to</a><br>
<a name="2331qlink1"><i>&gt; wisely select the values we give to the superintelligences, so that</i><br>
<i>&gt; they wouldn't *want* to hurt us. If nobody wants to commit crimes,</i><br>
<i>&gt; you don't need any police.</i><br>

<p>
And others have posted similar thoughts.

<p>
Guys, please, trust the programmers on programming questions, OK?</a>  The kinds
<a name="2327qlink2">of things you are talking about sound reasonable, and might even</a> be possible
<a name="2327qlink3">in a static system, but they are not even theoretically possible in the
situation we are discussing.  The problem is that we don't know how</a> to built
<a name="2327qlink4">a Transhuman AI - all we can do is make something that might evolve</a> into one
<a name="2327qlink5">on its own.  If we try to put constraints on that evolution then the
</a>
constraints also have to evolve, and they must do so in synch with the rest
of the system.</a>

<p>
Now, in the real world we can't even program a simple, static program
without bugs.  The more complex the system becomes, the more errors there
will be.  Given that a seed AI would consist of at least several hundred
thousand lines of arcane, self-modifying code, it is impossible to predict
its behavior with any great precision.  Any static morality module will
eventually break or be circumvented, and a dynamic one will itself mutate in
unpredictable ways.<a name="2340qlink1">  The best that we can do is teach it how do deduce its
own rules, and hope it comes up with a moral system requires</a> it to be nice
to fellow sentients.

<p>
<a name="2361qlink1">Besides, schemes to artificially impose a particular moral system on the AI
rely on mind control, plain and simple.  The smarter the AI becomes, the
more likely it is to realize that you are trying to control it.  Now, is
mind control wrong in your moral system?  How about in the one you gave the
AI?  In either case, how is the AI likely to react?  This is a recipe for
disaster, no matter how things work out.</a>

<p>
Billy Brown
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2322.html">[ Next ]</a><a href="2320.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2287.html">Nick Bostrom</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2331.html">Robin Hanson</a>
</ul>
</body></html>
