<!-- received="Tue Dec 15 19:54:32 1998 MST" -->
<!-- sent="Tue, 15 Dec 1998 21:53:59 -0500" -->
<!-- name="Dan Clemmensen" -->
<!-- email="Dan@Clemmensen.ShireNet.com" -->
<!-- subject="Re: Singularity: AI Morality" -->
<!-- id="367720C7.7DB87A80@clemmensen.shirenet.com" -->
<!-- inreplyto="Singularity: AI Morality" -->
<!-- version=1.10, linesinbody=33 -->
<html><head><title>extropy: Re: Singularity: AI Morality</title>
<meta name=author content="Dan Clemmensen">
<link rel=author rev=made href="mailto:Dan@Clemmensen.ShireNet.com" title ="Dan Clemmensen">
</head><body>
<h1>Re: Singularity: AI Morality</h1>
Dan Clemmensen (<i>Dan@Clemmensen.ShireNet.com</i>)<br>
<i>Tue, 15 Dec 1998 21:53:59 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3026">[ date ]</a><a href="index.html#3026">[ thread ]</a><a href="subject.html#3026">[ subject ]</a><a href="author.html#3026">[ author ]</a>
<!-- next="start" -->
<li><a href="3027.html">[ Next ]</a><a href="3025.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2907.html">Samael</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Samael wrote: 
<br>
<a href="2699.html#3026qlink1">&gt; Dan Clemmensen wrote:</a><br>
<pre>
&gt;&gt;Same problem. This only works if all AIs are inhibited fron extending their
&gt;&gt;"strong goals": This si very hard to do using traditional computers. Essentially,
&gt;&gt;you will either permit the AI to program itself, or not. I feel that most AI
&gt;&gt;researchers will be tempted to permit the AI to program itself. Only one such
&gt;&gt;researcher needs to do this to break your containment system. Do you feel that
&gt;&gt;A self-extending AI must intrinsically have strong and un-self-modifiable goals
&gt;&gt;to exist, or do you feel that all AI researchers will correctly implement this
&gt;&gt;feature, or do you have another reason?
</pre>
<br>
<a href="2907.html#3026qlink2">&gt; 1) One must have a reason to do something before one does it.</a><br>
<i>&gt; 2) If one has an overarching goal, one would modify one's subgoals to reach</i><br>
<i>&gt; the overarching goal but would not modify the overarching goal, because one</i><br>
<i>&gt; would not have a reason to do so.</i><br>
<i>&gt; </i><br>
<i>&gt; Why would an AI modify it's overriding goals?  What reason would it have?</i><br>
<i>&gt; If it's been programmed with the motive 'Painting things red is good', why</i><br>
<i>&gt; would it change that?  If it did change that (or at last consider what it</i><br>
<i>&gt; meant and why it wanted it), it may well come to the conclusion that</i><br>
<i>&gt; 'painting things red is no better than increasing my own intelligence'  but</i><br>
<i>&gt; why would it want to increase its own intelligence?  Why would it think</i><br>
<i>&gt; intelligence was important to it?  It's just another trait, only as</i><br>
<i>&gt; important as you are programmed to think it is.</i><br>

<p>
Presumably, the AI knows that it applies logic, reasoning, creativity,
and the other attributes of "intelligence" to achieving it "overriding
goals." Therefore, increasing its intelligence is an obvious subgoal to
just about any other goal.

<p>
However, you still haven't told me why you think that some AI researcher
somewhere won't use "increase your intelligence" as an overriding goal,
or add goal modification to an AI design.
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3027.html">[ Next ]</a><a href="3025.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2907.html">Samael</a>
<!-- nextthread="start" -->
</ul>
</body></html>
