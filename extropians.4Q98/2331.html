<!-- received="Wed Dec  9 10:34:01 1998 MST" -->
<!-- sent="Wed, 09 Dec 1998 09:33:04 -0800" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@econ.berkeley.edu" -->
<!-- subject="RE: Singularity: AI Morality" -->
<!-- id="3.0.3.32.19981209093304.00cce7a4@econ.berkeley.edu" -->
<!-- inreplyto="001201be2385$ce32a280$352501c0@mfg130" -->
<!-- version=1.10, linesinbody=31 -->
<html><head><title>extropy: RE: Singularity: AI Morality</title>
<meta name=author content="Robin Hanson">
<link rel=author rev=made href="mailto:hanson@econ.berkeley.edu" title ="Robin Hanson">
</head><body>
<h1>RE: Singularity: AI Morality</h1>
Robin Hanson (<i>hanson@econ.berkeley.edu</i>)<br>
<i>Wed, 09 Dec 1998 09:33:04 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2331">[ date ]</a><a href="index.html#2331">[ thread ]</a><a href="subject.html#2331">[ subject ]</a><a href="author.html#2331">[ author ]</a>
<!-- next="start" -->
<li><a href="2332.html">[ Next ]</a><a href="2330.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2321.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2340.html">Billy Brown</a>
</ul>
<!-- body="start" -->

<p>
Billy Brown writes:
<br>
<a href="2321.html#2331qlink1">&gt;&gt; wisely select the values we give to the superintelligences, ...</a><br>
<i>&gt;</i><br>
<i>&gt;Guys, please, trust the programmers on programming questions, OK?  ...</i><br>
<i>&gt;Now, in the real world we can't even program a simple, static program</i><br>
<i>&gt;without bugs.  The more complex the system becomes, the more errors there</i><br>
<i>&gt;will be.  Given that a seed AI would consist of at least several hundred</i><br>
<i>&gt;thousand lines of arcane, self-modifying code, it is impossible to predict</i><br>
<i>&gt;its behavior with any great precision.  Any static morality module will</i><br>
<i>&gt;eventually break or be circumvented, and a dynamic one will itself mutate in</i><br>
<i>&gt;unpredictable ways.  The best that we can do is teach it how do deduce its</i><br>
<i>&gt;own rules, and hope it comes up with a moral system requires it to be nice</i><br>
<a name="2340qlink2"><i>&gt;to fellow sentients.</i><br>

<p>
<a name="2343qlink1">Well we could do a little more; we might create lots of different AIs 
and observer how they treat each other in contained environments.</a>  We might
<a name="2340qlink3">then repeatedly select the ones whose behavior we deem "moral."  And once
we have creatures whose behavior seems stably "moral" we could</a> release them
<a name="2340qlink4">to participate in the big world. 
</a>

<p>
However, I'd expect evolutionary pressures to act again out</a> in the big world,
<a name="2340qlink5">and so our only real reason for confidence in continued "moral" behavior 
would be expectations that such behavior would be rewarded in a world when
most other creatures also act that way.</a>


<p>
Robin Hanson  
<pre>
hanson@econ.berkeley.edu     <a href="http://hanson.berkeley.edu/">http://hanson.berkeley.edu/</a>   
RWJF Health Policy Scholar             FAX: 510-643-8614 
140 Warren Hall, UC Berkeley, CA 94720-7360 510-643-1884     
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2332.html">[ Next ]</a><a href="2330.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2321.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2340.html">Billy Brown</a>
</ul>
</body></html>
