<!-- received="Thu Oct  1 16:38:39 1998 MST" -->
<!-- sent="Thu, 01 Oct 1998 15:34:38 -0700" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@econ.berkeley.edu" -->
<!-- subject="Re: Economic Growth Assuming Machine Intelligence" -->
<!-- id="3.0.3.32.19981001153438.0075e6dc@econ.berkeley.edu" -->
<!-- inreplyto="3613E9DB.6D4DB9B3@pobox.com" -->
<!-- version=1.10, linesinbody=36 -->
<html><head><title>extropy: Re: Economic Growth Assuming Machine Intelligence</title>
<meta name=author content="Robin Hanson">
<link rel=author rev=made href="mailto:hanson@econ.berkeley.edu" title ="Robin Hanson">
</head><body>
<h1>Re: Economic Growth Assuming Machine Intelligence</h1>
Robin Hanson (<i>hanson@econ.berkeley.edu</i>)<br>
<i>Thu, 01 Oct 1998 15:34:38 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#12">[ date ]</a><a href="index.html#12">[ thread ]</a><a href="subject.html#12">[ subject ]</a><a href="author.html#12">[ author ]</a>
<!-- next="start" -->
<li><a href="0013.html">[ Next ]</a>
<b>In reply to:</b> <a href="0011.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky writes:
<br>
<a href="0011.html#0012qlink1">&gt;&gt; ... Steady state growth rates could easily rise by an order of magnitude </a><br>
<i>&gt;&gt; or more.  But with steady growth, wages and per-intelligence consumption </i><br>
<i>&gt;&gt; could fall as fast as computer prices do.</i><br>
<i>&gt;</i><br>
<i>&gt;I shall leave aside my personal objections, since I think you may find</i><br>
<i>&gt;Gubrud's arguments more convincing; </i><br>
<i>&gt;"... any military production system fully automated by advanced artificial</i><br>
<i>&gt;intelligence, would lead to instability in a confrontation between rough</i><br>
<i>&gt;equals. ... feel pressured to preempt, ... close contact between forces at </i><br>
<i>&gt;sea and in space would give an advantage to the first to strike."</i><br>
<i>&gt;</i><br>
<i>&gt;In the event that you've read it already, which seems probable, I would just</i><br>
<i>&gt;like to say this:  The flip side of rapid progress is incredibly destructive</i><br>
<i>&gt;wars.  And if you think the currency meltdown is causing global</i><br>
<i>&gt;destabilization, the differential equations you blithely toss around would</i><br>
<i>&gt;shatter national economies like glass.  Since I don't believe a Weak</i><br>
<i>&gt;Singularity is probable, I can say dispassionately that the Weak Singularity</i><br>
<i>&gt;your paper models would probably end in the violent death of a significant</i><br>
<i>&gt;fraction of mankind.  </i><br>

<p>
I'm not sure what your "objection" is here.  I have read Gubrud and had a long
email conversation with him.  His basic thesis seems to be that new technology
induces military instability, an argument that is not particular to nanotech or 
AI.  I don't find this terribly convincing - technology is changing a lot 
faster now than a thousand years ago, but it's not clear wars are worse.   But 
even if Gubrud is right, how is that an objection to my analysis?  If machine
intelligence appears, it is a big new tech, so Gubrud predicts wars.  I predict
changes in growth rates, wages, and population.  How are these predictions in 
conflict?  

<p>
Robin Hanson  
<br>
hanson@econ.berkeley.edu    <a href="http://hanson.berkeley.edu/">http://hanson.berkeley.edu/</a>   
RWJF Health Policy Scholar, Sch. of Public Health   510-643-1884  
140 Warren Hall, UC Berkeley, CA 94720-7360    FAX: 510-643-8614
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0013.html">[ Next ]</a>
<b>In reply to:</b> <a href="0011.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
