<!-- received="Sun Dec  6 11:11:56 1998 MST" -->
<!-- sent="Sun, 06 Dec 1998 12:13:25 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Singularity: AI Morality" -->
<!-- id="366AC943.B3E1F8C@pobox.com" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=127 -->
<html><head><title>extropy: Singularity: AI Morality</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Singularity: AI Morality</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sun, 06 Dec 1998 12:13:25 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2153">[ date ]</a><a href="index.html#2153">[ thread ]</a><a href="subject.html#2153">[ subject ]</a><a href="author.html#2153">[ author ]</a>
<!-- next="start" -->
<li><a href="2154.html">[ Next ]</a><a href="2152.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2151.html">Nick Bostrom</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2185.html">Billy Brown</a>
</ul>
<!-- body="start" -->

<p>
Nick Bostrom wrote:
<br>
<i>&gt; </i><br>
<i>&gt; &gt; Eliezer Yudkowsky wrote:</i><br>
<i>&gt; &gt;</i><br>
<a href="2151.html#2153qlink1">&gt; &gt; Not at all!  If that is really and truly and objectively the moral thing to</a><br>
<i>&gt; &gt; do, then we can rely on the Post-Singularity Entities to be bound by the same</i><br>
<i>&gt; &gt; reasoning.  If the reasoning is wrong, the PSEs won't be bound by it.  If the</i><br>
<i>&gt; &gt; PSEs aren't bound by morality, we have a REAL problem</i><br>
<i>&gt; </i><br>
<i>&gt; Indeed. And this is another point where I seem to disagree with you.</i><br>
<i>&gt; I am not at all certain that being superintelligent implies being</i><br>
<i>&gt; moral. Certainly there are very intelligent humans that are also very</i><br>
<i>&gt; wicked; I don't see why once you pass a certain threshold of</i><br>
<i>&gt; intelligence then it is no longer possible to be morally bad. What I</i><br>
<i>&gt; might agree with, is that once you are sufficiently intelligent then</i><br>
<i>&gt; you should be able to recognize what's good and what's bad. But</i><br>
<i>&gt; whether you are motivated to act in accordance with these moral</i><br>
<i>&gt; convictions is a different question. What weight you give to moral</i><br>
<i>&gt; imperatives in planning your actions depends on how altruistic/moral</i><br>
<i>&gt; you are. We should therefore make sure that we build in strong moral</i><br>
<i>&gt; drives into the superintelligences. (Presumably, we would also want</i><br>
<i>&gt; to link these moral drives to a moral system that places a great</i><br>
<i>&gt; value on human survival; because that way we would increase our own</i><br>
<i>&gt; chances of survival.)</i><br>

<p>
<a name="2225qlink1">This, in my opinion, is exactly the wrong answer.  (See particularly the
"Prime Directive of AI" in "Coding a Transhuman AI".)  But think about what
you just said.  First you say that sufficient intelligence should be able to
recognize good and bad.  Then you say that we should build in a moral system
with a particular set of values.</a>  What if we get it wrong?  What if the two
values conflict?  Would you really want to be around the AI when that
happened?  I would prefer to be very far away, like the Magellanic Clouds.  It
might try to remove the source of the conflict.

<p>
Once again, we have a conflict between the self-propelled trajectory and the
convergence to truth.  Even putting on my "human allegiance" hat, I think
self-propelled trajectories would be a terrible idea because I have no goddamn
idea where they would wind up.<a name="2225qlink2">  Do you really know all the logical
consequences of placing a large value on human survival?  Would you care to
define "human" for me?  Oops!  Thanks to your overly rigid definition, you
will live for billions and trillions and googolplexes of years, prohibited
from uploading, prohibited even from ameliorating your own boredom, endlessly
screaming, until the soul burns out of your mind, after which you will
continue to scream.</a>  I would prefer a quick death to creating our own hells,
and that is what we would inevitably do.

<p>
It's not just the particular example.  It's not even that we can't predict all
the consequences of a particular system.  It's the trajectory.  I hope and
pray that the trajectory will converge to the (known) correct answers in any
case.  I really do.  Because if it doesn't converge there, I don't have the
goddamndest where it will end up.  Any errors we make in the initial
formulation will either cancel out or amplify.  If they cancel out, the AI
makes the correct moral choices.  If they build up, you have positive feedback
into intelligence and insanity.  If you're lucky, you'll wind up in a world of
incomprehensible magic, a world of twisted, insane genies obeying every order.
 If you're not lucky, you'll wind up in a hell beyond the ability of any of us
to imagine.

<p>
Of course, I could always be wrong.  I'd say there's a 1% chance that AI
coercions could get us into "paradise" where the alternative is extermination.
 You'll have to prohibit human intelligence enhancement, however, or put the
same coercions on us.  Imposing lasting coercions on the pre-existing human
goal system, even given that AI coercions work, takes another factor of 100
off the probability.<a name="2225qlink3">  If you can synchronize everyone's intelligence
enhancement perfectly, then eventually we'll probably coalesce into a
singleton indistinguishable from that resulting from an AI Transcend.</a>  And the
Amish will go kicking and screaming, so even the element of noncoercion is nonpresent.

<p>
<a name="2225qlink4"><a name="2154qlink2">Look, these forces are going to a particular place, and they are way, way,
waaaaaayyy too big for any of us to divert.  Think of the Singularity as this
titanic, three-billion-ton truck heading for us.  We can't stop it, but I
suppose we could manage to get run over trying to slow it down.</a></a>

<p>
<a href="2151.html#2153qlink2">&gt; &gt;, but I don't see any way</a><br>
<i>&gt; &gt; of finding this out short of trying it.</i><br>
<i>&gt; </i><br>
<a name="2165qlink1"><i>&gt; How to control an SI? Well, I think it *might* be possible through</i><br>
<i>&gt; programming the right values into the SIs,</i><br>
</a>

<p>
<a name="2165qlink2">We should program the AI to seek out *correct* answers, not</a> a particular set
of answers.

<p>
<a name="2225qlink5"><a name="2185qlink1"><a name="2165qlink3">&gt; but let's not go into</a> that<br>
<i>&gt; now.</i><br>
</a>
<p>
<a name="2185qlink2">Let's.  Please.  Now.</a>

</a>
<p>
<a href="2151.html#2153qlink3">&gt; &gt; There's a far better chance that delay makes things much, much worse.</a><br>
<i>&gt; </i><br>
<i>&gt; I think it will all depend on the circumstances at the time. For</i><br>
<i>&gt; example, what the state of art of nanotechnology is then. But you</i><br>
<i>&gt; can't say that sooner is *always* better, although it may be a good</i><br>
<i>&gt; rule of thumb. Clearly there are cases where it's more prudent to</i><br>
<i>&gt; take more precausions before launch. And in the case of the</i><br>
<i>&gt; singularity, we'd seem to be well advised to take as many precausions</i><br>
<i>&gt; as we have time for.</i><br>

<p>
I think that the amount of delay-caused deterioration depends on
circumstances, but not the sign.  Let's substitute "intelligence enhancement"
for "Singularity" and reconsider.  Is there really any circumstance under
which it is better to be stupid than smart, with the world at stake?  If
you're second-guessing the transhumans, maybe, but we know where that leads.

<p>
<a href="2151.html#2153qlink4">&gt; &gt; Why not leave the moral obligations to the SIs, rather than trying (futilely</a><br>
<i>&gt; &gt; and fatally) to impose your moral guesses on them?</i><br>
<i>&gt; </i><br>
<i>&gt; Because, as I said above, if we build them in the wrong way they may</i><br>
<i>&gt; not be moral.</i><br>

<p>
I hope not.  But "building in the wrong way" seems to me to imply a sloppy,
inelegant set of arbitrary, unsupported, ill-defined, and probably
self-contradictory assertions, rather than a tight chain of pure logic seeking
out the correct answers.

<p>
<a name="2225qlink6"><a name="2154qlink3"><a href="2151.html#2153qlink5">&gt; Plus: whether it's moral or not, we would want to make</a><br>
<i>&gt; sure that they are kind to us humans and allow us to upload.</i><br>

<p>
No, we would NOT want to make sure of that.  It would be immoral.  Every bit
as immoral as torturing little children to death, but with a much higher
certainty of evil.</a></a>
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2154.html">[ Next ]</a><a href="2152.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2151.html">Nick Bostrom</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2185.html">Billy Brown</a>
</ul>
</body></html>
