<!-- received="Thu Dec 10 02:18:31 1998 MST" -->
<!-- sent="Thu, 10 Dec 1998 09:15:34 -0000" -->
<!-- name="Samael" -->
<!-- email="Samael@dial.pipex.com" -->
<!-- subject="Re: Singularity: AI Morality" -->
<!-- id="009401be241d$a4ab64e0$0100a8c0@andy" -->
<!-- inreplyto="Singularity: AI Morality" -->
<!-- version=1.10, linesinbody=38 -->
<html><head><title>extropy: Re: Singularity: AI Morality</title>
<meta name=author content="Samael">
<link rel=author rev=made href="mailto:Samael@dial.pipex.com" title ="Samael">
</head><body>
<h1>Re: Singularity: AI Morality</h1>
Samael (<i>Samael@dial.pipex.com</i>)<br>
<i>Thu, 10 Dec 1998 09:15:34 -0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2370">[ date ]</a><a href="index.html#2370">[ thread ]</a><a href="subject.html#2370">[ subject ]</a><a href="author.html#2370">[ author ]</a>
<!-- next="start" -->
<li><a href="2371.html">[ Next ]</a><a href="2369.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2339.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->


<p>
-----Original Message-----
<br>
From: Billy Brown &lt;bbrown@conemsco.com&gt;
<br>
To: extropians@extropy.com &lt;extropians@extropy.com&gt;
Date: 09 December 1998 20:17
<br>
Subject: RE: Singularity: AI Morality


<p>
<a href="2359.html#2370qlink1">&gt;Samael wrote:</a><br>
<i>&gt;&gt; The problem with programs is that they have to be designed to _do_</i><br>
<i>&gt;&gt; something..</i><br>
<i>&gt;&gt;</i><br>
<i>&gt;&gt; Is your AI being designed to solve certain problems?  Is it being</i><br>
designed
<br>
<a href="2339.html#2370qlink2">&gt;&gt; to understand certain things?  What goals are you setting it?</a><br>
<i>&gt;&gt;</i><br>
<i>&gt;&gt; An AI will not want anything unless it has been given a goal (unless it</i><br>
<i>&gt;&gt;accidentally gains a goal through sloppy programming of course)..</i><br>
<i>&gt;</i><br>
<i>&gt;Actually, its Eliezer's AI, not mine - you can find the details on his web</i><br>
<i>&gt;site, at <a href="http://huitzilo.tezcat.com/~eliezer/AI_design.temp.html">http://huitzilo.tezcat.com/~eliezer/AI_design.temp.html</a>.</i><br>
<i>&gt;</i><br>
<i>&gt;On of the things that makes this AI different from a traditional</i><br>
<i>&gt;implementation is that it would be capable of creating its own goals based</i><br>
<i>&gt;on its (initially limited) understanding of the world.  I think you would</i><br>
<i>&gt;have to program in a fair number of initial assumptions to get the process</i><br>
<i>&gt;going, but after that the system evolves on its own - and it can discard</i><br>
<i>&gt;those initial assumptions if it concludes they are false.</i><br>


<p>
<a name="2573qlink1">But why would it _want_ to do anything?

<p>
<a name="2421qlink1">What's to stop it reaching the conclusion 'Life is pointless.  There is no
meaning anywhere' and just turning itself off?
</a>

</a>
<p>
Samael
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2371.html">[ Next ]</a><a href="2369.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2339.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
</body></html>
