<!-- received="Fri Dec 11 15:29:46 1998 MST" -->
<!-- sent="Fri, 11 Dec 1998 16:29:27 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Singularity: AI Morality" -->
<!-- id="002901be2555$b80b9fd0$352501c0@mfg130" -->
<!-- inreplyto="3.0.3.32.19981210120509.006f3c1c@econ.berkeley.edu" -->
<!-- version=1.10, linesinbody=40 -->
<html><head><title>extropy: RE: Singularity: AI Morality</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Singularity: AI Morality</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Fri, 11 Dec 1998 16:29:27 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2562">[ date ]</a><a href="index.html#2562">[ thread ]</a><a href="subject.html#2562">[ subject ]</a><a href="author.html#2562">[ author ]</a>
<!-- next="start" -->
<li><a href="2563.html">[ Next ]</a><a href="2561.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2420.html">Robin Hanson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2784.html">Robin Hanson</a>
</ul>
<!-- body="start" -->

<p>
Robin Hanson wrote:
<br>
<a name="2784qlink1"><a href="2420.html#2562qlink1">&gt; You can you possibly know this about AIs?  I know of a great many</a><br>
<i>&gt; programs that have been written by AI researchers that use conflicting</i><br>
<i>&gt; goal systems, where conflicts are resolved by some executive module.</i><br>
<i>&gt; Maybe those approaches will win out in the end.</a></i><br>

<p>
Sorry, I guess I should be more careful to use precise terminology.  Here's
what I was trying to say:

<p>
A sentient being will generally have goals.  It chooses the goals that it
will pursue, and it chooses the methods it will use to pursue them.  A
system of morality will influence both its choice of goals and the methods
it uses to achieve them.

<p>
In humans, there seem to be many different ways for a goal to be selected -
sometimes we make a logical choice, sometimes we rely on emotions, and
sometimes we act on impulse.  There also does not seem to be a unified
system for placing constraints on the methods used to pursue these goals -
sometimes a moral system's prohibitions are obeyed, and sometimes they are
ignored.

<p>
If you want to implement a sentient AI, there is no obvious reason to do
things this way.  It would make more sense to implement as many mechanisms
as you like for suggesting possible goals, then have a single system for
selecting which ones to pursue.  Likewise, if you are going to have
constraints on what methods you use to pursue a goal, it makes sense for
them to all be enforced by the same mechanism.

<p>
<a name="2604qlink1">An AI implemented in this fashion would exhibit what I call 'unified will'.
It would act on whatever moral system it believed in with a very</a> high degree
<a name="2604qlink2">of consistency, because the tenets of that system would be enforced
precisely and universally.  It could still face moral quandaries,</a> because it
<a name="2604qlink3">might have conflicting goals or limited information.  However, it would
never knowingly violate its own ethics, because it would always use</a> the same
<a name="2604qlink4">set of rules to make a decision.</a>

<p>
Billy Brown
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2563.html">[ Next ]</a><a href="2561.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2420.html">Robin Hanson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2784.html">Robin Hanson</a>
</ul>
</body></html>
