<!-- received="Sun Oct 25 11:05:07 1998 MST" -->
<!-- sent="Sun, 25 Oct 1998 13:13:02 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: QUICKIE PREDICTION NEEDED" -->
<!-- id="36336A2C.E98755B5@pobox.com" -->
<!-- inreplyto="QUICKIE PREDICTION NEEDED" -->
<!-- version=1.10, linesinbody=63 -->
<html><head><title>extropy: Re: QUICKIE PREDICTION NEEDED</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: QUICKIE PREDICTION NEEDED</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sun, 25 Oct 1998 13:13:02 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#488">[ date ]</a><a href="index.html#488">[ thread ]</a><a href="subject.html#488">[ subject ]</a><a href="author.html#488">[ author ]</a>
<!-- next="start" -->
<li><a href="0489.html">[ Next ]</a><a href="0487.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0479.html">E. Shaun Russell</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
E. Shaun Russell wrote:
<br>
<i>&gt; </i><br>
<a href="0481.html#0488qlink1">&gt; Eliezer wrote:</a><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt;By the year 2050 at the absolute latest (assuming no civilization-destroying</i><br>
<i>&gt; &gt;wars) I expect that there will be nothing recognizably mortal, much less</i><br>
<i>&gt; &gt;human, in our Solar System.  It's a matter of positive feedback.</i><br>
<i>&gt; </i><br>
<i>&gt;         Out of curiosity, what parameters are you basing this hypothesis on?</i><br>
<i>&gt; True, technological progress is being manifested at a rapidly quickening</i><br>
<i>&gt; pace, but the moral mindset of this planet's human habitat is generally</i><br>
<i>&gt; stubborn, pessimistic and myopic.  Given the opportunity of "right here,</i><br>
<i>&gt; right now" transcendence into a higher form, it is doubtful that every</i><br>
<i>&gt; single mortal would want to upload.</i><br>

<p>
Anyone who thinks there is going to be some kind of choice involved is being
naive and optimistic to the point of lunacy.  Do we give children a choice as
<a name="0508qlink1">to whether or not they have to grow up?</a>  Most imaginings on this subject
basically resemble a bunch of children who have never seen or heard of adults
discussing whether they ought to grow up and what kind of wonderful society
they'll make if they do.

<p>
<a href="0479.html#0488qlink2">&gt; There is far to much human stigma.</a><br>

<p>
First-graders attach a large stigma to education.  It hasn't made a noticeable
difference.  They are not major factors in politics.

<p>
<a href="0479.html#0488qlink3">&gt; Furthermore, the assumption that technology will have progressed *that* far</a><br>
<i>&gt; in the next fifty years is not an extrapolation that I would care to gamble</i><br>
<i>&gt; on.  Though I would honestly love for this claim to be true, my optimism</i><br>
<i>&gt; wanes in the face of my realism.  Show me good, hard, *tangible* evidence</i><br>
<i>&gt; for your hypothesis and I will probably believe it as well.</i><br>

<p>
Why does it matter (except to us) whether this happens in five years or five
hundred?  But anyway, we've *got* two-way computer-telepathy implants,
collaborative filtering, enough 'Net-connected computing power to match the
human brain, quantum computing, atomic-level manipulation, and neurological
intelligence enhancement.  Sure, most of these technologies are at the
vacuum-tube stage - but so what?  At this point, everything is just a matter
of predictable miniaturization and known paths of research.  No unexpected
brilliancies are needed, although I'm sure there will be many - who'd heard of
quantum computing a few years ago?  Where was the Web when _Mind Children_ was
written?  Had I.J. Good heard of nanotechnology?

<p>
All these lines, projected into the future, intersect in and about 2025.  If
just one breakthrough is needed (quite plausible), we could get it by 2005. 
In 2025 we'll have a lot of converging, synergetic Singularity tech, making it
the most arguable point.

<p>
By 2050 we'll be supersaturated.  A thousand times human processing power on
every desktop, home-neurosurgery kits, Parallel Scanning Tunneling Probe
atomic desktop printers, thousand-qubit CPUs, a Net with a *quadrillion* times
human power, million-neuron two-way interfaces...  Even uploading would be
possible at this point, given a Manhattan Project.  This scenario is
ludicrous.  The Singularity occurs long, long before, or grey goo eats the
world, or some paranoid blows up the planet.
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0489.html">[ Next ]</a><a href="0487.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0479.html">E. Shaun Russell</a>
<!-- nextthread="start" -->
</ul>
</body></html>
