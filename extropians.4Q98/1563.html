<!-- received="Sun Nov 22 16:50:53 1998 MST" -->
<!-- sent="Sun, 22 Nov 1998 17:51:53 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: SI: Singleton and programming" -->
<!-- id="3658A38C.A6041975@pobox.com" -->
<!-- inreplyto="8b9adfd5.36526af9@aol.com" -->
<!-- version=1.10, linesinbody=223 -->
<html><head><title>extropy: Re: SI: Singleton and programming</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: SI: Singleton and programming</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sun, 22 Nov 1998 17:51:53 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1563">[ date ]</a><a href="index.html#1563">[ thread ]</a><a href="subject.html#1563">[ subject ]</a><a href="author.html#1563">[ author ]</a>
<!-- next="start" -->
<li><a href="1564.html">[ Next ]</a><a href="1562.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1550.html">Eugene Leitl</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eugene Leitl wrote:
<br>
<i>&gt; </i><br>
<a href="1550.html#1563qlink1">&gt; Eliezer S. Yudkowsky writes:</a><br>
<i>&gt; </i><br>
<i>&gt;  &gt; I can't say that I believe in the scenario of a Singularity as a collection of</i><br>
<i>&gt;  &gt; individuals.  Goals converge at sufficiently high intelligence levels, just</i><br>
<i>&gt;  &gt; like pictures of the truth.</i><br>
<i>&gt; </i><br>
<i>&gt; Are you saying that evolution doesn't exist post Singularity, then? Can you</i><br>
<i>&gt; back up that claim?</i><br>

<p>
After reading through your whole post, I believe that we are operating on
drastically different and mutually incompatible assumptions about
post-Singularity environments.  To try and identify both ontological and
surface assumptions:

<p>
Eugene Leitl:
<br>
1.  Singularity contains multiple competing entities, with conflicting goals;
not only with actions intended to reverse another action, but possibly
entities trying to destroy or consume another entity.  AFUTD competition.
2.  Evolution is the best way to create a robust, efficient process in the
vast majority of cases; improvements on evolution (if any) will result in the
same stereotypical properties being exhibited by the evolved processes.
3.  Intelligent design is inherently fragile in that the failure (or
exploitation) of any part destroys the whole; this is a universal problem at
all levels of intelligence.

<p>
Eliezer Yudkowsky:
<br>
1.  Morality is contained within an objective (physical? unalterable?)
ontology.  Goal values post-Singularity converge to the objective values.  No competition.
2.  Evolution is the best way to do something without intelligence, and
consists of a moderately intelligent way to use blind search.  SIs can walk
all over evolution.  (Please see below before responding.)
3.  Human design is inherently fragile in that it tends to linear chains and
lack of fault-tolerance and no local optimization.  This is due to lack of
attention, no parallelism (all attention focused on only one object), and low
speed.  SIs and seed AIs don't share the problem.

<p>
<a href="1550.html#1563qlink2">&gt; Once you have your omega hardware (probably a kind of quantum dot</a><br>
<i>&gt; array) design, which should be very early along the Singularity, the</i><br>

<p>
See, I don't trust "quantum dot array".  Before the '90s, we would have given
a different answer.  In ten years, we'll have another answer.  Similarly, I
don't buy the Singularity using nanotechnology; it's a wonderful method by our
standards, but a few decades ago we would have seen Dyson-sized collections of
miniaturized vacuum tubes.  "Descriptor theory" a la Greg Bear or "wormhole
computing" I might buy, on the theory that it won't sound obsolete until we
actually know how to do it.

<p>
<a href="1550.html#1563qlink3">&gt;  &gt; It's not until you start talking about time travel (which I think is at least</a><br>
<i>&gt;  &gt; 80% probable post-Singularity) that you get real "inertialess" systems.  I</i><br>
<i>&gt; </i><br>
<i>&gt; If you have to resort to Planck energies for spacetime engineering,</i><br>
<i>&gt; you might have to build a particle accelerator around the galaxy. It</i><br>
<i>&gt; takes a while before you can assemble such a structure, and before</i><br>
<i>&gt; it can pick up enough steam to do something interesting, like blasting</i><br>
<i>&gt; a designer hole into spacetime.</i><br>

<p>
...and a 95% probability, given time travel, that it can be done (for
millisec. times) using micron-scale machinery.

<p>
<a href="1550.html#1563qlink4">&gt;  &gt; I'm not at all sure that evolution and singletons are compatible.  Evolution</a><br>
<i>&gt; </i><br>
<i>&gt; The worse for the singletons, then.</i><br>

<p>
The worse for evolution.  Would you agree with these statements?


<OL>
  <li>  Evolution can be improved by the injection of intelligent selection.
  <li>  Evolution can be improved by occasional global redesigns of locally
optimized competitors being reinjected into the pool.
  <li>  Evolution as presently carried out does not involve intelligent design,
either within the process, or as the origin of evolution.  (Theists disagree.)
  <li>  Evolution consists of using a vast number of random tries and multiple
generations of selection and recombination.
  <li>  Evolution is a blind search, or at least the basic units are blind.
  <li>  Insofar as evolution has no option but to be blind, there is no reason to
think that blindness is optional.
  <li>  As the search depth narrows, global redesign and intelligent mutation
predominate over local optimization and random mutation.
  <li>  The evolution of evolution converges to an intelligent, global design with
intelligent tweaks and intelligent tests.  If massive randomness is absolutely
necessary, it can be used selectively.


</OL>
<p>
<a href="1550.html#1563qlink5">&gt;  &gt; "survival" might be determined.  Even if one concedes internal differences,</a><br>
<i>&gt;  &gt; the externally observed behavior of every Singularity might be exactly the</i><br>
<i>&gt;  &gt; same - expand in all directions at lightspeed, dive into a black hole.  So</i><br>
<i>&gt;  &gt; there might not be room for differential inclusive reproductive success.</i><br>
<i>&gt; </i><br>
<i>&gt; So far we have not observed any Singularity or distant signatures</i><br>
<i>&gt; thereof. What makes you think you can predict details of any such</i><br>
<i>&gt; event?</i><br>

<p>
What makes you think you can predict the internal software design?  Sheer
hubris, just like me.  But that's not what I was saying.

<p>
Sometimes there's such a flagrant local and global optimum that all processes
converge there, whether that optimum be "expand at C" or "dive into black
hole".  I don't have to know where the optimum is to hypothesize that the
optimum exists.  With respect to the Singularity, it is unlikely that the two
best mutually exclusive methods of harvesting flops (the two above being a
good example) will produce roughly the same order of results; therefore all
Singularities choose the same option.  You might disagree on the grounds that
an immune-type competition might promote the value of "difference", as such,
regardless of the actual change.

<p>
<a href="1550.html#1563qlink6">&gt;  &gt; Internally, the evolution you propose has to occur in defiance of the</a><br>
<i>&gt;  &gt; superintelligent way to do things, or act on properties the superintelligence</i><br>
<i>&gt; </i><br>
<i>&gt; Well, you are intelligent.</i><br>

<p>
Incorrect.  The personal processing power I control is insignificant compared
to that necessary to run a simulation of Earth's evolutionary processes.

<p>
<a href="1550.html#1563qlink7">&gt; Are you in control of other intelligences?</a><br>
<i>&gt; Particularly these dumb, lowly things like bacteria, viruses, flies,</i><br>
<i>&gt; dust mites, ants, cockroaches, silverfish, pets, cars, ships, sealing</i><br>
<i>&gt; wax? Humankind have not spontaneously mutated into homogenous monocultured</i><br>
<i>&gt; Eliezers set into nice rows, why should a virtual ecology do that?</i><br>

<p>
You're right, particularly given that Eliezers are not intelligent.

<p>
<i>&gt; In</i><br>
<a href="1550.html#1563qlink8">&gt; case you build such a strange thing, due to a chance some part of some</a><br>
<i>&gt; clone somewhere might grow frisky, and rushes over the civilization</i><br>
<i>&gt; monoculture like a brush fire. It's instable as hell, the thing.</i><br>

<p>
Let me see if I understand.  Translating to my assumptions:
"Given any acceptance whatsoever of hardware or software error, stemming from
meteors, eganite flux, or intelligent-efficient programming, and given a
sufficiently large playing field such as 1e30 flops, pure error will
eventually produce a voracious subprocess.  There must therefore be internal
defenses against that subprocess.  The additional possibility of mutations in
the defenses causes the field as a whole to converge to TIERRA."

<p>
I disagree on the grounds that the maximum complexity of mutated attack forms
increases as the log of computing power, so the necessary defense also
increases as the log of computing power; thus internal defense will not be a
serious problem in the absence of deliberate attack (in which I do not believe).

<p>
<a href="1550.html#1563qlink9">&gt;  &gt; [ AFUTD's skrodes ]</a><br>
<i>&gt; </i><br>
<i>&gt; You said a Power would have noticed, but skrodes were built either by</i><br>
<i>&gt; the Blight or the Countermeasure (it's not clear by which), which both</i><br>
<i>&gt; ate normal Powers for breakfast. A Blight from a distance didn't look</i><br>
<i>&gt; particularly singular to a Power, and a lot of Powers meddled with the</i><br>
<i>&gt; lower zones. The hidden functionality of the skrode/its rider complex</i><br>
<i>&gt; may well have exceeded a casual scrutiny of a Power. All they'd see</i><br>
<i>&gt; would be a yet another Power artefact.</i><br>

<p>
It's an interesting question whether 1e70 flops can hide something from 1e40
flops in a design that takes up 1e9 bytes.  My guess is no.  The Blight has to
hide the flaw, and then hide the fact that it's hiding the flaw.  Maybe you
can get a Power to "skip over" the problem section by exploiting common flaws
in the nonconscious first-level parsers, but can you do it without a
characteristic pattern that would trigger an alarm?  Remember, any density or
incomprehensibility over and above what the Power can understand - a part of
the design that's opaque - would itself probably trigger an alarm.  "Golly,
this is the first one-gigabyte program I haven't been able to understand in
twenty decillion subjective years!"  One would have to convey the illusion of
transparency, and I really don't think that's possible unless some parts don't
get fully analyzed, i.e. unless the Power is willing to accept opaqueness as
normal because it would take too much computing power to go beyond it.

<p>
On the other hand, there is absolutely no way I can prove it, and Vinge had
Countermeasure hiding *inside* the Blight during the initial Transcendence, so
it's obvious that in AFUTD's Universe the assumptions are different.

<p>
<a href="1550.html#1563qlink10">&gt;  &gt; "A programmer with a codic cortex - by analogy to our current visual cortex -</a><br>
<i>&gt;  &gt; would be at a vast advantage in writing code.  Imagine trying to learn</i><br>
<i>&gt;  &gt; geometry or mentally rotate a 3D object without a visual cortex; that's what</i><br>
<i>&gt;  &gt; we do, when we write code without a module giving us an intuitive</i><br>
<i>&gt;  &gt; understanding.  An AI would no more need a "programming language" than we need</i><br>
<i>&gt;  &gt; a conscious knowledge of geometry or pixel manipulation to represent spatial</i><br>
<i>&gt;  &gt; objects; the sentences of assembly code would be perceived directly - during</i><br>
<i>&gt;  &gt; writing and during execution."</i><br>
<i>&gt; </i><br>
<i>&gt; A task you have a knack of, and doing for a long time changes you. You</i><br>
<i>&gt; sprout representational systems as you grow better and better. A</i><br>
<i>&gt; tabula rasa AI which was not designed to do machine language would</i><br>
<i>&gt; learn anything the hard way as well, exactly as a human. Of course if</i><br>
<i>&gt; it was more intelligent than a human it would grow much better than</i><br>
<i>&gt; that.</i><br>

<p>
You've just proved that compilers don't exist.
Oh, wait - "was not designed to do machine language".  But how would you get
an AI in the first place if it couldn't bootstrap itself?  And presumably
"more intelligent" includes a larger short-term memory or inherent searching
capabilities, even if its actual creativity is nil?  As far as I can tell, a
"tabula rasa AI" nonexists.

<p>
<a href="1550.html#1563qlink11">&gt;  &gt; Who needs a Power to get a skrode?  The first programming AIs will likely be</a><br>
<i>&gt;  &gt; that incomprehensible to us mere humans.  You know how much trouble it is to</i><br>
<i>&gt; </i><br>
<i>&gt; Thank you, GP comes up with plenty of compact, efficient, opaque</i><br>
<i>&gt; solutions humans have no idea of how they work. If you ask a person to</i><br>
<i>&gt; write a word-recognition circuit, he'll certainly will not build a 100</i><br>
<i>&gt; FPGA-cell large conundrum consisting of a mesh of autofeedbacked loops</i><br>
<i>&gt; exploiting the undocumented analog effects of the bare silicon.</i><br>

<p>
Yes, GP is another good example which shows the shortcomings of human
"intelligent" design.  Extending the analogy between visual cortex and codic
cortex to a load-supporting arch, we might say that human designs are Legos -
bright, monocolor, blocky structures with a global design but local fragility.
 GP creates an upside-down arch made from wood splinters packed tightly
together with a swirling texture; it's as beautiful as a rose, and any given
section is a lot stronger than the human Legos, but it's still upside-down. 
Actual intelligent design is nanotech; every atom is in a locally and globally
optimized design.

<p>
<a href="1550.html#1563qlink12">&gt; OO mirrors our physical world very much: independant objects interact</a><br>
<i>&gt; with lots of others via asynchronous messages. Many simulations are</i><br>
<i>&gt; made much more elegant this way. I think OOP is deeper than a mere</i><br>
<i>&gt; human fad.</i><br>

<p>
It is.  It goes right down into human cognitive structures.
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1564.html">[ Next ]</a><a href="1562.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1550.html">Eugene Leitl</a>
<!-- nextthread="start" -->
</ul>
</body></html>
