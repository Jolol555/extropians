<!-- received="Fri Dec  4 09:25:55 1998 MST" -->
<!-- sent="Fri, 4 Dec 1998 16:24:24 +0000" -->
<!-- name="Nick Bostrom" -->
<!-- email="bostrom@ndirect.co.uk" -->
<!-- subject="Re: Singularity: Individual, Borg, Death?" -->
<!-- id="199812041625.QAA21055@andromeda.hosts.netdirect.net.uk" -->
<!-- inreplyto="3666B5EB.39DB37FD@pobox.com" -->
<!-- version=1.10, linesinbody=77 -->
<html><head><title>extropy: Re: Singularity: Individual, Borg, Death?</title>
<meta name=author content="Nick Bostrom">
<link rel=author rev=made href="mailto:bostrom@ndirect.co.uk" title ="Nick Bostrom">
</head><body>
<h1>Re: Singularity: Individual, Borg, Death?</h1>
Nick Bostrom (<i>bostrom@ndirect.co.uk</i>)<br>
<i>Fri, 4 Dec 1998 16:24:24 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2093">[ date ]</a><a href="index.html#2093">[ thread ]</a><a href="subject.html#2093">[ subject ]</a><a href="author.html#2093">[ author ]</a>
<!-- next="start" -->
<li><a href="2094.html">[ Next ]</a><a href="2092.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2050.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2275.html">Alejandro Dubrovsky</a>
</ul>
<!-- body="start" -->

<p>
<a name="2275qlink1"><a name="2098qlink1">I think Eliezer's interesting argument is unsound, because I think 
one of it's premisses (RA1) is false.</a>
</a>
<p>
Eliezer wrote:

<p>
&gt;Now I'm not
<br>
<a href="2050.html#2093qlink1">&gt; dumb enough to think I have the vaguest idea what it's all for</a><br>
[snip]
<br>
<a href="2050.html#2093qlink2">&gt; Rational Assumptions:</a><br>
<i>&gt; RA1.  I don't know what the objective morality is and neither do you.</i><br>
<i>&gt; This distinguishes from past philosophies which have attempted to "prove"</i><br>
<i>&gt; their arguments using elaborate and spurious logic.  One does not "prove" a</i><br>
<i>&gt; morality; one assigns probabilities.</i><br>
[snip]
<br>
<a href="2050.html#2093qlink3">&gt; LA1 and RA1, called "Externalism", are not part of the Singularity logic per</a><br>
<i>&gt; se; these are simply the assumptions required to rationally debate</i><br>
<i>&gt; morality.</i><br>

<p>
I don't see why a rational debate about morality would be impossible 
if you or I knew the "objective morality". People often 
rationally debate issues even when one party already knows where the 
truth lies.

<p>
<a name="2098qlink2">As for RA1, I think it could well be argued that it is false. We may 
not know in detail and with certainty what the moral facts are (so 
sure we'd want to assign probabilities), but it doesn't follow that 
we know nothing about then at all. In fact, probably all the people 
on this list know that it is wrong to torture innocent people for a 
small amount of fun. We could no doubt write down a long list of 
moral statements that we would all agree are true. Do you mean that 
we all suffer from a huge illusion, and that we are all totally 
mistaken in believing these moral propositions?</a>

<p>
<a name="2098qlink3">Now, if we accept the position that maybe we already know quite a few 
simple moral truths, then your chain of reasoning is broken. No 
longer is it clear that the morally preferred action is to cause a 
singularity no matter what.</a> For example, if it is morally preferred 
<a name="2098qlink4">that the people who are currently alive get the chance to survive 
into the postsingularity world, then we would have to take this 
desideratum into account when deciding when and how hard to push for 
</a>the singularity.<a name="2098qlink5"> In the hypothetical case where we could dramatically 
increase the chances that we and all other humans would survive, by 
paying the relatively small price of postponing the singularity one 
year, then I feel pretty sure that the morally right thing to do 
would be to wait one year.</a>

<p>
<a name="2098qlink6">In reality there could well be some kind of tradeoff like</a> that. It's 
<a name="2098qlink7">good if superintelligent posthumans are created, but it's also good 
if *we* get to become such beings. And that can in some cases impose 
moral obligations on us to make sure that we ourselves can survive 
</a>the singularity. 

<p>
<a name="2275qlink2"><a name="2098qlink8">You might say that our human morality - our desire that *we* survive 
- is an arbitrary effect of our evolutionary history. Maybe so, but I 
don't see the relevance. If our morality is in that sense arbitrary, 
so what? You could say that the laws of physics are arbitrary, but 
that does not make them any less real. Don't forget that "moral" and 
"good" are words in a human language. It's not surprising, then, if 
their meaning is also in some way connected to human concepts and 
anthropocentric concerns.</a></a>

<p>
~~~
<br>
My own view is that it is indeed a very wise idea that humankind 
tries to put itself in a position where it will be better able than 
today to figure out what to do next, for example through intelligence 
amplification, education, research, information technology, 
collaborative information filtering, idea futures etc. - and 
ultimately by building superintelligence; hopefully by that time we 
will have been able to think through what could go wrong a little 
more so we know how to do it in a way that would allow ourselves to 
survive and to upload.

<p>
Nick Bostrom
<br>
<a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a>      n.bostrom@lse.ac.uk
Department of Philosophy, Logic and Scientific Method
London School of Economics
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2094.html">[ Next ]</a><a href="2092.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2050.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2275.html">Alejandro Dubrovsky</a>
</ul>
</body></html>
