<!-- received="Sun Nov 22 13:11:01 1998 MST" -->
<!-- sent="Sun, 22 Nov 1998 21:02:33 +0100" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: SI: Singleton and programming" -->
<!-- id="199811222002.VAA01073@lrz.uni-muenchen.de" -->
<!-- inreplyto="8b9adfd5.36526af9@aol.com" -->
<!-- version=1.10, linesinbody=211 -->
<html><head><title>extropy: Re: SI: Singleton and programming</title>
<meta name=author content="Eugene Leitl">
<link rel=author rev=made href="mailto:eugene.leitl@lrz.uni-muenchen.de" title ="Eugene Leitl">
</head><body>
<h1>Re: SI: Singleton and programming</h1>
Eugene Leitl (<i>eugene.leitl@lrz.uni-muenchen.de</i>)<br>
<i>Sun, 22 Nov 1998 21:02:33 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1550">[ date ]</a><a href="index.html#1550">[ thread ]</a><a href="subject.html#1550">[ subject ]</a><a href="author.html#1550">[ author ]</a>
<!-- next="start" -->
<li><a href="1551.html">[ Next ]</a><a href="1549.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1523.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1563.html">Eliezer S. Yudkowsky</a>
</ul>
<!-- body="start" -->

<p>
<a name="1563qlink1">Eliezer S. Yudkowsky writes:

<p>
<a href="1523.html#1550qlink1"> &gt; I can't say that I believe in the scenario of a Singularity as a collection of</a><br>
<i> &gt; individuals.  Goals converge at sufficiently high intelligence levels, just</i><br>
<i> &gt; like pictures of the truth.</i><br>
 
<p>
Are you saying that evolution doesn't exist post Singularity, then? Can you
back up that claim?
</a>
 
<p>
<a href="1523.html#1550qlink2"> &gt; What I'm pointing out is that there won't be "bottleneck" legacy systems.  If</a><br>
<i> &gt; old code is central, it will be redesigned.  Where does the vast excess of</i><br>

<p>
Let's say there is a provably optimal simple 'omega' hardware, which gives
you very little choice about how your 'code' (information patterns,
actually) at the deepest level will look like. The resulting substrate
supports a virtual ecology, starting with most primitive
autoreplicators a la Langdon loops, hardly more than our 
analogons of viroids (offspring of those designed or emerged 
spontaneously) and ending with godlike intelligences. Population 
pressure causes this substrate to expand, gobbling up the material 
realm and spitting out molecular circuitry blocks, instantly 
claimed by the duplicated virtual beasties. The sources of diversity
are both rational design and darwinian evolution. Both support
some conservation (legacy), as well as novelty due to coevolution 
concurrence (very much novelty in a Red Queen fitness landcape) 
but why should it be a bottleneck?

<p>
<a href="1523.html#1550qlink3"> &gt; computing power come from?  I'm assuming a constantly expanding pool.  In our</a><br>

<p>
<a name="1563qlink2">Once you have your omega hardware (probably a kind of quantum dot
array) design, which should be very early along the Singularity, the 
</a>
expansion rate of the pool is limited by the conversion rate 
of the material realm into circuitry, and the amount of 
matter available. Once you've converted and rearranged things 
optimally, further influx of substrate stops, unless you can shrug 
off physics as we know it and can make spacetime compute on its own. 
Signalling in a relativistic universe in a darwinian context limits 
you to relatively compact system anyway: the maximal grain size will 
thus be likely limited. You can hardly afford waiting to listen for 
tidings from locations lightminutes apart, while the local clock 
ticks at nanosecond and picosecond scale. Frozen statues don't take
well to erosion and bulldozers.

<p>
<a href="1523.html#1550qlink4"> &gt; days, there really is a lot of inertia that derives from human nature, even</a><br>
<i> &gt; after all optimization is accounted for.  I do think things will be faster.</i><br>
<i> &gt; </i><br>
<i> &gt; One interesting notion is that even if there's a completely unlimited amount</i><br>
<i> &gt; of computing power, aleph-null flops, the Singularity still might not be</i><br>
<i> &gt; autopotent.  The extent to which current computing power could be extended,</i><br>
<i> &gt; controlled, consolidated would still be finite.  There would be software</i><br>
<i> &gt; inertia even in the absence of hardware inertia, and there would still be hard</i><br>
<i> &gt; choices for optimization.  One would no longer conserve resources, or even</i><br>
<i> &gt; intelligence, but choices.</i><br>

<p>
I disagree. The way I see it, we will hit the computational limits set
by physics very quickly, and after that will simply engage in virtual
navelgazing/open-ended evolution. I don't buy Moravec's recurring
evolution shockwaves rewriting the laws of physics, as they are 
unfalsifyable using current knowledge. It may take a short while to evolve
nanoautoreplicators capable of following hard at the edge of the light
cone, but afterwards it's all plain sailing. Of course it can be that
traversable wormholes are buildable, but resources required are likely
to be so large that the second graduation night might be a few MYrs off.
 
<p>
<a name="1563qlink3"><a href="1523.html#1550qlink5"> &gt; It's not until you start talking about time travel (which I think is at least</a><br>
<i> &gt; 80% probable post-Singularity) that you get real "inertialess" systems.  I</i><br>

<p>
If you have to resort to Planck energies for spacetime engineering,
you might have to build a particle accelerator around the galaxy. It 
takes a while before you can assemble such a structure, and before 
it can pick up enough steam to do something interesting, like blasting
a designer hole into spacetime.
</a>

<p>
<a href="1523.html#1550qlink6"> &gt; cannot even begin to imagine what this might look like from the inside.  It is</a><br>
<i> &gt; incomprehensibility squared.</i><br>

<p>
<a href="1523.html#1550qlink7"> &gt; I think this confuses the disadvantages of human design with the disadvantages</a><br>
<i> &gt; of intelligent design in general.  Remember, evolved systems fail too.  Humans</i><br>
<i> &gt; go insane.  It's just a different kind of crash.  And, even using your</i><br>

<p>
Of course a fair fraction of these failures is due to the mutations,
without which evolutionary optimization would be impossible. Another
is a trade-off between optimizing forever and having to react as
quickly as possible. Red in tooth and claw doesn't breed perfect
systems, but robust ones. Any comparisons with products of human
engineering and what evolution came up are ridiculous. I'm still
wondering why so many people, programmers particularly, trust man-made
brittle systems. In biological systems, almost every component is
mission-critical (there goes that liver). A truly technological 
civilization based on current designs would go extinct overnight. 
Software glitches in a Brazil world? Urgh.

<p>
<a href="1523.html#1550qlink8"> &gt; assumptions, I'd rather suffer a general protection fault and be rebooted from</a><br>
<i> &gt; backups by an exoself using up 1% of resources, than go insane while using 75%</i><br>

<p>
Would you rather want to ward off several quick (well, quicker than
you) evolved systems BSOD'ing you with bug-exploiting nam-shubs? 
I'd say you'd lose, and the resources used up by you, by your exoself 
and your backups will be reclaimed by the victor meanies. If you rely 
on shared blocks, attacking that location would result in a vast 
payoff to the attacker. Why, suddenly it's not crowded anymore?

<p>
<a href="1523.html#1550qlink9"> &gt; Are PSEs more likely to suffer from asteroid strikes or Y2K?  If every date in</a><br>
<i> &gt; the world ran through a single module, we wouldn't have this problem.  Yes, I</i><br>
<i> &gt; know we would have other problems, but the operative word is "we".</i><br>
 
<p>
If there was a bug in that module, all of you would go
extinct. Descendants of any systems with a local nonfaulty 
module will happily take your place. No more brittle resource 
sharing anymore. 

<p>
Biological beings use internal clocks, synchronized by external
cycles. They only go awry when exposed to conditions (usually,
man-made) they are not meant to, and even then the results are 
usually not fatal. Better jetlagged than dead, thankyouverymuch.
  
<p>
<a href="1523.html#1550qlink10"> &gt; But they wouldn't be individuals - suppose Anders Sandburg has stripped from</a><br>
<i> &gt; him every algorithm he has in common with any other member of the human race,</i><br>
<i> &gt; and is given immersive perception and control of a fleem grobbler, one of the</i><br>
<i> &gt; five major independent fleem-grobbling systems.  Is he still Anders Sandburg? </i><br>

<p>
Suppose Anders Sandberg has stripped from him every physical structure
he has in common with any other member of the human race, and is given
immersive perception and control of a Microsoft Visual Basic compiler, one
of the five major independant Microsoft compilers, instead. Is he still Anders
Sandberg?

<p>
<a name="1563qlink4"><a href="1523.html#1550qlink11"> &gt; I'm not at all sure that evolution and singletons are compatible.  Evolution</a><br>

<p>
The worse for the singletons, then.
</a>

<p>
<a href="1523.html#1550qlink12"> &gt; relies on differential chances of survival, but with everything being</a><br>
<i> &gt; determined by intelligence, the human-understandable attributes like</i><br>

<p>
'everything being determined by intelligence'? Why that?

<p>
<a name="1563qlink5"><a href="1523.html#1550qlink13"> &gt; "survival" might be determined.  Even if one concedes internal differences,</a><br>
<i> &gt; the externally observed behavior of every Singularity might be exactly the</i><br>
<i> &gt; same - expand in all directions at lightspeed, dive into a black hole.  So</i><br>
<i> &gt; there might not be room for differential inclusive reproductive success.</i><br>
 
<p>
So far we have not observed any Singularity or distant signatures
thereof. What makes you think you can predict details of any such 
event?
</a>

<p>
<a name="1563qlink6"><a href="1523.html#1550qlink14"> &gt; Internally, the evolution you propose has to occur in defiance of the</a><br>
<i> &gt; superintelligent way to do things, or act on properties the superintelligence</i><br>

<p>
<a name="1563qlink7">Well, you are intelligent. Are you in control of other intelligences?
</a>
Particularly these dumb, lowly things like bacteria, viruses, flies,
dust mites, ants, cockroaches, silverfish, pets, cars, ships, sealing 
wax? Humankind have not spontaneously mutated into homogenous monocultured
Eliezers set into nice rows, why should a virtual ecology do that? In
</a>
<a name="1563qlink8">case you build such a strange thing, due to a chance some part of some
clone somewhere might grow frisky, and rushes over the civilization
monoculture like a brush fire. It's instable as hell, the thing.
</a>

 
<p>
<a name="1563qlink9"> &gt; [ AFUTD's skrodes ]<br>

<p>
You said a Power would have noticed, but skrodes were built either by
the Blight or the Countermeasure (it's not clear by which), which both
ate normal Powers for breakfast. A Blight from a distance didn't look
particularly singular to a Power, and a lot of Powers meddled with the
lower zones. The hidden functionality of the skrode/its rider complex
may well have exceeded a casual scrutiny of a Power. All they'd see
would be a yet another Power artefact.
</a>

<p>
<a name="1563qlink10"><a href="1523.html#1550qlink15"> &gt; "A programmer with a codic cortex - by analogy to our current visual cortex -</a><br>
<i> &gt; would be at a vast advantage in writing code.  Imagine trying to learn</i><br>
<i> &gt; geometry or mentally rotate a 3D object without a visual cortex; that's what</i><br>
<i> &gt; we do, when we write code without a module giving us an intuitive</i><br>
<i> &gt; understanding.  An AI would no more need a "programming language" than we need</i><br>
<i> &gt; a conscious knowledge of geometry or pixel manipulation to represent spatial</i><br>
<i> &gt; objects; the sentences of assembly code would be perceived directly - during</i><br>
<i> &gt; writing and during execution."</i><br>

<p>
A task you have a knack of, and doing for a long time changes you. You
sprout representational systems as you grow better and better. A
tabula rasa AI which was not designed to do machine language would
learn anything the hard way as well, exactly as a human. Of course if
it was more intelligent than a human it would grow much better than
that. 
</a>

<p>
<a name="1563qlink11"><a href="1523.html#1550qlink16"> &gt; Who needs a Power to get a skrode?  The first programming AIs will likely be</a><br>
<i> &gt; that incomprehensible to us mere humans.  You know how much trouble it is to</i><br>

<p>
Thank you, GP comes up with plenty of compact, efficient, opaque
solutions humans have no idea of how they work. If you ask a person to
write a word-recognition circuit, he'll certainly will not build a 100
FPGA-cell large conundrum consisting of a mesh of autofeedbacked loops
exploiting the undocumented analog effects of the bare silicon.
</a>

<p>
<a href="1523.html#1550qlink17"> &gt; get an AI to walk across a room?  Well, that's how hard it is for an AI to</a><br>
<i> &gt; teach a human to write code.</i><br>
<i> &gt; </i><br>
<i> &gt; OO programming is there for a reason, and that reason is transforming the raw</i><br>
<i> &gt; environment of assembly language into "objects" and "behaviors" comprehensible</i><br>
<i> &gt; to our cognition.  But OO may make about as much sense to an AI, as repainting</i><br>
<i> &gt; the Earth in regular patterns and basic shapes that combine to form computer</i><br>
<i> &gt; programs would make to us.  Different ontologies, different rules.</i><br>

<p>
<a name="1563qlink12">OO mirrors our physical world very much: independant objects interact
with lots of others via asynchronous messages. Many simulations are
made much more elegant this way. I think OOP is deeper than a mere
human fad.
</a>

<p>
ciao,
<br>
'gene
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1551.html">[ Next ]</a><a href="1549.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1523.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1563.html">Eliezer S. Yudkowsky</a>
</ul>
</body></html>
