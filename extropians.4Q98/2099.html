<!-- received="Fri Dec  4 12:41:53 1998 MST" -->
<!-- sent="Fri, 04 Dec 1998 13:43:18 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Singularity: Individual, Borg, Death?" -->
<!-- id="36683B52.E55449F1@pobox.com" -->
<!-- inreplyto="Singularity: Individual, Borg, Death?" -->
<!-- version=1.10, linesinbody=65 -->
<html><head><title>extropy: Re: Singularity: Individual, Borg, Death?</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Singularity: Individual, Borg, Death?</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Fri, 04 Dec 1998 13:43:18 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2099">[ date ]</a><a href="index.html#2099">[ thread ]</a><a href="subject.html#2099">[ subject ]</a><a href="author.html#2099">[ author ]</a>
<!-- next="start" -->
<li><a href="2100.html">[ Next ]</a><a href="2098.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2086.html">Alejandro Dubrovsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Alejandro Dubrovsky wrote:
<br>
<i>&gt; </i><br>
<a href="2086.html#2099qlink1">&gt; I'm assuming that what you are doing is trying to maximize the value of</a><br>
<i>&gt; the system.  I don't see, though how you can assume that the goals' values</i><br>
<i>&gt; are positive.  ie "Either life has meaning or it doesn't, but i don't see</i><br>
<i>&gt; any way of knowing if the discovery of the meaning of life is good or bad"</i><br>

<p>
There's no way of knowing whether life is good or bad, but it is unqualifiedly
good that a superintelligence find out which is the case and act on it.  In
either case, the correct value is served.

<p>
<a href="2086.html#2099qlink2">&gt; &gt; Logical Assumptions:</a><br>
<i>&gt; &gt; LA1.  Questions of morality have real answers - that is, unique,</i><br>
<i>&gt; &gt; observer-independent answers external from our opinions.</i><br>
<i>&gt; &gt; Justification:  If ~LA1, I can do whatever I want and there will be no true</i><br>
<i>&gt; &gt; reason why I am wrong; and what I want is to behave as if LA1 - thus making my</i><br>
<i>&gt; &gt; behavior rational regardless of the probability assigned to LA1.</i><br>
<i>&gt; </i><br>
<i>&gt; I disagree.  If there are multiple, observer-dependent answers, then</i><br>
<i>&gt; there's still a morality system that affects you and you could still be in</i><br>
<i>&gt; the wrong, and this situation would fall into ~LA1.</i><br>

<p>
<a name="2103qlink1">True.  Perhaps I should say opinion-independent rather than
observer-dependent.  Given the number of strange games physics plays</a> with
<a name="2103qlink2">observers, this is certainly possible.  But evolution is absolutely dependent
on the observing gene, since it's a differential competition; and unless I see
some excellent evidence, I'm not going to seriously consider the possibility
that our particular type of observer-independence translates exactly</a> into
<a name="2103qlink3">reality.  What I mean by observer-independence is really more like
"independent of our brand of observer-dependence".  There are various
interesting scenarios here.  But I don't see an observer-dependence scenario
of plausibility comparable to observer-independence which contributes a large
anti-Singularity factor.</a>

<p>
<a href="2086.html#2099qlink3">&gt; And even if LA1, i don't see how BEHAVING as if LA1 is more rational than</a><br>
<i>&gt; if behaving as if ~LA1.  As in John Clark's email about Pascal's wager,</i><br>
<i>&gt; the rational way to behave if LA1 might be to behave as if ~LA1, depending</i><br>
<i>&gt; the nature of the real moral answers.</i><br>

<p>
I agree completely.  Behaving as if the Singularity overrules all other moral
considerations and the end justifies the means is likely to get you slapped
down and tossed in prison.  One must respect the observer-dependent wishes of
others to get along.

<p>
<a href="2086.html#2099qlink4">&gt; My rejection of LA1 makes me unfit (by your conclusion, with which i</a><br>
<i>&gt; mostly agree with) to argue rationally about morality, and i suppose my</i><br>
<i>&gt; claim (like many others') is that you cannot argue rationally about</i><br>
<i>&gt; morality since LA1 seems very weak.</i><br>

<p>
Would you agree that LA1 is either true or false?  (Call this meta-LA1.)  We
might not be able to debate morality, but we can certainly debate LA1, which
is what we are doing.  For that matter, if you agree to a weaker version of
LA1 in which some things are "wrong" even if what's "right" is
observer-dependent; or if you have a proposed observer-dependent (but
non-arbitrary) system in which logic is subject to disproof; we can still have
a rational debate.

<p>
The key requirement is that there should be some method of disproof.
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2100.html">[ Next ]</a><a href="2098.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2086.html">Alejandro Dubrovsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
