<!-- received="Thu Dec 17 11:57:49 1998 MST" -->
<!-- sent="Thu, 17 Dec 1998 12:59:43 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Truth machines" -->
<!-- id="3679549C.CE0CC751@pobox.com" -->
<!-- inreplyto="Truth machines" -->
<!-- version=1.10, linesinbody=85 -->
<html><head><title>extropy: Re: Truth machines</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Truth machines</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Thu, 17 Dec 1998 12:59:43 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3142">[ date ]</a><a href="index.html#3142">[ thread ]</a><a href="subject.html#3142">[ subject ]</a><a href="author.html#3142">[ author ]</a>
<!-- next="start" -->
<li><a href="3143.html">[ Next ]</a><a href="3141.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3135.html">Robin Hanson</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Robin Hanson wrote:
<br>
<i>&gt; </i><br>
<a href="3135.html#3142qlink1">&gt; I agree with Eliezer that it is likely that we can find clear fMRI</a><br>
<i>&gt; signatures of conscious intentional lying.  What bothers me more is the</i><br>
<i>&gt; fact that we are only conscious of the smallest fraction of what goes on</i><br>
<i>&gt; in our heads.  Our unconsious selves are quite familiar with "lying" to</i><br>
<i>&gt; our conscious selves to help our genes to reproduce.</i><br>

<p>
Heh, heh, heh!  But you see, my dear Dr. Hanson, there's an immense
amount of evolutionary effort behind our ability to lie to ourselves. 
Does everyone see where this is going?

<p>
The development of an "unrationalization" device would have an impact
even larger (albeit less violent) than that of a truth machine.  This
may well be the most powerful form of near-current-tech intelligence
enhancement I know of, perhaps more powerful than neurosurgical Algernic
specialization.  The impact of this device is almost impossible to
estimate.  If the development of a truth machine represents a change in
the sociopolitical climate on the order of an Ice Age, then an
unrationalization device represents an asteroid strike.

<p>
Such a device is mentioned in passing in a Story In Progress of mine,
although it really deserves its own novel.

<p>
<a href="3135.html#3142qlink2">&gt; Romeos sincerely believe they'll "love her forever", even though it'll</a><br>
<i>&gt; be a different her in a week.  Politicians sincerely believe that public</i><br>
<i>&gt; schools are best for us all, even though for some reason they prefer to</i><br>
<i>&gt; send their kids to private schools.  Sales people find it easy to believe</i><br>
<i>&gt; that their product is really the best for you.  Most people find it easy</i><br>
<i>&gt; to think they are above average drivers, lovers, etc.  "The Moral Animal"</i><br>
<i>&gt; by Robert Wright discusses a lot of this sort of self-deception.  See</i><br>
<i>&gt; also: <a href="http://hanson.berkeley.edu/belieflikeclothes.html">http://hanson.berkeley.edu/belieflikeclothes.html</a></i><br>

<p>
A wonderful book; it was my introduction to evolutionary psychology. 
Required Reading.  Anyway, given the highly specialized behaviors, it
isn't at all hard to believe that there are specific brain areas involved.

<p>
<a href="3135.html#3142qlink3">&gt; As John Clark said:</a><br>
<i>&gt; &gt;Another problem is that the most dangerous and horrible monsters are</i><br>
<i>&gt; &gt;also sincere monsters.  I don't think it was just an act, I think Hitler</i><br>
<i>&gt; &gt;really thought Jews were subhuman and that he was doing a good thing by</i><br>
<i>&gt; &gt;butchering them.  Sincerity is a vastly overrated virtue.</i><br>
<i>&gt; </i><br>
<i>&gt; With a truth machine, people would want to avoid intellectuals, cynics,</i><br>
<i>&gt; and others who point out how their "sincere" beliefs are inconsistent and</i><br>
<i>&gt; self-serving.   After all, if they listened to such critics they might</i><br>
<i>&gt; adopt beliefs which would hurt them socially.  "In all likelihood, I'll</i><br>
<i>&gt; only love you this much for a week."  I fear people might instead begin</i><br>
<i>&gt; a strong and perhaps bloody supression of critical voices.  Sorta like</i><br>
<i>&gt; what happens in war time suppressing those who aren't sure the war is a</i><br>
<i>&gt; good idea, only much more so.</i><br>

<p>
We appear to be in definite agreement on one thing:  A truth machine
would cause a gigantic political upheaval.  A lot of social arrangements
would snap like twigs and the balance of power everywhere would
drastically alter.  It would be the Y2K problem of the social fabric.

<p>
Politicians with all their dirty linen suddenly aired.  Church officials
- what happens if the machine red-lights the Pope?  Dictators stamping
out all traces of rebellion.  Children of religious parents punished for
their hidden doubts.  Consensual criminals hunted down.  Chairmen who
don't really care about their employees ousted.  Ninety-nine people with
dirty linen and one altruist trying to get all use of the machine
declared a civil rights violation.  UN bans.  Uprisings.  Black-market
machines used for blackmail.  Congressbeing Lawyer replaced with
Congressbeing Fanatic, Congressbeing Rationalizer, and Congressbeing Idiot.

<p>
On the whole, I really think it would be a good idea if the
"unrationalization" machine were introduced first.

<p>
Singularity permitting, I expect the Age of the Neurohackers to begin
within 15 years.  An article in _Wired_ once pointed out that the reason
the great biotech frontier hasn't gone mainstream is because biotech is
so expensive; there are no hackers, no cowboys.  He expected biohackers
in 30 years, I think.  I expect the first neurohackers in 10.

<p>
Ah, it's an exciting time to be alive!
<br>
But don't forget the canned food and shotguns.
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3143.html">[ Next ]</a><a href="3141.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3135.html">Robin Hanson</a>
<!-- nextthread="start" -->
</ul>
</body></html>
