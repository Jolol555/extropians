<!-- received="Wed Dec  9 15:04:49 1998 MST" -->
<!-- sent="Wed, 09 Dec 1998 17:04:57 -0500" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: Singularity: AI Morality (AI containment)" -->
<!-- id="366EF409.45EDCEDE@posthuman.com" -->
<!-- inreplyto="Singularity: AI Morality (AI containment)" -->
<!-- version=1.10, linesinbody=62 -->
<html><head><title>extropy: Re: Singularity: AI Morality (AI containment)</title>
<meta name=author content="Brian Atkins">
<link rel=author rev=made href="mailto:brian@posthuman.com" title ="Brian Atkins">
</head><body>
<h1>Re: Singularity: AI Morality (AI containment)</h1>
Brian Atkins (<i>brian@posthuman.com</i>)<br>
<i>Wed, 09 Dec 1998 17:04:57 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2354">[ date ]</a><a href="index.html#2354">[ thread ]</a><a href="subject.html#2354">[ subject ]</a><a href="author.html#2354">[ author ]</a>
<!-- next="start" -->
<li><a href="2355.html">[ Next ]</a><a href="2353.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2352.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2357.html">Billy Brown</a>
</ul>
<!-- body="start" -->

<p>
<a name="2357qlink1">Well ok imagine something like this: a large corporate decides
to develop its own "slave SI AI" so that it can take down its
corporate rivals more easily. So it builds a secure facility
for it, totally cut off from the Internet or other computers,
in a physically secure room with very very strict security
procedures for the humans interacting with it. Now unless
some kind of silly plot device happens like on TV, this
should keep the AI contained, no? (assuming no wild physics
discoveries)</a>

<p>
Billy Brown wrote:
<br>
<i>&gt; </i><br>
<a href="2352.html#2354qlink1">&gt; Brian Atkins writes:</a><br>
<i>&gt; &gt; I'm curious if there has been a previous discussion on this</i><br>
<i>&gt; &gt; list regarding the secure containment of an AI (let's say a</i><br>
<i>&gt; &gt; SI AI for kicks)? Many people on the list seem to be saying</i><br>
<i>&gt; &gt; that no matter what you do, it will manage to break out of</i><br>
<i>&gt; &gt; the containment. I think that seems a little far-fetched....</i><br>
<i>&gt; </i><br>
<i>&gt; Here's why I don't think containment is feasible for an SI:</i><br>
<i>&gt; </i><br>
<i>&gt; 1) No one has ever achieved it for programs written by humans.  Some</i><br>
<i>&gt; operating systems get close, but I can't think of one that has never had a</i><br>
<i>&gt; serious security issue.</i><br>
<i>&gt; </i><br>
<i>&gt; 2) Supporting an SI AI would require much faster machines than what we have</i><br>
<i>&gt; now, running much more complex programs.  This makes the problem even worse.</i><br>
<i>&gt; </i><br>
<i>&gt; 3) An AI will be much better at programming than humans.  That means that</i><br>
<i>&gt; its efforts to get around our security will be much sneakier, and more</i><br>
<i>&gt; complex, that those of human hackers.  See #1.</i><br>
<i>&gt; </i><br>
<i>&gt; Even if you make a perfectly secure sandbox, we still aren't safe.  Never</i><br>
<i>&gt; underestimate the security risk posed by social engineering:</i><br>
<i>&gt; </i><br>
<i>&gt; 4) You have to have human/AI contact to have any idea what the AIs are like.</i><br>
<i>&gt; This opens up lots of potential problems - the AI can talk someone into</i><br>
<i>&gt; letting it out, bribe them to do it, 'give away' useful (or fantastically</i><br>
<i>&gt; valuable) programs that contain seeds of itself, etc.</i><br>
<i>&gt; </i><br>
<i>&gt; 5) Don't forget the legal front.  The AI could try to convince people that</i><br>
<i>&gt; it is a person, and you are keeping it as a slave (not hard to do, since</i><br>
<i>&gt; that's exactly what is happening).  If it acts as its own lawyer, you're</i><br>
<i>&gt; probably going to loose the case.</i><br>
<i>&gt; </i><br>
<i>&gt; 6)  Do reporters ever talk to the AI?  Of course they do.  Think of the PR</i><br>
<i>&gt; campaign the 'poor, helpless, exploited' AI could mount.</i><br>
<i>&gt; </i><br>
<i>&gt; Some of these problems are bigger than others, but that isn't the point.</i><br>
<i>&gt; The real problem is that I thought of all these approaches in the space of</i><br>
<i>&gt; 15 minutes, and I'm only human.  What is something with an IQ of 1,000 (or</i><br>
<i>&gt; worse, 1,000,000) going to think of?</i><br>
<i>&gt; </i><br>
<i>&gt; Billy Brown</i><br>
<i>&gt; bbrown@conemsco.com</i><br>

<pre>
-- 
The future has arrived; it's just not evenly distributed.
                                                       -William Gibson
______________________________________________________________________
Visit Hypermart at <a href="http://www.hypermart.net">http://www.hypermart.net</a> for free business hosting!
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2355.html">[ Next ]</a><a href="2353.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2352.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2357.html">Billy Brown</a>
</ul>
</body></html>
