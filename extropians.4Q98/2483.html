<!-- received="Fri Dec 11 04:44:48 1998 MST" -->
<!-- sent="Fri, 11 Dec 1998 11:36:26 -0000" -->
<!-- name="Samael" -->
<!-- email="Samael@dial.pipex.com" -->
<!-- subject="Re: Singularity: AI Morality" -->
<!-- id="02c201be24fa$7d0ed2e0$0100a8c0@andy" -->
<!-- inreplyto="Singularity: AI Morality" -->
<!-- version=1.10, linesinbody=45 -->
<html><head><title>extropy: Re: Singularity: AI Morality</title>
<meta name=author content="Samael">
<link rel=author rev=made href="mailto:Samael@dial.pipex.com" title ="Samael">
</head><body>
<h1>Re: Singularity: AI Morality</h1>
Samael (<i>Samael@dial.pipex.com</i>)<br>
<i>Fri, 11 Dec 1998 11:36:26 -0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2483">[ date ]</a><a href="index.html#2483">[ thread ]</a><a href="subject.html#2483">[ subject ]</a><a href="author.html#2483">[ author ]</a>
<!-- next="start" -->
<li><a href="2484.html">[ Next ]</a><a href="2482.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2408.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->


<p>
-----Original Message-----
<br>
From: Billy Brown &lt;bbrown@conemsco.com&gt;
<br>
To: extropians@extropy.com &lt;extropians@extropy.com&gt;
Date: 10 December 1998 20:26
<br>
Subject: RE: Singularity: AI Morality


<p>
<a href="2408.html#2483qlink1">&gt;Samael wrote:</a><br>
<i>&gt;&gt; &gt; In an AI, there is only one goal system.  When it is trying to decide</i><br>
if
<br>
<i>&gt;an</i><br>
<a href="2408.html#2483qlink2">&gt;&gt; &gt; action is moral, it evaluates it against whatever rules it uses for</a><br>
such
<br>
<a href="2408.html#2483qlink3">&gt;&gt; &gt; things and comes up with a single answer.  There is no 'struggle to do</a><br>
<i>&gt;the</i><br>
<i>&gt;&gt; &gt; right thing', because there are no conflicting motivations..</i><br>
<i>&gt;&gt;</i><br>
<i>&gt;&gt; Unless it has numerous different factos which contribute towards it's</i><br>
<i>&gt;rules..</i><br>
<i>&gt;&gt; After all, it would probably have the same problems with certain</i><br>
<i>&gt;situations</i><br>
<i>&gt;&gt; that we would.  Would it think that the ends justify the means?  What</i><br>
<i>&gt;&gt; variance would it allow for different possibilities?  It would be better</i><br>
<i>&gt;at</i><br>
<i>&gt;&gt; predicting outcomes from its actions, but it stil wouldn't be perfect..</i><br>
<i>&gt;&gt;</i><br>
<i>&gt;&gt; Samael</i><br>
<i>&gt;</i><br>
<i>&gt;The AI won't necessarily have a clear answer to a moral question, any more</i><br>
<i>&gt;than we do.  However, my point is that it won't have more than one answer -</i><br>
<i>&gt;there is no 'my heart says yes but my mind says no' phenomenon.</i><br>


<p>
But it might have an 'objective' viewpoint.

<p>
"Tell me oh wise AI, is it moral to X'

<p>
"Oh suplicant, you are a utilitarian, so it is indeed moral to X.  I would
advise you to not let your christian neighbours find out as according to
their moral system, it is wrong to X."

<p>
Samael
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2484.html">[ Next ]</a><a href="2482.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2408.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
</body></html>
