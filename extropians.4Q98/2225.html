<!-- received="Mon Dec  7 18:16:55 1998 MST" -->
<!-- sent="Tue, 8 Dec 1998 01:15:26 +0000" -->
<!-- name="Nick Bostrom" -->
<!-- email="bostrom@ndirect.co.uk" -->
<!-- subject="Re: Singularity: AI Morality" -->
<!-- id="199812080116.BAA08441@andromeda.hosts.netdirect.net.uk" -->
<!-- inreplyto="366AC943.B3E1F8C@pobox.com" -->
<!-- version=1.10, linesinbody=92 -->
<html><head><title>extropy: Re: Singularity: AI Morality</title>
<meta name=author content="Nick Bostrom">
<link rel=author rev=made href="mailto:bostrom@ndirect.co.uk" title ="Nick Bostrom">
</head><body>
<h1>Re: Singularity: AI Morality</h1>
Nick Bostrom (<i>bostrom@ndirect.co.uk</i>)<br>
<i>Tue, 8 Dec 1998 01:15:26 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2225">[ date ]</a><a href="index.html#2225">[ thread ]</a><a href="subject.html#2225">[ subject ]</a><a href="author.html#2225">[ author ]</a>
<!-- next="start" -->
<li><a href="2226.html">[ Next ]</a><a href="2224.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2153.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky wrote:

<p>
<a href="2153.html#2225qlink1">&gt; This, in my opinion, is exactly the wrong answer.  (See particularly the</a><br>
<i>&gt; "Prime Directive of AI" in "Coding a Transhuman AI".)  But think about what</i><br>
<i>&gt; you just said.  First you say that sufficient intelligence should be able to</i><br>
<i>&gt; recognize good and bad.  Then you say that we should build in a moral system</i><br>
<i>&gt; with a particular set of values.</i><br>

<p>
Yes. What I mean is this: whatever moral system we have (whether we 
have defined it explicitly or it is only implicitly manifested in our 
use of such moral terms as "good", "right" etc.), a superintelligence 
would be able to figure it out and to understand how it was intended 
to be applied to particular cases. So what we have to do is (1) 
define a moral system that would place a great value on our own 
survival (as well as on our gradual metamorphoses into posthumans); 
and (2) to give the superintelligence a strong desire to live by this 
moral system.

<p>
&gt;  What if we get it wrong?<br>

<p>
Then possibly we're fucked.

<p>
<a href="2153.html#2225qlink2">&gt; Do you really know all the logical</a><br>
<i>&gt; consequences of placing a large value on human survival?  Would you care to</i><br>
<i>&gt; define "human" for me?  Oops!  Thanks to your overly rigid definition, you</i><br>
<i>&gt; will live for billions and trillions and googolplexes of years, prohibited</i><br>
<i>&gt; from uploading, prohibited even from ameliorating your own boredom, endlessly</i><br>
<i>&gt; screaming, until the soul burns out of your mind, after which you will</i><br>
<i>&gt; continue to scream. </i><br>

<p>
I think the risk of this happening is pretty slim and it can be made 
smaller through<a name="2240qlink1"> building smart safeguards into the moral system. For 
example, rather than rigidly prescribing a certain treatment for 
humans, we could add a clause allowing for democratic decisions by 
humans or human descendants to overrule other laws. I bet you could 
think of some good safety-measures if you put your mind</a> to it.


<p>
<a href="2153.html#2225qlink3">&gt; If you can synchronize everyone's intelligence</a><br>
<i>&gt; enhancement perfectly, then eventually we'll probably coalesce into a</i><br>
<i>&gt; singleton indistinguishable from that resulting from an AI Transcend.</i><br>

<p>
That could easily happen even if we don't synchronize everyone's 
intelligence enhancements.


<p>
<a href="2153.html#2225qlink4">&gt; Look, these forces are going to a particular place, and they are way, way,</a><br>
<i>&gt; waaaaaayyy too big for any of us to divert.  Think of the Singularity as this</i><br>
<i>&gt; titanic, three-billion-ton truck heading for us.  We can't stop it, but I</i><br>
<i>&gt; suppose we could manage to get run over trying to slow it down.</i><br>

<p>
To use your analogy, what I am proposing is that we try to latch on 
to it somehow - the earlier the better, since it get's harder as it 
picks up speed - and try to get into the driver's seat. Then we can 
drive it safely to where we want it to go.


<p>
<a href="2153.html#2225qlink5">&gt; &gt; but let's not go into that</a><br>
<i>&gt; &gt; now.</i><br>
<i>&gt; </i><br>
<i>&gt; Let's.  Please.  Now.</i><br>

<p>
How to contol a superintelligence? An interesting topic. I hope to 
write a paper on that during the Christmas holiday. 


<p>
<a href="2153.html#2225qlink6">&gt; &gt; Plus: whether it's moral or not, we would want to make</a><br>
<i>&gt; &gt; sure that they are kind to us humans and allow us to upload.</i><br>
<i>&gt; </i><br>
<i>&gt; No, we would NOT want to make sure of that.  It would be immoral.  Every bit</i><br>
<i>&gt; as immoral as torturing little children to death, but with a much higher</i><br>
<i>&gt; certainty of evil.</i><br>

<p>
I suppose we have to agree to disagree on that one. But even if it 
were slighly immoral to place a premium on human survival, I still 
think we should do it - simply because we want to survive. You are 
asking too much if you want us to be coldblodedly engineer our own 
martyrdom. I would not vote for that policy.

<p>
Sure, a few humans who refuse to upload might be inefficient and a 
waste of resources, but there are enough resources in the universe 
that we can afford that. Let's be generous. Even from your moral 
point-of-view, it would seem a wise moral insurance policy - for what 
if human life turned out to have great moral value after all, and we 
allowed a selfish superintelligence to destroy it. The outcome would 
have been hundreds of times worse than what Hitler did.

<p>
Nick Bostrom
<br>
<a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a>      n.bostrom@lse.ac.uk
Department of Philosophy, Logic and Scientific Method
London School of Economics
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2226.html">[ Next ]</a><a href="2224.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2153.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
