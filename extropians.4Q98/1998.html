<!-- received="Wed Dec  2 12:16:10 1998 MST" -->
<!-- sent="Wed, 02 Dec 1998 14:09:08 -0500" -->
<!-- name="Michael Lorrey" -->
<!-- email="retroman@together.net" -->
<!-- subject="Re: Singularity: Individual, Borg, Death? was Re: the few, the proud..." -->
<!-- id="36659053.822EF704@together.net" -->
<!-- inreplyto="Singularity: Individual, Borg, Death? was Re: the few, the proud..." -->
<!-- version=1.10, linesinbody=126 -->
<html><head><title>extropy: Re: Singularity: Individual, Borg, Death? was Re: the few, the proud...</title>
<meta name=author content="Michael Lorrey">
<link rel=author rev=made href="mailto:retroman@together.net" title ="Michael Lorrey">
</head><body>
<h1>Re: Singularity: Individual, Borg, Death? was Re: the few, the proud...</h1>
Michael Lorrey (<i>retroman@together.net</i>)<br>
<i>Wed, 02 Dec 1998 14:09:08 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1998">[ date ]</a><a href="index.html#1998">[ thread ]</a><a href="subject.html#1998">[ subject ]</a><a href="author.html#1998">[ author ]</a>
<!-- next="start" -->
<li><a href="1999.html">[ Next ]</a><a href="1997.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1969.html">Paul Hughes</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Paul Hughes wrote:

<p>
<a href="1969.html#1998qlink1">&gt; "Eliezer S. Yudkowsky" wrote:</a><br>
<i>&gt;</i><br>
<i>&gt; &gt;  I grew up knowing I was a genius and knowing that I could almost</i><br>
<i>&gt; &gt; certainly be part of something big if I played my cards right.</i><br>
<i>&gt;</i><br>
<i>&gt; I have no doubt of that.  However since I am probably not a genius (yet), I have up</i><br>
<i>&gt; to this point been unable to argue with you on the finer points of your reasoning.</i><br>
<i>&gt; However, after abundant contemplation, I'm beginning to notice what may be some</i><br>
<i>&gt; logical inconsistencies in your position.</i><br>
<i>&gt;</i><br>
<i>&gt; For starters, if I had to come down to choosing logic vs. survival, I'd choose my</i><br>
<i>&gt; survival every time.  I can think of at least three life-threatening situations</i><br>
<i>&gt; where my gut instinct saved my life over all other logical objections.</i><br>

<p>
In split second emergencies, the old flight or fight reflex works remarkably well.
Stopping to think tends to get you killed, but only because the logical left brain tends
to cross wires with the right brain, which is the typical seat of instinctive eye/body
coordination.

<p>
<a href="1969.html#1998qlink2">&gt; &gt; I am not acting on wishes.  I do not at this time project that the Singularity</a><br>
<i>&gt; &gt; will result in the gratification of any of the desires that initially</i><br>
<i>&gt; &gt; motivated me, not pride, not the acknowledgement of greatness, not the fun,</i><br>
<i>&gt; &gt; and probably not even the knowledge of success.  The personal drama that once</i><br>
<i>&gt; &gt; captivated me is irrelevant.  I am acting on logic.</i><br>
<i>&gt;</i><br>
<i>&gt; Ok, so your acting on logic.  If I understand you correctly, all of the things that</i><br>
<i>&gt; makes life enjoyable (fun) are irrelevant because logically our only outcomes are</i><br>
<i>&gt; singularity or oblivion?  Either we face the inevitable or accept extinction?</i><br>

<p>
More like, accept that things are going to change. Being stubbornly against change is
only going to cost you.

<p>
<i>&gt;</i><br>
<i>&gt;</i><br>
<a href="1969.html#1998qlink3">&gt; &gt; Now, I happen to think that even from humanity's perspective, a rapid</a><br>
<i>&gt; &gt; Singularity is the best way to go, because I don't see a feasible alternative.</i><br>
<i>&gt;</i><br>
<i>&gt; Can you concisely explain why a a non-rapid path to the Singularity is unfeasible?</i><br>

<p>
Well, there are several possible excuses for this opinion.

<p>
One is that since the reactionary factions in society are all opposed to this change,
and will tend to try to stamp it out in a luddite/fundamentalist backlash, you need a
minimum rate of growth to attain a minimum individual power base size that is effective
in defending against such attacks, within a minimum time period such that you get too
powerful for the luddites before they notice that you (a transhuman entity) are actually
in existence. Personally I think that a soft singularity is the way to avoid such civil
strife. Many people tend to get reactionary when the rate of change exceeds a certain
multiple of the rate which the individual perceives to have existed in childhood. So,
thinking rationally, a lower rate of growth will tend to minimize interest in such
reactionary opinions in the general population, thus marginalizing their impact on
society at large.

<p>
Another rationalization for this opinion is based on some faulty notions of malthusian
scarcity. Since acheiving a transhuman state will require x amount of resources, we need
to keep technology advancing enough such that efficiency gains from technology outweigh
increases in population diluting the per capita resource base. People that still think
that population is growing too rapidly will tend to beleive that a fast track to the
singularity is needed, otherwize the trend will stall out and civilization will fall
into a dark age. Since recent studies of populations show that as populations get a)
more educated (especially the female population), and b) more wealthy, and c) more
healthy, then the per capita child production tends to drop off, such that industrial
nations now have negative population growth when immigration is not counted. All
population growth worldwide is concentrated in the 3rd world. So, the fewer nations that
remain in a 3rd world development status should lead to a lower global population growth
rate. This sort of reasoning is supported by such statistics as energy cost stats. On a
current dollar rate, energy of all kinds is cheaper today than at any time in human
history. Resource costs of all kinds is deflating on a regular, annual basis, and has
been for most of this decade. What is good about this is it makes acheiving the
singularity more affordable for greater percentages of the population.

<p>
<i>&gt;</i><br>
<i>&gt;</i><br>
<a href="1969.html#1998qlink4">&gt; &gt; The only way that any of us</a><br>
<i>&gt; &gt; can "not die" is through a friendly Singularity.  If that's impossible, well,</i><br>
<i>&gt; &gt; at least our deaths will be the ethically correct thing to do.  Sooner or</i><br>
<i>&gt; &gt; later human civilization will perish or go through a Singularity.  This I</i><br>
<i>&gt; &gt; guarantee.  What can you possibly accomplish, for yourself or for anyone, by</i><br>
<i>&gt; &gt; delaying it?</i><br>
<i>&gt;</i><br>
<i>&gt; My life for starters.  Unlike you Elizier, I could care less about the singularity</i><br>
<i>&gt; if it means the end of my existence.  What I do care about is the continued</i><br>
<i>&gt; existence of my memetic conscious self and others in my memesphere (which includes</i><br>
<i>&gt; even you Eliezer).  Now if that means that I must embrace the Singularity or face</i><br>
<i>&gt; death, then you and I are in agreement.  I'm very willing to embrace logical</i><br>
<i>&gt; outcomes, but only under the condition of my continued existence.  If somehow</i><br>
<i>&gt; 'logic' prevented me from surviving a situation, then 'logic' would have to go.</i><br>
<i>&gt; Call me selfish, call me egocentric; but I'm not about to put my faith in an</i><br>
<i>&gt; unknowable singularity (god) over my own self-directed consciousness.  I would</i><br>
<i>&gt; rather lead a long and futile quest for perfection as an individual, rather than</i><br>
<i>&gt; join a more potentially satisfactory collective borganism.</i><br>
<i>&gt;</i><br>
<i>&gt; I'll agree that if a non-rapid path to a friendly singularity is not possible, then</i><br>
<i>&gt; the logical thing for anyone who cares about their continued existence is to embrace</i><br>
<i>&gt; a rapid Singularity.</i><br>
<i>&gt;</i><br>
<i>&gt; &gt; But that could all be rationalization.  It's not the reasoning I use.  I wish</i><br>
<i>&gt; &gt; to do the right thing, which is a question that is resolved by intelligence,</i><br>
<i>&gt; &gt; and thus I am required to create a higher intelligence to accept orders from.</i><br>
<i>&gt;</i><br>
<i>&gt; Can you accept the possibility that this higher intelligence could be internally</i><br>
<i>&gt; generated rather than externally dictated? Assuming its possible, which would you</i><br>
<i>&gt; rather have?</i><br>

<p>
I would obviously want myself to be an active participant into a transhuman existence,
not merely in a parental role.

<p>
<i>&gt;</i><br>
<i>&gt;</i><br>
<a href="1969.html#1998qlink5">&gt; &gt; My allegiance is to the Singularity first, humanity second, and I'm tired of</a><br>
<i>&gt; &gt; pretending otherwise to myself.</i><br>
<i>&gt;</i><br>
<i>&gt; My allegiance is to myself first, that part of humanity I care about the most, then</i><br>
<i>&gt; the Singularity.  Like you Elizier I'm seduced by what the Singularity portends and</i><br>
<i>&gt; want to achieve and immerse myself in higher intelligence, but only as a</i><br>
<i>&gt; transformation of my ego rather than a deletion of it.</i><br>

<p>
This is the sort of sentiment which will be a limiting factor on a fast hard
singularity. The more people feel that they have no personal stake in the singularity,
the less likely they are to support it.

<p>
Mike Lorrey
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1999.html">[ Next ]</a><a href="1997.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1969.html">Paul Hughes</a>
<!-- nextthread="start" -->
</ul>
</body></html>
