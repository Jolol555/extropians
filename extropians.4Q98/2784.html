<!-- received="Mon Dec 14 11:18:40 1998 MST" -->
<!-- sent="Mon, 14 Dec 1998 10:17:59 -0800" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@econ.berkeley.edu" -->
<!-- subject="RE: Singularity: AI Morality" -->
<!-- id="3.0.3.32.19981214101759.00c38a38@econ.berkeley.edu" -->
<!-- inreplyto="002901be2555$b80b9fd0$352501c0@mfg130" -->
<!-- version=1.10, linesinbody=28 -->
<html><head><title>extropy: RE: Singularity: AI Morality</title>
<meta name=author content="Robin Hanson">
<link rel=author rev=made href="mailto:hanson@econ.berkeley.edu" title ="Robin Hanson">
</head><body>
<h1>RE: Singularity: AI Morality</h1>
Robin Hanson (<i>hanson@econ.berkeley.edu</i>)<br>
<i>Mon, 14 Dec 1998 10:17:59 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2784">[ date ]</a><a href="index.html#2784">[ thread ]</a><a href="subject.html#2784">[ subject ]</a><a href="author.html#2784">[ author ]</a>
<!-- next="start" -->
<li><a href="2785.html">[ Next ]</a><a href="2783.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2562.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Billy Brown wrote:
<br>
<a href="2562.html#2784qlink1">&gt;&gt; You can you possibly know this about AIs?  I know of a great many</a><br>
<i>&gt;&gt; programs that have been written by AI researchers that use conflicting</i><br>
<i>&gt;&gt; goal systems, where conflicts are resolved by some executive module.</i><br>
<i>&gt;&gt; Maybe those approaches will win out in the end.</i><br>
<i>&gt;...</i><br>
<i>&gt;In humans, there seem to be many different ways for a goal to be selected -</i><br>
<i>&gt;sometimes we make a logical choice, sometimes we rely on emotions, and</i><br>
<i>&gt;sometimes we act on impulse.  There also does not seem to be a unified</i><br>
<i>&gt;system for placing constraints on the methods used to pursue these goals -</i><br>
<i>&gt;sometimes a moral system's prohibitions are obeyed, and sometimes they are</i><br>
<i>&gt;ignored.</i><br>
<i>&gt;</i><br>
<a name="2826qlink1"><i>&gt;If you want to implement a sentient AI, there is no obvious reason to do</i><br>
<i>&gt;things this way.  It would make more sense to implement as many mechanisms</i><br>
<i>&gt;as you like for suggesting possible goals, then have a single system for</i><br>
<i>&gt;selecting which ones to pursue.  </i><br>

<p>
There is no obvious reason to do things the way humans do, but there is 
no obvious reason not to either.  I think it is fair to say that few things 
are obvious about high level AI organization and design.</a>


<p>
Robin Hanson  
<pre>
hanson@econ.berkeley.edu     <a href="http://hanson.berkeley.edu/">http://hanson.berkeley.edu/</a>   
RWJF Health Policy Scholar             FAX: 510-643-8614 
140 Warren Hall, UC Berkeley, CA 94720-7360 510-643-1884     
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2785.html">[ Next ]</a><a href="2783.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2562.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
</body></html>
