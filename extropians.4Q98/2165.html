<!-- received="Sun Dec  6 20:53:38 1998 MST" -->
<!-- sent="Sun, 6 Dec 1998 19:31:48 -0800" -->
<!-- name="Zenarchy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: Singularity: AI Morality" -->
<!-- id="00ef01be2195$a1c2e260$027519d0@J.R.Molloy" -->
<!-- inreplyto="Singularity: AI Morality" -->
<!-- version=1.10, linesinbody=78 -->
<html><head><title>extropy: Re: Singularity: AI Morality</title>
<meta name=author content="Zenarchy">
<link rel=author rev=made href="mailto:jr@shasta.com" title ="Zenarchy">
</head><body>
<h1>Re: Singularity: AI Morality</h1>
Zenarchy (<i>jr@shasta.com</i>)<br>
<i>Sun, 6 Dec 1998 19:31:48 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2165">[ date ]</a><a href="index.html#2165">[ thread ]</a><a href="subject.html#2165">[ subject ]</a><a href="author.html#2165">[ author ]</a>
<!-- next="start" -->
<li><a href="2166.html">[ Next ]</a><a href="2164.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2153.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
From: Eliezer S. Yudkowsky
<br>
Nick asked:
<br>
<a href="2153.html#2165qlink1">&gt;&gt; How to control an SI? Well, I think it *might* be possible through</a><br>
<i>&gt;&gt; programming the right values into the SIs,</i><br>


<p>
Eliezer replied:
<br>
<a href="2153.html#2165qlink2">&gt;We should program the AI to seek out *correct* answers, not a particular</a><br>
set
<br>
<a href="2153.html#2165qlink3">&gt;of answers.</a><br>
<i>&gt;</i><br>
<i>&gt;&gt; but let's not go into that</i><br>
<i>&gt;&gt; now.</i><br>
<i>&gt;</i><br>
<i>&gt;Let's.  Please.  Now.</i><br>

<p>
Yes, it comes to that, doesn't it? If the builders can't control their SI,
then it has no sane value for them. Yet they rush to build it anyway. Why?
Because their own intellect convinces them of the merit of intelligence per
se.

<p>
Concern that humans will relinquish intellectual supremacy to computers --
artificial super-brains, the SI, with an intellectual power far beyond
anything we can now comprehend, or that through genetic engineering, humans
will grow SI to order or combine both, so that super-grown organic brains
can plug in to a super-computer, or computer super-chips implanted into
human brains.

<p>
Would the SI take humanity on a quantum leap of consciousness, cutting our
deadly connections with the whole ugly history of the past, or would they
too get caught in humanity's idiotic conditionings and memetic attractors?

<p>
The question does not seem serious enough for the man in the street to stop
and consider. First of all, as many on this list know, this singularity
shall occur, arguments to the contrary deserve no attention. Knowing that we
can't avoid it, no need exists to try. The social order already has
intelligence well under control. (Einstein left an estate worth about
$30,000, but a has-been horse opera star like Reagan got the White House.)
The SI, with intelligence a million times that of Einstein, can produce
better science than ever before.

<p>
The real danger for humanity lies in the inability to control ignorance and
stupidity, not in a lack of control of intelligence. Stupid people feel
threatened by super intelligence. Smart and savvy people welcome the
solutions provided by super accurate machine thinking. Now we can relax and
spend our time meditating.

<p>
The SI cannot start any war on religious, political, romantic, or
territorial grounds. Those things appeal to far less intelligent brains.
(btw, with an SI in charge, we won't need a Mussolini to keep the trains on
time. &lt;g&gt;) Who fears the SI? The churches, the politicians, the anti-sex
leagues, the power brokers, and their minions. Big government in particular
has much to lose if automation replaces bloated bureaus. For my own part, I
consider robots nicer people than politicos, theologues, and matriarchs.
They seem like the nicest people you can find, and they never tire, and they
never retire. So now all smart people can take a break and meditate. No more
armies, no more wars -- life consists of carnivals and cabarets.

<p>
The SI reminds me of Hymie Goldberg, who answers a classified advertisement
in a newspaper which says, "Opportunity of a lifetime!" He is given an
address and finds himself face to face with old man Finkelstein.
"What I'm looking for," explains old man Fink, "is somebody to do all my
worrying for me. Your job will be to shoulder all my cares."
<pre>
"That's quite a job," says Hymie. "how much do I get paid?"
"You will get one hundred fifty thousand dollars a year," says old man Fink,
"to make every worry of mine your own."
"Okay," says Hymie, "when do I get paid?"
"Aha!" says Fink. "That's your first worry."

</pre>
<p>
When it creates super-intelligence, then humanity can enjoy super-sanity.

<p>
<a name="2170qlink1">As for morality, just feed all the world's laws into the SI, let it combine
them into an average, and presto! the one-size-fits-all universal human
ethos, free with every Big Mac.
</a>

<p>
-zen
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2166.html">[ Next ]</a><a href="2164.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2153.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
