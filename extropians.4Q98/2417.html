<!-- received="Thu Dec 10 13:00:57 1998 MST" -->
<!-- sent="Thu, 10 Dec 1998 12:00:03 -0800" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@econ.berkeley.edu" -->
<!-- subject="Re: Singularity: AI Morality" -->
<!-- id="3.0.3.32.19981210120003.00ccbe58@econ.berkeley.edu" -->
<!-- inreplyto="366EC891.31073418@pobox.com" -->
<!-- version=1.10, linesinbody=30 -->
<html><head><title>extropy: Re: Singularity: AI Morality</title>
<meta name=author content="Robin Hanson">
<link rel=author rev=made href="mailto:hanson@econ.berkeley.edu" title ="Robin Hanson">
</head><body>
<h1>Re: Singularity: AI Morality</h1>
Robin Hanson (<i>hanson@econ.berkeley.edu</i>)<br>
<i>Thu, 10 Dec 1998 12:00:03 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2417">[ date ]</a><a href="index.html#2417">[ thread ]</a><a href="subject.html#2417">[ subject ]</a><a href="author.html#2417">[ author ]</a>
<!-- next="start" -->
<li><a href="2418.html">[ Next ]</a><a href="2416.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2343.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky writes:
<br>
<a href="2343.html#2417qlink1">&gt;&gt; Well we could do a little more; we might create lots of different AIs</a><br>
<i>&gt;&gt; and observer how they treat each other in contained environments.  We might</i><br>
<i>&gt;&gt; then repeatedly select the ones whose behavior we deem "moral."  And once</i><br>
<i>&gt;&gt; we have creatures whose behavior seems stably "moral" we could release them</i><br>
<i>&gt;&gt; to participate in the big world.</i><br>
<i>&gt;</i><br>
<i>&gt;Anything that can safely be stuffed into a contained environment isn't any</i><br>
<i>&gt;sort of AI that we need to worry about.  Such threat management techniques are</i><br>
<i>&gt;useful only against programs that can be filed and forgotten.  Remember, we're</i><br>
<i>&gt;talking about Culture Minds and Vingean Powers, not your mail filter.  Yours</i><br>
<i>&gt;is a way to ensure the integrity of the global data network, not to protect</i><br>
<i>&gt;the survival of humanity.</i><br>

<p>
Superpowers will evolve from smaller powers, and so when only smaller powers 
are possible, we might try to make them "moral" in the hopes that their 
descendants will inherit some of that.

<p>
<i>&gt;As for pulling this trick on genuine SIs: ... I would rather have three million</i><br>
<a href="2343.html#2417qlink2">&gt;lines of Asimov Laws written in COBOL than run evolutionary simulations!  </a><br>

<p>
Putting in lots of lines of Law code is also only something you can do to
smaller powers, in the hopes that the larger powers they become won't want
to rip those lines out.  

<p>
Robin Hanson  
<pre>
hanson@econ.berkeley.edu     <a href="http://hanson.berkeley.edu/">http://hanson.berkeley.edu/</a>   
RWJF Health Policy Scholar             FAX: 510-643-8614 
140 Warren Hall, UC Berkeley, CA 94720-7360 510-643-1884     
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2418.html">[ Next ]</a><a href="2416.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2343.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
