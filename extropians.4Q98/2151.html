<!-- received="Sun Dec  6 09:33:44 1998 MST" -->
<!-- sent="Sun, 6 Dec 1998 16:32:18 +0000" -->
<!-- name="Nick Bostrom" -->
<!-- email="bostrom@ndirect.co.uk" -->
<!-- subject="Re: Singularity: Individual, Borg, Death?" -->
<!-- id="199812061633.QAA27982@andromeda.hosts.netdirect.net.uk" -->
<!-- inreplyto="366837F4.289EA3C5@pobox.com" -->
<!-- version=1.10, linesinbody=136 -->
<html><head><title>extropy: Re: Singularity: Individual, Borg, Death?</title>
<meta name=author content="Nick Bostrom">
<link rel=author rev=made href="mailto:bostrom@ndirect.co.uk" title ="Nick Bostrom">
</head><body>
<h1>Re: Singularity: Individual, Borg, Death?</h1>
Nick Bostrom (<i>bostrom@ndirect.co.uk</i>)<br>
<i>Sun, 6 Dec 1998 16:32:18 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2151">[ date ]</a><a href="index.html#2151">[ thread ]</a><a href="subject.html#2151">[ subject ]</a><a href="author.html#2151">[ author ]</a>
<!-- next="start" -->
<li><a href="2152.html">[ Next ]</a><a href="2150.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2098.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky wrote:

<p>
<a href="2098.html#2151qlink1">&gt; &gt; &gt; Rational Assumptions:</a><br>
<i>&gt; &gt; &gt; RA1.  I don't know what the objective morality is and neither do you.</i><br>
<i>&gt; &gt; [snip]</i><br>
<i>&gt; &gt; &gt; LA1 and RA1, called "Externalism", are not part of the Singularity logic per</i><br>
<i>&gt; &gt; &gt; se; these are simply the assumptions required to rationally debate</i><br>
<i>&gt; &gt; &gt; morality.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; I don't see why a rational debate about morality would be impossible</i><br>
<i>&gt; &gt; if you or I knew the "objective morality". People often</i><br>
<i>&gt; &gt; rationally debate issues even when one party already knows where the</i><br>
<i>&gt; &gt; truth lies.</i><br>

<p>
<a href="2098.html#2151qlink2">&gt; For two people to argue rationally, it is necessary for each to accept the</a><br>
<i>&gt; possibility of being wrong, or they won't listen.  Maybe this doesn't hold on</i><br>
<i>&gt; the other side of dawn, but it surely holds true for the human race.</i><br>

<p>
<a name="2152qlink1">I have often rationally debated positions that I thought I knew were 
</a>correct. I have even on occasion been involved in rational debates 
where *both* parties agreed where the truth lies, but one party 
decided to play the devil's advocate. The game then consists of 
coming up with good arguments for or against the proposition that 
both recognize as true. So I don't agree with you that if you or I 
knew the "objective morality" then it would be impossible for us to 
rationally debate about morality. Isn't that simply false?


<p>
<a href="2098.html#2151qlink3">&gt; First of all, I don't know that it is wrong to torture innocent people for a</a><br>
<i>&gt; small amount of fun.  It is, with at least 90% probability, not right.  I</i><br>
<i>&gt; can't be at all certain that "2+2 = 4", but I can be almost totally certain</i><br>
<i>&gt; that "2+2 does not uniquely equal 83.47".</i><br>

<p>
<a name="2152qlink2">The way I use the word "know" you don't have to be exactly 100% 
certain about something in order to know it. But in any case, nothing 
in my argument changes if you use a probabilistic statement instead.</a>


<p>
<a href="2098.html#2151qlink4">&gt; &gt; For example, if it is morally preferred</a><br>
<i>&gt; &gt; that the people who are currently alive get the chance to survive</i><br>
<i>&gt; &gt; into the postsingularity world, then we would have to take this</i><br>
<i>&gt; &gt; desideratum into account when deciding when and how hard to push for</i><br>
<i>&gt; &gt; the singularity.</i><br>
<i>&gt; </i><br>
<a name="2153qlink1"><i>&gt; Not at all!  If that is really and truly and objectively the moral thing to</i><br>
<i>&gt; do, then we can rely on the Post-Singularity Entities to be bound by the same</i><br>
<i>&gt; reasoning.  If the reasoning is wrong, the PSEs won't be bound by it.  If the</i><br>
<i>&gt; PSEs aren't bound by morality, we have a REAL problem</i><br>

<p>
Indeed. And this is another point where I seem to disagree with you. 
I am not at all certain that being superintelligent implies being 
moral. Certainly there are very intelligent humans that are also very 
wicked; I don't see why once you pass a certain threshold of 
intelligence then it is no longer possible to be morally bad. What I 
might agree with, is that once you are sufficiently intelligent then 
you should be able to recognize what's good and what's bad. But 
whether you are motivated to act in accordance with these moral 
convictions is a different question. What weight you give to moral 
imperatives in planning your actions depends on how altruistic/</a>moral 
you are. We should therefore make sure that we build in strong moral 
drives into the superintelligences. (Presumably, we would also want 
to link these moral drives to a moral system that places a great 
value on human survival; because that way we would increase our own 
chances of survival.)

<p>
<a name="2153qlink2"><i>&gt;, but I don't see any way</i><br>
<a href="2098.html#2151qlink5">&gt; of finding this out short of trying it.</a><br>

<p>
How to control an SI? Well, I think it *might* be possible through 
programming the right values into the SIs,</a> but let's not go into that 
now.

<p>
<i>&gt;  Or did you mean that we should push</i><br>
<a href="2098.html#2151qlink6">&gt; faster and harder for the Singularity, given that 150,000 people die every day?</a><br>

<p>
That is a consideration, though we have to put it in perspective, 
i.e. consider it in the context of the total number of sentiences 
that have died or may come to live.

<p>
<a href="2098.html#2151qlink7">&gt; &gt; In the hypothetical case where we could dramatically</a><br>
<i>&gt; &gt; increase the chances that we and all other humans would survive, by</i><br>
<i>&gt; &gt; paying the relatively small price of postponing the singularity one</i><br>
<i>&gt; &gt; year, then I feel pretty sure that the morally right thing to do</i><br>
<i>&gt; &gt; would be to wait one year.</i><br>
<i>&gt; </i><br>
<i>&gt; For me, it would depend on how it affected the chance of Singularity.  I don't</i><br>
<i>&gt; morally care when the Singularity happens, as long as it's in the next</i><br>
<i>&gt; thousand years or so.  After all, it's been fifteen billion years already; the</i><br>
<i>&gt; tradeoff between time and probability is all in favor of probability.  From my</i><br>
<i>&gt; understanding of causality, "urgency" is quite likely to be a human</i><br>
<i>&gt; perversion.  So why am I in such a tearing hurry?  Because the longer the</i><br>
<i>&gt; Singularity takes, the more likely that humanity will wipe itself out.</i><br>

<p>
That is also a valid consideration.

<p>
<i>&gt;  I</i><br>
<a href="2098.html#2151qlink8">&gt; won't go so far as to say that I'm in a hurry for your sake, not the</a><br>
<i>&gt; Singularity's - although I would personally prefer that my grandparents make</i><br>
<i>&gt; it - but delay hurts everyone.</i><br>

<p>
Unless a one or two year delay would give us time to fine-tune the 
goal systems of the SIs, so that they would be more likely to be 
moral (and kind to us humans).

<p>
<a href="2098.html#2151qlink9">&gt; &gt; In reality there could well be some kind of tradeoff like that.</a><br>
<i>&gt; </i><br>
<a name="2153qlink3"><i>&gt; There's a far better chance that delay makes things much, much worse.</i><br>

<p>
I think it will all depend on the circumstances at the time. For 
example, what the state of art of nanotechnology is then. But you 
can't say that sooner is *always* better, although it may be a good 
rule of thumb. Clearly there are cases where it's more prudent to 
take more precausions before launch. And in the case of the 
singularity, we'd seem to be well advised to take as many precausions 
as we have time</a> for.


<p>
<i>&gt;</i><br>
<a href="2098.html#2151qlink10">&gt; &gt; It's</a><br>
<i>&gt; &gt; good if superintelligent posthumans are created, but it's also good</i><br>
<i>&gt; &gt; if *we* get to become such beings. And that can in some cases impose</i><br>
<i>&gt; &gt; moral obligations on us to make sure that we ourselves can survive</i><br>
<i>&gt; &gt; the singularity.</i><br>
<i>&gt; </i><br>
<a name="2153qlink4"><i>&gt; Why not leave the moral obligations to the SIs, rather than trying (futilely</i><br>
<i>&gt; and fatally) to impose your moral guesses on them?</i><br>

<p>
Because, as I said above, if we build them in the wrong way they may 
<a name="2153qlink5">not be moral.</a> Plus: whether it's moral or not, we would want to make 
sure that they are kind to us humans and allow us to upload.
</a>

<p>
Nick Bostrom
<br>
<a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a>      n.bostrom@lse.ac.uk
Department of Philosophy, Logic and Scientific Method
London School of Economics
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2152.html">[ Next ]</a><a href="2150.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2098.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
