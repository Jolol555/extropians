<!-- received="Mon Dec 14 16:52:54 1998 MST" -->
<!-- sent="Mon, 14 Dec 1998 18:52:15 -0500" -->
<!-- name="Dan Clemmensen" -->
<!-- email="Dan@Clemmensen.ShireNet.com" -->
<!-- subject="Re: Singularity: AI Morality" -->
<!-- id="3675A4AF.6A7B6F80@clemmensen.shirenet.com" -->
<!-- inreplyto="Singularity: AI Morality" -->
<!-- version=1.10, linesinbody=32 -->
<html><head><title>extropy: Re: Singularity: AI Morality</title>
<meta name=author content="Dan Clemmensen">
<link rel=author rev=made href="mailto:Dan@Clemmensen.ShireNet.com" title ="Dan Clemmensen">
</head><body>
<h1>Re: Singularity: AI Morality</h1>
Dan Clemmensen (<i>Dan@Clemmensen.ShireNet.com</i>)<br>
<i>Mon, 14 Dec 1998 18:52:15 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2855">[ date ]</a><a href="index.html#2855">[ thread ]</a><a href="subject.html#2855">[ subject ]</a><a href="author.html#2855">[ author ]</a>
<!-- next="start" -->
<li><a href="2856.html">[ Next ]</a><a href="2854.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2708.html">Samael</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="2907qlink1">Samael wrote:
<br>
<i>&gt; </i><br>
<a href="2708.html#2855qlink1">&gt; From: Dan Clemmensen &lt;Dan@Clemmensen.ShireNet.com&gt;</a><br>
<i>&gt; &gt;Samael wrote:</i><br>
<i>&gt; &gt;&gt;</i><br>
<i>&gt; &gt;&gt; But why would it _want_ to do anything?</i><br>
<i>&gt; &gt;&gt;</i><br>
<i>&gt; &gt;&gt; What's to stop it reaching the conclusion 'Life is pointless.</a>  There is</i><br>
<i>&gt; no</i><br>
<a name="2907qlink2"><i>&gt; &gt;&gt; meaning anywhere' and just turning itself off?</i><br>
<i>&gt; &gt;&gt;</i><br>
<i>&gt; &gt;Nothing stops any particular AI from deciding to do this. However, this</i><br>
<i>&gt; &gt;doesn't stop the singularity unless it happens to every AI.</i><br>

<p>
<a href="2708.html#2855qlink2">&gt; &gt;The singularity only takes one AI that decides to extend itself</a> rather than</a><br>
<i>&gt; &gt;terminating.</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt;If you are counting on AI self-termination to stop the Singularity, you'll</i><br>
<a name="2907qlink4"><i>&gt; &gt;have to explain why affects every single AI.</i><br>
<i>&gt; </i><br>
<i>&gt; I don't expect it will, because I expect the AI's to be prgorammed with</i><br>
<i>&gt; strong goals that it will not think about.</i><br>

<p>
Same problem. This only works if all AIs are inhibited fron extending their
"strong goals": This si very hard to do using traditional computers.</a> Essentially,
<a name="2907qlink5">you will either permit the AI to program itself, or not. I feel</a> that most AI
<a name="2907qlink6">researchers will be tempted to permit the AI to program itself. Only</a> one such
<a name="2907qlink7">researcher needs to do this to break your containment system. Do</a> you feel that
<a name="2907qlink8">A self-extending AI must intrinsically have strong and un-self-</a>modifiable goals
<a name="2907qlink9">to exist, or do you feel that all AI researchers will correctly</a> implement this
<a name="2907qlink10">feature, or do you have another reason?
</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2856.html">[ Next ]</a><a href="2854.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2708.html">Samael</a>
<!-- nextthread="start" -->
</ul>
</body></html>
