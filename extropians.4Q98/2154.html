<!-- received="Sun Dec  6 12:21:29 1998 MST" -->
<!-- sent="Sun, 6 Dec 1998 20:12:34 +0100" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: Singularity: AI Morality" -->
<!-- id="199812061921.LAA12272@geocities.com" -->
<!-- inreplyto="Singularity: AI Morality" -->
<!-- version=1.10, linesinbody=27 -->
<html><head><title>extropy: Re: Singularity: AI Morality</title>
<meta name=author content="den Otter">
<link rel=author rev=made href="mailto:neosapient@geocities.com" title ="den Otter">
</head><body>
<h1>Re: Singularity: AI Morality</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Sun, 6 Dec 1998 20:12:34 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2154">[ date ]</a><a href="index.html#2154">[ thread ]</a><a href="subject.html#2154">[ subject ]</a><a href="author.html#2154">[ author ]</a>
<!-- next="start" -->
<li><a href="2155.html">[ Next ]</a><b>In reply to:</b> <a href="2153.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->



<hr>
<br>
<a href="1869.html#2154qlink1">&gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</a><br>

<p>
<a href="2153.html#2154qlink2">&gt; Look, these forces are going to a particular place, and they are way, way,</a><br>
<i>&gt; waaaaaayyy too big for any of us to divert.  Think of the Singularity as this</i><br>
<i>&gt; titanic, three-billion-ton truck heading for us.  We can't stop it, but I</i><br>
<i>&gt; suppose we could manage to get run over trying to slow it down.</i><br>

<p>
Or you could try to hop aboard and grab the wheel. Get very rich,
focus your efforts on upoading, and there's a considerable chance
that you *cause* the Singularity and become its God. Difficult, yes,
but certainly not impossible.
 
<p>
<a href="2153.html#2154qlink3">&gt; &gt; Plus: whether it's moral or not, we would want to make</a><br>
<i>&gt; &gt; sure that they are kind to us humans and allow us to upload.</i><br>
<i>&gt; </i><br>
<i>&gt; No, we would NOT want to make sure of that.  It would be immoral.  Every bit</i><br>
<i>&gt; as immoral as torturing little children to death, but with a much higher</i><br>
<i>&gt; certainty of evil.</i><br>

<p>
Obviously AI is *not* the smart way to cause a Singularity. Placing
yourself at the mercy of your creation (or anyone, for that matter) is 
never a good idea. The only value of AI research is that it may help 
us to understand the mechanisms of uploading better. 
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2155.html">[ Next ]</a>
<b>In reply to:</b> <a href="2153.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
