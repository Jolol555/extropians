<!-- received="Mon Dec 21 08:10:43 1998 MST" -->
<!-- sent="Mon, 21 Dec 1998 16:11:45 +0100" -->
<!-- name="DELRIVIERE" -->
<!-- email="delriviere@brutele.be" -->
<!-- subject="Re: Singularity: AI Morality" -->
<!-- id="000d01be2cf4$3a5d7bc0$b2c145c2@p150.brutele.be" -->
<!-- inreplyto="Singularity: AI Morality" -->
<!-- version=1.10, linesinbody=46 -->
<html><head><title>extropy: Re: Singularity: AI Morality</title>
<meta name=author content="DELRIVIERE">
<link rel=author rev=made href="mailto:delriviere@brutele.be" title ="DELRIVIERE">
</head><body>
<h1>Re: Singularity: AI Morality</h1>
DELRIVIERE (<i>delriviere@brutele.be</i>)<br>
<i>Mon, 21 Dec 1998 16:11:45 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3322">[ date ]</a><a href="index.html#3322">[ thread ]</a><a href="subject.html#3322">[ subject ]</a><a href="author.html#3322">[ author ]</a>
<!-- next="start" -->
<li><a href="3323.html">[ Next ]</a><a href="3321.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2826.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky wrote:

<p>
<a href="2826.html#3322qlink1">&gt; I would say that humans have many conflicting goal systems,</a><br>
<i>&gt; overlapping where</i><br>
<i>&gt; they shouldn't and unlinked where they should.  I would demand an</i><br>
<i>&gt; extremely</i><br>
<i>&gt; good reason before giving an AI more than one goal system.  Multiple</i><br>
<i>&gt; intuitions, yes; multiple goals and multiple intentions; but not more</i><br>
<i>&gt; than one</i><br>
<i>&gt; goal system - and absolutely not an silicoendocrine system to</i><br>
<i>&gt; duplicate the</i><br>
<i>&gt; stresses we experience as the result of conflict.  Even if there are</i><br>
<i>&gt; good</i><br>
<i>&gt; evolutionary reasons!  It's just too much of a risk.</i><br>
<i>&gt;</i><br>

<p>
Perhaps only to make the AI able not to be trapped in dead ends,
situations where he is unable to make a next move to progress in his set
of goals and he can't make a come back to choose an other path. To
continue to exist it has to be able to choose or define a new goal
system perhaps conflicting a lot with the previous one. Of course, it's
probably only useful if the continuation of the AI-process for itself is
valuable.

<p>
How to make an AI enjoy the existence for itself, the ability to define
internally his goals if he understand that his goals are only arbitrary
choices in an ocean of possibilities? The reward of pleasure ? The fear
of pain ? what's next if the AI is able to rewire itself to feel an
eternity of orgasm and is able to doom his rewired opponents in an
eternity of pain ?

<p>
What else for such AI system than to be a robot to implement an
arbitrary set of goals (but no value for existence in itself) or an
expanding orgasm generator megastructure?

<p>
How could we give to a machine able to rewire itself the ability to
enjoy life and implement goals, if it is able to understand the
arbitrary nature of a set of goals?


<p>
delriviere
<br>
christophe
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3323.html">[ Next ]</a><a href="3321.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2826.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
