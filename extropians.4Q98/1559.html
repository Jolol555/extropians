<!-- received="Sun Nov 22 16:33:52 1998 MST" -->
<!-- sent="Sun, 22 Nov 1998 23:32:42 +0000" -->
<!-- name="Nick Bostrom" -->
<!-- email="bostrom@ndirect.co.uk" -->
<!-- subject="Re: SI: Singleton and programming" -->
<!-- id="199811222333.XAA08450@andromeda.hosts.netdirect.net.uk" -->
<!-- inreplyto="3655A112.3E7817C1@pobox.com" -->
<!-- version=1.10, linesinbody=35 -->
<html><head><title>extropy: Re: SI: Singleton and programming</title>
<meta name=author content="Nick Bostrom">
<link rel=author rev=made href="mailto:bostrom@ndirect.co.uk" title ="Nick Bostrom">
</head><body>
<h1>Re: SI: Singleton and programming</h1>
Nick Bostrom (<i>bostrom@ndirect.co.uk</i>)<br>
<i>Sun, 22 Nov 1998 23:32:42 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1559">[ date ]</a><a href="index.html#1559">[ thread ]</a><a href="subject.html#1559">[ subject ]</a><a href="author.html#1559">[ author ]</a>
<!-- next="start" -->
<li><a href="1560.html">[ Next ]</a><a href="1558.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1443.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1806.html">Peter C. McCluskey</a>
</ul>
<!-- body="start" -->

<p>
<a name="1806qlink1">I'm still looking for a really good definition of what a singleton 
is (maybe I will finally have time to finish that paper this 
Christman holiday?). However, I can say this about the concept I had 
in mind:


<OL>
  <li>  It does not imply a unity of mind. The singleton could have one 
unitary mind, or it could contain lots of independent minds (e.g. 
human minds).

  <li>  It has more to do with global efficiency. Robin Hanson's paper 
about burning the cosmic commons in a Darwinian race to colonize 
space depicts a scenario that is not compatible with the singleton 
hypothesis since it would be globally wasteful.

  <li>  You may ask, efficient for what? On this the singleton hypothesis 
is silent. One can imagine any of a large number of global goals 
either of which could be adopted by a singleton (e.g. the goal to 
allow humans and posthumans to freely persue their goals without 
</a>
being coerced.) 



</OL>
<p>
And finally, why bother about what happens after the singularity (if 
indeed there will be one)? Eliezer thinks that it can take care of 
itself. Well, I think that for all we know, several different 
post-singularity paths may be possible, and which one will actually 
be real might depend on human choices between today and the final 
moments before the singularity. We therefore want to understand what 
the possibilities are so we can try to bring about the one we like 
best.

<p>
Nick Bostrom
<br>
<a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a>      n.bostrom@lse.ac.uk
Department of Philosophy, Logic and Scientific Method
London School of Economics
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1560.html">[ Next ]</a><a href="1558.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1443.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1806.html">Peter C. McCluskey</a>
</ul>
</body></html>
