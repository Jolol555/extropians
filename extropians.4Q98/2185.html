<!-- received="Mon Dec  7 09:27:12 1998 MST" -->
<!-- sent="Mon, 7 Dec 1998 10:26:53 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Singularity: AI Morality" -->
<!-- id="000901be21fe$684a9280$352501c0@mfg130" -->
<!-- inreplyto="366AC943.B3E1F8C@pobox.com" -->
<!-- version=1.10, linesinbody=49 -->
<html><head><title>extropy: RE: Singularity: AI Morality</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Singularity: AI Morality</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Mon, 7 Dec 1998 10:26:53 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2185">[ date ]</a><a href="index.html#2185">[ thread ]</a><a href="subject.html#2185">[ subject ]</a><a href="author.html#2185">[ author ]</a>
<!-- next="start" -->
<li><a href="2186.html">[ Next ]</a><a href="2184.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2153.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2225.html">Nick Bostrom</a>
</ul>
<!-- body="start" -->

<p>
Eliezer Yudkowsky wrote:
<br>
&gt; We should program the AI to seek out *correct* answers, not a particular<br>
set
<br>
&gt; of answers..<br>

<p>
<a href="2153.html#2185qlink1">&gt; &gt; but let's not go into that</a><br>
<i>&gt; &gt; now..</i><br>

<p>
<a href="2153.html#2185qlink2">&gt; Let's.  Please.  Now..</a><br>

<p>
<a name="2206qlink1">If we are going to write a seed AI, then I agree with you that it is
absolutely critical that its goal system function on a purely rational
basis.  There is no way a group of humans is going to impose durable
artificial</a> constrains on a self-modifying system of that complexity.
<a name="2206qlink2">However, this only begs the question - why build a seed AI?</a>

<p>
<a name="2206qlink3">More specifically, why attempt to create a sentient, self-enhancing entity?
Not only is this an *extremely* dangerous undertaking,</a> but it requires that
<a name="2206qlink4">we solve the Hard Problem of Sentience using merely human mental faculties.</a>

<p>
<a name="2206qlink5">Creating a non-sentient AI with similar capabilities would be both less
complex and less hazardous.  We could use the same approach you outlined in
'Coding a Transhuman AI', with the following changes:


<OL>
  <li>  Don't implement a complete goal system.  Instead, the AI is instantiated
with a single arbitrary top-level goal, and it stops running when that goal
</a>
is completed.

<a name="2205qlink1">  <li>  Don't try to implement full self-awareness.  The various domdules</a> need to
<a name="2206qlink6"><a name="2205qlink2">be able to interface with each other, but we don't need to create one for
'thinking about thought'.</a></a>

<a name="2206qlink7">  <li>  Don't make it self-enhancing.  We want an AI that can write and modify
other programs, but can't re-code itself while it is running.</a>


</OL>
<p>
<a name="2206qlink8">The result of this project would be a very powerful tool, rather than a
sentient being.  It could be used to solve a wide variety of problems,
including writing better AIs, so it would offer most of the same benefits as
a sentient AI.  It would have a flatter enhancement trajectory, but it could
be implemented much sooner.</a>  As a result, we might be able to get human
<a name="2206qlink9">enhancement off the ground fast enough to avoid an 'AI takes over the world'</a>
scenario.

<p>
Billy Brown
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2186.html">[ Next ]</a><a href="2184.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2153.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2225.html">Nick Bostrom</a>
</ul>
</body></html>
