<!-- received="Thu Dec 10 09:36:06 1998 MST" -->
<!-- sent="Thu, 10 Dec 1998 16:25:43 -0000" -->
<!-- name="Samael" -->
<!-- email="Samael@dial.pipex.com" -->
<!-- subject="Re: Singularity: AI Morality" -->
<!-- id="013701be2459$bc284e80$0100a8c0@andy" -->
<!-- inreplyto="Singularity: AI Morality" -->
<!-- version=1.10, linesinbody=51 -->
<html><head><title>extropy: Re: Singularity: AI Morality</title>
<meta name=author content="Samael">
<link rel=author rev=made href="mailto:Samael@dial.pipex.com" title ="Samael">
</head><body>
<h1>Re: Singularity: AI Morality</h1>
Samael (<i>Samael@dial.pipex.com</i>)<br>
<i>Thu, 10 Dec 1998 16:25:43 -0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2388">[ date ]</a><a href="index.html#2388">[ thread ]</a><a href="subject.html#2388">[ subject ]</a><a href="author.html#2388">[ author ]</a>
<!-- next="start" -->
<li><a href="2389.html">[ Next ]</a><a href="2387.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2380.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2408.html">Billy Brown</a>
</ul>
<!-- body="start" -->


<p>
-----Original Message-----
<br>
From: Billy Brown &lt;bbrown@conemsco.com&gt;
<br>
To: extropians@extropy.com &lt;extropians@extropy.com&gt;
Date: 10 December 1998 16:19
<br>
Subject: RE: Singularity: AI Morality


<p>
<a href="2380.html#2388qlink1">&gt;christophe delriviere wrote:</a><br>
<i>&gt;&gt; A lot are assuming that if a "smarter" AI has a particular moral system,</i><br>
<i>&gt;&gt; he will follow it if he believe for a little time it is the moral system</i><br>
<i>&gt;&gt; (say 5 micro seconds :)  )....</i><br>
<i>&gt;&gt;</i><br>
<i>&gt;&gt; I can't see why, We probably all have some moral system, but we surely</i><br>
<i>&gt;&gt; don't always follow it. I'm a strong relativist and I strongly feel that</i><br>
<i>&gt;&gt; there is not true objective moral system, but there is of course one</i><br>
<i>&gt;&gt; somewhat hardwired in my brain, statistically I follow it almost all the</i><br>
<i>&gt;&gt; time, but from time to time I do an act wrong in this moral system and</i><br>
<i>&gt;&gt; because I also think it's totally subjective, i'm feeling bad and don't</i><br>
<i>&gt;&gt; feel bad about it at the same time. The later wins after mostly in a</i><br>
<i>&gt;&gt; little time. I'm sure a greater intelligence will have the ability to be</i><br>
<i>&gt;&gt; strongly multiplex in his world views and will be able to deal with</i><br>
<i>&gt;&gt; strong contradictions and inconsistencies in his beliefs ;)....</i><br>
<i>&gt;</i><br>
<i>&gt;The phenomenon you describe is an artifact of the cognitive architecture of</i><br>
<i>&gt;the human mind.  Humans have more than one system for "what do I do</i><br>
next?" -
<br>
<a href="2380.html#2388qlink2">&gt;you have various instinctive drives, a complex mass of conscious and</a><br>
<i>&gt;unconscious desires, and a conscious moral system.  When you are trying to</i><br>
<i>&gt;decide what to do about something, you will usually get responses from</i><br>
<i>&gt;several of these goal systems.  Sometimes the moral system wins the</i><br>
<i>&gt;argument, and sometimes it doesn't.  I suspect that the conscious moral</i><br>
<i>&gt;system takes longer to come up with an opinion than the other systems,</i><br>
which
<br>
<a href="2380.html#2388qlink3">&gt;would explain why people tend to ignore it in a crisis situation.</a><br>
<i>&gt;</i><br>
<i>&gt;In an AI, there is only one goal system.  When it is trying to decide if an</i><br>
<i>&gt;action is moral, it evaluates it against whatever rules it uses for such</i><br>
<i>&gt;things and comes up with a single answer.  There is no 'struggle to do the</i><br>
<a name="2408qlink3"><i>&gt;right thing', because there are no conflicting motivations.</i><br>


</a>
<p>
Unless it has numerous different factos which contribute towards it's rules.
<a name="2408qlink4">After all, it would probably have the same problems</a> with certain situations
<a name="2408qlink5">that we would.  Would it think that the ends justify the means?  What
variance would it allow for different possibilities?  It would</a> be better at
<a name="2408qlink6">predicting outcomes from its actions, but it stil wouldn't be perfect.

<p>
Samael
</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2389.html">[ Next ]</a><a href="2387.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2380.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2408.html">Billy Brown</a>
</ul>
</body></html>
