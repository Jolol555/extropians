<!-- received="Thu Dec 10 07:33:53 1998 MST" -->
<!-- sent="Thu, 10 Dec 1998 08:33:32 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Singularity: AI Morality" -->
<!-- id="001b01be244a$117c4810$352501c0@mfg130" -->
<!-- inreplyto="366F224D.2CCE6321@bigfoot.com" -->
<!-- version=1.10, linesinbody=35 -->
<html><head><title>extropy: RE: Singularity: AI Morality</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Singularity: AI Morality</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Thu, 10 Dec 1998 08:33:32 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2380">[ date ]</a><a href="index.html#2380">[ thread ]</a><a href="subject.html#2380">[ subject ]</a><a href="author.html#2380">[ author ]</a>
<!-- next="start" -->
<li><a href="2381.html">[ Next ]</a><a href="2379.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2361.html">christophe delriviere</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2420.html">Robin Hanson</a>
</ul>
<!-- body="start" -->

<p>
<a name="2388qlink1">christophe delriviere wrote:
<br>
<a href="2361.html#2380qlink1">&gt; A lot are assuming that if a "smarter" AI has a particular moral system,</a><br>
<i>&gt; he will follow it if he believe for a little time it is the moral system</i><br>
<i>&gt; (say 5 micro seconds :)  )....</i><br>
<i>&gt;</i><br>
<i>&gt; I can't see why, We probably all have some moral system, but we surely</i><br>
<i>&gt; don't always follow it. I'm a strong relativist and I strongly feel that</i><br>
<i>&gt; there is not true objective moral system, but there is of course one</i><br>
<i>&gt; somewhat hardwired in my brain, statistically I follow it almost all the</i><br>
<i>&gt; time, but from time to time I do an act wrong in this moral system and</i><br>
<i>&gt; because I also think it's totally subjective, i'm feeling bad and don't</i><br>
<i>&gt; feel bad about it at the same time. The later wins after mostly in a</i><br>
<i>&gt; little time. I'm sure a greater intelligence will have the ability to be</i><br>
<i>&gt; strongly multiplex in his world views and will be able to deal with</i><br>
<i>&gt; strong contradictions and inconsistencies in his beliefs</a> ;)....</i><br>

<p>
The phenomenon you describe is an artifact of the cognitive architecture of
<a name="2420qlink1">the human mind.  Humans have more than one system for "what do I do next?" -
<a name="2388qlink2">you have various instinctive drives, a complex mass of conscious and
unconscious desires, and a conscious moral system.  When you are trying to
decide what to do about something, you will usually get responses from
several of these goal systems.  Sometimes the moral system wins the
argument, and sometimes it doesn't.  I suspect that the conscious moral
system takes longer to come up with an opinion than the other systems,</a> which
</a>
<a name="2388qlink3">would explain why people tend to ignore it in a crisis situation.

<p>
<a name="2408qlink1">In an AI, there is only one goal system.  When it is trying to decide</a> if an
<a name="2408qlink2">action is moral, it evaluates it against whatever rules it uses for such
things and comes up with a single answer.  There is no 'struggle</a> to do the
right thing', because there are no conflicting motivations.</a>

<p>
Billy Brown
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2381.html">[ Next ]</a><a href="2379.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2361.html">christophe delriviere</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2420.html">Robin Hanson</a>
</ul>
</body></html>
