<!-- received="Sun Nov  8 18:31:40 1998 MST" -->
<!-- sent="Sat, 07 Nov 1998 22:35:40 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Technology evolves, ergo automation evolves, until..." -->
<!-- id="36451F91.17700119@pobox.com" -->
<!-- inreplyto="363E96DE.E04C0806@pobox.com" -->
<!-- version=1.10, linesinbody=43 -->
<html><head><title>extropy: Re: Technology evolves, ergo automation evolves, until...</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Technology evolves, ergo automation evolves, until...</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sat, 07 Nov 1998 22:35:40 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#900">[ date ]</a><a href="index.html#900">[ thread ]</a><a href="subject.html#900">[ subject ]</a><a href="author.html#900">[ author ]</a>
<!-- next="start" -->
<li><a href="0901.html">[ Next ]</a><a href="0899.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0832.html">Eugene Leitl</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eugene Leitl wrote:
<br>
<i>&gt; </i><br>
<a href="0832.html#0900qlink1">&gt; Eliezer S. Yudkowsky writes:</a><br>
<i>&gt; </i><br>
<i>&gt;  &gt; I could build a transhuman intelligence for the same estimated price.  In</i><br>
<i>&gt; </i><br>
<i>&gt; Uh, if you don't mind I would like to hear how you would like to</i><br>
<i>&gt; accomplish this especially at the low end of things (about 1 G$). A</i><br>
<i>&gt; time schedle (MMM taken into account) would be nice as well.</i><br>

<p>
I didn't hear any time schedules from the self-replicator people.  Come to
think of it, I didn't hear any time schedules from the people who said they'd
remodel the basement in two weeks.

<p>
It's a Pure Intuitive figure, based on how many programmers and how much time
I think it would take to do everything listed in "Coding a Transhuman AI". 
But as I once said, "This is the last program the human race ever needs to
write.  It makes sense that it would be the largest and the most complex."

<p>
<a href="0832.html#0900qlink2">&gt; Also, the claim 'I could build' it would seem to become objectionable,</a><br>
<i>&gt; since you'd wind up more a glorified administrator than an</i><br>
<i>&gt; implementer. The superintelligence would then be the product of a</i><br>
<i>&gt; superorganism (team) than a single individual (Eliezer).</i><br>

<p>
Don't be too sure of that.  It's harder for me to design organizations than to
design computer architectures, and it takes a much larger and more secure
power base to implement them - but the level of design necessary just to free
myself up is a hell of a lot less than what it takes to design a seed AI.

<p>
The range of the estimate reflects the degree to which I can change the rules
of the game.  A seed AI is a quick kill.  But if that quick kill isn't
possible, one simply recurses on creating more and more sophisticated forms of
intelligence enhancement, ranging from collaborative filtering to Algernic neurosurgery.
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0901.html">[ Next ]</a><a href="0899.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0832.html">Eugene Leitl</a>
<!-- nextthread="start" -->
</ul>
</body></html>
