<!-- received="Wed Dec  9 14:09:00 1998 MST" -->
<!-- sent="Wed, 9 Dec 1998 15:08:38 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Singularity: AI Morality (AI containment)" -->
<!-- id="001901be23b8$1aba8aa0$352501c0@mfg130" -->
<!-- inreplyto="366ECDCD.73CB5968@posthuman.com" -->
<!-- version=1.10, linesinbody=45 -->
<html><head><title>extropy: RE: Singularity: AI Morality (AI containment)</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Singularity: AI Morality (AI containment)</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Wed, 9 Dec 1998 15:08:38 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2352">[ date ]</a><a href="index.html#2352">[ thread ]</a><a href="subject.html#2352">[ subject ]</a><a href="author.html#2352">[ author ]</a>
<!-- next="start" -->
<li><a href="2353.html">[ Next ]</a><a href="2351.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2341.html">Brian Atkins</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="2354qlink1">Brian Atkins writes:
<br>
<a href="2341.html#2352qlink1">&gt; I'm curious if there has been a previous discussion on this</a><br>
<i>&gt; list regarding the secure containment of an AI (let's say a</i><br>
<i>&gt; SI AI for kicks)? Many people on the list seem to be saying</i><br>
<i>&gt; that no matter what you do, it will manage to break out of</i><br>
<i>&gt; the containment. I think that seems a little far-fetched....</i><br>

<p>
Here's why I don't think containment is feasible for an SI:


<OL>
  <li>  No one has ever achieved it for programs written by humans.  Some
operating systems get close, but I can't think of one that has never had a
serious security issue.

  <li>  Supporting an SI AI would require much faster machines than what we have
now, running much more complex programs.  This makes the problem even worse.

  <li>  An AI will be much better at programming than humans.  That means that
its efforts to get around our security will be much sneakier, and more
complex, that those of human hackers.  See #1.


</OL>
<p>
Even if you make a perfectly secure sandbox, we still aren't safe.</a>  Never
underestimate the security risk posed by social engineering:

<p>
4) You have to have human/AI contact to have any idea what the AIs are like.
This opens up lots of potential problems - the AI can talk someone into
letting it out, bribe them to do it, 'give away' useful (or fantastically
valuable) programs that contain seeds of itself, etc.

<p>
5) Don't forget the legal front.  The AI could try to convince people that
it is a person, and you are keeping it as a slave (not hard to do, since
that's exactly what is happening).  If it acts as its own lawyer, you're
probably going to loose the case.

<p>
6)  Do reporters ever talk to the AI?  Of course they do.  Think of the PR
campaign the 'poor, helpless, exploited' AI could mount.

<p>
Some of these problems are bigger than others, but that isn't the point.
The real problem is that I thought of all these approaches in the space of
15 minutes, and I'm only human.  What is something with an IQ of 1,000 (or
worse, 1,000,000) going to think of?

<p>
Billy Brown
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2353.html">[ Next ]</a><a href="2351.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2341.html">Brian Atkins</a>
<!-- nextthread="start" -->
</ul>
</body></html>
