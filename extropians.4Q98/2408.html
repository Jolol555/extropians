<!-- received="Thu Dec 10 11:59:58 1998 MST" -->
<!-- sent="Thu, 10 Dec 1998 12:59:38 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Singularity: AI Morality" -->
<!-- id="002101be246f$3e211d80$352501c0@mfg130" -->
<!-- inreplyto="013701be2459$bc284e80$0100a8c0@andy" -->
<!-- version=1.10, linesinbody=29 -->
<html><head><title>extropy: RE: Singularity: AI Morality</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Singularity: AI Morality</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Thu, 10 Dec 1998 12:59:38 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2408">[ date ]</a><a href="index.html#2408">[ thread ]</a><a href="subject.html#2408">[ subject ]</a><a href="author.html#2408">[ author ]</a>
<!-- next="start" -->
<li><a href="2409.html">[ Next ]</a><a href="2407.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2388.html">Samael</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="2483qlink1">Samael wrote:
<br>
<a href="2380.html#2408qlink1">&gt; &gt; In an AI, there is only one goal system.  When it is trying</a> to decide if</a><br>
an
<br>
<a name="2483qlink2"><a href="2380.html#2408qlink2">&gt; &gt; action is moral, it evaluates it against whatever rules it uses</a> for such</a><br>
<a name="2483qlink3"><i>&gt; &gt; things and comes up with a single answer.  There is no 'struggle to do</i><br>
the
<br>
<a href="2388.html#2408qlink3">&gt; &gt; right thing', because there are no conflicting motivations..</a><br>
<i>&gt;</i><br>
<i>&gt; Unless it has numerous different factos which contribute towards it's</i><br>
rules..
<br>
<a href="2388.html#2408qlink4">&gt; After all, it would probably have the same problems with certain</a><br>
situations
<br>
<a href="2388.html#2408qlink5">&gt; that we would.  Would it think that the ends justify the means?  What</a><br>
<i>&gt; variance would it allow for different possibilities?  It would be better</i><br>
at
<br>
<a href="2388.html#2408qlink6">&gt; predicting outcomes from its actions, but it stil wouldn't be perfect..</a><br>
<i>&gt;</i><br>
<i>&gt; Samael</i><br>

<p>
The AI won't necessarily have a clear answer to a moral question, any more
than we do.  However, my point is that it won't have more than one answer -
there is no 'my heart says yes but my mind says no' phenomenon.</a>

<p>
Billy Brown
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2409.html">[ Next ]</a><a href="2407.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2388.html">Samael</a>
<!-- nextthread="start" -->
</ul>
</body></html>
