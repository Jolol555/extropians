<!-- received="Wed Dec  9 11:45:20 1998 MST" -->
<!-- sent="Wed, 9 Dec 1998 12:41:05 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Singularity: AI Morality" -->
<!-- id="001701be23a3$7b648820$352501c0@mfg130" -->
<!-- inreplyto="3.0.3.32.19981209093304.00cce7a4@econ.berkeley.edu" -->
<!-- version=1.10, linesinbody=29 -->
<html><head><title>extropy: RE: Singularity: AI Morality</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Singularity: AI Morality</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Wed, 9 Dec 1998 12:41:05 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2340">[ date ]</a><a href="index.html#2340">[ thread ]</a><a href="subject.html#2340">[ subject ]</a><a href="author.html#2340">[ author ]</a>
<!-- next="start" -->
<li><a href="2341.html">[ Next ]</a><a href="2339.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2331.html">Robin Hanson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2369.html">Eugene Leitl</a>
</ul>
<!-- body="start" -->

<p>
Robin Hanson wrote:
<br>
<a href="2321.html#2340qlink1">&gt; &gt; The best that we can do is teach it how do deduce its</a><br>
<i>&gt; &gt; own rules, and hope it comes up with a moral system requires it to be</i><br>
nice
<br>
<a href="2331.html#2340qlink2">&gt; &gt; to fellow sentients..</a><br>
<i>&gt;</i><br>
<i>&gt; Well we could do a little more; we might create lots of different AIs</i><br>
<i>&gt; and observer how they treat each other in contained environments.  We</i><br>
might
<br>
<a href="2331.html#2340qlink3">&gt; then repeatedly select the ones whose behavior we deem "moral."  And once</a><br>
<i>&gt; we have creatures whose behavior seems stably "moral" we could release</i><br>
them
<br>
<a href="2331.html#2340qlink4">&gt; to participate in the big world.</a><br>
<i>&gt;</i><br>
<i>&gt; However, I'd expect evolutionary pressures to act again out in the big</i><br>
world,
<br>
<a href="2331.html#2340qlink5">&gt; and so our only real reason for confidence in continued "moral" behavior</a><br>
<i>&gt; would be expectations that such behavior would be rewarded in a world when</i><br>
<i>&gt; most other creatures also act that way..</i><br>

<p>
If we can do this, the project has failed.  Even a mildly Transhuman AI will
be able to deduce what is going on and fool us about its morality.  A Power
will find some way around any security we can create, either through coding
or social engineering.

<p>
Billy Brown
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2341.html">[ Next ]</a><a href="2339.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2331.html">Robin Hanson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2369.html">Eugene Leitl</a>
</ul>
</body></html>
