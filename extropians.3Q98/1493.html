<!-- received="Fri Aug 14 09:27:11 1998 MDT" -->
<!-- sent="Fri, 14 Aug 1998 08:26:31 -0700" -->
<!-- name="Peter C. McCluskey" -->
<!-- email="pcm@rahul.net" -->
<!-- subject="Re: 5,000,000,000 transhumans?" -->
<!-- id="199808141526.AA09894@foxtrot.rahul.net" -->
<!-- inreplyto="199808091911.MAA17502@smtp02.primenet.com" -->
<!-- version=1.10, linesinbody=66 -->
<html><head><title>extropians: Re: 5,000,000,000 transhumans?</title>
<meta name=author content="Peter C. McCluskey">
<link rel=author rev=made href="mailto:pcm@rahul.net" title ="Peter C. McCluskey">
</head><body>
<h1>Re: 5,000,000,000 transhumans?</h1>
Peter C. McCluskey (<i>pcm@rahul.net</i>)<br>
<i>Fri, 14 Aug 1998 08:26:31 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1493">[ date ]</a><a href="index.html#1493">[ thread ]</a><a href="subject.html#1493">[ subject ]</a><a href="author.html#1493">[ author ]</a>
<!-- next="start" -->
<li><a href="1494.html">[ Next ]</a><a href="1492.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1335.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->


<p>
<a name="1540qlink1"> neosapient@geocities.com (den Otter) writes:
<br>
<a href="1335.html#1493qlink1">&gt;In the case of SI, any head start, no matter how slight, can mean all</a><br>
<i>&gt;the difference in the world. A SI can think, and thus strike, much</i><br>
<i>&gt;faster than less intelligent, disorganized masses. Before others</i><br>
<i>&gt;could transcend themselves or organize resistance, it would be too late.</i><br>

<p>
 I predict that the first SI will think slower than the best humans.
</a>
<a name="1540qlink2">Why would people wait to upload or create an artificial intelligence
until CPU power supports ultrafast minds if they can figure out how
to do it earlier?
</a>

<p>
<a name="1540qlink3"> We have plenty of experience with small differences in speed of thought,
and have seen no sign that thinking faster than others is enough to
make total conquest (as opposed to manipulation) possible.
</a>

<p>
<a href="1335.html#1493qlink2">&gt;That could be the case (roughly 33 %), but it wouldn't be the most</a><br>
<i>&gt;rational approach. Simply put: more (outside) diversity means also</i><br>
<i>&gt;a greater risk of being attacked, with possibly fatal consequences.</i><br>
<i>&gt;If a SI is the only intelligent being in the universe, then it's</i><br>
<i>&gt;presumably safe. In any case *safer* than with known others around.</i><br>


<OL>
<a name="1540qlink4">  <li>  attempting world conquest is very risky, as it unites billions
of minds around the goal of attempting to conquer you. This should
</a>
be especially true in a period of rapid technological change when
many new inventions are popping up but haven't had their military
implications tested. I can't see how the risks of diversity could
be as big as this.
<a name="1540qlink5">  <li>  there's no quick way for the SI to determine that there aren't
other civilizations in the universe which might intend to exterminate
malevolent SIs.
</a>
<a name="1540qlink6">  <li>  given the power you are (mistakenly) assuming the SI has over the
rest of the world, I suspect the SI could also insure it's safety by
hindering the technological advances of others.
</a>


</OL>
<p>
<a href="1335.html#1493qlink3">&gt;&gt; Let me put it this way: I'm pretty sure your view is incorrect, because I</a><br>
<i>&gt;&gt; expect to be one of the first superintelligences, and I intend to uplift</i><br>
<i>&gt;&gt; others.</i><br>
<i>&gt;</i><br>
<a name="1540qlink7"><i>&gt;A bold claim indeed ;) However, becoming a SI will probably change</i><br>
<i>&gt;your motivational system, making any views you hold at the beginning</i><br>
<i>&gt;of transcension rapidly obscolete. Also, being one of the first may</i><br>
<i>&gt;not be good enough. Once a SI is operational, mere hours, minutes and</i><br>
<i>&gt;even seconds could be the difference between success and total failure.</i><br>

<p>
 An unusual enough claim that I will assume it is way off unless
someone constructs a carefull argument in favor of it. 
</a>

<p>
<a name="1540qlink8"><a href="1335.html#1493qlink4">&gt;After all, a "malevolent" SI could (for example) easily sabotage most of</a><br>
<i>&gt;the earth's computer systems, including those of the competition, and</i><br>
<i>&gt;use the confusion to gain a decisive head start.</i><br>
</a>

<p>
 Sabotaging computer systems sounds like a good way of reducing the
malevolent SI's power. It's power over others is likely to come from
<a name="1540qlink10">it's ability to use those systems better. Driving people to reduce
their dependance on computers would probably insure they are more
independant of the SI's area of expertise.
</a>
<a name="1540qlink11"> Also, there will probably be enough secure OSs by then that sabotaging
them wouldn't be as easy as you imply (i.e. the SI would probably need
to knock out the power system).
</a>
<pre>
-- 
------------------------------------------------------------------------
Peter McCluskey          | Critmail (<a href="http://crit.org/critmail.html">http://crit.org/critmail.html</a>):
<a href="http://www.rahul.net/pcm">http://www.rahul.net/pcm</a> | Accept nothing less to archive your mailing list
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1494.html">[ Next ]</a><a href="1492.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1335.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
</body></html>
