<!-- received="Sat Aug 15 21:02:04 1998 MDT" -->
<!-- sent="Sat, 15 Aug 1998 23:01:04 -0400" -->
<!-- name="Doug Bailey" -->
<!-- email="Doug.Bailey@ey.com" -->
<!-- subject="Re: Who's Afraid of the SI?" -->
<!-- id="35cd477a.150798@eyllpwt003.ey.com" -->
<!-- inreplyto="Who's Afraid of the SI?" -->
<!-- version=1.10, linesinbody=117 -->
<html><head><title>extropians: Re: Who's Afraid of the SI?</title>
<meta name=author content="Doug Bailey">
<link rel=author rev=made href="mailto:Doug.Bailey@ey.com" title ="Doug Bailey">
</head><body>
<h1>Re: Who's Afraid of the SI?</h1>
Doug Bailey (<i>Doug.Bailey@ey.com</i>)<br>
<i>Sat, 15 Aug 1998 23:01:04 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1555">[ date ]</a><a href="index.html#1555">[ thread ]</a><a href="subject.html#1555">[ subject ]</a><a href="author.html#1555">[ author ]</a>
<!-- next="start" -->
<li><a href="1556.html">[ Next ]</a><a href="1554.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1548.html">Bryan Moss</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="1561qlink1">Bryan Moss wrote:

<p>
<a href="1548.html#1555qlink1">&gt; In my opinion the reality the AI inhabits is just</a><br>
<i>&gt; as real as the one we inhabit. The argument is</i><br>
<i>&gt; that the AI would not have any concept of what a</i><br>
<i>&gt; human is (in our sense) and would therefore not be</i><br>
<i>&gt; capable of being malicious (or benevolent) towards</i><br>
<i>&gt; us. This does not mean it can't be dangerous, just</i><br>
<i>&gt; that the possibility of the AI causing intentional</i><br>
<i>&gt; harm is highly unlikely (unfortunately you quoted</i><br>
<i>&gt; a sentence in which I did not make this as clear</i><br>
<i>&gt; as I could have).</i><br>

<p>
Bryan later states:

<p>
<a href="1548.html#1555qlink2">&gt; Yes, but the AI does not know there's a financial</a><br>
<i>&gt; system to attack. You're anthropomorphizing,</i><br>
<i>&gt; whereas the AI (to coin a term) would be</i><br>
<i>&gt; AImorphizing. You see a financial system; the AI</i><br>
<i>&gt; sees physical law.</i><br>

<p>
I'm not comfortable with the idea that an AI able to
match human-levels of cognition would not be able to
understand the "concept of what a human is", be able
to fathom how humans view themselves, and be able to
act in a harmful</a> way towards humans. I think you are
exhibiting a bit of anthropic hubris or not giving
AIs a fair shake.

<p>
<a name="1561qlink2">An AI might not have any use for the stock market but
that does not mean it can not discover its existence;
determine its significance (by the concentration of
computer power and security protocols humans have
dedicated to market systems); investigate, discover,
and comprehend the conceptual meaning of the markets;
and on the stage of necessity act in a way as to 
disrupt them.</a>

<p>
<a name="1561qlink3">The thought process an AI goes through to reach its
conclusions may be different (or it may be the same)
but it should be capable of reaching a conclusion that
led to any action a human might conclude to take. Perhaps
an AI's ultimate objective is to maximize its information
processing rate. At some point its efforts to reach that
objective might conflict with the goals of humans to
operate their information infrastructure. The AI might
decide to "commandeer" system resources at the expense
of human information processing demands.</a>

<p>
Doug
<br>
doug.bailey@ey.com

<p>
AI point of view:

<p>
The AI sees a series of objects jumping around in
what appears to be a random way. Being a
scientist, the AI decides to investigate further.
It is not long before the AI has found patterns in
the system and is capable of making short-term
predictions.

<p>
Human point of view:

<p>
This is an AI trained to predict trends in the
stock market. Fortunately it has had a good
success rate of short-term predictions and has
made a significant amount of money.

<p>
The "jumping objects" of the AI's world are
natural phenomenon, much like electron clouds or
planetary orbits. And, as all rational AI's and
humans know, asking "why" jumping objects and
electrons exist is a religious endeavour.

<p>
A social AI is similar, it gets spatial
<br>
information, gestures, heat patterns and it acts
on them according to its programming. Even if it
can reprogram itself it would be a massive stroke
of luck if it decided to turn on us. Remember, an
AI that has evolved around us is likely to have no
concept of resources or power (in the Hitler
sense). And an AI designed to share resources
would have no idea what it was "really" doing. In
my opinion, neither would be likely to cause us
any intentional harm.

<p>
We cannot say what motivations an AI might
develop, but we can attach a high probability that
they won't be like human motivations (since we
know the course of human evolution). An AI that is
like a human (rather than `human-like') would have
to evolve along a similar path as a human.

<p>
<a href="1545.html#1555qlink3">&gt; second, the AI can easily operate in real space</a><br>
<i>&gt; even with today's technology, and can readily</i><br>
<i>&gt; design and implement even better robotic</i><br>
<i>&gt; technology.</i><br>

<p>
The patterns of "real space" are no different to
the patterns of stocks falling and rising.

<p>
<a href="1545.html#1555qlink4">&gt; Why do you feel that the AI is further removed</a><br>
<i>&gt; from "reality" than your own intelligence is?</i><br>

<p>
I feel that my motivations and the motivations of
the AI are more likely to be different than they
are to be similar. And I think that there are more
available courses of action that do not harm
humans, than there are those that do. The view of
a super intelligence that is like a disease,
gorvernment, or corporation (expand and engulf)
is, imho, completely unfounded.

<p>
I could be wrong.

<p>
BM
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1556.html">[ Next ]</a><a href="1554.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1548.html">Bryan Moss</a>
<!-- nextthread="start" -->
</ul>
</body></html>
