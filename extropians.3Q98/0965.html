<!-- received="Wed Jul 22 19:25:02 1998 MDT" -->
<!-- sent="Wed, 22 Jul 1998 18:24:57 -0700 (PDT)" -->
<!-- name="Damien R. Sullivan" -->
<!-- email="phoenix@ugcs.caltech.edu" -->
<!-- subject="Re: The Singularity" -->
<!-- id="199807230124.SAA16932@pride.ugcs.caltech.edu" -->
<!-- inreplyto="13750.10533.43272.38076@liposome.genebee.msu.su" -->
<!-- version=1.10, linesinbody=77 -->
<html><head><title>extropians: Re: The Singularity</title>
<meta name=author content="Damien R. Sullivan">
<link rel=author rev=made href="mailto:phoenix@ugcs.caltech.edu" title ="Damien R. Sullivan">
</head><body>
<h1>Re: The Singularity</h1>
Damien R. Sullivan (<i>phoenix@ugcs.caltech.edu</i>)<br>
<i>Wed, 22 Jul 1998 18:24:57 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#965">[ date ]</a><a href="index.html#965">[ thread ]</a><a href="subject.html#965">[ subject ]</a><a href="author.html#965">[ author ]</a>
<!-- next="start" -->
<li><a href="0966.html">[ Next ]</a><a href="0964.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0952.html">Eugene Leitl</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0953.html">Robin Hanson</a>
</ul>
<!-- body="start" -->

<p>
Robin wrote:
<br>
<a href="0963.html#0965qlink1">&gt; Damien has an essay on the topic at:</a><br>
<i>&gt; <a href="http://www.ugcs.caltech.edu/~phoenix/vinge/antising.html">http://www.ugcs.caltech.edu/~phoenix/vinge/antising.html</a></i><br>

<p>
God's claws, I'd forgotten about that.  And I'm not even ashamed of it, after
2 years.

<p>
On Jul 22, 11:19am, Eugene Leitl wrote:

<p>
<a href="0952.html#0965qlink2">&gt; doesn't it seem to be a bit presumptious the bipedal ape's position on </a><br>
<i>&gt; the smartness scala can't be topped by similiar increases? Our senses</i><br>

<p>
No more so than assuming that no computer language will be more powerful in
capability than the ones we have.  In Hofstadter's terms, assuming that Floop
is the top language, and that there is no Gloop.  Assuming that
Turing-completeness is, well, complete, as far as ways to do computation go.
With quasi-Darwinism for creativity.

<p>
I forget what the proofs in this area actually state.  But to me
incomprehensible SI means

<p>
<a href="0952.html#0965qlink3">&gt; Strange, I really have trouble believing in an SI I can understand to</a><br>
<i>&gt; a meaningful extent. If I could, it wouldn't be an SI, or I would be</i><br>
<i>&gt; its peer. It could be governed by some simple laws (the degenerated</i><br>

<p>
Perhaps we have different definitions of SI.  I think of Della Lu, or a
Culture Mind, or a greedy polis.  Bigger and faster, but understandable, both
in principle and (eventually) in detail.  The detailed understanding might be 
obsolete when it came, like an 8086 trying to decrypt keys generated by a 786
and changed every day, but it would still be understanding.  But the Mind is
still a Super Intelligence.  

<p>
<a href="0952.html#0965qlink4">&gt; magickal (it could or could not), but we wouldn't be able to tell</a><br>
<i>&gt; which way it would turn out to be. The Singularity is a developmental</i><br>
<i>&gt; _prediction horizont_, after all.</i><br>
 
<p>
People in 1798 couldn't predict how 1898 would come out.  1898 couldn't
predict how 1998 came out.  On the other hand, Franklin thought of cryonics.

<p>
The prediction horizon Vinge worries about is not being unable to predict how
things will turn out, but being unable to imagine possibilities at all.  It's
the business of SF writies to be wrong about their projections; an SF writer
is in trouble when they can't project.  Robin and I say there _are_ boundary
conditions we can predict, and lots and lots of possibilities we can imagine,
and if we play enough we may well get close (without knowing it ahead of time)
to whatever actually happens, and whatever does happen will very likely be
explainable to us.

<p>
Because we can play universal Turing machine, which can compute anything.
Unless we run out of memory.

<p>
<a href="0952.html#0965qlink5">&gt; 'Sufficiently advanced technology is indistinguishable from magic'?</a><br>

<p>
Not an axiom in my world.

<p>
<a href="0952.html#0965qlink6">&gt;  &gt; magical SI through AI seems a bit incoherent; you exploit the</a><br>
<i>&gt;  &gt; Church-Turing thesis, then assume the result does something beyond</i><br>
<i>&gt;  &gt; Turing-completeness...</i><br>

<p>
Strong AI, which is tied up in the concept of the Singularity, assumes
Turing's worldview and proofs.  To assume that the self-modifying AI can
become something we are intrinsically incapable of understanding is to assume
that there's something beyond Turing-complete languages and UTMs.  I don't
know if I could prove this to be thoroughly inconsistent, but it seems
slightly incoherent to me.  Or inconsilient, to use E.O. Wilson's word.

<p>
Or: if the computer can emulate me, I can emulate the computer.

<p>
-xx- Damien R. Sullivan X-)

<p>
For God made Ewan Gillies
<br>
God gave him wings to fly
<br>
But only from the land where he belonged.
But I'd fight with God himself
<br>
For the light in Ewan's eye
<br>
Or any man who tells me he was wrong.
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0966.html">[ Next ]</a><a href="0964.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0952.html">Eugene Leitl</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0953.html">Robin Hanson</a>
</ul>
</body></html>
