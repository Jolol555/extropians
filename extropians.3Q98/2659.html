<!-- received="Fri Sep 25 21:08:26 1998 MDT" -->
<!-- sent="Fri, 25 Sep 1998 20:08:21 -0700 (PDT)" -->
<!-- name="Damien R. Sullivan" -->
<!-- email="phoenix@ugcs.caltech.edu" -->
<!-- subject="Re: Professional intuitions" -->
<!-- id="199809260308.UAA26172@sloth.ugcs.caltech.edu" -->
<!-- inreplyto="360C10E0.55BFA388@pobox.com" -->
<!-- version=1.10, linesinbody=74 -->
<html><head><title>extropians: Re: Professional intuitions</title>
<meta name=author content="Damien R. Sullivan">
<link rel=author rev=made href="mailto:phoenix@ugcs.caltech.edu" title ="Damien R. Sullivan">
</head><body>
<h1>Re: Professional intuitions</h1>
Damien R. Sullivan (<i>phoenix@ugcs.caltech.edu</i>)<br>
<i>Fri, 25 Sep 1998 20:08:21 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2659">[ date ]</a><a href="index.html#2659">[ thread ]</a><a href="subject.html#2659">[ subject ]</a><a href="author.html#2659">[ author ]</a>
<!-- next="start" -->
<li><a href="2660.html">[ Next ]</a><a href="2658.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2650.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2662.html">Hara Ra</a>
</ul>
<!-- body="start" -->

<p>
<a name="2694qlink1"><a name="2662qlink1">On Sep 25,  2:51pm, "Eliezer S. Yudkowsky" wrote:

<p>
<a href="2650.html#2659qlink1">&gt; and he doesn't sound like a crank, he didn't say anything I know</a> is false, so</a><br>
<a name="2662qlink2"><i>&gt; I have no choice but to trust his conclusions - to default to Drexler</a> on the</i><br>
<a name="2662qlink3"><i>&gt; matter of nanotechnology.</i><br>
 
<p>
Tangent:
<br>
I haven't read it yet, but most chemists I know laugh.  So I default to a
neutral state: he may have some nice equations; they have actual experience
with reaction chemistry.

</a>
</a>
<p>
<a href="2650.html#2659qlink2">&gt; Oh, I'm sure you understand AI!  Enough to come up original ideas, for that</a><br>
<i>&gt; matter.  Still, you've got more powerful intuitions in social science.  I</i><br>
<i>&gt; daresay that you may even be leveraging your understanding of AI with your</i><br>

<p>
<a name="2674qlink1">Tangent: Robin studied physics from 1977 to 1984, and worked in AI from 1984
to 1993.  *Then* he switched to economics.  This is from his web site.  I
don't know where he feels he has more powerful intuitions, but I wouldn't
assume they were in economics.  Or that your intuitions are better than his,
at least based on experience.</a>

<p>
<a href="2650.html#2659qlink3">&gt; superintelligence and the future.  It doesn't matter whether we both possess</a><br>
<i>&gt; invent-level intelligence in the field, because our specialties are different.</i><br>
 
<p>
Specialty?  What's Robin's specialty?  He's currently focused on econ, but he
spent half your life in AI.

<p>
<a name="2674qlink2"><a href="2650.html#2659qlink4">&gt; The Big Bang was instantaneous.  Supernovas are very bright.  State-vector</a><br>
<i>&gt; reduction is sudden and discontinuous.  A computer crashing loses all the</i><br>
<i>&gt; memory at once.  The thing is, human life can't survive in any of these areas,</i><br>
<i>&gt; nor in a Singularity, so our intuitions don't deal with them.  The Universe is</i><br>

<p>
I'm not sure what analogy you're trying to make here.  The one I'm
constructing out of these instances is "Yes, the universe is full of many
sudden phenomena.  Big, simple, destructive phenomena.  But the destruction of
the Singularity is a side-effect; what it *is* is a sudden increase in
complexity.  A creative discontinuity, not a destructive one."

<p>
And a sudden jump in complexity is implausible.  The closest analogies which
come to mind are crystallization and the probable spread of bacteria through
the early oceans.  I'm not sure the former is valid; as for the latter, we
don't know for certain what happened, and there's a unique qualitative</a> change
there: the rise of self-replicating entities on an otherwise dead planet.

<p>
Hmm.  For judging suddenness of a new level the proper view is probably the
level just below.  No fair judging the Industrial Revolution by dogs or
rocks, or the Singularity by uncontacted New Guinea highlanders.  The rise and
spread of bacteria is unique because the previous level -- rocks -- had no
perception.  Of course, if we take the previous level as liquid reaction time,
it probably took a while.

<p>
And so I'd disqualify the rise and spread of Homo sapiens; geologically a
blip, but for large mammals not that sudden.  Certainly African and Eurasian
animals were able to cope, until we went to the next level or two.

<p>
So I'd say the Singularity idea is saying that the level of literate and
corporate humans will give rise to a level of self-improving AIs in a manner
which will seem discontinuous to that human level.  Robin and I say "Yeah,
right."  I don't think supernovae and crashing computers are good analogies to
get us to admit our intuition might be wrong.

<p>
<a href="2650.html#2659qlink5">&gt; The phenomena I'm talking about would have destroyed life as we know it if</a><br>
<i>&gt; they had appeared in any point in the past.  How, exactly, am I supposed to</i><br>
<i>&gt; find an analogy?  It's like trying to find an analogy for Earth falling into</i><br>

<p>
By finding a phenomenon that destroyed life as it knew it, while increasing
complexity.  (Pedants: I know supernovae create elements for the rest of us,
but the complexity boost happens later, far away, not in the explosion.)

<p>
-xx- Twirlip of Greymist X-)

<p>
Death is for animals; immortality for gods.  Technology is the means by
which we move from one state to the other.
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2660.html">[ Next ]</a><a href="2658.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2650.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2662.html">Hara Ra</a>
</ul>
</body></html>
