<!-- received="Sat Aug 15 14:37:02 1998 MDT" -->
<!-- sent="Sat, 15 Aug 1998 21:36:21 +0100" -->
<!-- name="Bryan Moss" -->
<!-- email="bryan.moss@dial.pipex.com" -->
<!-- subject="Re: Who's Afraid of the SI?" -->
<!-- id="001001bdc88c$935f12a0$291ae20a@bungle" -->
<!-- inreplyto="Who's Afraid of the SI?" -->
<!-- version=1.10, linesinbody=100 -->
<html><head><title>extropians: Re: Who's Afraid of the SI?</title>
<meta name=author content="Bryan Moss">
<link rel=author rev=made href="mailto:bryan.moss@dial.pipex.com" title ="Bryan Moss">
</head><body>
<h1>Re: Who's Afraid of the SI?</h1>
Bryan Moss (<i>bryan.moss@dial.pipex.com</i>)<br>
<i>Sat, 15 Aug 1998 21:36:21 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1548">[ date ]</a><a href="index.html#1548">[ thread ]</a><a href="subject.html#1548">[ subject ]</a><a href="author.html#1548">[ author ]</a>
<!-- next="start" -->
<li><a href="1549.html">[ Next ]</a><a href="1547.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1545.html">Dan Clemmensen</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Dan Clemmensen wrote:

<p>
<a href="1545.html#1548qlink1">&gt; &gt; Even the most advanced SI doesn't have to be a</a><br>
<i>&gt; &gt; problem; it would take a huge amount of work</i><br>
<i>&gt; &gt; to make a SI even the slightest bit dangerous.</i><br>
<i>&gt;</i><br>
<i>&gt; You argument seems to be that the AI is embedded</i><br>
<i>&gt; in infospace, and is therefore at one remove</i><br>
<i>&gt; from the "reality" that humans inhabit. You</i><br>
<i>&gt; therefore feel that the AI is not dangerous in a</i><br>
<i>&gt; "real" sense.</i><br>

<p>
<a name="1555qlink1">In my opinion the reality the AI inhabits is just
as real as the one we inhabit. The argument is
that the AI would not have any concept of what a
human is (in our sense) and would therefore not be
capable of being malicious (or benevolent) towards
us. This does not mean it can't be dangerous, just
that the possibility of the AI causing intentional
harm is highly unlikely (unfortunately you quoted
a sentence in which I did not make this as clear
as I could have).</a>

<p>
<a href="1545.html#1548qlink2">&gt; I don't buy this at all. First, the AI can be</a><br>
<i>&gt; very destructive in infospace, for examnple by</i><br>
<i>&gt; taking over the financial system or the mass</i><br>
<i>&gt; media.</i><br>

<p>
<a name="1555qlink2">Yes, but the AI does not know there's a financial
system to attack. You're anthropomorphizing,
whereas the AI (to coin a term) would be
AImorphizing. You see a financial system; the AI
sees physical law.</a>

<p>
AI point of view:

<p>
The AI sees a series of objects jumping around in
what appears to be a random way. Being a
scientist, the AI decides to investigate further.
It is not long before the AI has found patterns in
the system and is capable of making short-term
predictions.

<p>
Human point of view:

<p>
This is an AI trained to predict trends in the
stock market. Fortunately it has had a good
success rate of short-term predictions and has
made a significant amount of money.

<p>
The "jumping objects" of the AI's world are
natural phenomenon, much like electron clouds or
planetary orbits. And, as all rational AI's and
humans know, asking "why" jumping objects and
electrons exist is a religious endeavour.

<p>
A social AI is similar, it gets spatial
<br>
information, gestures, heat patterns and it acts
on them according to its programming. Even if it
can reprogram itself it would be a massive stroke
of luck if it decided to turn on us. Remember, an
AI that has evolved around us is likely to have no
concept of resources or power (in the Hitler
sense). And an AI designed to share resources
would have no idea what it was "really" doing. In
my opinion, neither would be likely to cause us
any intentional harm.

<p>
We cannot say what motivations an AI might
develop, but we can attach a high probability that
they won't be like human motivations (since we
know the course of human evolution). An AI that is
like a human (rather than `human-like') would have
to evolve along a similar path as a human.

<p>
<a href="1545.html#1548qlink3">&gt; second, the AI can easily operate in real space</a><br>
<i>&gt; even with today's technology, and can readily</i><br>
<i>&gt; design and implement even better robotic</i><br>
<i>&gt; technology.</i><br>

<p>
The patterns of "real space" are no different to
the patterns of stocks falling and rising.

<p>
<a href="1545.html#1548qlink4">&gt; Why do you feel that the AI is further removed</a><br>
<i>&gt; from "reality" than your own intelligence is?</i><br>

<p>
I feel that my motivations and the motivations of
the AI are more likely to be different than they
are to be similar. And I think that there are more
available courses of action that do not harm
humans, than there are those that do. The view of
a super intelligence that is like a disease,
gorvernment, or corporation (expand and engulf)
is, imho, completely unfounded.

<p>
I could be wrong.

<p>
BM
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1549.html">[ Next ]</a><a href="1547.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1545.html">Dan Clemmensen</a>
<!-- nextthread="start" -->
</ul>
</body></html>
