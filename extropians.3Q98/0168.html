<!-- received="Sat Jul  4 02:44:30 1998 MDT" -->
<!-- sent="Sat, 04 Jul 1998 01:00:44 -0700" -->
<!-- name="Hara Ra" -->
<!-- email="harara@shamanics.com" -->
<!-- subject="Re: The AI revolution" -->
<!-- id="Version.32.19980704005632.00e803e0@mail.scruznet.com" -->
<!-- inreplyto="b497m1ww93q.fsf@void.nada.kth.se" -->
<!-- version=1.10, linesinbody=41 -->
<html><head><title>extropians: Re: The AI revolution</title>
<meta name=author content="Hara Ra">
<link rel=author rev=made href="mailto:harara@shamanics.com" title ="Hara Ra">
</head><body>
<h1>Re: The AI revolution</h1>
Hara Ra (<i>harara@shamanics.com</i>)<br>
<i>Sat, 04 Jul 1998 01:00:44 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#168">[ date ]</a><a href="index.html#168">[ thread ]</a><a href="subject.html#168">[ subject ]</a><a href="author.html#168">[ author ]</a>
<!-- next="start" -->
<li><a href="0169.html">[ Next ]</a><a href="0167.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0069.html">Anders Sandberg</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0163.html">Hara Ra</a>
</ul>
<!-- body="start" -->

<p>
And so we have a logical argument for "intuition". Obviously the robot
will use what knowlege it has to decide. And this information may be
limited and faulty. 

<p>
<a href="0069.html#0168qlink1">&gt;But what is best? You have to supply the robot with valuations in</a><br>
<i>&gt;order to have this kind of reasoning. Asimov's laws have the advantage</i><br>
<i>&gt;of being clear what a robot may and may not do, and do not require</i><br>
<i>&gt;open ended reasoning ("... but if I save him, what if he is a killer?</i><br>
<i>&gt;But if he is a killer..."). </i><br>
<i>&gt;</i><br>
The whole point of Asimov's Laws is that they are ultimately unclear.

<p>
<a href="0069.html#0168qlink2">&gt;&gt;  I think a</a><br>
<i>&gt;&gt; robot could logically calculate that a person living is better than a</i><br>
<i>&gt;&gt; person dieing and by induction that 100 people living and only one</i><br>
<i>&gt;&gt; dieing is better than one person living and 100 dieing.</i><br>
<i>&gt;</i><br>
<i>&gt;This kind of reasoning was most likely too unconstrained for Asimov or</i><br>
<i>&gt;his contemporaries - or anybody building a robot today. Imagine the</i><br>
<i>&gt;litigation if your robot does something that leads to the death of</i><br>
<i>&gt;somebody, and it is not possible to show that this was a clear logical</i><br>
<i>&gt;results of the laws of robotics. People would feel much more at home</i><br>
<i>&gt;with a robot that simply couldn't harm them due to the first law, than</i><br>
<i>&gt;a robot that just *might* harm them because it had deduced that it was</i><br>
<i>&gt;for the best due to some obscure twist of logic.</i><br>
<i>&gt;</i><br>
I belive it took 10,000 years to invent the Zero'th Law. And the Robots did
it, not the humans. How logical. Also, chaos theory hadn't been invented yet.


<pre>
O--------------------------------O 
| Hara Ra &lt;harara@shamanics.com&gt; | 
| Box 8334 Santa Cruz, CA 95061  |
|                                |
| Death is for animals;          |
| immortality for gods.          |
| Technology is the means by     |
</pre>
<br>
| which we make the transition.  |
<pre>
O--------------------------------O
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0169.html">[ Next ]</a><a href="0167.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0069.html">Anders Sandberg</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0163.html">Hara Ra</a>
</ul>
</body></html>
