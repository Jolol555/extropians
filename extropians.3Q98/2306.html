<!-- received="Fri Sep 11 14:17:34 1998 MDT" -->
<!-- sent="Fri, 11 Sep 1998 15:24:00 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Singularity: Human AI to superhuman" -->
<!-- id="35F986C1.6B1227C4@pobox.com" -->
<!-- inreplyto="Singularity: Human AI to superhuman" -->
<!-- version=1.10, linesinbody=131 -->
<html><head><title>extropians: Re: Singularity: Human AI to superhuman</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Singularity: Human AI to superhuman</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Fri, 11 Sep 1998 15:24:00 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2306">[ date ]</a><a href="index.html#2306">[ thread ]</a><a href="subject.html#2306">[ subject ]</a><a href="author.html#2306">[ author ]</a>
<!-- next="start" -->
<li><a href="2307.html">[ Next ]</a><a href="2305.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2298.html">Emmanuel Charpentier</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="2354qlink1">Emmanuel Charpentier wrote:
<br>
<i>&gt; </i><br>
<a href="2298.html#2306qlink1">&gt; ---"Eliezer S. Yudkowsky" &lt;sentience@pobox.com&gt; wrote:</a><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; I disagree with the basic concept that human brains are</i><br>
<i>&gt; &gt; based on neural nets.</i><br>
<i>&gt; </i><br>
<i>&gt;  :DDD You're joking, aren't you? How many neurones are there in a</i><br>
<i>&gt; brain again? What else do you propose for memory, processes, learning,</i><br>
<i>&gt; pain/pleasure taking place in you and me.</i><br>

</a>
<p>
Perhaps it would be better to say that "association" is not the foundation of
<a name="2354qlink2">thought.  The source of "pattern", in memory, learning, pain</a> and pleasure,
<a name="2354qlink3">does not derive from the associational nature of neural networks,</a> but programs
<a name="2354qlink4">which use neural networks for processing power.  A brain</a> is not necessarily
<a name="2354qlink5">built on neurons any more than a spreadsheet is built</a> on silicon atoms.  The
<a name="2354qlink6">properties of small human-built neural networks, such</a> as the association of
<a name="2354qlink7">features, will not necessarily show up as high-level properties</a> of the human brain.

<p>
<a href="2298.html#2306qlink2">&gt; &gt;  Human</a><br>
<a name="2354qlink8"><i>&gt; &gt; brains use more powerful principles.</i><br>
<i>&gt; </i><br>
<i>&gt;   You need to give me some hinsight here. I don't see what you mean.</i><br>

<p>
For example, I think that the cerebellum performs some type</a> of constraint
<a name="2354qlink9">propagation, or rather constraint assembly, and that symbolic</a> memory is based
<a name="2354qlink10">on abstracting a set of high-level constraints which</a> the cerebellum assembles.
<a name="2354qlink11"> While the constraint propagation is almost certainly optimized</a> on the neural
<a name="2354qlink12">level, there are no "neural networks" I am aware</a> of that perform constraint
<a name="2354qlink13">propagation, since that activity is fundamentally distinct</a> from "association"
as we know it.

<p>
<a href="2298.html#2306qlink3">&gt;    Of course, I shall agree with you on the fact that current</a><br>
<i>&gt; artificial neural network are not really human like, they are still</i><br>
<i>&gt; yet pattern catchers. But add many more layers, intra layers synapses</i><br>
<i>&gt; (and surely many other things), and you can code just about anything.</i><br>

<p>
Likewise, my PowerPC 604 can simulate a neural network.  But a program running
on that neural network would still be based on parallel computing and
association, not a Von Neumann architecture and arithmetic.  Human neurons are
not necessarily as simple as they're made out to be; it has been proposed that
each neuron is actually the equivalent of a personal computer.  Penrose has
proposed that each microtubule dimer is a unit of complex computation and that
it uses quantum computation to boot.  Association, a very simple feature which
appeared in the first Perceptron neural net, is not necessarily the foundation
of the brain.

<p>
<a href="2298.html#2306qlink4">&gt; &gt; My AI is based on neither neural nets, nor rule-based systems.</a><br>
<i>&gt; Symbols _may_</i><br>
<i>&gt; &gt; be fundamentally based on association, but they are still only a</i><br>
<i>&gt; part of an AI architecture.</i><br>
<i>&gt; </i><br>
<i>&gt;    From what I've read, you mostly think that you need to code all</i><br>
<i>&gt; abilities and 'somehow' have them work together through some world</i><br>
<i>&gt; model, central to have all the module communicate between each other.</i><br>
<i>&gt; But according to me, what you do is simply code a body of features for</i><br>
<i>&gt; an AI. You don't give it any ability concerning memory, learning,</i><br>
<i>&gt; imagination, your basic human thingies. Or do you think you simply</i><br>
<i>&gt; have to add modules whose work will be to 'memorise', 'imagine',</i><br>
<i>&gt; 'streamline memory' whatever else is not taken care of by the</i><br>
<i>&gt; domdules? (domaine module)</i><br>

<p>
I do think that memory, abstract thought, reflexive reasoning, and other
aspects of consciousness will have to be programmed in deliberately.  They
will not emerge spontaneously.  Some (very basic) aspects may be implicit in
every domdule, some forms may be explicit and separate domdules, but they will
not appear unless we summon them.

<p>
I do not say that coding an AI is a matter of throwing a group of abilities
into a pot.  Even Lenat, creator of the encyclopedic Cyc, is binding all the
tiny facts together with a highly elegant language and many abstract reasoning
modules.  The challenge in creating a seed AI is 80% core architecture and s0%
throwing on more intuitions.  The very high creative challenge is purely core
architecture, especially the creation of symbols, reflexivity, and world-model
synchronization to bind together multiple domdules.

<p>
<a href="2298.html#2306qlink5">&gt;     I don't think you can ever link together an autocad module and an</a><br>
<i>&gt; OCR module and say "tadam, here I got the base for an AI". What you do</i><br>
<i>&gt; is put together functionnalities, not integrate them (no matter the</i><br>
<i>&gt; amount of code).</i><br>

<p>
Not without the core architecture, no.  An OCR plus CAD yields OCR+CAD unless
there's a synergy, symbols that apply to both domdules, visualizations with
components in both models.

<p>
<a name="2354qlink14"><a href="2298.html#2306qlink6">&gt;      Markov nets would probably do a better job at it, at least it</a><br>
<i>&gt; allows to 'associate' things together!</i><br>

<p>
I doubt very strongly indeed that the memory/symbolic</a> domdule (equivalent of
<a name="2354qlink15">our hippocampus) could be implemented by a simple Markov net.</a>

<p>
<a name="2354qlink16"><a href="2298.html#2306qlink7">&gt;     Come on, human body and brain do it all the time. That's what</a><br>
<i>&gt; happen when you become an expert on a task: you don't need to think</i><br>
<i>&gt; about it! It's wired!!! And you didn't answer about perfection: you</i><br>
<i>&gt; can't design perfection into an AI, and have that AI work its way</i><br>
<i>&gt; around in an unperfect (from our models point of view) universe!!!</i><br>

<p>
You're confusing high-level "perfection" with low-</a>level "perfection".  When
<a name="2354qlink17">was the last time your neurons got confused over a matter</a> of philosophy?  When
<a name="2354qlink18">did your neurons get bored with firing?  You run on an AI Advantage,</a> but you
<a name="2354qlink19">can't use it consciously - can't tell your neurons to multiply two
twenty-digit numbers for you, even though they could do</a> it in a second; you
<a name="2354qlink20">have to use your entire brain and probably a paper-and-pencil</a> to perform this
<a name="2354qlink21">simple procedure, and even then you'll drop a digit or two.</a>  AIs will still
<a name="2354qlink22">use error-prone high-level conscious thought to solve</a> uncertain problems;
<a name="2354qlink23">they'll simply have the capability of pouring in massive amounts</a> of low-level
<a name="2354qlink24">procedural thought when necessary.</a>

<p>
<i>&gt; ...</i><br>
<i>&gt; </i><br>
<a href="2298.html#2306qlink8">&gt; &gt; No offense, but these arguments always sound like "But fish are the</a><br>
<i>&gt; result of</i><br>
<i>&gt; &gt; billions of years of evolution!  How could they be outmatched by a</i><br>
<i>&gt; mere</i><br>
<i>&gt; &gt; nuclear submarine?"</i><br>
<i>&gt; </i><br>
<i>&gt; I still think evolution has come up with solutions that are pretty</i><br>
<i>&gt; effective, and we will have to somehow copy them before improving</i><br>
<i>&gt; and/or completely changing design.</i><br>

<p>
I did.  Pattern-catchers are copied from the brain and evolution, reflexive
traces and symbols and goals and causality and pretty much everything else are
copied from the mind.  (Although adaptive code is pretty much a programmer's
invention, and the goal system was completely reworked, and so on.)
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2307.html">[ Next ]</a><a href="2305.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2298.html">Emmanuel Charpentier</a>
<!-- nextthread="start" -->
</ul>
</body></html>
