<!-- received="Fri Sep 11 20:28:04 1998 MDT" -->
<!-- sent="Sat, 12 Sep 1998 14:26:15 +1200" -->
<!-- name="J. Maxwell Legg" -->
<!-- email="income@ihug.co.nz" -->
<!-- subject="Re: Singularity: Human AI to superhuman" -->
<!-- id="35F9DBC7.F5398962@ihug.co.nz" -->
<!-- inreplyto="Singularity: Human AI to superhuman" -->
<!-- version=1.10, linesinbody=17 -->
<html><head><title>extropians: Re: Singularity: Human AI to superhuman</title>
<meta name=author content="J. Maxwell Legg">
<link rel=author rev=made href="mailto:income@ihug.co.nz" title ="J. Maxwell Legg">
</head><body>
<h1>Re: Singularity: Human AI to superhuman</h1>
J. Maxwell Legg (<i>income@ihug.co.nz</i>)<br>
<i>Sat, 12 Sep 1998 14:26:15 +1200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2314">[ date ]</a><a href="index.html#2314">[ thread ]</a><a href="subject.html#2314">[ subject ]</a><a href="author.html#2314">[ author ]</a>
<!-- next="start" -->
<li><a href="2315.html">[ Next ]</a><a href="2313.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2312.html">Hal Finney</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->





<p>
Hal Finney wrote:

<p>
<a href="2312.html#2314qlink1">&gt; One thing I'm wondering about is whether the goal of superhuman</a><br>
<i>&gt; intelligence is well defined.</i><br>
<i>&gt;</i><br>
<i>&gt; Eliezer lays out a scenario in which a machine designs a better machine,</i><br>
<i>&gt; and that one designs a still better one, and so on.  But better at what?</i><br>

<p>
There seems to be a shortage of good economists capable of designing a system
to break the back of those luciferian old boy networks who hide behind the
price system as a cover for their nefarious deeds.
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2315.html">[ Next ]</a><a href="2313.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2312.html">Hal Finney</a>
<!-- nextthread="start" -->
</ul>
</body></html>
