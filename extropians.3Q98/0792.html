<!-- received="Fri Jul 17 09:10:23 1998 MDT" -->
<!-- sent="Fri, 17 Jul 1998 19:13:13 +0400 (MSD)" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene@liposome.genebee.msu.su" -->
<!-- subject="Re: The Singularity" -->
<!-- id="13743.25683.363134.720800@liposome.genebee.msu.su" -->
<!-- inreplyto="Version.32.19980712023935.00e34780@mail.scruznet.com" -->
<!-- version=1.10, linesinbody=28 -->
<html><head><title>extropians: Re: The Singularity</title>
<meta name=author content="Eugene Leitl">
<link rel=author rev=made href="mailto:eugene@liposome.genebee.msu.su" title ="Eugene Leitl">
</head><body>
<h1>Re: The Singularity</h1>
Eugene Leitl (<i>eugene@liposome.genebee.msu.su</i>)<br>
<i>Fri, 17 Jul 1998 19:13:13 +0400 (MSD)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#792">[ date ]</a><a href="index.html#792">[ thread ]</a><a href="subject.html#792">[ subject ]</a><a href="author.html#792">[ author ]</a>
<!-- next="start" -->
<li><a href="0793.html">[ Next ]</a><a href="0791.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0778.html">Robin Hanson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0798.html">Robin Hanson</a>
</ul>
<!-- body="start" -->

<p>
Robin Hanson writes:
<br>
[...] 
<br>
<a name="0798qlink1"><a href="0778.html#0792qlink1"> &gt; You probably can't think of a reasonable way to calculate the temperature</a><br>
<i> &gt; of a black hole either, but that doesn't mean other people can't do it.</i><br>
<i> &gt; Do you mean to claim more than that *you* *now* haven't thought of</i><br>
<i> &gt; something you like?</i><br>
 
<p>
Good point, but afaik there is no consistent theory of human actions,
either, and we're a good deal more predictable than an SI. Ergodic
systems are intrinsically unpredictable, and you can't prove the SI is 
not occasionally ergodic.
</a>
 
<p>
<a href="0778.html#0792qlink2"> &gt; &gt;Since the SI will be vastly more intelligent than humans, IMO we may not</a><br>
<i> &gt; &gt;be able to comprehend its motivations, much less predict them. The SI will</i><br>
<i> &gt; &gt;be so smart that its actions are constrained only by the laws of physics,</i><br>
<i> &gt; &gt;and it will choose a course of action based on its motivations.</i><br>
<i> &gt; </i><br>
<i> &gt; Why do you assume such a strong association between intelligence and</i><br>
<i> &gt; motivations?  It seems to me that intelligence doesn't change one's</i><br>
<i> &gt; primary purposes much at all, though it may change one's tactics as one</i><br>
<i> &gt; better learns the connection between actions and consequences.</i><br>

<p>
<a name="0798qlink2">I could reason evolutionary, but I have no idea whether this still applies
to the SI. Areas of human enterprise is certainly unapplicable.
</a>

<p>
'gene
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0793.html">[ Next ]</a><a href="0791.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0778.html">Robin Hanson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0798.html">Robin Hanson</a>
</ul>
</body></html>
