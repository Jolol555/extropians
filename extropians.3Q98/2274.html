<!-- received="Thu Sep 10 11:58:24 1998 MDT" -->
<!-- sent="Thu, 10 Sep 1998 18:59:16 +0100" -->
<!-- name="Bryan Moss" -->
<!-- email="bryan.moss@dial.pipex.com" -->
<!-- subject="Re: Singularity: The nanotech-uploading argument" -->
<!-- id="001401bddce4$e05eb800$291ae20a@bungle" -->
<!-- inreplyto="Singularity: The nanotech-uploading argument" -->
<!-- version=1.10, linesinbody=60 -->
<html><head><title>extropians: Re: Singularity: The nanotech-uploading argument</title>
<meta name=author content="Bryan Moss">
<link rel=author rev=made href="mailto:bryan.moss@dial.pipex.com" title ="Bryan Moss">
</head><body>
<h1>Re: Singularity: The nanotech-uploading argument</h1>
Bryan Moss (<i>bryan.moss@dial.pipex.com</i>)<br>
<i>Thu, 10 Sep 1998 18:59:16 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2274">[ date ]</a><a href="index.html#2274">[ thread ]</a><a href="subject.html#2274">[ subject ]</a><a href="author.html#2274">[ author ]</a>
<!-- next="start" -->
<li><a href="2275.html">[ Next ]</a><a href="2273.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2239.html">Nick Bostrom</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Nick Bostrom wrote:

<p>
<a href="2239.html#2274qlink1">&gt; [...] Drexler has sketched out a simple</a><br>
<i>&gt; mechanical nanocomputer the size of a sugar cube</i><br>
<i>&gt; with a processing power one million times that</i><br>
<i>&gt; of a human brain.</i><br>
<i>&gt;</i><br>
<i>&gt; [...] Hence we can run the best human brains at</i><br>
<i>&gt; a speed where after one day they will have</i><br>
<i>&gt; experienced more than two thousand subjective</i><br>
<i>&gt; years each.</i><br>

<p>
This ignores the (apparently) important role of
chaos in the brain. Computers, as we all know,
have a difficult time simulating chaotic systems
so uploading to a conventional computer would be
foolish. (And the faster you run the simulation,
the more foolish you look.)

<p>
Obviously this could be a problem with regular
Artificial Intelligence as well. It could be that
to have a complex adaptive intelligence you need
an analog architecture.

<p>
<a href="2239.html#2274qlink2">&gt; During this time they will almost certainly have</a><br>
<i>&gt; managed to design more effective architectures</i><br>
<i>&gt; such as electronic nanocomputers and quantum</i><br>
<i>&gt; computers.</i><br>

<p>
An interesting question would be how many uploads
you would need. I would hazard a guess that a lone
human, given all the time in the world, couldn't
recreate the achievements of a human society.
Perhaps how we communicate and what we create is
just as important as how we think. This can just
as easily be applied to artificial intelligence.

<p>
<i>&gt; Since the uploads can control a molecular lab at</i><br>
<a href="2239.html#2274qlink3">&gt; molecular speeds, they are not slowed down by</a><br>
<i>&gt; having to rely on humans to carry out trial-and-</i><br>
<i>&gt; error testing.</i><br>

<p>
Of course, if an upload (or AI) did require a
radically different architecture it might not
conform to the same scaling laws as conventional
computing. Timing may well be all-important, and
the upload (or AI) might have to stay at the same
speed as the rest of us.

<p>
<a href="2239.html#2274qlink4">&gt; It is perfectly possible that the singularity</a><br>
<i>&gt; happens before uploading takes place, but this</i><br>
<i>&gt; argument shows it will happen no later.</i><br>

<p>
It is perfectly possible that the singularity is a
method of not having to go into deep historical
context when writing science fiction novels.

<p>
BM
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2275.html">[ Next ]</a><a href="2273.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2239.html">Nick Bostrom</a>
<!-- nextthread="start" -->
</ul>
</body></html>
