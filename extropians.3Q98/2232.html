<!-- received="Tue Sep  8 12:42:27 1998 MDT" -->
<!-- sent="Tue, 08 Sep 1998 13:48:27 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Singularity: Are posthumans understandable?" -->
<!-- id="35F57BF5.1A334C03@pobox.com" -->
<!-- inreplyto="Singularity: Are posthumans understandable?" -->
<!-- version=1.10, linesinbody=43 -->
<html><head><title>extropians: Re: Singularity: Are posthumans understandable?</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Singularity: Are posthumans understandable?</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Tue, 08 Sep 1998 13:48:27 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2232">[ date ]</a><a href="index.html#2232">[ thread ]</a><a href="subject.html#2232">[ subject ]</a><a href="author.html#2232">[ author ]</a>
<!-- next="start" -->
<li><a href="2233.html">[ Next ]</a><a href="2231.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2205.html">Hal Finney</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Hal Finney wrote:
<br>
<i>&gt; </i><br>
<a href="2205.html#2232qlink1">&gt; If we look at the analogy in this way, it suggests that we may expect</a><br>
<i>&gt; to be able to understand some aspects of posthuman behavior, without</i><br>
<i>&gt; coming anywhere close to truly understanding and appreciating the full</i><br>
<i>&gt; power of their thoughts.  Their mental life may be far beyond anything</i><br>
<i>&gt; we can imagine, but we could still expect to draw some simple</i><br>
<i>&gt; conclusions about how they will behave, things which are at the level</i><br>
<i>&gt; which we can understand.  Perhaps Robin's reasoning based on</i><br>
<i>&gt; fundamental principles of selection and evolution would fall into this</i><br>
<i>&gt; category.</i><br>

<p>
<a href="2209.html#2232qlink2">&gt;From http://pobox.com/~sentience/sing_analysis.html#motivate</a><br>

<p>
As far as I can tell, there are only three real questions about SI
motivations.


<OL>
  <li>  Do they still care about efficiency, or do they have an
    infinite/adequate supply of power?  Do they need atoms for
    anything?
  <li>  Do they consider humans valuable?
  <li>  If so, would they force-upgrade us?


</OL>
<p>
If (1) but not (2), we're dead.  If (1) and (2), we either turn
into PSEs or stay humans forever, whichever is more efficient. 
(Or perhaps only the "valuable" part of us will remain...)  If (2)
and (3), we turn into PSEs.  If (2) but not (3), we stick around
in Permutation City until we grow up.  If neither (1) nor (2), we
probably get all the capacity we want anyhow on the theory that it
encourages Singularities, or else we just get left alone with our
atoms.  In short, the basic interplay between these three
motivations determines our survival and/or godhood.

<p>
I am pleased to announce that I see excellent arguments in favor
of both sides of all three questions, which arguments change on a
monthly basis, so I'm not going to bother mentioning them.
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2233.html">[ Next ]</a><a href="2231.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2205.html">Hal Finney</a>
<!-- nextthread="start" -->
</ul>
</body></html>
