<!-- received="Mon Aug 10 06:16:27 1998 MDT" -->
<!-- sent="Mon, 10 Aug 1998 14:12:43 +0200" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: 5,000,000,000 transhumans?" -->
<!-- id="35CEE2A9.2CC5@geocities.com" -->
<!-- inreplyto="5,000,000,000 transhumans?" -->
<!-- version=1.10, linesinbody=138 -->
<html><head><title>extropians: Re: 5,000,000,000 transhumans?</title>
<meta name=author content="den Otter">
<link rel=author rev=made href="mailto:neosapient@geocities.com" title ="den Otter">
</head><body>
<h1>Re: 5,000,000,000 transhumans?</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Mon, 10 Aug 1998 14:12:43 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1335">[ date ]</a><a href="index.html#1335">[ thread ]</a><a href="subject.html#1335">[ subject ]</a><a href="author.html#1335">[ author ]</a>
<!-- next="start" -->
<li><a href="1336.html">[ Next ]</a><a href="1334.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1297.html">Max More</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Max More wrote:
<br>
<i>&gt; </i><br>
<a href="1297.html#1335qlink1">&gt; At 01:37 PM 8/9/98 +0200, Den Otter wrote:</a><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt;Yes, but this is fundamentally different; godhood isn't something</i><br>
<i>&gt; &gt;that one would sell (or give away) like one would do with minor</i><br>
<i>&gt; &gt;technological advances such as phones, TVs cars etc. Just like nukes</i><br>
<i>&gt; &gt;were (and are) only for a select few, so will hyperintelligence,</i><br>
<i>&gt; &gt;nanotech, uploading etc. initially be only available to a select</i><br>
<i>&gt; &gt;group, which will most likely use them to become gods. There is</i><br>
<i>&gt; &gt;no rational reason to distribute this kind of power once you have</i><br>
<i>&gt; &gt;it.</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt;Powerful businessmen still need others to make and buy their products,</i><br>
<i>&gt; &gt;and dictators and presidents still need their people to stay in power</i><br>
<i>&gt; &gt;&amp; to keep the country running, but a SI needs NO-ONE, it's</i><br>
<i>&gt; &gt;supremely autonomous. I can't imagine why it would share its</i><br>
<i>&gt; &gt;awesome power with creatures that are horribly primitive from its point</i><br>
<i>&gt; &gt;of view. Would *we* uplift ants/mice/dogs/monkeys to rule the world</i><br>
<i>&gt; &gt;as our equals? I think not.</i><br>
<i>&gt; </i><br>
<i>&gt; "No rational reason" is a strong claim. I doubt your claim. First, your</i><br>
<i>&gt; view surely depends on a Singularitarian view that superintelligence will</i><br>
<i>&gt; come all at once, with those achieving it pulling vastly far away from</i><br>
<i>&gt; everyone else. I don't expect things to work out that way. </i><br>

<p>
New technologies usually are the domain of government/company research 
facilities, then the relatively few rich and powerful that can afford
the prototypes, then the well-off middle classes etc. Years or even
decades go by before a new technology is available to the general
population. IMO, the pioneers of AI, nanotech, uploading, chip-brain
interface etc,(which are all interconnected btw, so that the emergence
of one these technologies would considerably speed up the advent of the
others) will figure out how to transcend at approximately the same time
that the new products would have normally become more widely available;
i.e. when the major bugs have been taken out.

<p>
<a name="1493qlink1">In the case of SI, any head start, no matter how slight, can mean all
the difference in the world. A SI can think, and thus strike, much
faster than less intelligent, disorganized masses. Before others
could transcend themselves or organize resistance, it would be too late.</a>
So yes, someone will be the first to have SI, and unless the rest 
is only seconds or minutes behind (or has vastly superior technology), 
they *will* be neutralized *if* the SI is malevolent, or simply cold
and rational. 

<p>
<i>&gt; I've explained</i><br>
<a href="1297.html#1335qlink2">&gt; some of my thinking in the upcoming Singularity feature that Robin Hanson</a><br>
<i>&gt; is putting together for Extropy Online.</i><br>

<p>
I'm looking forward to reading it...I do beleive I have a 
solid case though: the chances that SI will be only for a small group
or even just one entity are *at least* 50%, likely more.
 
<p>
<a href="1297.html#1335qlink3">&gt; Second, I also doubt that the superintelligence scenario is so radically</a><br>
<i>&gt; different from today's powerful business people. [I don't say "businessmen"</i><br>
<i>&gt; since this promotes an unfortunate assumption about gender and business.]</i><br>

<p>
Well, that's the catch; SIs won't be "people" in any normal
sense, so there's absolutely no guarantee that they will behave (more or
less) like people. The situation is radically different because SI is
radically different, basically an alien life form.

<p>
<a href="1297.html#1335qlink4">&gt; You could just as well say that today's extremely wealthy and powerful</a><br>
<i>&gt; business should have no need to benefit poor people. Yet, here we have oil</i><br>
<i>&gt; companies building hospitals and providing income in central Africa. </i><br>

<p>
This is a typical feature of our interdependent society. Since no-one
has the power to be fully autonomous (do whatever you want and get
away with it), PR is important. 

<p>
<a href="1297.html#1335qlink5">&gt; I just</a><br>
<i>&gt; don't buy the idea that each single SI will do everyone alone.</i><br>

<p>
Surely you mean every*thing*? ;) IMO, a real SI will be fully
autonomous; it can soak up most of humanity's knowledge via
the web, reprogram and improve itself (evolve), manipulate its
surroundings with nanoprobes, macrobots etc. If it wants something, it
can simply take it. In other words, it's a world in itself.

<p>
<a href="1297.html#1335qlink6">&gt; Specialization and division of labor will still apply.</a><br>

<p>
Yes, but it will be parts of the being itself that will perform
the specialized tasks, not outside entities. 

<p>
<i>&gt; at some SI's will</i><br>
<a href="1297.html#1335qlink7">&gt; want to help the poor humans upgrade because that will mean adding to the</a><br>
<i>&gt; pool of superintelligences with different points of view and different</i><br>
<i>&gt; interests.</i><br>

<p>
<a name="1493qlink2">That could be the case (roughly 33 %), but it wouldn't be the most
rational approach. Simply put: more (outside) diversity means also
a greater risk of being attacked, with possibly fatal consequences.
If a SI is the only intelligent being in the universe, then it's
presumably safe. In any case *safer* than with known others around.</a>
Now, how can outside diversity benefit the SI? Through a) entertainment
b) providing extra computing power to solve problems that might
threaten the SI, like the end of the universe (if there is such a thing
at all). Do these advantages outweigh the risks? Rationally speaking,
they don't. A SI has ample computing power and intelligence to entertain 
itself and to solve just about any problem (it's auto-evolving and
can always grow extra brain mass to increase its output -- it can
even turn the whole known universe into a computer, given enough
time. And time it has, being technically immortal). Why would it
want other entities around that are powerful, unpredictable, beyond 
its control? There is a lot more at stake for a SI (eternity), so
presumably it will be *very* careful. I think that the need for company
is a typically human thing, us being social apes and all, and it no
longer applies to SI.
 
<p>
<a name="1493qlink3"><a href="1297.html#1335qlink8">&gt; Let me put it this way: I'm pretty sure your view is incorrect, because I</a><br>
<i>&gt; expect to be one of the first superintelligences, and I intend to uplift</i><br>
<i>&gt; others.</i><br>

<p>
A bold claim indeed ;) However, becoming a SI will probably change
your motivational system, making any views you hold at the beginning
of transcension rapidly obscolete. Also, being one of the first may
not be good enough. Once a SI is operational, mere hours, minutes and
even seconds could be the difference between success and total failure.</a>
<a name="1493qlink4">After all, a "malevolent" SI could (for example) easily sabotage most of
the earth's computer systems, including those of the competition, and
use the confusion to gain a decisive head start.</a>

<p>
<a href="1297.html#1335qlink9">&gt; Or, are you planning on trying to stop me from bringing new members</a><br>
<i>&gt; into the elite club of SIs?</i><br>

<p>
I'm not planning to stop anyone, but what I want is irrelevant anyway
since I don't really expect to be among the lucky few that make it
(I will certainly try, though). Others *will* likely try to stop you,
not just from helping your fellow man, but also from transcending
yourself. After all, people won't be selected for their upright morals,
but for wealth, power, efficiency (ruthlessness) and plain luck. And
who's to tell how ascending will influence one's character. It
certainly wouldn't be unreasonable to assume that there's an
equal chance that SIs will be indifferent, evil or benevolent (or
some strange mix of these traits, way beyond our current 
understanding).
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1336.html">[ Next ]</a><a href="1334.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1297.html">Max More</a>
<!-- nextthread="start" -->
</ul>
</body></html>
