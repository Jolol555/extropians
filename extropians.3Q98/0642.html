<!-- received="Mon Jul 13 10:08:44 1998 MDT" -->
<!-- sent="Mon, 13 Jul 1998 10:08:46 -0600 (MDT)" -->
<!-- name="Michael Nielsen" -->
<!-- email="mnielsen@tangelo.phys.unm.edu" -->
<!-- subject="Re: Moore's law" -->
<!-- id="Pine.SUN.3.91.980709103550.29197B-100000@tangelo.phys.unm.edu" -->
<!-- inreplyto="35A42E5A.C92C5272@clemmensen.shirenet.com" -->
<!-- version=1.10, linesinbody=113 -->
<html><head><title>extropians: Re: Moore's law</title>
<meta name=author content="Michael Nielsen">
<link rel=author rev=made href="mailto:mnielsen@tangelo.phys.unm.edu" title ="Michael Nielsen">
</head><body>
<h1>Re: Moore's law</h1>
Michael Nielsen (<i>mnielsen@tangelo.phys.unm.edu</i>)<br>
<i>Mon, 13 Jul 1998 10:08:46 -0600 (MDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#642">[ date ]</a><a href="index.html#642">[ thread ]</a><a href="subject.html#642">[ subject ]</a><a href="author.html#642">[ author ]</a>
<!-- next="start" -->
<li><a href="0643.html">[ Next ]</a><a href="0641.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0452.html">Dan Clemmensen</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->


<p>
On Wed, 8 Jul 1998, Dan Clemmensen wrote:

<p>
Dan, I hope you don't mind the rather delayed nature of my responses.   
Email sometimes is treated as something to be responded to 
instantly.  By contrast, I've found that I quite enjoy extended 
discussion like this, often with a considerable break between rounds, for 
reflection. 

<p>
<a href="0452.html#0642qlink1">&gt; Michael Nielsen wrote:</a><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; I may as well state one of my main interests in this: whether we'll ever</i><br>
<i>&gt; &gt; have enough computational power to set up a good "breeding ground"</i><br>
<i>&gt; &gt; for AIs -- an artificial environment optimized to produce artifical</i><br>
<i>&gt; &gt; intelligence by means of selective pressure.  Using proctonumerology, I'd</i><br>
<i>&gt; &gt; guess a figure of about 10^40 operations ought to be enough.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; I haven't a clue. What does Moravic say, and should we believe him? Are</i><br>
<i>&gt; these operations per second?</i><br>

<p>
No, I was speaking of the total number of operations necessary to evolve 
and intelligence.  My thinking was that a population of 10^15 entities 
roughly as complex as a human brain, evolving for 10^10 years or so, with 
the same speed of operation as the brain (1Hz, to a first 
approximation) ought to evolve into something quite interesting.  After 
all, that's not a bad approximation to the human evolutionary path.

<p>
Moravec's argument doesn't seem especially sound to me.  I forgot the 
numbers -- they're much more modest than those I gave above -- but he 
seems to be implicitly assuming that the brain is just a big blob of 
hardware, in the sense that once we have the hardware necessary to 
support a brain-equivalent device, we'll be able to create one.  This 
neglect of software and the specific details of arhitecture seems highly 
dubious to me.

<p>
A point I quite like is that chemical reactions -- the basis for life as 
we know it -- typically progress at a rate on the order of Hz.  There 
are some fast exceptions, but, in any computational system, the relevant 
timescales are usually the slow ones. Solid state devices can work at 
GigaHerz or more.  Human beings have a factor of a 10^9 or so over nature 
in this regard.  Which is a good thing, because Nature has been going at 
it for ~10^9 years.  Nature's advantage due to size and parallelism 
are still considerable, though -- we need to work on those things.
 
<p>
<a href="0452.html#0642qlink2">&gt; &gt; I don't know how the proposed nanomechanical schemes work. In</a><br>
<i>&gt; &gt; commonly used electronic schemes, I believe that the depth is O(log N) for</i><br>
<i>&gt; &gt; random access, but the number of operations is O(N log N), at least in the</i><br>
<i>&gt; &gt; schemes I'm familiar with.  Are you absolutely sure that the number of</i><br>
<i>&gt; &gt; operations in a nanomechanical addressing systems is O(log N)?</i><br>
<i>&gt; </i><br>
<i>&gt; Nope, I made it up as I went along, mentally designing the system on the</i><br>
<i>&gt; fly and probably messing it up severely. I now strongly suspect that between</i><br>
<i>&gt; the two of us we are thinking about three different things: depth with O(log N)</i><br>
<i>&gt; complexity, number of gates needed with O(N), and energy dissipation, which will</i><br>
<i>&gt; depend strongly implementation, with 0(N) required for a minimized depth (i.e.,</i><br>
<i>&gt; depth of 2, fastest algorithm) and O(log N) required for a slow but energy-efficient</i><br>
<i>&gt; approach with one address bit per clock.</i><br>
<i>&gt; &gt;  </i><br>
<i>&gt; &gt; can we drop the 2020 date?</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; Sure, It's more fun. </i><br>

<p>
Great.

<p>
<a href="0452.html#0642qlink3">&gt; &gt; &gt; By contrast, you raise the issue of error correction.</a><br>
<i>&gt; &gt; &gt; However, even very powerful ECC schemes require less than doubling the amount</i><br>
<i>&gt; &gt; &gt; of volume needed to store a word.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; That's not really true.  To do fault-tolerant computation the best known</i><br>
<i>&gt; &gt; overhead, so far as I know, goes polylogarithmically in the size of the</i><br>
<i>&gt; &gt; computation, with some (fairly large) constant factor in front of the</i><br>
<i>&gt; &gt; first term in the polylog factor.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; Here are are again shifting back and forth between ECC for storage and ECC for</i><br>
<i>&gt; computation.</i><br>

<p>
Not really.  Much the same issues come up in both case, unless your 
storage is fantastically reliable.  Why?  Suppose your storage is not 
fantastically reliable.  Then you will need to do a "little" bit of error 
correction.  Unfortunately, that involves gates... which themselves need 
to be error corrected, and so on, down the line.   For this 
reason, there's not so much difference between memory and dynamics, unless 
the memory is fantastically reliable, so no error correction at all needs 
to be done.  I don't consider that likely for the sort of applications I 
have in mind, so I'm just using the dynamical figure for all error 
correction.

<p>
<a href="0452.html#0642qlink4">&gt; I was addressing storage, wherein we are not using reversable</a><br>
<i>&gt; logic. For storage, as I recall the Hamming distance goes up rapidly with a modest</i><br>
<i>&gt; increase in ECC bits. In conjunction with cashing, the bulk memory words</i><br>
<i>&gt; subject to ECC will be long. The nuber of ECC bits goes up O(log N) with</i><br>
<i>&gt; word length and linearly (?) with the number of errored bits that can be corrected.</i><br>

<p>
I believe it is linearly; not sure.

<p>
<a href="0452.html#0642qlink5">&gt; &gt; If I can find the time, I'll try to look up some of the papers analyzing</a><br>
<i>&gt; &gt; errors in reversible computation.  As I recall, there were a few in the</i><br>
<i>&gt; &gt; early 80s, which I've never read.</i><br>
<i>&gt; There was at east one paper delivered at the 1997 foresight conference:</i><br>
<i>&gt;       <a href="http://WWW.FORESIGHT.ORG/Conferences/MNT05/Abstracts/Frakabst.html">http://WWW.FORESIGHT.ORG/Conferences/MNT05/Abstracts/Frakabst.html</a></i><br>
<i>&gt; That deals with reversible computation.</i><br>

<p>
A very interesting article, thanks for the link.  It doesn't 
seem to say much about errors or error correction, though.  Some work has 
been done on these subjects (in reversible computing); I don't recall the 
results.

<p>
Michael Nielsen 

<p>
<a href="http://wwwcas.phys.unm.edu/~mnielsen/index.html">http://wwwcas.phys.unm.edu/~mnielsen/index.html</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0643.html">[ Next ]</a><a href="0641.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0452.html">Dan Clemmensen</a>
<!-- nextthread="start" -->
</ul>
</body></html>
