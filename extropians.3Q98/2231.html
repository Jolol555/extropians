<!-- received="Tue Sep  8 12:25:51 1998 MDT" -->
<!-- sent="Tue, 08 Sep 1998 11:21:36 -0700" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@econ.berkeley.edu" -->
<!-- subject="Re: Human AI to superhuman (Re: Max More)" -->
<!-- id="3.0.3.32.19980908112136.010e05c8@econ.berkeley.edu" -->
<!-- inreplyto="35F4B16C.D419E7F1@pobox.com" -->
<!-- version=1.10, linesinbody=63 -->
<html><head><title>extropians: Re: Human AI to superhuman (Re: Max More)</title>
<meta name=author content="Robin Hanson">
<link rel=author rev=made href="mailto:hanson@econ.berkeley.edu" title ="Robin Hanson">
</head><body>
<h1>Re: Human AI to superhuman (Re: Max More)</h1>
Robin Hanson (<i>hanson@econ.berkeley.edu</i>)<br>
<i>Tue, 08 Sep 1998 11:21:36 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2231">[ date ]</a><a href="index.html#2231">[ thread ]</a><a href="subject.html#2231">[ subject ]</a><a href="author.html#2231">[ author ]</a>
<!-- next="start" -->
<li><a href="2232.html">[ Next ]</a><a href="2230.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2209.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="2235qlink1">Eliezer S. Yudkowsky writes:</a>
<br>
<a href="2209.html#2231qlink1">&gt;Quoting Max More:  </a><br>
<i>&gt;      "... I have no doubt that</i><br>
<i>&gt;      human level AI (or computer networked intelligence) will be</i><br>
<i>&gt;      achieved at some point. But to move from this immediately to</i><br>
<i>&gt;      drastically superintelligent thinkers seems to me doubtful."   </i><br>
<i>&gt;...</i><br>
<i>&gt;the seed AI's power either remains constant or has a definite maximum ...</i><br>
<i>&gt;efficiency ... defined as ... the levels of intelligence achievable at </i><br>
<i>&gt;each level of power ... more intelligence makes it possible for the AI </i><br>
<i>&gt;to better optimize its own code. ...</i><br>
<i>&gt;the basic hypothesis of seed AI can be described as postulating </i><br>
<i>&gt;a Transcend Point; a point at which each increment of intelligence yields </i><br>
<i>&gt;an increase in efficiency that yields an equal or greater increment of </i><br>
<i>&gt;intelligence, or at any rate an increment that sustains the reaction.  </i><br>
<i>&gt;This behavior of de/di is assumed to carry the seed AI to the Singularity </i><br>
<i>&gt;Point, where each increment of intelligence yields an increase of efficiency </i><br>
<i>&gt;and power that yield a reaction-sustaining increment of intelligence. </i><br>
<i>&gt;It so happens that all humans operate, by and large, at pretty much the </i><br>
<i>&gt;same level of intelligence.  ... the brain doesn't self-enhance, only </i><br>
<i>&gt;self-optimize a prehuman subsystem.  ...</i><br>
<a name="2235qlink2"><i>&gt;You can't draw conclusions from one system to the other.  The</i><br>
<i>&gt;genes give rise to an algorithm that optimizes itself and then programs</i><br>
<i>&gt;the brain according to genetically determined architectures ...</i><br>

<p>
But where *do* you draw your conclusions from, if not by analogy with
other intelligence growth processes?  Saying that "superintelligence is 
nothing like anything we've ever known, so my superfast growth estimates 
are as well founded as any other" would be a very weak argument.  Do you
have any stronger argument?</a> 

<p>
<a name="2235qlink3">We humans have been improving ourselves in a great many ways for a long time.
By a six year old's definition of intelligence ("she's so smart; look at all 
the things she knows and can do"), we are vastly more intelligent that our 
ancestors of a hundred thousand years ago.  Much of that intelligence is 
embodied in our social organization, but even when people try their hardest
to measure individual intelligence, divorced from social supports, they 
still find that such intelligence has been increasing dramatically with time.</a>

<p>
<a name="2235qlink4">This<a name="2271qlink3"> experience with intelligence growth seems highly relevant to me.</a>  
First, we see that the effect of smarter creatures being better able to 
implement any one improvement is counteracted by the fact that one tries the
easy big win improvements first.  Second, we see that growth is social; it 
is the whole world economy that is improving together, not any one creature
improving itself.  Third, we see that easy big win improvements are very rare;
growth is mainly due to the accumulation of many small improvements. 
(Similar lessons come from our experience trying to write AI programs.)</a> 

<p>
<a name="2235qlink5">Now it is true that AIs should be able to more easily modify certain 
aspects of their cognitive architectures.  But it is also true that human 
economic growth is partly due to slowly accumulating more ways to more 
easily modify aspects of our society and ourselves.  The big question is:
why should we believe that an isolated "seed AI" will find a very long stream 
of easy big win improvements in its cognitive architecture, when this seems 
contrary to our experience with similar intelligence growth processes?</a>  



<p>
Robin Hanson  
<br>
hanson@econ.berkeley.edu    <a href="http://hanson.berkeley.edu/">http://hanson.berkeley.edu/</a>   
RWJF Health Policy Scholar, Sch. of Public Health   510-643-1884  
140 Warren Hall, UC Berkeley, CA 94720-7360    FAX: 510-643-8614
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2232.html">[ Next ]</a><a href="2230.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2209.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
