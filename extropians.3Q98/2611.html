<!-- received="Thu Sep 24 10:55:27 1998 MDT" -->
<!-- sent="Thu, 24 Sep 1998 09:51:22 -0700" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@econ.berkeley.edu" -->
<!-- subject="AI big wins (was: Punctuated Equilibrium Theory)" -->
<!-- id="3.0.3.32.19980924095122.00724a18@econ.berkeley.edu" -->
<!-- inreplyto="36099182.86E492EA@pobox.com" -->
<!-- version=1.10, linesinbody=30 -->
<html><head><title>extropians: AI big wins (was: Punctuated Equilibrium Theory)</title>
<meta name=author content="Robin Hanson">
<link rel=author rev=made href="mailto:hanson@econ.berkeley.edu" title ="Robin Hanson">
</head><body>
<h1>AI big wins (was: Punctuated Equilibrium Theory)</h1>
Robin Hanson (<i>hanson@econ.berkeley.edu</i>)<br>
<i>Thu, 24 Sep 1998 09:51:22 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2611">[ date ]</a><a href="index.html#2611">[ thread ]</a><a href="subject.html#2611">[ subject ]</a><a href="author.html#2611">[ author ]</a>
<!-- next="start" -->
<li><a href="2612.html">[ Next ]</a><a href="2610.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2596.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2618.html">Eliezer S. Yudkowsky</a>
</ul>
<!-- body="start" -->

<p>
<a name="2618qlink1">Eliezer S. Yudkowsky writes:
<br>
<a href="2596.html#2611qlink1">&gt;Well, my other reason for expecting a breakthrough/bottleneck architecture,</a><br>
<i>&gt;even if there are no big wins, is that there's positive feedback involved,</i><br>
<i>&gt;which generally turns even a smooth curve steep/flat.  And I think my</i><br>
<i>&gt;expectation about a sharp jump upwards after architectural ability is</i><br>
<i>&gt;independent of whether my particular designs actually get there or not.  In</i><br>
<i>&gt;common-sense terms, the positive feedback arrives after the AI has the ability</i><br>
<i>&gt;humans use to design programs.</i><br>

<p>
Let me repeat my call for you to clarify what appears to be a muddled argument.
We've had "positive feedback", in the usual sense of the term, for a long time.
We've also been able to modify and design AI architectures for a long time. 
Neither of these considerations obviously suggests a break with history.</a> 

<p>
<a name="2618qlink2"><a href="2596.html#2611qlink2">&gt;My understanding of the AI Stereotype is that the youngster only has a single</a><br>
<i>&gt;great paradigm, and is loath to abandon it.  I've got whole toolboxes full ...</i><br>

<p>
I think you're mistaken - lots of those cocky youngsters have full toolboxes.
("Yup, mosta gunslingers get kilt before winter - but they mosta got only one 
 gun, and looky how many guns I got!")</a>





<p>
Robin Hanson  
<br>
hanson@econ.berkeley.edu    <a href="http://hanson.berkeley.edu/">http://hanson.berkeley.edu/</a>   
RWJF Health Policy Scholar, Sch. of Public Health   510-643-1884  
140 Warren Hall, UC Berkeley, CA 94720-7360    FAX: 510-643-8614
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2612.html">[ Next ]</a><a href="2610.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2596.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2618.html">Eliezer S. Yudkowsky</a>
</ul>
</body></html>
