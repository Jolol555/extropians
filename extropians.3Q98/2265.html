<!-- received="Thu Sep 10 06:15:42 1998 MDT" -->
<!-- sent="Thu, 10 Sep 1998 05:15:35 -0700 (PDT)" -->
<!-- name="Emmanuel Charpentier" -->
<!-- email="emmanuel_charpentier@yahoo.com" -->
<!-- subject="Re: Human AI to superhuman (Re: Max More)" -->
<!-- id="19980910121535.18594.rocketmail@send101.yahoomail.com" -->
<!-- inreplyto="Human AI to superhuman (Re: Max More)" -->
<!-- version=1.10, linesinbody=60 -->
<html><head><title>extropians: Re: Human AI to superhuman (Re: Max More)</title>
<meta name=author content="Emmanuel Charpentier">
<link rel=author rev=made href="mailto:emmanuel_charpentier@yahoo.com" title ="Emmanuel Charpentier">
</head><body>
<h1>Re: Human AI to superhuman (Re: Max More)</h1>
Emmanuel Charpentier (<i>emmanuel_charpentier@yahoo.com</i>)<br>
<i>Thu, 10 Sep 1998 05:15:35 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2265">[ date ]</a><a href="index.html#2265">[ thread ]</a><a href="subject.html#2265">[ subject ]</a><a href="author.html#2265">[ author ]</a>
<!-- next="start" -->
<li><a href="2266.html">[ Next ]</a><a href="2264.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2235.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
---"Eliezer S. Yudkowsky" &lt;sentience@pobox.com&gt; wrote:
<br>
<i>&gt;</i><br>
<a href="2235.html#2265qlink1">&gt; Robin Hanson wrote:</a><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Eliezer S. Yudkowsky writes:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; &gt;You can't draw conclusions from one system to the other.  The</i><br>
<i>&gt; &gt; &gt;genes give rise to an algorithm that optimizes itself and then</i><br>
programs
<br>
<a href="2209.html#2265qlink2">&gt; &gt; &gt;the brain according to genetically determined architectures ...</a><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; But where *do* you draw your conclusions from, if not by analogy</i><br>
with
<br>
<a href="2235.html#2265qlink3">&gt; &gt; other intelligence growth processes?  Saying that</a><br>
"superintelligence is
<br>
<a href="2235.html#2265qlink4">&gt; &gt; nothing like anything we've ever known, so my superfast growth</a><br>
estimates
<br>
<a href="2235.html#2265qlink5">&gt; &gt; are as well founded as any other" would be a very weak argument. </a><br>
Do you
<br>
<a href="2235.html#2265qlink6">&gt; &gt; have any stronger argument?</a><br>
<i>&gt; </i><br>
<i>&gt; Basically, "I designed the thing and this is how I think it will</i><br>
work and this
<br>
<a href="2235.html#2265qlink7">&gt; is why."  There aren't any self-enhancing intelligences in Nature,</a><br>
and the
<br>
<a href="2235.html#2265qlink8">&gt; behavior produced by self-enhancement is qualitatively distinct.  In</a><br>
short,
<br>
<a href="2235.html#2265qlink9">&gt; this is not a time for analogic reasoning.</a><br>

<p>
 Excuse me for carrying on (I hope I'm not being a pain, or pointless,
or boring...), but<a name="2283qlink6"> I would say that, IMHO, the first Artficial
Intelligence we will create will mostly have the same characteristics
as us. The same defaults, the same qualities, no magic wand. And
self-enhancement is no better, I don't see why it would make anything
other than geniuses with any sort of add-on as can be imagined.</a>

<p>
<a name="2283qlink7">  And we (as humans in flesh) will probably have a cutting edge for a
long time: evolution has made us, we are very strongly a part of the
world, we have instincts (quite entangled sometimes). Those instincts
can cover everything from pain/pleasure to basic wirings of the brain,
to algorithms for creating more wirings through what we call games,
tests, experiments. And we have a body.</a>

<p>
   This could mean that singularity doesn't have to happen. Just the
usual exponential knowledge growth. And if we might ever stumble upon
a new architecture for intelligence (real new, not just adding or
improving parts), this change might or might not be of great
importance. But how can we know. 

<p>
<a name="2283qlink8">    Well, it is true that AI still (seem to) have the speed/power
advantage (and possibly eli's others abilities), but do that mean it
will change all the cards? Lead to that end of history that seems to
</a>be the singularity???

<p>
     Manu.
<hr>
<br>
DO YOU YAHOO!?
<br>
Get your free @yahoo.com address at <a href="http://mail.yahoo.com">http://mail.yahoo.com</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2266.html">[ Next ]</a><a href="2264.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2235.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
