<!-- received="Thu Sep 24 13:10:40 1998 MDT" -->
<!-- sent="Thu, 24 Sep 1998 14:17:31 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: AI big wins (was: Punctuated Equilibrium Theory)" -->
<!-- id="360A9ABC.4812706E@pobox.com" -->
<!-- inreplyto="3.0.3.32.19980910101936.00b29c90@econ.berkeley.edu" -->
<!-- version=1.10, linesinbody=83 -->
<html><head><title>extropians: Re: AI big wins (was: Punctuated Equilibrium Theory)</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: AI big wins (was: Punctuated Equilibrium Theory)</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Thu, 24 Sep 1998 14:17:31 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2618">[ date ]</a><a href="index.html#2618">[ thread ]</a><a href="subject.html#2618">[ subject ]</a><a href="author.html#2618">[ author ]</a>
<!-- next="start" -->
<li><a href="2619.html">[ Next ]</a><a href="2617.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2611.html">Robin Hanson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2621.html">Robin Hanson</a>
</ul>
<!-- body="start" -->

<p>
Robin Hanson wrote:
<br>
<i>&gt; </i><br>
<a href="2611.html#2618qlink1">&gt; Eliezer S. Yudkowsky writes:</a><br>
<i>&gt; &gt;Well, my other reason for expecting a breakthrough/bottleneck architecture,</i><br>
<i>&gt; &gt;even if there are no big wins, is that there's positive feedback involved,</i><br>
<i>&gt; &gt;which generally turns even a smooth curve steep/flat.  And I think my</i><br>
<i>&gt; &gt;expectation about a sharp jump upwards after architectural ability is</i><br>
<i>&gt; &gt;independent of whether my particular designs actually get there or not.  In</i><br>
<i>&gt; &gt;common-sense terms, the positive feedback arrives after the AI has the ability</i><br>
<i>&gt; &gt;humans use to design programs.</i><br>
<i>&gt; </i><br>
<i>&gt; Let me repeat my call for you to clarify what appears to be a muddled argument.</i><br>
<i>&gt; We've had "positive feedback", in the usual sense of the term, for a long time.</i><br>
<i>&gt; We've also been able to modify and design AI architectures for a long time.</i><br>
<i>&gt; Neither of these considerations obviously suggests a break with history.</i><br>

<p>
Sigh.  Okay, one more time:  The total trajectory is determined by the
relation between power (raw teraflops), optimization (the speed and size of
code) and intelligence (the ability to do interesting things with code or
invent fast-infrastructure technologies).

<p>
Is intelligence well-defined?  Of course not, because it consists of symbolic
architecture and goals and causality and memories and analogies and
similarities and concepts and simulations and twenty different modules of
domain-specific intuitions and God knows what else.  The best we can do is
focus down on code-writing intelligence, which suggests specific metrics such
as particular programming tasks, or fast-infrastructure intelligence, which
focuses on domains such as molecular engineering and protein folding.

<p>
Is power well-defined?  Pretty much.  A teraflops from a thousand parallel
gigaflops processors isn't the same as a linear teraflops, but all this means
is that the type of power shades over into optimization.

<p>
Is optimization well-defined?  Yes.  If intelligence A can come up with the
same solution in the same time using half the processing power or RAM of
intelligence B, intelligence A is more optimized.  If intelligence A writes
code with similar speedups, intelligence A has better optimizing ability.

<p>
Given constant power, the trajectory at time T is determined by whether the AI
can optimize itself enough to get an intelligence boost which further
increases the ability at optimization enough for another intelligence boost. 
<a name="2621qlink2">Presumably the sum of this series converges to a finite amount.  If the amount
</a>
<a name="2621qlink3">is small, we say the trajectory bottlenecks; if the amount is large, we say a
breakthrough has occurred.  The key question is whether the intelligence
reached is able to build fast-infrastructure nanotechnology and the like, or
of exhibiting unambiguously better-than-human abilities in all domains.
</a>

<p>
Now that I've basically repeated everything I said earlier, will you please
ask specific questions about a specific term?  I'm starting to feel like I
can't win, or that your requests for clarification are only conversational tactics...

<p>
<a href="2611.html#2618qlink2">&gt; &gt;My understanding of the AI Stereotype is that the youngster only has a single</a><br>
<i>&gt; &gt;great paradigm, and is loath to abandon it.  I've got whole toolboxes full ...</i><br>
<i>&gt; </i><br>
<i>&gt; I think you're mistaken - lots of those cocky youngsters have full toolboxes.</i><br>
<i>&gt; ("Yup, mosta gunslingers get kilt before winter - but they mosta got only one</i><br>
<i>&gt;  gun, and looky how many guns I got!")</i><br>

<p>
Heh.  I must be moving in the wrong circles, because the "cocky youngsters" I
encounter generally send me email filled with silly, overcomplex,
badly-defined theories based on a single unworkable "insight" which they
praise to the high heavens.  While I plead guilty to the charges of not
defining every damn term I use in an email discussion, I think that the
original literature - my Web page - is fairly good about doing so.  I don't
gush about how wonderful the paradigms are, although I've been known to warn
about the horrible consequences of ignoring them.  And above all else, I don't
attribute Moral Significance, the universal bane of youngsters.  I may do so
when discussing the ultimate point of building an AI, and in the very special
case of an AI's ultimate goals, but I don't do so when I'm talking about the
workhorse principles.

<p>
<a name="2621qlink4">At this point, the key question for me is "How much of _Coding a Transhuman
AI_ did you actually read?"  You really can't judge my tendency to
</a>
oversimplify from a 5K post about a 200K page.  And can you tell me where I
can find the writings of these eager youngsters to which you keep referring?
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2619.html">[ Next ]</a><a href="2617.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2611.html">Robin Hanson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2621.html">Robin Hanson</a>
</ul>
</body></html>
