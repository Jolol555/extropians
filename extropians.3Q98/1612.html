<!-- received="Mon Aug 17 15:12:30 1998 MDT" -->
<!-- sent="Mon, 17 Aug 1998 22:11:47 +0100" -->
<!-- name="Bryan Moss" -->
<!-- email="bryan.moss@dial.pipex.com" -->
<!-- subject="Re: Evolving AIs and the term "Virus" (was "Who's Afraid of the SI?")" -->
<!-- id="001a01bdca23$d8d75a20$291ae20a@bungle" -->
<!-- inreplyto="Evolving AIs and the term "Virus" (was "Who's Afraid of the SI?")" -->
<!-- version=1.10, linesinbody=91 -->
<html><head><title>extropians: Re: Evolving AIs and the term "Virus" (was "Who's Afraid of the SI?")</title>
<meta name=author content="Bryan Moss">
<link rel=author rev=made href="mailto:bryan.moss@dial.pipex.com" title ="Bryan Moss">
</head><body>
<h1>Re: Evolving AIs and the term "Virus" (was "Who's Afraid of the SI?")</h1>
Bryan Moss (<i>bryan.moss@dial.pipex.com</i>)<br>
<i>Mon, 17 Aug 1998 22:11:47 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1612">[ date ]</a><a href="index.html#1612">[ thread ]</a><a href="subject.html#1612">[ subject ]</a><a href="author.html#1612">[ author ]</a>
<!-- next="start" -->
<li><a href="1613.html">[ Next ]</a><a href="1611.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1596.html">Doug Bailey</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Doug Bailey wrote:

<p>
<a href="1596.html#1612qlink1">&gt; &gt; Firstly, it would not understand *our*</a><br>
<i>&gt; &gt; "concept of what a human is", I wouldn't be so</i><br>
<i>&gt; &gt; arrogant as to assume our concept is "the"</i><br>
<i>&gt; &gt; concept (some biological chauvinism on your</i><br>
<i>&gt; &gt; part ;).</i><br>
<i>&gt;</i><br>
<i>&gt; Why couldn't it understand our concept of</i><br>
<i>&gt; ourselves? It does not have to adopt that</i><br>
<i>&gt; concept for itself but it could understand how</i><br>
<i>&gt; we view ourselves.</i><br>

<p>
I no longer have my post, but didn't I say
something like, "It could fathom our concept of
ourselves, but it would not share that concept"?
It's the fact that it doesn't have to share our
point of view to understand and interact with us
that is the basis of my argument. And if that's
the case, an AI that lives among us doesn't have
to be hostile like us. The second part of my
argument is that because the AI shares a different
course of evolution to us, it's less likely to
share *our* hostility. This does not mean it can't
be dangerous, but it's more likely these dangers
would be something we would call "computer error"
rather than "malicious intent". And I also believe
these computer errors can be contained.

<p>
<a href="1596.html#1612qlink2">&gt; Besides, it would not have to understand how we</a><br>
<i>&gt; understand ourselves to inflict harm upon us. We</i><br>
<i>&gt; don't understand how dolphins view themselves</i><br>
<i>&gt; but we have certainly inflicted harm upon them.</i><br>

<p>
The idea is that they don't have be like us to act
like us, that they can interact with humans
without having all the violent impulses and need
for power that we have.

<p>
<a href="1596.html#1612qlink3">&gt; &gt; Well an AI that's goal is to "maximise its</a><br>
<i>&gt; &gt; information processing rate" sounds like a</i><br>
<i>&gt; &gt; virus to me.</i><br>
<i>&gt;</i><br>
<i>&gt; [...] Just because we view it as a virus does</i><br>
<i>&gt; not mean it will not exist at some point, care</i><br>
<i>&gt; about what we think, or be responsive to our</i><br>
<i>&gt; demands.</i><br>

<p>
But where does the motivation to care about what
we think and be responsive to our goals come from?
If we `evolve' an AI that wants to "maximise its
information processing rate" at what point does it
suddenly realise that humans are the problem and
humans must be destroyed. And if I intentionally
put those components in to the AI, where do I get
them?

<p>
<a href="1596.html#1612qlink4">&gt; [...] As I've described above, "virus" is a</a><br>
<i>&gt; perspective judgement. Additionally, a</i><br>
<i>&gt; completely benevolent AI could become a "virus"</i><br>
<i>&gt; through its own evolvability or simple intent to</i><br>
<i>&gt; solve a problem.</i><br>

<p>
Basically what I'm saying is this:

<p>
If we create a `good' AI then it is likely to stay
`good'. This is because, A) to exhibit behaviour
we consider good does not mean the AI has to be
identical to us, and B) the probability that `bad'
behaviour will emerge from this AI is small. (A
and B are related because if the AI is not like us
it can have good behaviour without bad behaviour.)

<p>
Although simple behaviour that is harmful to us
can evolve from an AI that exhibits good
behaviour. Complex harmful behaviour would have to
evolve with similar initial condition, and in a
similar environment, to us. (By complex I mean the
sort of cunning AI you see in science fiction, by
simple I mean things we'd generally prescribe to
"computer error" and will usually be able to
control.)

<p>
Of course, a `bad' AI can be `evolved' in a lab or
software department just as easily as a `good'
one. But I'm not addressing purposeful weapons
here, just emergent behaviour.

<p>
BM
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1613.html">[ Next ]</a><a href="1611.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1596.html">Doug Bailey</a>
<!-- nextthread="start" -->
</ul>
</body></html>
