<!-- received="Tue Jul  7 21:47:51 1998 MDT" -->
<!-- sent="Tue, 7 Jul 1998 21:47:59 -0600 (MDT)" -->
<!-- name="Michael Nielsen" -->
<!-- email="mnielsen@tangelo.phys.unm.edu" -->
<!-- subject="Re: Moore's law" -->
<!-- id="Pine.SUN.3.91.980628175202.4891B-100000@tangelo.phys.unm.edu" -->
<!-- inreplyto="3595840E.FFFB586F@clemmensen.shirenet.com" -->
<!-- version=1.10, linesinbody=118 -->
<html><head><title>extropians: Re: Moore's law</title>
<meta name=author content="Michael Nielsen">
<link rel=author rev=made href="mailto:mnielsen@tangelo.phys.unm.edu" title ="Michael Nielsen">
</head><body>
<h1>Re: Moore's law</h1>
Michael Nielsen (<i>mnielsen@tangelo.phys.unm.edu</i>)<br>
<i>Tue, 7 Jul 1998 21:47:59 -0600 (MDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#414">[ date ]</a><a href="index.html#414">[ thread ]</a><a href="subject.html#414">[ subject ]</a><a href="author.html#414">[ author ]</a>
<!-- next="start" -->
<li><a href="0415.html">[ Next ]</a><a href="0413.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0006.html">Steve Mynott</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
On Sat, 27 Jun 1998, Dan Clemmensen wrote:

<p>
<i>&gt; Michael Nielsen wrote:</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Playing Devil's Advocate here...</i><br>
<i>&gt;</i><br>
<i>&gt; No problem with devil's advocacy. Please take my responses constructivly</i><br>
<i>&gt; and not as an argument. As I see it, we are exploring the problem</i><br>
<i>&gt; together.</i><br>

<p>
Okay. I'm having fun with it, and your arguments have made me less 
pessimistic than before.  

<p>
<a name="0452qlink1">I may as well state one of my main interests in this: whether we'll ever 
have enough computational power to set up a good "breeding ground" 
for AIs -- an artificial environment optimized to produce artifical 
intelligence by means of selective pressure.  Using proctonumerology, I'd 
guess a figure of about 10^40 operations ought to be enough.</a>

<p>
<a name="0452qlink2"><i>&gt; &gt; On Fri, 26 Jun 1998, Dan Clemmensen wrote:</a></i><br>
<i>&gt; &gt; </i><br>
<a href="0006.html#0414qlink1">&gt; &gt; &gt; Michael Nielsen wrote:</a><br>
<i>&gt;</i><br>
[memory addressing overhead]
<br>
<i>&gt; </i><br>
<i>&gt; Addressing in a nanomechanical system occurs only as part of a read or write</i><br>
<i>&gt; operation. When no I/O is occurring, no energy is dissipated. Dissipation per</i><br>
<i>&gt; access depends on the size of the memory, O(logN) for random access,</i><br>
<i>&gt; which is negligible and is mitigated further by caching.</i><br>

<p>
I don't know how the proposed nanomechanical schemes work. In 
commonly used electronic schemes, I believe that the depth is O(log N) for 
random access, but the number of operations is O(N log N), at least in the 
schemes I'm familiar with.  Are you absolutely sure that the number of 
operations in a nanomechanical addressing systems is O(log N)?

<p>
On a side note, I really need to read "Nanosystems" one of these days...

<p>
<i>&gt; &gt; &gt; Three-dimensional</i><br>
<i>&gt; &gt; &gt; storage in a nonomechanical system, using 100 atoms per bit alllows</i><br>
<i>&gt; &gt; &gt; a fair number of extra atoms for "overhead" functions such as support</i><br>
<i>&gt; &gt; &gt; and heat-conduction. The energy dissipated to read or write a bit</i><br>
<i>&gt; &gt; &gt; nanomechanically should be very small compared to that needed by current</i><br>
<i>&gt; &gt; &gt; technology,</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Well, it would need to be. Suppose (conservatively), that you are going</i><br>
<i>&gt; &gt; to have 10^6 layers, each storing bits with a density of 100 atoms / bit;</i><br>
<i>&gt; &gt; a storage density of 1 bit for every few nanometers squared.  Such a</i><br>
<i>&gt; &gt; device will have roughly 10^12 more bits stored on it than current</i><br>
<i>&gt; &gt; commerical chips.  I forget the exact numbers, but the dissipation rate</i><br>
<i>&gt; &gt; per logical operation is something like 10^6 kT in current chips.  That</i><br>
<i>&gt; &gt; means you have a major problem unless everything is done completely</i><br>
<i>&gt; &gt; dissipation free.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; I'm not sure that 10^6 layers is conservative. It's nanomechanically conservative</i><br>
<i>&gt; from a static structural standpoint (i.e., we could build it) but not from a</i><br>
<i>&gt; Moore's Law standpoint. We started this discussion with Moore's law in 2020.</i><br>

<p>
<a name="0452qlink3">Okay.  I am, I suppose, trying to see how far we can push the 3d 
architecture idea at this point.  We already do it to some extent -- I am 
told that 20 layers is not that uncommon in a chip -- but I would like to 
know how much further we can go; can we drop the 2020 date?</a>

<p>
<i>&gt; Moore's law ( in one form) calls for doubling density every 1.5 years, or roughly</i><br>
<i>&gt; an increase of 16,000 by 2020. I can get this with an areal decrease from the current</i><br>
<i>&gt; 10^6 nm^2/bit to 50 nm^2/bit without going into the third dimension at all. It's not</i><br>
<i>&gt; unreasonable to assume a decrease in energy per I/O on the same order, so I don't</i><br>
<i>&gt; have to invoke any of the other mechanisms.</i><br>

<p>
Okay. 

<p>
<i>&gt; &gt; &gt; and diamondoid should be able to operate at much higher</i><br>
<i>&gt; &gt; &gt; temperatures than silicon-based devices. Diamondoid is a much better conductor</i><br>
<i>&gt; &gt; &gt; of heat than silicon, also.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; These are good points, but they only buy you a tiny amount.</i><br>
<i>&gt; </i><br>
<i>&gt; As you see from the above, these points can affect the densities by a factor</i><br>
<i>&gt; of ten to one hundred or so.</i><br>

<p>
Okay.  That's quite a non-trivial gain.  Of course, those technologies 
may run into their own problems (remember bubble memories?), but they're 
certainly possibilities I wasn't aware of.

<p>
<a name="0452qlink4"><i>&gt; By contrast, you raise the issue of error correction.</i><br>
<i>&gt; However, even very powerful ECC schemes require less than doubling the amount</i><br>
<i>&gt; of volume needed to store a word.</i><br>

<p>
That's not really true.  To do fault-tolerant computation the best known 
overhead, so far as I know, goes polylogarithmically in the size of the 
computation, with some (fairly large) constant factor in front of the 
first term in the polylog factor.</a> 

<p>
<a name="0452qlink5">The error correction schemes you are talking about will buy you a little, 
but ultimately, fixing one bit errors goes only a tiny way to fixing the 
error problem.  Much more powerful error correction techniques are 
needed to really solve the error problem, unless, as in today's 
computers, you have an incredibly low fundamental error rate.  Even 
then, there's a real question of how many operations you want to do.  If 
you "only" want to do 10^{17} operations, then error rates of about 
10^{-17} are fine -- which is why today's computers don't use error 
correctyion.  Presumably, for AI we would like to do far more operations 
than that -- on the order of 10^{30} does not seem unreasonable. 

<p>
Assuming a fundamental error rate of about 10^{-5} for reversible 
computation, that implies a heavy error correction overhead.  Doing a 
calculation of how much overhead for error correction would be required 
would take quite a while, but it's safe to say that most of the work</a> going 
on in the computer would actually be error correction, not computation.

<p>
<a name="0452qlink6">If I can find the time, I'll try to look up some of the papers analyzing 
errors in reversible computation.  As I recall, there were a few in the 
early 80s, which I've never read.</a> 

<p>
Michael Nielsen

<p>
<a href="http://wwwcas.phys.unm.edu/~mnielsen/index.html">http://wwwcas.phys.unm.edu/~mnielsen/index.html</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0415.html">[ Next ]</a><a href="0413.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0006.html">Steve Mynott</a>
<!-- nextthread="start" -->
</ul>
</body></html>
