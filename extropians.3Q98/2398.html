<!-- received="Mon Sep 14 17:35:35 1998 MDT" -->
<!-- sent="Mon, 14 Sep 1998 18:42:14 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: AI Prime Directive" -->
<!-- id="35FDA9C2.70B61BAD@pobox.com" -->
<!-- inreplyto="AI Prime Directive" -->
<!-- version=1.10, linesinbody=38 -->
<html><head><title>extropians: Re: AI Prime Directive</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: AI Prime Directive</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Mon, 14 Sep 1998 18:42:14 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2398">[ date ]</a><a href="index.html#2398">[ thread ]</a><a href="subject.html#2398">[ subject ]</a><a href="author.html#2398">[ author ]</a>
<!-- next="start" -->
<li><a href="2399.html">[ Next ]</a><a href="2397.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2396.html">Michael Lorrey</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Michael Lorrey wrote:
<br>
<i>&gt; </i><br>
<a href="2396.html#2398qlink1">&gt; I don't know if you understood what I was saying. It seems like you interpreted my proposed</a><br>
<i>&gt; statement in exactly the opposite way in which it was intended. As far as I can see,</i><br>
<i>&gt; telling the AI to model all possible decisions with the goal of reaching the best choice</i><br>
<i>&gt; for the AI's own best long term rational self interest does two things. a) it gives the AI</i><br>
<i>&gt; maximum freedom of choice, and b) if libertarian theory is correct, it also minimizes the</i><br>
<i>&gt; infringements upon others.</i><br>

<p>
Yeah, that's what I thought you were saying.  You're not supposed to "tell"
the AI ANYTHING.  That's what I'm saying.  I'm not saying it as a moral
philosopher; I'm saying it as a computer programmer.  I know damn well you're
acting from the highest of altruistic moral purposes, which is exactly what
I'm afraid of.  More damage has been wreaked by moral altruism than greed and
stupidity have ever dreamed.  On the Great Scale Of Things, saying "we ought
to do this" is always overruled by "that won't work and trying would cause
tremendous damage".

<p>
<a href="http://pobox.com/~sentience/AI_design.temp.html#PrimeDirective">http://pobox.com/~sentience/AI_design.temp.html#PrimeDirective</a>

<p>
And READ it, because that's where the answers are.

<p>
<a href="2396.html#2398qlink2">&gt; In such a scenario, if you don't give an AI a</a><br>
<i>&gt; root alignment, what are you going to do to the AI's programming so that it can develop its</i><br>
<i>&gt; own root alignment?</i><br>

<p>
<a href="http://pobox.com/~sentience/AI_design.temp.html#det_igs">http://pobox.com/~sentience/AI_design.temp.html#det_igs</a>

<p>
&gt; It must have at least ONE primary goal.<br>

<p>
It doesn't have to be forced on it by the programmer. 
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2399.html">[ Next ]</a><a href="2397.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2396.html">Michael Lorrey</a>
<!-- nextthread="start" -->
</ul>
</body></html>
