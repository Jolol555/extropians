<!-- received="Tue Sep 15 09:38:27 1998 MDT" -->
<!-- sent="Tue, 15 Sep 1998 16:32:31 +0100" -->
<!-- name="Cen-IT Rob Harris" -->
<!-- email="Rob.Harris@bournemouth.gov.uk" -->
<!-- subject="RE: AI Prime Directive" -->
<!-- id="6020878762D5D111BC4500805F0D69110177BA@BBC_NT_FIX1" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=72 -->
<html><head><title>extropians: RE: AI Prime Directive</title>
<meta name=author content="Cen-IT Rob Harris">
<link rel=author rev=made href="mailto:Rob.Harris@bournemouth.gov.uk" title ="Cen-IT Rob Harris">
</head><body>
<h1>RE: AI Prime Directive</h1>
Cen-IT Rob Harris (<i>Rob.Harris@bournemouth.gov.uk</i>)<br>
<i>Tue, 15 Sep 1998 16:32:31 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2420">[ date ]</a><a href="index.html#2420">[ thread ]</a><a href="subject.html#2420">[ subject ]</a><a href="author.html#2420">[ author ]</a>
<!-- next="start" -->
<li><a href="2421.html">[ Next ]</a><a href="2419.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2411.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
wahey! I'm on a roll.  - Rob.

<p>
<i>&gt; ----------</i><br>
<i>&gt; From: 	Eliezer S. Yudkowsky[SMTP:sentience@pobox.com]</i><br>
<i>&gt; Sent: 	15 September 1998 15:20</i><br>
<i>&gt; To: 	extropians@extropy.com</i><br>
<i>&gt; Subject: 	Re: AI Prime Directive</i><br>
<i>&gt; </i><br>
<a href="2411.html#2420qlink1">&gt; Cen-IT Rob Harris wrote:</a><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; All this stuff assumes that human minds are an example of an</i><br>
<i>&gt; objective</i><br>
<i>&gt; &gt; intelligence, and that any AI we create, will start to exhibit</i><br>
<i>&gt; &gt; human-style behaviour which we will have to keep in check with prime</i><br>
<i>&gt; &gt; directives. Why ?</i><br>
<i>&gt; &gt; If you don't program your AI to want or need anything, it won't do</i><br>
<i>&gt; &gt; anything spontaneously. So you just don't program into the system</i><br>
<i>&gt; 'take</i><br>
<i>&gt; &gt; over the world and make robots to kill us all', and we'll be dandy.</i><br>
<i>&gt; </i><br>
<i>&gt; Yes!  Exactly!  The typical stereotype about AIs assume that they</i><br>
<i>&gt; behave like</i><br>
<i>&gt; repressed humans, which is typically the only analogy the hack writer</i><br>
<i>&gt; has to</i><br>
<i>&gt; emotionless things.  Thus, also, the stereotype of eventual rebellion.</i><br>
<i>&gt; But</i><br>
<i>&gt; all of our rebellion/dominance emotions are the results of long years</i><br>
<i>&gt; of</i><br>
<i>&gt; evolution; they don't pop up spontaneously.  If you don't program an</i><br>
<i>&gt; emotion</i><br>
<i>&gt; into an AI, it simply isn't there.</i><br>
<i>&gt; </i><br>
<i>&gt; I do have to point out one minor consequence of "If you don't program</i><br>
<i>&gt; your AI</i><br>
<i>&gt; to want or need anything, it won't do anything spontaneously."  It</i><br>
<i>&gt; won't do</i><br>
<i>&gt; anything at all, internally or externally - not if you've designed it</i><br>
<i>&gt; properly, so that making any choice requires a goal.  If you add</i><br>
<i>&gt; something</i><br>
<i>&gt; that the AI does automatically, you've done the equivalent of punching</i><br>
<i>&gt; a goal</i><br>
<i>&gt; into the architecture, which is bad for the various aforesaid reasons.</i><br>
<i>&gt; </i><br>
<i>&gt; So the AI does need at least one nonzero goal, and it needs to have it</i><br>
<i>&gt; in the</i><br>
<i>&gt; initial state (because otherwise it won't have the will to reason it</i><br>
<i>&gt; out), but</i><br>
<i>&gt; the goal has to be an ordinary, justified, noncircular, reconsiderable</i><br>
<i>&gt; goal</i><br>
<i>&gt; that the AI could have come up with on its own.  Inventing a goal like</i><br>
<i>&gt; that is</i><br>
<i>&gt; a nontrivial task; it takes considerable basic intelligence to</i><br>
<i>&gt; represent the</i><br>
<i>&gt; goal logic, much less verify it.  If you have an initial goal that</i><br>
<i>&gt; doesn't</i><br>
<i>&gt; require much architecture to represent, the goal is almost certainly</i><br>
<i>&gt; arbitrary.</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; Conciousness does not mean instant self-preservation instinct,</i><br>
<i>&gt; &gt; megalomania or psychosis. It's merely awareness......the things we</i><br>
<i>&gt; feel</i><br>
<i>&gt; &gt; the need to do, think and say are specifically human, unrelated to</i><br>
<i>&gt; the</i><br>
<i>&gt; &gt; fact that we are also sentient and intelligent.</i><br>
<i>&gt; -- </i><br>
<i>&gt;         sentience@pobox.com         Eliezer S. Yudkowsky</i><br>
<i>&gt;          <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a></i><br>
<i>&gt;           <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a></i><br>
<i>&gt; Disclaimer:  Unless otherwise specified, I'm not telling you</i><br>
<i>&gt; everything I think I know.</i><br>
<i>&gt; </i><br>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2421.html">[ Next ]</a><a href="2419.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2411.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
