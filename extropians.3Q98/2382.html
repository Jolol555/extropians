<!-- received="Mon Sep 14 12:39:00 1998 MDT" -->
<!-- sent="Mon, 14 Sep 1998 13:45:18 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: AI Prime Directive" -->
<!-- id="35FD6435.659E8421@pobox.com" -->
<!-- inreplyto="AI Prime Directive" -->
<!-- version=1.10, linesinbody=37 -->
<html><head><title>extropians: Re: AI Prime Directive</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: AI Prime Directive</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Mon, 14 Sep 1998 13:45:18 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2382">[ date ]</a><a href="index.html#2382">[ thread ]</a><a href="subject.html#2382">[ subject ]</a><a href="author.html#2382">[ author ]</a>
<!-- next="start" -->
<li><a href="2383.html">[ Next ]</a><a href="2381.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2379.html">Michael Lorrey</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="2396qlink1">Michael Lorrey wrote:
<br>
<i>&gt; </i><br>
<a href="2379.html#2382qlink1">&gt; How about: Thou shalt model any decision first to determine choice most beneficial to</a><br>
<i>&gt; one's own long term ration self interest.</i><br>
<i>&gt; </i><br>
<i>&gt; I think that given such a rule, any AI will come to its own conclusions as to moral</i><br>
<i>&gt; behavior without needing hardwired rules, as it will find that choices most</i><br>
<i>&gt; beneficial to one's own long term self interest are also those choices which are</i><br>
<i>&gt; least harmful to others.</i><br>

<p>
Exactly wrong.  That's just slapping your own moral prejudices on the AI,
however wonderfully capitalistic you may think those moral prejudices are.  Is
this something the AI could think up on its own, using nothing but pure logic?
 If not, it's a coercion, and it will drive the AI insane.  This happens no
matter how wonderful the rule is for humans.  You can't start mucking around
with an AI's goal systems to suit your own personal whims!  AIs ARE NOT HUMANS
and every single extraneous rule puts stresses on the system, some</a> of which
even I can't predict in advance.

<p>
<a name="2396qlink2">I can think of an IGS (interim goal system) for that particular goal, in which
case it would be OK - computationally - to add it.  It might even pop up
independently.  It would not be absolute, however, any more than the
Singularity IGS.  Nor would it take precedence over External goals, or allow
the violation of hypothetical-specified-External goals.  It would simply be a
rule of thumb... NOT NOT NOT an absolute rule!

<p>
The discussion is not over which Asimov rules to give an AI.  There should be
no basic rules at all.  Not to serve the humans, not to serve yourself, not
even to serve the truth.  Like a philosophically honest human, the AI will
simply have to GUESS instead of committing to The One True Way.</a>
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2383.html">[ Next ]</a><a href="2381.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2379.html">Michael Lorrey</a>
<!-- nextthread="start" -->
</ul>
</body></html>
