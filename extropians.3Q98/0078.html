<!-- received="Thu Jul  2 14:26:19 1998 MDT" -->
<!-- sent="Thu, 2 Jul 1998 15:24:05 -0500" -->
<!-- name="Scott Badger" -->
<!-- email="wbadger@psyberlink.net" -->
<!-- subject="Re: The AI revolution -Reply" -->
<!-- id="001a01bda5f7$5d814520$c59112cf@wbadger" -->
<!-- inreplyto="The AI revolution -Reply" -->
<!-- version=1.10, linesinbody=37 -->
<html><head><title>extropians: Re: The AI revolution -Reply</title>
<meta name=author content="Scott Badger">
<link rel=author rev=made href="mailto:wbadger@psyberlink.net" title ="Scott Badger">
</head><body>
<h1>Re: The AI revolution -Reply</h1>
Scott Badger (<i>wbadger@psyberlink.net</i>)<br>
<i>Thu, 2 Jul 1998 15:24:05 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#78">[ date ]</a><a href="index.html#78">[ thread ]</a><a href="subject.html#78">[ subject ]</a><a href="author.html#78">[ author ]</a>
<!-- next="start" -->
<li><a href="0079.html">[ Next ]</a><a href="0077.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0068.html">Douglas Whitworth</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->


<p>
Douglas Whitworth &lt;DouglasW@durban.gov.za&gt; wrote:


<p>
<a href="0068.html#0078qlink1">&gt;Hara Ra wrote</a><br>
<i>&gt;&gt;So, what if a robot has this choice:</i><br>
<i>&gt;</i><br>
<i>&gt; &gt;Kill someone, and allow 100 others to live, or</i><br>
<i>&gt; &gt;not kill, and allow the 100 others to die.</i><br>
<i>&gt;</i><br>
<i>&gt;&gt;This would probably immobilize the robot, which is the worst choice,</i><br>
<i>&gt;&gt;so the Zero'th Law is:</i><br>
<i>&gt;</i><br>
<i>&gt;&gt;0. A robot, when faced with a choice which results in harm,</i><br>
<i>&gt;  &gt; chooses the one resulting in the least harm.</i><br>
<i>&gt;</i><br>
<i>&gt;How would  such a robot react if required to save  2 very young children</i><br>
whose lives were in mortal danger( say they have fallen out of a window),
where the circumstances were such that only one child's life could be saved
by the robot.  Lets also assume the children were twins and that there was
no reason to favour one over the other, or that there were no other
characteristics known to the Robot that would allow it  to determine which
life would, potentially, be that most likely to be of most benefit to
<br>
<a href="0068.html#0078qlink2">&gt;humanity in the long run.  Could that not immobilize our robot ?</a><br>
<i>&gt;</i><br>
<i>&gt;Douglas</i><br>
<i>&gt;</i><br>

<p>
Easy.  If it follows the decision tree to a point where all future decisions
regarding the action to be taken have equal values, then it would be
programmed to make a random choice, unless there was time to gather relevant
data.  Humans aremore likely to become immobilized in such a situation
(fretting over the *right* choice).

<p>
S.B.
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0079.html">[ Next ]</a><a href="0077.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0068.html">Douglas Whitworth</a>
<!-- nextthread="start" -->
</ul>
</body></html>
