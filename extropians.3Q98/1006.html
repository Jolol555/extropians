<!-- received="Fri Jul 24 13:02:47 1998 MDT" -->
<!-- sent="Fri, 24 Jul 1998 15:00:48 -0400" -->
<!-- name="Doug Bailey" -->
<!-- email="Doug.Bailey@ey.com" -->
<!-- subject="Optimal Intelligent State?" -->
<!-- id="199807241902.PAA25516@gateway2.ey.com" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=51 -->
<html><head><title>extropians: Optimal Intelligent State?</title>
<meta name=author content="Doug Bailey">
<link rel=author rev=made href="mailto:Doug.Bailey@ey.com" title ="Doug Bailey">
</head><body>
<h1>Optimal Intelligent State?</h1>
Doug Bailey (<i>Doug.Bailey@ey.com</i>)<br>
<i>Fri, 24 Jul 1998 15:00:48 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1006">[ date ]</a><a href="index.html#1006">[ thread ]</a><a href="subject.html#1006">[ subject ]</a><a href="author.html#1006">[ author ]</a>
<!-- next="start" -->
<li><a href="1007.html">[ Next ]</a><a href="1005.html">[ Previous ]</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2184.html">Anders Sandberg</a>
</ul>
<!-- body="start" -->

<p>
As I continue to develop my thinking about various
transhuman and related subject areas, I've come across
an intuitive conclusion that I'd like to hear feedbback
on.

<p>
As intelligence and the societies/civilizations/structurecomplexes
 it creates become governed more by directed
intelligent evolution than natural evolution, it seems to
me that it will only make changes that increase its
overall "state".<a name="1066qlink2"> Assuming we evaluate the state of a
civilization by comparing it to the limits prescribed by
our current understanding of physical laws, it appears that
there should be an "optimal state" that intelligent systems
should tend towards. Regardless of the initial environment,
conditions, substrates, characteristics, etc. of 
civilizations, it seems that once they has harnessed the
</a>
power of directed evolution that they should tend to
strive for this "optimal state".

<p>
This observation raises some interesting questions:

<p>
1 - Does this have any implications concerning the Great Filter
concept, i.e., is there something about this gravitation
towards the "optimal state" which could explain the lack
of evidence of intelligence having filled the cosmos? For
example, perhaps the "singularity"-type acceleration 
towards this optimal state happens before a civilization
can expand significantly into its surrounding environment.
If the "optimal state" inhibits further expansion (for reasons
I can't fathom) that would explain why we don't see signs
of these civilizations.

<p>
2 - What insights can we draw into the nature of this
"optimal state" from our current knowledge of the natural
limits (e.g., the Planck Length, the speed of light, etc.)?

<p>
3 - What's the merit of evaluating optimality based on physical
limitations? What other possible criteria might there be?

<p>
4 - What would a civilization do once it reached the "optimal
state"? In concert with #1, would such a civilization "die"
of boredom having exhausted all the wonders of existence?

<p>
5 - Maybe its errant to think of the "optimal state" as an
evolutionary pinnacle and instead as an evolutionary
singularity. What could possibly lie on the other side of
an evolutionary singularity?

<p>
Doug Bailey
<br>
doug.bailey@ey.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1007.html">[ Next ]</a><a href="1005.html">[ Previous ]</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2184.html">Anders Sandberg</a>
</ul>
</body></html>
