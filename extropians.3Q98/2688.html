<!-- received="Sun Sep 27 13:39:25 1998 MDT" -->
<!-- sent="Sun, 27 Sep 1998 12:39:16 -0700 (PDT)" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@econ.berkeley.edu" -->
<!-- subject="Re: AI big wins" -->
<!-- id="v03020902b233d639621f@[136.152.63.65]" -->
<!-- inreplyto="360D9B18.A73A1E08@pobox.com" -->
<!-- version=1.10, linesinbody=35 -->
<html><head><title>extropians: Re: AI big wins</title>
<meta name=author content="Robin Hanson">
<link rel=author rev=made href="mailto:hanson@econ.berkeley.edu" title ="Robin Hanson">
</head><body>
<h1>Re: AI big wins</h1>
Robin Hanson (<i>hanson@econ.berkeley.edu</i>)<br>
<i>Sun, 27 Sep 1998 12:39:16 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2688">[ date ]</a><a href="index.html#2688">[ thread ]</a><a href="subject.html#2688">[ subject ]</a><a href="author.html#2688">[ author ]</a>
<!-- next="start" -->
<li><a href="2689.html">[ Next ]</a><a href="2687.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2653.html">Robin Hanson</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky writes:
<br>
<a href="2653.html#2688qlink1">&gt;&gt; As the AI progresses there are contrary forces working for and against</a><br>
<i>&gt;&gt; accelerated growth.  As the AI gets more optimized, it is able to implement</i><br>
<i>&gt;&gt; any one optimization idea in a shorter time.  It may also be able to</i><br>
<i>&gt;&gt;evaluate</i><br>
<i>&gt;&gt; each idea in a shorter time.  But working against this is the fact that the</i><br>
<i>&gt;&gt; AI will wisely first work on the easiest most likely to succeed ideas.  ..</i><br>
<i>&gt;</i><br>
<i>&gt;Now _that's_ the kind of argument I wanted to hear!  Thanks, Hanson.</i><br>

<p>
This was the argument I gave in my first comment on Vinge.

<p>
<i>&gt;....  You are dealing with fragmentary</i><br>
<i>&gt;increments, assuming that the AI's time to complete any one task is the</i><br>
<i>&gt;occurrence that happens on our time scale.  But I'm thinking in terms of a</i><br>
<i>&gt;series of AIs created by human programmers, and that the entire potential of a</i><br>
<i>&gt;given model of AI will be achieved in a run over the course of a few hours at</i><br>
<i>&gt;the most, or will bog down in a run that would take centuries to complete.</i><br>
<i>&gt;...  In either case, the programmer (or more likely the Manhattan</i><br>
<i>&gt;Project) sighs, sits down, tries to fiddle with O or I and add abilities, and</i><br>
<i>&gt;tries running the AI again. ... But what matters is not</i><br>
<i>&gt;the level it starts at, but the succession of levels, and when you "zoom out"</i><br>
<i>&gt;to that perspective, the key steps are likely to be changes to the fundamental</i><br>
<i>&gt;architecture, not optimization.</i><br>

<p>
The same argument seems to apply at this broader level.  The programmer has
a list of ideas for fundamental architecture changes, which vary in how
likely they are to succeed, how big a win they would be if they worked,
and how much trouble they are to implement.  The programmer naturally tries
the best ideas first.
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2689.html">[ Next ]</a><a href="2687.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2653.html">Robin Hanson</a>
<!-- nextthread="start" -->
</ul>
</body></html>
