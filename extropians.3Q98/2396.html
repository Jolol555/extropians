<!-- received="Mon Sep 14 16:41:12 1998 MDT" -->
<!-- sent="Mon, 14 Sep 1998 18:55:25 -0400" -->
<!-- name="Michael Lorrey" -->
<!-- email="retroman@together.net" -->
<!-- subject="Re: AI Prime Directive" -->
<!-- id="35FD9EDD.7B24540C@together.net" -->
<!-- inreplyto="AI Prime Directive" -->
<!-- version=1.10, linesinbody=64 -->
<html><head><title>extropians: Re: AI Prime Directive</title>
<meta name=author content="Michael Lorrey">
<link rel=author rev=made href="mailto:retroman@together.net" title ="Michael Lorrey">
</head><body>
<h1>Re: AI Prime Directive</h1>
Michael Lorrey (<i>retroman@together.net</i>)<br>
<i>Mon, 14 Sep 1998 18:55:25 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2396">[ date ]</a><a href="index.html#2396">[ thread ]</a><a href="subject.html#2396">[ subject ]</a><a href="author.html#2396">[ author ]</a>
<!-- next="start" -->
<li><a href="2397.html">[ Next ]</a><a href="2395.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2382.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->



<p>
Eliezer S. Yudkowsky wrote:

<p>
<a href="2382.html#2396qlink1">&gt; Michael Lorrey wrote:</a><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; How about: Thou shalt model any decision first to determine choice most beneficial to</i><br>
<i>&gt; &gt; one's own long term ration self interest.</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; I think that given such a rule, any AI will come to its own conclusions as to moral</i><br>
<i>&gt; &gt; behavior without needing hardwired rules, as it will find that choices most</i><br>
<i>&gt; &gt; beneficial to one's own long term self interest are also those choices which are</i><br>
<i>&gt; &gt; least harmful to others.</i><br>
<i>&gt;</i><br>
<i>&gt; Exactly wrong.  That's just slapping your own moral prejudices on the AI,</i><br>
<i>&gt; however wonderfully capitalistic you may think those moral prejudices are.  Is</i><br>
<i>&gt; this something the AI could think up on its own, using nothing but pure logic?</i><br>
<i>&gt;  If not, it's a coercion, and it will drive the AI insane.  This happens no</i><br>
<i>&gt; matter how wonderful the rule is for humans.  You can't start mucking around</i><br>
<i>&gt; with an AI's goal systems to suit your own personal whims!  AIs ARE NOT HUMANS</i><br>
<i>&gt; and every single extraneous rule puts stresses on the system, some of which</i><br>
<i>&gt; even I can't predict in advance.</i><br>

<p>
<a name="2398qlink1">I don't know if you understood what I was saying. It seems like you interpreted my proposed
statement in exactly the opposite way in which it was intended. As far as I can see,
telling the AI to model all possible decisions with the goal of reaching the best choice
for the AI's own best long term rational self interest does two things. a) it gives the AI
maximum freedom of choice, and b) if libertarian theory is correct, it also minimizes the
infringements upon others.</a>

<p>
<i>&gt;</i><br>
<i>&gt;</i><br>
<a href="2382.html#2396qlink2">&gt; I can think of an IGS (interim goal system) for that particular goal, in which</a><br>
<i>&gt; case it would be OK - computationally - to add it.  It might even pop up</i><br>
<i>&gt; independently.  It would not be absolute, however, any more than the</i><br>
<i>&gt; Singularity IGS.  Nor would it take precedence over External goals, or allow</i><br>
<i>&gt; the violation of hypothetical-specified-External goals.  It would simply be a</i><br>
<i>&gt; rule of thumb... NOT NOT NOT an absolute rule!</i><br>
<i>&gt;</i><br>
<i>&gt; The discussion is not over which Asimov rules to give an AI.  There should be</i><br>
<i>&gt; no basic rules at all.  Not to serve the humans, not to serve yourself, not</i><br>
<i>&gt; even to serve the truth.  Like a philosophically honest human, the AI will</i><br>
<i>&gt; simply have to GUESS instead of committing to The One True Way.</i><br>
<i>&gt;</i><br>

<p>
And how does one 'guess'? Doing a random stab? or modeling all possible outcomes? Which is
more 'intelligent'? A philosophically honest human IMHO, sees Looking Out for Number One as
synonymous in a cost-benefit analysis with Attaining Maximum Good for As Many As Possible
(though this is not necessarily commutative).<a name="2398qlink2"> In such a scenario, if you don't give an AI a
root alignment, what are you going to do to the AI's programming so that it can develop its
own root alignment?</a> It must have at least ONE primary goal. if you do not give an AI at
least one primary goal, all you will have accomplished is to create the cyber equivalent of
a 'trust fund baby', which IMHO is one of the least productive individuals in society, thus
making the point of having an AI worthless...

<p>
To paraphrase Heinlein: "An AI who has nothing worth scratching one's backup copy, has
nothing worth retaining an active copy on one's neural net"

<p>
The original:"One who has nothing worth dying for, also has nothing worth living for."

<p>
Mike Lorrey
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2397.html">[ Next ]</a><a href="2395.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2382.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
