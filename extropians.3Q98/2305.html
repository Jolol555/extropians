<!-- received="Fri Sep 11 13:41:06 1998 MDT" -->
<!-- sent="Fri, 11 Sep 1998 12:36:49 -0700" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@econ.berkeley.edu" -->
<!-- subject="Re: Singularity: Human AI to superhuman" -->
<!-- id="3.0.3.32.19980911123649.006f4ae0@econ.berkeley.edu" -->
<!-- inreplyto="001901bddd40$b1aa10c0$13934d0c@flrjs" -->
<!-- version=1.10, linesinbody=45 -->
<html><head><title>extropians: Re: Singularity: Human AI to superhuman</title>
<meta name=author content="Robin Hanson">
<link rel=author rev=made href="mailto:hanson@econ.berkeley.edu" title ="Robin Hanson">
</head><body>
<h1>Re: Singularity: Human AI to superhuman</h1>
Robin Hanson (<i>hanson@econ.berkeley.edu</i>)<br>
<i>Fri, 11 Sep 1998 12:36:49 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2305">[ date ]</a><a href="index.html#2305">[ thread ]</a><a href="subject.html#2305">[ subject ]</a><a href="author.html#2305">[ author ]</a>
<!-- next="start" -->
<li><a href="2306.html">[ Next ]</a><a href="2304.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2288.html">John Clark</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
John Clark writes:
<br>
<i>&gt;&gt;... AI progress ... suggesting the importance of lots of little</i><br>
<a href="2288.html#2305qlink1">&gt;&gt;insights which require years of reading and experience to accumulate.</a><br>
<i>&gt;</i><br>
<i>&gt;That's a good point but the question remains, even if a billion small</i><br>
<i>&gt;insights are needed what happens if their rate of discovery increases</i><br>
<i>&gt;astronomically? ... artificial intelligence is only one path toward the</i><br>
<i>&gt;singularity, another is Nanotechnology, perhaps another is Quantum</i><br>
<i>&gt;Computers, and you only need one path to go somewhere.</i><br>

<p>
<a name="2308qlink1">The basic problem with singularity discussions is that lots of people 
see big fast change coming, but few seem to agree on what that is or 
why they think that.  Discussion quickly fragment into an enumeration
of possiblities, and no one view is subject to enough critical analysis 
to really make progress.

<p>
I've tried to deal with this by focusing everyone's attention on the 
opinions of the one person most associated with the word "singularity."
But success has been limited, as many prefer to talk about their own
concept of and analysis in support of "singularity".</a>  

<p>
<a name="2308qlink2">In the above I was responding to Eliezer Yudkowsky's analysis, which
is based on his concept of a few big wins.  To respond to your question,
I'd have to hear your analysis of why we might see an astronomical
increase in the rate of insights.<a name="2318qlink1">  Right now, though, I'd really rather
draw folks' attention to Vinge's concept and analysis.  So I haven't 
responded to Nick Bostrom's nanotech/upload analysis, since Vinge 
explicitly disavows</a> it.  And I guess I should stop responding to Eliezer.
(Be happy to discuss your other singularity concepts in a few weeks.)</a>  

<p>
<a href="2288.html#2305qlink2">&gt;&gt;There has been *cultural* evolution, but cultural evolution is Lamarkian.</a><br>
<i>&gt;</i><br>
<i>&gt;Yes but the distinction between physical and cultural evolution would</i><br>
<i>&gt;evaporate for an AI, it would all be Lamarkian and that's why it would</i><br>
<i>&gt;change so fast. </i><br>

<p>
Well this suggests it would be *faster*, all else equal, but how fast and
is all else really equal, that's what's at issue.


<p>
Robin Hanson  
<br>
hanson@econ.berkeley.edu    <a href="http://hanson.berkeley.edu/">http://hanson.berkeley.edu/</a>   
RWJF Health Policy Scholar, Sch. of Public Health   510-643-1884  
140 Warren Hall, UC Berkeley, CA 94720-7360    FAX: 510-643-8614
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2306.html">[ Next ]</a><a href="2304.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2288.html">John Clark</a>
<!-- nextthread="start" -->
</ul>
</body></html>
