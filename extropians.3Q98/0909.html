<!-- received="Mon Jul 20 20:57:28 1998 MDT" -->
<!-- sent="Mon, 20 Jul 1998 22:56:51 -0400" -->
<!-- name="Dan Clemmensen" -->
<!-- email="Dan@Clemmensen.ShireNet.com" -->
<!-- subject="Re: The Singularity" -->
<!-- id="35B40373.1F622E10@clemmensen.shirenet.com" -->
<!-- inreplyto="Version.32.19980712023935.00e34780@mail.scruznet.com" -->
<!-- version=1.10, linesinbody=67 -->
<html><head><title>extropians: Re: The Singularity</title>
<meta name=author content="Dan Clemmensen">
<link rel=author rev=made href="mailto:Dan@Clemmensen.ShireNet.com" title ="Dan Clemmensen">
</head><body>
<h1>Re: The Singularity</h1>
Dan Clemmensen (<i>Dan@Clemmensen.ShireNet.com</i>)<br>
<i>Mon, 20 Jul 1998 22:56:51 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#909">[ date ]</a><a href="index.html#909">[ thread ]</a><a href="subject.html#909">[ subject ]</a><a href="author.html#909">[ author ]</a>
<!-- next="start" -->
<li><a href="0910.html">[ Next ]</a><a href="0908.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0897.html">Robin Hanson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0915.html">Eugene Leitl</a>
</ul>
<!-- body="start" -->

<p>
Robin Hanson wrote:
<br>
<i>&gt; </i><br>
<a href="0897.html#0909qlink1">&gt; Dan C. wrote:</a><br>
<i>&gt; &gt;&gt; &gt;Since the SI will be vastly more intelligent than humans, IMO we may not</i><br>
<i>&gt; &gt;&gt; &gt;be able to comprehend its motivations, much less predict them. The SI will</i><br>
<i>&gt; &gt;&gt; &gt;be so smart that its actions are constrained only by the laws of physics,</i><br>
<i>&gt; &gt;&gt; &gt;and it will choose a course of action based on its motivations.</i><br>
<i>&gt; &gt;&gt;</i><br>
<i>&gt; &gt;&gt; Why do you assume such a strong association between intelligence and</i><br>
<i>&gt; &gt;&gt; motivations?  It seems to me that intelligence doesn't change one's</i><br>
<i>&gt; &gt;&gt; primary purposes much at all, though it may change one's tactics as one</i><br>
<i>&gt; &gt;&gt; better learns the connection between actions and consequences.</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt;Human motivation is less complex than the motivations of ants?</i><br>
<i>&gt; </i><br>
<i>&gt; You lost me here.</i><br>

<p>
OOPS, I lost myself, also. I meant to say:
<p>
   Human motivation is the same as the motivations of ants?
I intended this to mean that I think that human motivations are a lot more
complicated than the motivation of ants, and that the motivations of an
SI will be proportionately more complex.

<p>
<i>&gt; </i><br>
<a href="0897.html#0909qlink2">&gt; &gt;Robin, the reason I produced the list of motivations and actions was</a><br>
<i>&gt; &gt;to attempt to provide specific examples. Can you reccomend a way for</i><br>
<i>&gt; &gt;me, or another human or group of humans or construct of humans (short of</i><br>
<i>&gt; &gt;an SI) to reliably assign probabilities to that list?</i><br>
<i>&gt; </i><br>
<i>&gt; Dan had written:</i><br>
<i>&gt; &gt;...</i><br>
<i>&gt; &gt;M: SI wants to maximize its power long-term</i><br>
<i>&gt; &gt;A: SI sends replicator probes to in all directions.</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt;M: SI wants to die.</i><br>
<i>&gt; &gt;A: SI terminates itself. ...</i><br>
<i>&gt; </i><br>
<i>&gt; It seems to me that the motivations of future entities can be predicted</i><br>
<i>&gt; as a combination of</i><br>
<i>&gt; 1) Selection effects.  What motivations would tend to be selected for</i><br>
<i>&gt;    in a given situation?</i><br>
I don't see this. Selection appplies to populations, and requires
<a name="0920qlink3">replication and mutation. It may be that there is a population of
SIs. I don't think so, but I could be wrong. However, its irrelevant, since
only one SI is important to us: the one generated in our singularity.
</a>

<p>
<a href="0897.html#0909qlink3">&gt; 2) Legacy motivations.  Descendants of current creatures will likely</a><br>
<i>&gt;    retain much of their motivations, translated to a new context.</i><br>
<i>&gt; </i><br>
<i>&gt; Wanting to die isn't favored by selection or legacy, except for certain</i><br>
<i>&gt; translation possibilities.   Spatial colonization will be selected for,</i><br>
<i>&gt; and has lots of legacy pushing for it as well.</i><br>
<i>&gt; </i><br>
<i>&gt; Increases in intelligence allow creatures to anticipate selection effects,</i><br>
<i>&gt; accelerating their effects of creatures who prefer to be selected.</i><br>
<i>&gt; Increases in intelligence also raise the possibilities of creatures</i><br>
<i>&gt; attempting to integrate their otherwise disparate legacy preferences</i><br>
<i>&gt; under simpler unifying principles.  Otherwise, I don't see how increases</i><br>
<i>&gt; in intelligence should lead us to expect much change in motivations.</i><br>
<i>&gt; </i><br>

<p>
<a name="0920qlink4">This whole line of reasoning neglects the fact that the SI has control of
its own motivations. The SI may choose to (i.e., be motivated to) retain
</a>
legacy motivations, but this is by no means certain, or even likely.
By my own rules, I can assign no probability to it either way :-)
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0910.html">[ Next ]</a><a href="0908.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0897.html">Robin Hanson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0915.html">Eugene Leitl</a>
</ul>
</body></html>
