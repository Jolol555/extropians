<!-- received="Mon Aug 17 09:27:31 1998 MDT" -->
<!-- sent="Mon, 17 Aug 1998 11:25:03 -0400" -->
<!-- name="Doug Bailey" -->
<!-- email="Doug.Bailey@ey.com" -->
<!-- subject="Evolving AIs and the term "Virus" (was "Who's Afraid of the SI?")" -->
<!-- id="35cd73e0.170798@eyllpwt003.ey.com" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=76 -->
<html><head><title>extropians: Evolving AIs and the term "Virus" (was "Who's Afraid of the SI?")</title>
<meta name=author content="Doug Bailey">
<link rel=author rev=made href="mailto:Doug.Bailey@ey.com" title ="Doug Bailey">
</head><body>
<h1>Evolving AIs and the term "Virus" (was "Who's Afraid of the SI?")</h1>
Doug Bailey (<i>Doug.Bailey@ey.com</i>)<br>
<i>Mon, 17 Aug 1998 11:25:03 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1596">[ date ]</a><a href="index.html#1596">[ thread ]</a><a href="subject.html#1596">[ subject ]</a><a href="author.html#1596">[ author ]</a>
<!-- next="start" -->
<li><a href="1597.html">[ Next ]</a><a href="1595.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1561.html">Bryan Moss</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Bryan Moss wrote:

<p>
<a name="1612qlink1"><a href="1561.html#1596qlink1">&gt; Firstly, it would not understand *our* "concept of</a><br>
<i>&gt; what a human is", I wouldn't be so arrogant as to</i><br>
<i>&gt; assume our concept is "the" concept (some</i><br>
<i>&gt; biological chauvinism on your part ;).</i><br>

<p>
Why couldn't it understand our concept of ourselves?
It does not have to adopt that concept for itself but
it could understand how we view ourselves.</a> Besides, it
<a name="1612qlink2">would not have to understand how we understand ourselves
to inflict harm upon us. We don't understand how dolphins
view themselves but we have certainly inflicted harm upon
them.</a> The scenario I provided in my previous posting
supplies a plausible scenario where this could occur. It
would not need to be an SI, just an AI.
 
<p>
<a href="1561.html#1596qlink2">&gt; &gt; The thought process an AI goes through to reach</a><br>
<i>&gt; &gt; its conclusions may be different (or it may be</i><br>
<i>&gt; &gt; the same) but it should be capable of reaching a</i><br>
<i>&gt; &gt; conclusion that led to any action a human might</i><br>
<i>&gt; &gt; conclude to take. Perhaps an AI's ultimate</i><br>
<i>&gt; &gt; objective is to maximize its information</i><br>
<i>&gt; &gt; processing rate. At some point its efforts to</i><br>
<i>&gt; &gt; reach that objective might conflict with the</i><br>
<i>&gt; &gt; goals of humans to operate their information</i><br>
<i>&gt; &gt; infrastructure. The AI might decide to</i><br>
<i>&gt; &gt; "commandeer" system resources at the expense of</i><br>
<i>&gt; &gt; human information processing demands.</i><br>
<i>&gt; </i><br>
<a name="1612qlink3"><i>&gt; Well an AI that's goal is to "maximise its</i><br>
<i>&gt; information processing rate" sounds like a virus</i><br>
<i>&gt; to me.</a> </i><br>

<p>
Sure, we'd view it as a virus. If B infringes on A's
resources to ensure the survival or continued evolution
of B at the expense of A, A is going to consider B a
virus. Many different A/B relationships exist: humanity and
the rest of the biosphere, Americans and Native Americans, 
You and a Trojan virus, You and cancer, Society and an evolving
AI. Just because we view it as a virus does not mean it
will not exist at some point, care about what we think, or
be responsive to our demands.

<p>
<a href="1561.html#1596qlink3">&gt; I can't see any use for such a thing beyond</a><br>
<i>&gt; a weapon.</i><br>

<p>
Such an AI might simply evolve into such a state where it
wishes to increase its internal complexity, computational
ability, etc. What if an AI encounters a problem, assigns
the problem a time horizon and learns that its present
computational capacity will not allow it to address the problem
before the time horizon expires? It might determine that
it could acquire additional capacity and then tackle the problem
before the time horizon expired. We can't assume every type
of AI that will exist in the future will be the ones we
"allow" to exist through creation. Human-level AIs (and even less
robust entities) will have some level of evolvability.

<p>
<a href="1561.html#1596qlink4">&gt; But AI or not, your</a><br>
<i>&gt; interface AI is not likely to `evolve' into a</i><br>
<i>&gt; virus.</i><br>

<p>
<a name="1612qlink4">As I've described above, "virus" is a perspective judgement.
Additionally, a completely benevolent AI could become a "virus"
through its own evolvability or simple intent to solve a problem.
Safeguards might be put in place to ensure that an AI
realizes the negative utlity of encroaching on human resources.
</a>
But how long before an AI evolves to the point that it
"reconsiders" such a premise and determines its need to solve
a particular problem or set of problems exceeds the value
of humanity's need for X amount of resources.

<p>
Doug
<br>
doug.bailey@ey.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1597.html">[ Next ]</a><a href="1595.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1561.html">Bryan Moss</a>
<!-- nextthread="start" -->
</ul>
</body></html>
