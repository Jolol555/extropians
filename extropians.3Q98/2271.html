<!-- received="Thu Sep 10 11:23:44 1998 MDT" -->
<!-- sent="Thu, 10 Sep 1998 10:19:36 -0700" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@econ.berkeley.edu" -->
<!-- subject="Singularity: Human AI to superhuman" -->
<!-- id="3.0.3.32.19980910101936.00b29c90@econ.berkeley.edu" -->
<!-- inreplyto="35F583D4.4CE0C0BA@pobox.com" -->
<!-- version=1.10, linesinbody=87 -->
<html><head><title>extropians: Singularity: Human AI to superhuman</title>
<meta name=author content="Robin Hanson">
<link rel=author rev=made href="mailto:hanson@econ.berkeley.edu" title ="Robin Hanson">
</head><body>
<h1>Singularity: Human AI to superhuman</h1>
Robin Hanson (<i>hanson@econ.berkeley.edu</i>)<br>
<i>Thu, 10 Sep 1998 10:19:36 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2271">[ date ]</a><a href="index.html#2271">[ thread ]</a><a href="subject.html#2271">[ subject ]</a><a href="author.html#2271">[ author ]</a>
<!-- next="start" -->
<li><a href="2272.html">[ Next ]</a><a href="2270.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2235.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2594.html">Eliezer S. Yudkowsky</a>
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky writes:
<br>
<a href="2235.html#2271qlink1">&gt;&gt; &gt;You can't draw conclusions from one system to the other.  ... </a><br>
<i>&gt;&gt; But where *do* you draw your conclusions from, if not by analogy with</i><br>
<i>&gt;&gt; other intelligence growth processes?  ...</i><br>
<i>&gt;</i><br>
<i>&gt;Basically, "I designed the thing and this is how I think it will work and this</i><br>
<i>&gt;is why."  ...  this is not a time for analogic reasoning.  ... A seed AI </i><br>
<i>&gt;trajectory consists of a series of sharp snaps and bottlenecks. ... either the</i><br>
<i>&gt;going is easy or the going is very hard ... this is the behavior exhibited by </i><br>
<i>&gt;all current AIs. ...</i><br>
<i>&gt;The history of AI seems to me to consist of a few big wins in a vast wasteland</i><br>
<i>&gt;of useless failures.  HEARSAY II, Marr's 2.5D vision, neural nets, Copycat,</i><br>
<i>&gt;EURISKO.  Sometimes you have a slow improvement in a particular field when the</i><br>
<i>&gt;principles are right but there just isn't enough computing power - voice</i><br>
<i>&gt;recognition, for example.  Otherwise:  Breakthroughs and bottlenecks.</i><br>

<p>
Sounds like you *are* making analogies, but with your impression of AI progress.

<p>
<a name="2288qlink1"><a name="2284qlink1">If most of AI progress comes from a half dozen big win insights, then you 
should be able to write them all down on one short page.  Anyone who read and 
understood that page would be nearly as good an AI programmer as anyone else.
This is very far from the case, suggesting the importance of lots of little
insights which require years of reading and experience to accumulate.</a></a> 

<p>
<a name="2284qlink2">You cite EURISKO as a big win, but ignore the big lesson its author, Lenat,
drew from it: that we've found most of the big wins, and progress now depends 
on collecting lots and lots of small knowledge chunks.  How can one read Lenat 
on CYC and get any other impression?</a>  

<p>
<a href="2235.html#2271qlink2">&gt;&gt; We humans have been improving ourselves in a great many ways for a long time.</a><br>
<i>&gt;</i><br>
<a name="2284qlink3"><i>&gt;There aren't any self-enhancing intelligences in Nature, ...  The [human]</i><br>
<i>&gt;structure is still a lot different.  What you have is humans being</i><br>
<i>&gt;optimized by evolution.  "A" being optimized by "B".  This is a lot different</i><br>
<i>&gt;than a seed AI, which is "C" being optimized by "C".  Even if humans take</i><br>
<i>&gt;control of genetics, "A" being optimized by "B" being optimized by "A" is</i><br>
<i>&gt;still vastly different from "C" being optimized by "C", in terms of trajectory.</i><br>

<p>
AIs *will* be subject to evolutionary selection, just as humans have been.
They will have "genes," things that code for their structure, and some 
structures will reproduce more, inducing more similar genes in future AIs.  
AI evolution may be Lamarkian, vs. Darwinian evolution for DNA, but evolution 
there will</a> be. 

<p>
<a href="2231.html#2271qlink3">&gt;&gt; [human] experience with intelligence growth seems highly relevant to me.</a><br>
<i>&gt;&gt; ... growth is mainly due to the accumulation of many small improvements. ...</i><br>
<i>&gt;</i><br>
<a name="2284qlink4"><i>&gt;With respect to human genetic evolution, I agree fully, but only for the past</i><br>
<i>&gt;50,000 years.  </i><br>

<p>
But<a name="2288qlink2"> there hasn't been much human genetic evolution over the last 50K years!</a>
<a name="2288qlink3">There has been *cultural* evolution, but cultural evolution is Lamarkian.</a> 
Cultures can choose to change what adults teach the young, and thereby change
themselves.  They are as "self-enhancing" as you like.</a> 

<p>
<a name="2284qlink5"><a href="2235.html#2271qlink4">&gt;On any larger scale, punctuated equilibrium seems to be the</a><br>
<i>&gt;rule; slow stability for eons, then a sudden leap.  The rise of the</i><br>
<i>&gt;Cro-Magnons was a very sharp event.  A fundamental breakthrough leads to a</i><br>
<i>&gt;series of big wins, after _that_ it's slow optimization until the next big </i><br>
<i>&gt;win opens up a new vista.  A series of breakthroughs and bottlenecks.  ...</i><br>

<p>
<a name="2288qlink4">There many have been breakthroughs within "short" times, though these times 
were only "short" when compared to a million years.</a>  But if so they seem to 
have been breakthroughs in the growth rates feasible, not in the absolute 
level.  The total number of Cro-Magnons didn't increase by a factor of one 
hundred in a few years, for example.</a>

<p>
<a name="2284qlink6"><a href="2235.html#2271qlink5">&gt;every technological aid to intelligence (such as writing or the printing press)</a><br>
<i>&gt;produces sharp decreases in time-scale - well, what godforsaken reason is</i><br>
<i>&gt;there to suppose that the trajectory will be slow and smooth?  It goes against</i><br>
<i>&gt;everything I know about complex systems.</i><br>

<p>
It seems you now *are* accepting analogies to the history of human intelligence 
growth.  And this history seems mainly to show only sharp transitions in growth
rates, not in levels.  See: <a href="http://hanson.berkeley.edu/longgrow.html">http://hanson.berkeley.edu/longgrow.html</a>  History
does suggest future increases in growth rates, but this is far from support for
the sudden "AI gods in a few weeks" scenarios proposed.  
</a>





<p>
Robin Hanson  
<br>
hanson@econ.berkeley.edu    <a href="http://hanson.berkeley.edu/">http://hanson.berkeley.edu/</a>   
RWJF Health Policy Scholar, Sch. of Public Health   510-643-1884  
140 Warren Hall, UC Berkeley, CA 94720-7360    FAX: 510-643-8614
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2272.html">[ Next ]</a><a href="2270.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2235.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2594.html">Eliezer S. Yudkowsky</a>
</ul>
</body></html>
