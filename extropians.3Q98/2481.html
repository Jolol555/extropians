<!-- received="Fri Sep 18 11:37:36 1998 MDT" -->
<!-- sent="Fri, 18 Sep 1998 10:33:11 -0700" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@econ.berkeley.edu" -->
<!-- subject="Singularity: Human AI to superhuman" -->
<!-- id="3.0.3.32.19980918103311.0070ac8c@econ.berkeley.edu" -->
<!-- inreplyto="36018DDE.27F7AFC@pobox.com" -->
<!-- version=1.10, linesinbody=77 -->
<html><head><title>extropians: Singularity: Human AI to superhuman</title>
<meta name=author content="Robin Hanson">
<link rel=author rev=made href="mailto:hanson@econ.berkeley.edu" title ="Robin Hanson">
</head><body>
<h1>Singularity: Human AI to superhuman</h1>
Robin Hanson (<i>hanson@econ.berkeley.edu</i>)<br>
<i>Fri, 18 Sep 1998 10:33:11 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2481">[ date ]</a><a href="index.html#2481">[ thread ]</a><a href="subject.html#2481">[ subject ]</a><a href="author.html#2481">[ author ]</a>
<!-- next="start" -->
<li><a href="2482.html">[ Next ]</a><a href="2480.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2475.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2544.html">Peter C. McCluskey</a>
</ul>
<!-- body="start" -->

<p>
<a href="2475.html#2481qlink1">&gt;&gt; Eliezer S. Yudkowsky seems to be the only person here willing to defend</a><br>
<i>&gt;                                                   ^^^^</i><br>
<i>&gt;I object to the implication that I'm fighting a lone cause.  </i><br>

<p>
I meant no such implication.

<p>
<a name="2544qlink1"><a href="2475.html#2481qlink2">&gt;There are many others, not-surprisingly including AIers (like</a><br>
<i>&gt;Moravec), and nanotechnologists (like Drexler), and others who actually deal</i><br>
<i>&gt;in the technology of transcendence, who are also Strong Singularitarians.  In</i><br>
<i>&gt;fact, I can't think offhand of a major SI-technologist who's heard of the</i><br>
<i>&gt;Strong Singularity but prefers the Soft.</i><br>

<p>
Is it clear they mean the same thing by "strong singularity"?  And if so many 
people agree, why can I find no published (i.e., subject to editorial review)
coherent analysis in favor of explosive growth?</a>  

<p>
<a href="2475.html#2481qlink3">&gt;&gt; "explosive growth," by which I mean sudden very rapid world economic growth.</a><br>
<i>&gt;</i><br>
<i>&gt;I don't know or care about "very rapid world economic growth".  I think that</i><br>
<i>&gt;specific, concrete things will happen, i.e. the creation of superintelligence</i><br>
<i>&gt;operating at a billion times a human's raw power, ...</i><br>

<p>
But it's not clear what "X times human power" means.  We already have machines 
this much faster than humans at arithmetic.  It seems to me that the important
measure of intelligence is its ability to solve real problems, and the natural
measure of this is the income such thinkers can command.  If you reject this,
you need to propose some substitute.

<p>
Even if you do this, there is also the question of *when* you say this will 
happen.  Most people would accept it may happen within a billion
years, but you seem to be saying more.  Economic growth rates seem to me a 
natural "when" measure, but if you reject this, you need a substitute. 

<p>
<a href="2475.html#2481qlink4">&gt;&gt; So I'd like to see what his best argument is for this.</a><br>
<i>&gt;</i><br>
<i>&gt;Arguments AGAINST a Singularity generally use the following assumptions:</i><br>

<p>
Until the claim is clarified, and at least one argument offered for it, 
there is no need to consider arguments against it. 

<p>
<a href="2475.html#2481qlink5">&gt;Arguments FOR a Singularity usually require some, but not all, of the</a><br>
<i>&gt;following assumptions: ...</i><br>
<i>&gt;* There exists at least one fast-infrastructure technology, such as</i><br>
<i>&gt;nanotechnology, which is capable of manufacturing massive computer power in a</i><br>
<i>&gt;short time.</i><br>
<i>&gt;* Intelligence above a certain level can rapidly improve itself or design a</i><br>
<i>&gt;successor, using self-sustaining increments of optimization or fast infrastructure.</i><br>
<i>&gt;... (Many of these arguments are technical, rather than philosophical, which is as</i><br>
<i>&gt;it should be.)</i><br>

<p>
To make a technical argument, you need to make<a name="2487qlink1"> "short time", "rapidly", and 
</a>
"self-sustaining" more precise.  

<p>
<a href="2475.html#2481qlink6">&gt;&gt; If no analogies are relevant, and so this is all theory driven, can the theory</a><br>
<i>&gt;&gt; be stated concisely?  If not, where did you get the theory you use?  Does</i><br>
<i>&gt;&gt; anyone else use it, and what can be its empirical support, if analogies are</i><br>
<i>&gt;&gt; irrelevant?</i><br>
<i>&gt;</i><br>
<i>&gt;The _hypothesis_ can be stated concisely:  Transhuman intelligence will move</i><br>
<i>&gt;through a very fast trajectory from O(5X) human intelligence to nanotechnology</i><br>
<i>&gt;and billions of times human intelligence.  The valid reasons for believing</i><br>
<i>&gt;this to be the most probable hypothesis are accessible only through technical</i><br>
<i>&gt;knowledge, as is usually the case.</i><br>

<p>
I think you will find I have sufficient technical background to understand 
whatever reasons you may offer.  I have skimmed through a lot of your web pages,
including <a href="http://www.tezcat.com/~eliezer/AI_design.temp.html">http://www.tezcat.com/~eliezer/AI_design.temp.html</a> , but find the
closest you get to an analysis of times/speeds is in <a href="http://www.tezcat.com/~eliezer/singularity.html">http://www.tezcat.com/~eliezer/singularity.html</a> which I will respond to in my
next post.
 


<p>
Robin Hanson  
<br>
hanson@econ.berkeley.edu    <a href="http://hanson.berkeley.edu/">http://hanson.berkeley.edu/</a>   
RWJF Health Policy Scholar, Sch. of Public Health   510-643-1884  
140 Warren Hall, UC Berkeley, CA 94720-7360    FAX: 510-643-8614
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2482.html">[ Next ]</a><a href="2480.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2475.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2544.html">Peter C. McCluskey</a>
</ul>
</body></html>
