<!-- received="Mon Sep  7 15:39:32 1998 MDT" -->
<!-- sent="Mon, 07 Sep 1998 14:35:09 -0700" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@econ.berkeley.edu" -->
<!-- subject="Singularity - Clarifying Timing Claims" -->
<!-- id="3.0.3.32.19980907143509.006fcc68@econ.berkeley.edu" -->
<!-- inreplyto="3.0.3.32.19980907110059.00c61974@econ.berkeley.edu" -->
<!-- version=1.10, linesinbody=99 -->
<html><head><title>extropians: Singularity - Clarifying Timing Claims</title>
<meta name=author content="Robin Hanson">
<link rel=author rev=made href="mailto:hanson@econ.berkeley.edu" title ="Robin Hanson">
</head><body>
<h1>Singularity - Clarifying Timing Claims</h1>
Robin Hanson (<i>hanson@econ.berkeley.edu</i>)<br>
<i>Mon, 07 Sep 1998 14:35:09 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2201">[ date ]</a><a href="index.html#2201">[ thread ]</a><a href="subject.html#2201">[ subject ]</a><a href="author.html#2201">[ author ]</a>
<!-- next="start" -->
<li><a href="2202.html">[ Next ]</a><a href="2200.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2189.html">Robin Hanson</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
[In this post, I wear my participant hat. RH]<br>

<p>
Vinge has mostly satisfied me that he does not literally mean that 
post-singularity things are "unknowable," though he for some reason
still likes to use that word.  It is hard to object much to his
refined descriptor, "qualitatively less understandable," and I
grant the possibility of "styles of cognition possible to higher 
technologies that are radically different from ours."   Vinge even 
acknowledges that future self-aware modules may not be much larger 
than we are.  

<p>
Given this, let me focus on what I think is now the central issue:
timing.  

<p>
I think that most thoughtful people will acknowledge that it is hard
to place limits on what our descendants will accomplish if substantial
numbers of them survive for another billion years, and continue to 
attempt to improve their technology over this time.  This implies both 
that super-human artificial intelligences (AIs) may well eventually 
appear, and that we now may find it very difficult to envision what 
they and their world will evolve into. 

<p>
In contrast, Vinge's "singularity" concept seems to me to claim much 
more, so much more that it becomes a radical claim few people endorse.  
It claims that these super-human AIs will appear within the next few 
decades, and then within a few years transform themselves and their 
world into something very difficult for us to imagine.  The essential 
difference between this view and the widely acceptable view I describe 
above is one of timing.  What most people acknowledge to be possible 
over a billion years, Vinge suggests is likely to transpire within just
a few years.  

<p>
Now Vinge does acknowledge the possibility of a slower transition, and
this does serve to signal that Vinge is a reasonable person.  But it
does not substitute for an analysis in favor of his radical claim.  

<p>
<a name="2206qlink1">Max More and I both took issue with Vinge's timing claim, but Vinge
just refers Max More to his reply to Nick Bostrom, which</a> is:

<p>
  To me, what Nick calls "Verticality" is just a plausible side effect of
  the creation of superintelligence. This is mostly by analogy with past
  progress:
<br>
   o Before humankind, the kingdom of life evolved by natural
<pre>
     selection -- a kind of "simulation" which proceeded as one with
     the real world. 
   o Humans have the ability to run simulations internally (far more
     so than other animals). Adaptation is several orders of magnitude
     faster than in the previous phase.
   o We humans now are developing devices which can run simulations
     faster than our internal, biological "hardware" can do. I think
     it's plausible that the accompanying speedup will have the
     appearance of "Verticality" over the human phase.

</pre>
<p>
Vinge seems to be just rephrasing this paragraph from his original paper:  

<p>
  ... When greater-than-human
<br>
  intelligence drives progress, that progress will be much more rapid.
  In fact, there seems no reason why progress itself would not involve
  the creation of still more intelligent entities -- on a still-shorter
  time scale. The best analogy that I see is with the evolutionary past:
  Animals can adapt to problems and make inventions, but often no faster
  than natural selection can do its work -- the world acts as its own
  simulator in the case of natural selection. We humans have the ability
  to internalize the world and conduct "what if's" in our heads; we can
  solve many problems thousands of times faster than natural selection.
  Now, by creating the means to execute those simulations at much higher
  speeds, we are entering a regime as radically different from our human
  past as we humans are from the lower animals.

<p>
<a name="2233qlink2"><a name="2206qlink2">Before this discussion can proceed further, I think we need to get clear
on what exactly Vinge is claiming in these two passages.  I'd be most 
interested in how others translate these, but my reading is:

<p>
 "Progress" rates increase with the speed of the processors involved.</a>

<p>
<a name="2206qlink3">Now it's not clear what "progress" metrics are valid here, but if 
economists' usual measures are valid, such as growth rate in world 
product, there are two immediate problems with this theory:


<OL>
  <li>  Progress rates increased greatly over the last hundred thousand 
   years until a century ago without any change in the cycle speed of 
</a></a>
   the processors involved.
<a name="2206qlink4">  <li>  Computer processor speeds have increased greatly over the last 
   century without much increase in rates of progress.</a>  


</OL>
<p>
(Furthermore, even if we accept this paraphrased claim, it is not 
clear that we must accept Vinge's timing claims.  For this, I think 
we'd need do say more about just how rapidly progress increases with 
processor speed.)





<p>
Robin Hanson  
<br>
hanson@econ.berkeley.edu    <a href="http://hanson.berkeley.edu/">http://hanson.berkeley.edu/</a>   
RWJF Health Policy Scholar, Sch. of Public Health   510-643-1884  
140 Warren Hall, UC Berkeley, CA 94720-7360    FAX: 510-643-8614
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2202.html">[ Next ]</a><a href="2200.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2189.html">Robin Hanson</a>
<!-- nextthread="start" -->
</ul>
</body></html>
