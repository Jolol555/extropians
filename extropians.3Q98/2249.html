<!-- received="Wed Sep  9 04:18:27 1998 MDT" -->
<!-- sent="Wed, 9 Sep 1998 12:09:10 +0200" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: Singularity, Breaker of Dreams" -->
<!-- id="199809091018.DAA11320@geocities.com" -->
<!-- inreplyto="Singularity, Breaker of Dreams" -->
<!-- version=1.10, linesinbody=172 -->
<html><head><title>extropians: Re: Singularity, Breaker of Dreams</title>
<meta name=author content="den Otter">
<link rel=author rev=made href="mailto:neosapient@geocities.com" title ="den Otter">
</head><body>
<h1>Re: Singularity, Breaker of Dreams</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Wed, 9 Sep 1998 12:09:10 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2249">[ date ]</a><a href="index.html#2249">[ thread ]</a><a href="subject.html#2249">[ subject ]</a><a href="author.html#2249">[ author ]</a>
<!-- next="start" -->
<li><a href="2250.html">[ Next ]</a><a href="2248.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2229.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->



<hr>
<br>
&gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;<br>

<p>
<a href="2229.html#2249qlink1">&gt; den Otter wrote:</a><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; ----------</i><br>
<i>&gt; &gt; &gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; &gt; Sooner or later, some generation will face the choice</i><br>
<i>&gt; &gt; &gt; between Singularity and extinction.  Why push it off, even if we</i><br>
<i>&gt; &gt; &gt; could?  And besides, we might not die at all.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; But we can't rely on that, can we?</i><br>
<i>&gt; </i><br>
<i>&gt; Depends on what you mean by "rely".  If you mean, "Can we assume that the</i><br>
<i>&gt; probability is 90%?", the answer is "No".  If you mean, "Can we behave as if</i><br>
<i>&gt; the probability is 90%?", the answer is "Yes".  Our world is dynamically</i><br>
<i>&gt; unstable, and is being acted on by powerful forces and positive feedbacks</i><br>
<i>&gt; which serve to destabilize it further.  Under the circumstances, the only</i><br>
<i>&gt; decision we can make effectively is whether we'll die in nuclear war or</i><br>
<i>&gt; nanowar, or whether a Singularity will occur.  These are the two stable</i><br>
<i>&gt; states, and the Universe is filled with stable things.</i><br>

<p>
The question is: how likely is it that there will be a world-wide disaster 
in the few extra years/months that it presumably takes to create SI 
from upgraded/uploaded humans (as opposed to creating SI from AI).
This must then be weighed against the possibility of (super) AI 
terminating our existence. If you include the possibility of moving to
space (surely quite feasible in a decade or two), then the scales tip
towards waiting with SI until humans can be uplifted, IMO. Of course,
this assumes that AI will be easier than uplifting, which is very likely
but not completely certain.

<p>
<a href="2229.html#2249qlink2">&gt; The forces inside a Singularity are powerful, complex, and far more dependent</a><br>
<i>&gt; on external factors then the initial conditions.  To the extent that initial</i><br>
<i>&gt; conditions do have effect, they must use unstable forms of insanity to shield</i><br>
<i>&gt; the AI from the external truth.  In short, trying to control the Singularity</i><br>
<i>&gt; would result in a world scoured bare and THEN Transcenscion.  While I might</i><br>
<i>&gt; find this outcome acceptable, I don't think anyone else would - and even from</i><br>
<i>&gt; my perspective it's too dangerous; what if the Blight scours the Earth bare</i><br>
<i>&gt; and then commits suicide?</i><br>

<p>
So, if I understand you correctly, you would have peace with your own demise
as long as your "brainchild", the Blight, lives? That's...traditional. Personally,
I wouldn't be even satisfied with an uploaded copy of myself to continue
"my" existence, let alone some completely alien form of life.

<p>
<a href="2229.html#2249qlink3">&gt; I couldn't command the future even if I had the complete source code of a</a><br>
<i>&gt; Macintosh-compatible seed AI in front of me right now.  Choose between</i><br>
<i>&gt; pre-existing possibilities, perhaps, but not add possibilities that weren't</i><br>
<i>&gt; there before.</i><br>

<p>
Of course, if you use AI to cause a Singularity you have effectively
handed over control. Enhancing yourself (with chip implants, for
example) to the point of SI (or at least significantly increased 
intelligence) would on the other hand give you a fair amount of
control over things to come. No guarantees, but certainly a
fighting chance.
 
<p>
<a href="2229.html#2249qlink4">&gt; &gt; The facts are simple:</a><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; 1) at this time, only a handful of people really grasp the enormity</i><br>
<i>&gt; &gt; of the coming changes, and (almost certainly) most of them are</i><br>
<i>&gt; &gt; transhumanists (unfortunately, even in this select group many</i><br>
<i>&gt; &gt; can't/won't understand the consequences of a Singularity, but</i><br>
<i>&gt; &gt; that aside).</i><br>
<i>&gt; </i><br>
<i>&gt; Sounds a bit elitist to me.  </i><br>

<p>
The pot and the kettle? Yes, it's elitist in the sense that only a small
group of people (will) see the Singularity coming, and even less will
try to "surf its wave" (instead of trying to run away and getting washed
over). On the other hand it isn't elitist at all since everyone is free to
join this group of survivalists. I'm not exculding anyone, on the contrary:
I've told hunderds of people (via the web) about the things to come,
but (almost) no-one would listen.

<p>
&gt; Are you sure *you're* one of the Chosen?<br>

<p>
I have chosen myself, and so can you or anyone else. I
know that Max for example has chosen himself too, so
at least I'm in good company. ;-) At the moment it may 
mean little, but at least it's a beginning. I accept that
the most probable outcome is failure (and death), but
since there is nothing to lose, I might as well go for it
(the same reason why I've signed up for suspension).

<p>
<a href="2229.html#2249qlink5">&gt; Seriously, my perspective doesn't really allow for dividing humanity into</a><br>
<i>&gt; groups like that.  </i><br>

<p>
I'm quite sure most of humanity will never know what hit them
(some will still be in the stone age when it happens). Most
people seem to even be in denial with regard to Y2K, which is
*nothing* compared to the Singularity. A few people see it coming, 
some might even try to influence the event, but most won't do
anything. Humanity is already devided. This is no personal 
preference, but a fact.

<p>
<a href="2229.html#2249qlink6">&gt; Where thinking is concerned, you've got rocks, mortals, and</a><br>
<i>&gt; Post-Singularity Entities.  Sorting mortals by intelligence is as silly as</i><br>
<i>&gt; separating rocks or PSEs.</i><br>

<p>
As in nature, intelligence (in this case mainly foresight) will be the
great discriminator. The "best adapted" mortals will become their
own successors, the PSEs. The fate of the rest is very uncertain.
 
<p>
<a href="2229.html#2249qlink7">&gt; &gt; 2) This gives us a *huge* edge, further increased by the high</a><br>
<i>&gt; &gt; concentration of scientific/technological talent in the &gt;H community.</i><br>
<i>&gt; </i><br>
<i>&gt; No, it doesn't.  We ain't got no money.  We ain't got no power. </i><br>

<p>
Correct. A major flaw, which deserves more attention than any other.
Money=power, money=life. It is time everybody woke up and 
realized that without money, all our future dreams are just hot air.

<pre>
&gt;_Zyvex_ might
</pre>
<br>
<a href="2229.html#2249qlink8">&gt; be said to have an edge because it's doing work in nanotechnology.  The MIT</a><br>
<i>&gt; labs might be said to have an edge.  If you're really generous, I could be</i><br>
<i>&gt; said to have an edge because of "Coding A Transhuman AI" or "Algernon's Law". </i><br>
<i>&gt; What I'm trying to convey is that each individual has ver own "edge".  Not</i><br>
<i>&gt; only that, but I think that if all the Extropians worked together it would</i><br>
<i>&gt; simply slow things down.</i><br>

<p>
Individual edges are small, and useless if you don't see the big picture. To
successfully create SI *from humans*, broad co-operation is needed.
Besides, I don't trust Zyvex or MIT *with my life*, and neither should
you. I want to be there when SI is born, nothing less. Co-operation,
a "Singularity Club", would increase the personal chances of survival
of all people involved. It is the rational thing to do for the egoist and
altruist alike. 
 
<p>
<a href="2229.html#2249qlink9">&gt; &gt; 3) If we start preparing now, by keeping a close eye on the</a><br>
<i>&gt; &gt; the development of technologies that could play a major part</i><br>
<i>&gt; &gt; in the Singularity (nanotech, AI, implants, intelligence</i><br>
<i>&gt; &gt; augmentation, human-machine interfaces in general etc.)</i><br>
<i>&gt; &gt; and by aquiring wealth (by any effective means) to set up a</i><br>
<i>&gt; &gt; SI research facility, then we have a real chance of success.</i><br>
<i>&gt; </i><br>
<i>&gt; Go ahead.  Don't let me stop you.</i><br>

<p>
If I could do it by all myself, I wouldn't be wasting my time on this
list, would I? ;-)
 
<p>
<a href="2229.html#2249qlink10">&gt; &gt; Note: the goal should (obviously) be creating SI by "uplifting"</a><br>
<i>&gt; &gt; (gradually uploading and augmenting) humans, *not* from AIs.</i><br>
<i>&gt; </i><br>
<i>&gt; Too damn slow.  Turnaround time on neurological enhancement is probably at</i><br>
<i>&gt; least a decade and probably more - for real effectiveness, you have to start</i><br>
<i>&gt; in infancy.  I can't rely on the world surviving that long.</i><br>

<p>
<a href="2229.html#2249qlink11">&gt; Also, I trust humans even less than I trust AIs.</a><br>

<p>
At least you know this devil (humans). The only (rational) reason
why we would want to have the Singularity asap is our innate mortality. If 
science can find a way around that without the help of SI, then there is
much less of a rush. You can move to space to keep safe from WW3
and like mishaps, and work on the slower but more reliable 
SI-from-humans-approach. Everyone keeps a close eye on the others,
and when the technology is ready, everyone uploads and gets 
augmented at the same time. Then...all bets are off.

<p>
<a href="2229.html#2249qlink12">&gt; Also, the first-stage neurological transhumans will just create AIs, since</a><br>
<i>&gt; it's easier to design a new mind than untangle an evolved one.</i><br>

<p>
The main question should not be "what's easier", but "what's safer". You
may trust AI more than humans (I can get into that), but do you trust AI 
more than *yourself*? 
 
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2250.html">[ Next ]</a><a href="2248.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2229.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
