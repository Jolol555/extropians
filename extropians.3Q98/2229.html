<!-- received="Tue Sep  8 12:16:21 1998 MDT" -->
<!-- sent="Tue, 08 Sep 1998 13:14:10 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Singularity, Breaker of Dreams" -->
<!-- id="35F573EF.D4A51379@pobox.com" -->
<!-- inreplyto="Singularity, Breaker of Dreams" -->
<!-- version=1.10, linesinbody=103 -->
<html><head><title>extropians: Re: Singularity, Breaker of Dreams</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Singularity, Breaker of Dreams</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Tue, 08 Sep 1998 13:14:10 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2229">[ date ]</a><a href="index.html#2229">[ thread ]</a><a href="subject.html#2229">[ subject ]</a><a href="author.html#2229">[ author ]</a>
<!-- next="start" -->
<li><a href="2230.html">[ Next ]</a><a href="2228.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2177.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="2249qlink1">den Otter wrote:
<br>
<i>&gt; </i><br>
<i>&gt; ----------</i><br>
<a href="2177.html#2229qlink1">&gt; &gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</a><br>
<i>&gt; </i><br>
<i>&gt; &gt; Sooner or later, some generation will face the choice</i><br>
<i>&gt; &gt; between Singularity and extinction.  Why push it off, even if we</i><br>
<i>&gt; &gt; could?  And besides, we might not die at all.</i><br>
<i>&gt; </i><br>
<i>&gt; But we can't rely on that, can we?</i><br>

<p>
Depends on what you mean by "rely".  If you mean, "Can we assume that the
probability is 90%?", the answer is "No".  If you mean, "Can we behave as if
the probability is 90%?", the answer is "Yes".  Our world is dynamically
unstable, and is being acted on by powerful forces and positive feedbacks
which serve to destabilize it further.  Under the circumstances, the only
decision we can make effectively is whether we'll die in nuclear war or
nanowar, or whether a Singularity will occur.  These are the two stable
states, and the Universe is filled with stable things.</a>

<p>
<a name="2249qlink2">The forces inside a Singularity are powerful, complex, and far more dependent
on external factors then the initial conditions.  To the extent that initial
conditions do have effect, they must use unstable forms of insanity to shield
the AI from the external truth.  In short, trying to control the Singularity
would result in a world scoured bare and THEN Transcenscion.  While I might
find this outcome acceptable, I don't think anyone else would - and even from
my perspective it's too dangerous; what if the Blight scours the Earth bare
and then commits suicide?</a>

<p>
<a href="2177.html#2229qlink2">&gt; This isn't completely true. We *can* command the future if we finish</a><br>
<i>&gt; the dash for SI in first place. If you lose yourself in defeatism you're</i><br>
<i>&gt; already as good as dead.</i><br>

<p>
<a name="2249qlink3">I couldn't command the future even if I had the complete source code of a
Macintosh-compatible seed AI in front of me right now.  Choose between
pre-existing possibilities, perhaps, but not add possibilities that weren't
there before.</a>

<p>
<a name="2249qlink4"><i>&gt; The facts are simple:</i><br>
<i>&gt; </i><br>
<a href="2177.html#2229qlink3">&gt; 1) at this time, only a handful of people really grasp the enormity</a><br>
<i>&gt; of the coming changes, and (almost certainly) most of them are</i><br>
<i>&gt; transhumanists (unfortunately, even in this select group many</i><br>
<i>&gt; can't/won't understand the consequences of a Singularity, but</i><br>
<i>&gt; that aside).</i><br>

<p>
Sounds a bit elitist to me.</a>  Are you sure *you're* one of the Chosen?

<p>
<a name="2249qlink5">Seriously, my perspective doesn't really allow for dividing humanity into
<a name="2249qlink6">groups like</a> that.  Where thinking is concerned, you've got rocks, mortals, and
Post-Singularity Entities.  Sorting mortals by intelligence is as silly as
separating rocks or PSEs.</a>

<p>
<a name="2249qlink7"><a href="2177.html#2229qlink4">&gt; 2) This gives us a *huge* edge, further increased by the high</a><br>
<i>&gt; concentration of scientific/technological talent in the &gt;H community.</i><br>

<p>
No, it doesn't.  We ain't got no money.  We ain't got no power.  _Zyvex_ might
</a>
<a name="2249qlink8">be said to have an edge because it's doing work in nanotechnology.  The MIT
labs might be said to have an edge.  If you're really generous, I could be
said to have an edge because of "Coding A Transhuman AI" or "Algernon's Law". 
What I'm trying to convey is that each individual has ver own "edge".  Not
only that, but I think that if all the Extropians worked together it would
simply slow things down.</a>

<p>
<a name="2249qlink9"><a href="2177.html#2229qlink5">&gt; 3) If we start preparing now, by keeping a close eye on the</a><br>
<i>&gt; the development of technologies that could play a major part</i><br>
<i>&gt; in the Singularity (nanotech, AI, implants, intelligence</i><br>
<i>&gt; augmentation, human-machine interfaces in general etc.)</i><br>
<i>&gt; and by aquiring wealth (by any effective means) to set up a</i><br>
<i>&gt; SI research facility, then we have a real chance of success.</i><br>

<p>
Go ahead.  Don't let me stop</a> you.

<p>
<a name="2249qlink10"><a href="2177.html#2229qlink6">&gt; Note: the goal should (obviously) be creating SI by "uplifting"</a><br>
<i>&gt; (gradually uploading and augmenting) humans, *not* from AIs.</i><br>

<p>
Too damn slow.  Turnaround time on neurological enhancement is probably at
least a decade and probably more - for real effectiveness, you have to start
in infancy.  I can't rely on the world surviving that long.</a>

<p>
<a name="2249qlink11">Also, I trust humans even less than I trust AIs.</a>

<p>
<a name="2256qlink1"><a name="2249qlink12">Also, the first-stage neurological transhumans will just create AIs, since
it's easier to design a new mind than untangle an evolved one.</a></a>

<p>
<a href="2177.html#2229qlink7">&gt; &gt; To the</a><br>
<i>&gt; &gt; day when it is said, in the tradition of Oppenheimer who looked upon</i><br>
<i>&gt; &gt; another sun:  "I am become Singularity, Breaker of Dreams."</i><br>
<i>&gt; </i><br>
<i>&gt; "Now I've become death, the destroyer of worlds."- The First SI?</i><br>

<p>
Not really.  Poetically speaking, the idea is that a scientist or hacker on
the verge of a powerful new technology briefly takes on the Aspect of whatever
it is he has created.  (This is hypothesized as a fluctuation in the social
emotions and causal attribution, not a literally real event.)  The analogy was
between the Aspects of two great changes, not between two forms of death.
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2230.html">[ Next ]</a><a href="2228.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2177.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
</body></html>
