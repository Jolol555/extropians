<!-- received="Wed Sep 16 10:42:26 1998 MDT" -->
<!-- sent="Wed, 16 Sep 1998 09:38:11 -0700" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@econ.berkeley.edu" -->
<!-- subject="Singularity: Vinge responds" -->
<!-- id="3.0.3.32.19980916093811.00b5b6fc@econ.berkeley.edu" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=93 -->
<html><head><title>extropians: Singularity: Vinge responds</title>
<meta name=author content="Robin Hanson">
<link rel=author rev=made href="mailto:hanson@econ.berkeley.edu" title ="Robin Hanson">
</head><body>
<h1>Singularity: Vinge responds</h1>
Robin Hanson (<i>hanson@econ.berkeley.edu</i>)<br>
<i>Wed, 16 Sep 1998 09:38:11 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2437">[ date ]</a><a href="index.html#2437">[ thread ]</a><a href="subject.html#2437">[ subject ]</a><a href="author.html#2437">[ author ]</a>
<!-- next="start" -->
<li><a href="2438.html">[ Next ]</a><a href="2436.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2391.html">Doug Bailey</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2444.html">Nick Bostrom</a>
</ul>
<!-- body="start" -->

<p>
Vinge asks me to forward the following to the list:
<hr>

<p>
<a name="2441qlink1">Notions of great change raise the vision of all sorts of things that
might be called a singularity. In the past, discussions of what I've
written about have more often than not spread out to things quite
different.

<p>
Early on in this discussion I got my point distilled down to:
1. The creation of superhuman intelligence appears to be a plausible
<p>
   eventuality if our technical progress proceeds for another few
   years.
<br>
2. The existence of superhuman intelligence would yield forms of 
<p>
   progress that are qualitatively less understandable than advances
   of the past.

<p>
Given that, however, the form of the post-human environment is
not at all specified or restricted! I like speculation about it,
and I like to speculate about it (usually after acknowledging
that I shouldn't have any business doing so :-). The speculation
often leads to conflicting scenarios; some I regard as more
likely than others. But if they arise from the original point,
I feel they are relevant.</a>

<p>
For planning purposes, my vision the high level taxonomy is:

<p>
 o (the null): The singularity doesn't happen (or is
   not recognizable).
<br>
 o We have a hard takeoff.
<br>
 o We have a soft takeoff.

<p>
There is a large variety of mechanisms for each of these.
(Some, such a bio-tech advances, might be only indirectly
connected with Moore's Law.)

<p>
Thus, I don't consider that I have written off Nick's upload
scenario. (Actually, Robin may have a better handle on what
I've said on this than I do, so maybe I have words to eat.)
Uploading has a special virtue in that it sidesteps most
people's impossibility arguments. Of course, in its most
conservative form it gives only weak superhumanity (as the
clock rate is increased for the uploads). If that were all
we had, then the participants would not become much greater
within themselves. Even the consequences of immortality would
be limited to the compass of the original blueprint. But to 
the outside world, a form of superhumanity would exist.

<p>
In my 1993 essay, I cited several mechanisms:
 o AI [PS: high possibility of a hard takeoff]
 o IA [PS: I agree this might be slow in developing. Once it
   happened, it might be a explosive. (Also, it can sneak up on
   us, out of research that might not seem relevant to the
   public.)]
<br>
 o Growing out of the Internet
<br>
 o Biological

<p>
Since then:
 
<p>
 o The evolutionary path of fine-grained distributed systems
   has impressed me a lot, and I see some very interesting
   singularity scenarios arising from it.
 o Greg Stockman (and ~Marvin) have made the Metaman scenario
   much more plausible to me: probably a very "gentle" take off.
   (At AAAI-82 one of the people in the audience said he figured
   this version had happened centuries ago.)

<p>
Discussion of others is of interest, too.

<p>
If I were betting (really foolish now :-), as of 
Tue Sep 15 10:58:51 PDT 1998 I would rate likelihoods
(from most to least probable):


<OL>
  <li>  very hard takeoff with fine-grained distribution;
  <li>  no Singularity because we never figure out how to get
    beyond software engineering and we fail to manage complexity;
  <li>  IA  (tied in likelihood with:) 4. Metaman
  <li>  ...


  <UL>
    <li>  Vernor


  </UL>

</OL>
<p>
PS: I like Doug Bailey's postscript:
<br>
<a href="2391.html#2437qlink1">&gt;[Note: My apologies for the less-than-stellar organization but I wrote this in</a><br>
<i>&gt;one pass. If I had waited and posted it after I had time to optimize it, it</i><br>
<i>&gt;would have never seen the light of day.]</i><br>

<p>
I find myself near-frozen by the compulsion to optimize :-)


<p>
Robin Hanson  
<br>
hanson@econ.berkeley.edu    <a href="http://hanson.berkeley.edu/">http://hanson.berkeley.edu/</a>   
RWJF Health Policy Scholar, Sch. of Public Health   510-643-1884  
140 Warren Hall, UC Berkeley, CA 94720-7360    FAX: 510-643-8614
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2438.html">[ Next ]</a><a href="2436.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2391.html">Doug Bailey</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2444.html">Nick Bostrom</a>
</ul>
</body></html>
