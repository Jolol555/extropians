<!-- received="Wed Jul 22 12:17:02 1998 MDT" -->
<!-- sent="Wed, 22 Jul 1998 22:19:36 +0400 (MSD)" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene@liposome.genebee.msu.su" -->
<!-- subject="Re: The Singularity" -->
<!-- id="13750.10533.43272.38076@liposome.genebee.msu.su" -->
<!-- inreplyto="199807220102.SAA03592@pride.ugcs.caltech.edu" -->
<!-- version=1.10, linesinbody=46 -->
<html><head><title>extropians: Re: The Singularity</title>
<meta name=author content="Eugene Leitl">
<link rel=author rev=made href="mailto:eugene@liposome.genebee.msu.su" title ="Eugene Leitl">
</head><body>
<h1>Re: The Singularity</h1>
Eugene Leitl (<i>eugene@liposome.genebee.msu.su</i>)<br>
<i>Wed, 22 Jul 1998 22:19:36 +0400 (MSD)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#952">[ date ]</a><a href="index.html#952">[ thread ]</a><a href="subject.html#952">[ subject ]</a><a href="author.html#952">[ author ]</a>
<!-- next="start" -->
<li><a href="0953.html">[ Next ]</a><a href="0951.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0303.html">Damien R. Sullivan</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0963.html">Robin Hanson</a>
</ul>
<!-- body="start" -->

<p>
Damien R. Sullivan writes:
 
<p>
<i> &gt; Actually I'm not certain of the limits of insect sensors themselves,</i><br>
 &gt; although I think they are fairly severe.  But the combination of tiny<br>
 &gt; sensors and teeny brains adds up to rather limited perceptions.<br>
 
<p>
But insects perceive and represent infinitely more than viroids, so
<a name="0965qlink2">doesn't it seem to be a bit presumptious the bipedal ape's position on 
the smartness scala can't be topped by similiar increases? Our senses
</a>
are limited as well, as does our perception. We augment our
sensomotorics _and_ our minds (Mathematica 3.0 for Linux is great!)
allright, but as a whole we're already participating in upgrades
towards godhead. These upgrades are gradual, and not invasive nor
result in extracorporeal agents of complexities similiar to ours
(unless we consider the metaman level) yet, but, after all, we've 
only begun with our ascent.
 
<p>
<a name="0953qlink1"> &gt; I really have trouble believing in SI we can't understand at all.<br>

<p>
<a name="0965qlink3">Strange, I really have trouble believing in an SI I can understand to
a meaningful extent. If I could, it wouldn't be an SI, or I would be
its peer. It could be governed by some simple laws (the degenerated
</a>
mindstuff I was hinted at), but how the heck would we-current now? The 
</a>
whole argument about the Singularity was not whether it would be
<a name="0965qlink4">magickal (it could or could not), but we wouldn't be able to tell
which way it would turn out to be. The Singularity is a developmental
_prediction horizont_, after all.
</a>

<p>
 &gt; Things that move too fast, sure.  Things that are Einstein+Feynman+Bach<br>
 &gt; +Turing+Darwin rolled up, sure.  But not the magical SI.  In fact,<br>

<p>
<a name="0965qlink5">'Sufficiently advanced technology is indistinguishable from magic'?
</a>
Sure, unwittedly we might be at the verge of a GUT/TOE, but this
doesn't mean we can instantly transform that knowledge into applications.

<p>
<a name="0965qlink6"><a name="0963qlink1"><i> &gt; magical SI through AI seems a bit incoherent; you exploit the</i><br>
 &gt; Church-Turing thesis, then assume the result does something beyond<br>
 &gt; Turing-completeness...<br>
</a>
 
<p>
I can't follow your reasoning here. Would you care to explain?
</a>

<p>
<a href="0303.html#0952qlink1"> &gt; -xx- Damien R. Sullivan X-)</a><br>

<p>
'gene
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0953.html">[ Next ]</a><a href="0951.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0303.html">Damien R. Sullivan</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0963.html">Robin Hanson</a>
</ul>
</body></html>
