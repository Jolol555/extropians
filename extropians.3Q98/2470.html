<!-- received="Thu Sep 17 11:33:46 1998 MDT" -->
<!-- sent="Thu, 17 Sep 1998 10:29:49 -0700" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@econ.berkeley.edu" -->
<!-- subject="Re: Singularity: Human AI to superhuman" -->
<!-- id="3.0.3.32.19980917102949.00b4f92c@econ.berkeley.edu" -->
<!-- inreplyto="35F87667.335C87C4@pobox.com" -->
<!-- version=1.10, linesinbody=30 -->
<html><head><title>extropians: Re: Singularity: Human AI to superhuman</title>
<meta name=author content="Robin Hanson">
<link rel=author rev=made href="mailto:hanson@econ.berkeley.edu" title ="Robin Hanson">
</head><body>
<h1>Re: Singularity: Human AI to superhuman</h1>
Robin Hanson (<i>hanson@econ.berkeley.edu</i>)<br>
<i>Thu, 17 Sep 1998 10:29:49 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2470">[ date ]</a><a href="index.html#2470">[ thread ]</a><a href="subject.html#2470">[ subject ]</a><a href="author.html#2470">[ author ]</a>
<!-- next="start" -->
<li><a href="2471.html">[ Next ]</a><a href="2469.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2284.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2592.html">Robin Hanson</a>
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky seems to be the only person here willing to defend 
<br>
"explosive growth," by which I mean sudden very rapid world economic growth. <br>
So I'd like to see what his best argument is for this.  But Eliezer, if 
we're going to make any progress, we're going to have to *focus*.   I don't 
want to wander off on a dozen irrelevant tangents.

<p>
So first, please make clear which (if any) other intelligence growth
processes you will accept as relevant analogies.  These include the evolution of 
the biosphere, recent world economic growth, human and animal learning, the 
adaptation of corporations to new environments, the growth of cities, the
domestication of animals, scientific progress, AI research progress, advances
in computer hardware, or the experience of specific computer learning programs.  

<p>
You seemed to say no analogies are relevant, and then your last response to me
touches on AI, learning general relativity, Cro-Magnons, Lamarkian biology, 
the rise of cities, and many other topics.   I don't want to talk about these 
things if they are tangential to your argument.

<p>
If no analogies are relevant, and so this is all theory driven, can the theory
be stated concisely?  If not, where did you get the theory you use?  Does 
anyone else use it, and what can be its empirical support, if analogies are 
irrelevant?  



<p>
Robin Hanson  
<br>
hanson@econ.berkeley.edu    <a href="http://hanson.berkeley.edu/">http://hanson.berkeley.edu/</a>   
RWJF Health Policy Scholar, Sch. of Public Health   510-643-1884  
140 Warren Hall, UC Berkeley, CA 94720-7360    FAX: 510-643-8614
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2471.html">[ Next ]</a><a href="2469.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2284.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2592.html">Robin Hanson</a>
</ul>
</body></html>
