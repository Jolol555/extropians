<!-- received="Mon Sep 14 11:52:04 1998 MDT" -->
<!-- sent="Mon, 14 Sep 1998 14:06:22 -0400" -->
<!-- name="Michael Lorrey" -->
<!-- email="retroman@together.net" -->
<!-- subject="Re: AI Prime Directive" -->
<!-- id="35FD5B1D.1E9382E2@together.net" -->
<!-- inreplyto="AI Prime Directive" -->
<!-- version=1.10, linesinbody=39 -->
<html><head><title>extropians: Re: AI Prime Directive</title>
<meta name=author content="Michael Lorrey">
<link rel=author rev=made href="mailto:retroman@together.net" title ="Michael Lorrey">
</head><body>
<h1>Re: AI Prime Directive</h1>
Michael Lorrey (<i>retroman@together.net</i>)<br>
<i>Mon, 14 Sep 1998 14:06:22 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2379">[ date ]</a><a href="index.html#2379">[ thread ]</a><a href="subject.html#2379">[ subject ]</a><a href="author.html#2379">[ author ]</a>
<!-- next="start" -->
<li><a href="2380.html">[ Next ]</a><a href="2378.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2328.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky wrote:

<p>
<a href="2328.html#2379qlink1">&gt; Damien Broderick wrote:</a><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; At 09:06 AM 9/11/98 -0500, Eliezer wrote:</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; [Broderick wrote that what Eliezer wrote:]</i><br>
<i>&gt; &gt; &gt;&gt; &lt; Never allow arbitrary, illogical, or untruthful goals to enter the AI. &gt;</i><br>
<i>&gt; &gt; &gt;&gt; reflects a touching faith in human powers of understanding and consistency.</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; &gt;I'm not quite sure what you mean by this. [Eliezer]</i><br>
<i>&gt; &gt; [Broderick again]</i><br>
<i>&gt; &gt; Isn't it obvious?  How can any limited mortal know in advance what another</i><br>
<i>&gt; &gt; intelligence, or itself at a different time and in other circumstances,</i><br>
<i>&gt; &gt; might regard as `arbitrary, illogical, or untruthful'?  Popper spank.</i><br>
<i>&gt;</i><br>
<i>&gt; So let's throw in all the coercions we want, since nobody can really know</i><br>
<i>&gt; anything anyhow?  That's suicidal!  I didn't say the Prime Directive was easy</i><br>
<i>&gt; or even achievable; I said we should try, and never ever violate it deliberately.</i><br>
<i>&gt;</i><br>
<i>&gt; Perhaps the interpretation of the Prime Directive is too dependent on context,</i><br>
<i>&gt; and it should be amended to read:</i><br>
<i>&gt;</i><br>
<i>&gt; "No damn coercions and no damn lies; triple-check all the goal reasoning and</i><br>
<i>&gt; make sure the AI knows it's fallible, but aside from that let the AI make up</i><br>
<i>&gt; its own bloody mind."</i><br>

<p>
<a name="2409qlink1"><a name="2382qlink1">How about: Thou shalt model any decision first to determine</a> choice most beneficial to
<a name="2409qlink2">one's own long term ration self interest.</a>

<p>
I think that given such a rule, any AI will come to its own conclusions as to moral
behavior without needing hardwired rules, as it will find that choices most
beneficial to one's own long term self interest are also those choices which are
least harmful to others.</a>

<p>
Mike Lorrey
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2380.html">[ Next ]</a><a href="2378.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2328.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
