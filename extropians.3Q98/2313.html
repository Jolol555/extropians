<!-- received="Fri Sep 11 20:15:31 1998 MDT" -->
<!-- sent="Fri, 11 Sep 1998 21:15:38 -0500" -->
<!-- name="Joe E. Dees" -->
<!-- email="jdees0@students.uwf.edu" -->
<!-- subject="Re: Singularity: Human AI to superhuman" -->
<!-- id="199809120215.VAA10569@castaway.uwf.edu" -->
<!-- inreplyto="199809120126.SAA20798@hal.sb.rain.org" -->
<!-- version=1.10, linesinbody=78 -->
<html><head><title>extropians: Re: Singularity: Human AI to superhuman</title>
<meta name=author content="Joe E. Dees">
<link rel=author rev=made href="mailto:jdees0@students.uwf.edu" title ="Joe E. Dees">
</head><body>
<h1>Re: Singularity: Human AI to superhuman</h1>
Joe E. Dees (<i>jdees0@students.uwf.edu</i>)<br>
<i>Fri, 11 Sep 1998 21:15:38 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2313">[ date ]</a><a href="index.html#2313">[ thread ]</a><a href="subject.html#2313">[ subject ]</a><a href="author.html#2313">[ author ]</a>
<!-- next="start" -->
<li><a href="2314.html">[ Next ]</a>
<b>In reply to:</b> <a href="2312.html">Hal Finney</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<pre>
Date sent:      	Fri, 11 Sep 1998 18:26:09 -0700
From:           	Hal Finney &lt;hal@rain.org&gt;
To:             	extropians@extropy.com
Subject:        	Re: Singularity: Human AI to superhuman
Send reply to:  	extropians@extropy.com

</pre>
<p>
<a href="2312.html#2313qlink1">&gt; One thing I'm wondering about is whether the goal of superhuman</a><br>
<i>&gt; intelligence is well defined.</i><br>
<i>&gt; </i><br>
<i>&gt; Eliezer lays out a scenario in which a machine designs a better machine,</i><br>
<i>&gt; and that one designs a still better one, and so on.  But better at what?</i><br>
<i>&gt; It has to be more than just designing the next machine, because otherwise</i><br>
<i>&gt; the improvement is meaningless.  There must be some grounding, some</i><br>
<i>&gt; objective criteria, that can be used to determine if one design is better</i><br>
<i>&gt; than another.</i><br>
<i>&gt; </i><br>
<i>&gt; In the initial stages, conventional IQ tests would probably be useful</i><br>
<i>&gt; as a metric.  Machines which could score ever higher on such tests would</i><br>
<i>&gt; probably be able to better design new versions of themselves.</i><br>
<i>&gt; </i><br>
<i>&gt; But this might not be the fastest path towards optimization.  Consider the</i><br>
<i>&gt; "Browns" of Niven and Pournelle's Motie stories, idiot savant engineers,</i><br>
<i>&gt; able to design and build things with lightning efficiency.  They might</i><br>
<i>&gt; not do well on IQ tests, but they would be ideal for designing new AIs</i><br>
<i>&gt; with even more extreme talents.  How do the AIs (and we ourselves, in</i><br>
<i>&gt; the early stages) decide whether to take this route or to emphasize more</i><br>
<i>&gt; general abilities?  Which mental skills should they retain and enhance,</i><br>
<i>&gt; assuming they have a range of designs available to themselves?</i><br>
<i>&gt; </i><br>
<i>&gt; Even if we want to stick with IQ tests, these run out of steam at some</i><br>
<i>&gt; point as the AIs become too smart.  Then the AIs have to start creating</i><br>
<i>&gt; new IQ tests which will challenge the next generation.  Can this be done</i><br>
<i>&gt; in a reliable and meaningful way?  How can I judge an intelligence which</i><br>
<i>&gt; is greater than my own?</i><br>
<i>&gt; </i><br>
<i>&gt; I suppose one possibility is to take problems that I can solve in a week</i><br>
<i>&gt; and ask for an intelligence which can solve them in a day.  But is greater</i><br>
<i>&gt; speed all we seek in superhuman AI?  It seems to me that more intelligent</i><br>
<i>&gt; people are not only able to solve problems faster, but they are able to</i><br>
<i>&gt; exhibit a greater depth of understanding, an ability to intuitively deal</i><br>
<i>&gt; with some problems that less intelligent people can't handle at all.</i><br>
<i>&gt; How do you evaluate an intelligence which has abilities that you can't</i><br>
<i>&gt; understand?</i><br>
<i>&gt; </i><br>
<i>&gt; A couple of other minor points.  First, these are not self-improving</i><br>
<i>&gt; machines.  Each generation designs a new machine which is different</i><br>
<i>&gt; from itself.  Chances are that its identity can't carry over to the new</i><br>
<i>&gt; design, assuming there is substantial architectural change.  So what</i><br>
<i>&gt; we have is a series of generations of new machines and new identities.</i><br>
<i>&gt; Eliezer's point about goal drift becomes more relevant when each new</i><br>
<i>&gt; machine is a new individual, one only poorly understood by its creators.</i><br>
<i>&gt; It would be a shame if some machine along the path developed a hobby</i><br>
<i>&gt; and bent the skills of later machines into the superhuman equivalent of</i><br>
<i>&gt; inventing anagrams.</i><br>
<i>&gt; </i><br>
<i>&gt; It may also be that the whole idea of ever-increasing superintelligence</i><br>
<i>&gt; is incoherent.  Intelligence may turn out to be just a matter of searching</i><br>
<i>&gt; through a solution space.  Brains are only moderately good at this,</i><br>
<i>&gt; with more intelligent people having more efficient search capabilities.</i><br>
<i>&gt; If so, then we may hit a ceiling in terms of intelligence per amount of</i><br>
<i>&gt; computer power.  We can move beyond human genius levels but quickly reach</i><br>
<i>&gt; limits beyond which search problems explode exponentially.  The result is</i><br>
<i>&gt; that a tenfold increase in computer power brings only a modest improvement</i><br>
<i>&gt; in problem-solving ability.</i><br>
<i>&gt; </i><br>
<i>&gt; Hal</i><br>

<p>
What about the internal construction of more complete and more 
accurate (i.e. more reliable) internal models of our surroundings 
(cosmology) and the most efficient ways they and those we share 
them with can be manipulated or influenced (physics, psychology)?  
This is, after all, what gives our brains and senses their usefulness 
and survival value, and ultimately, is the engine which has driven 
cognitive development and given us our evolutionary edge.  The only 
problem I can see with such a criterion is our lack of a standard (a 
perfect model) by which to evaluate those produced by our 
candidates.
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2314.html">[ Next ]</a>
<b>In reply to:</b> <a href="2312.html">Hal Finney</a>
<!-- nextthread="start" -->
</ul>
</body></html>
