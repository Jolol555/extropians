<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: How To Live In A Simulation</title>
<meta name="Author" content="Robert J. Bradbury (bradbury@aeiveos.com)">
<meta name="Subject" content="Re: How To Live In A Simulation">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: How To Live In A Simulation</h1>
<!-- received="Thu Mar 15 17:28:33 2001" -->
<!-- isoreceived="20010316002833" -->
<!-- sent="Thu, 15 Mar 2001 16:28:33 -0800 (PST)" -->
<!-- isosent="20010316002833" -->
<!-- name="Robert J. Bradbury" -->
<!-- email="bradbury@aeiveos.com" -->
<!-- subject="Re: How To Live In A Simulation" -->
<!-- id="Pine.UW2.4.20.0103151555430.8401-100000@www.aeiveos.com" -->
<!-- inreplyto="4.3.2.7.2.20010315170619.048fb940@nlb28.mail.yale.edu" -->
<strong>From:</strong> Robert J. Bradbury (<a href="mailto:bradbury@aeiveos.com?Subject=Re:%20How%20To%20Live%20In%20A%20Simulation&In-Reply-To=&lt;Pine.UW2.4.20.0103151555430.8401-100000@www.aeiveos.com&gt;"><em>bradbury@aeiveos.com</em></a>)<br>
<strong>Date:</strong> Thu Mar 15 2001 - 17:28:33 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0699.html">J. R. Molloy: "Re: Kurzweil's new Singularity/AI page"</a>
<li><strong>Previous message:</strong> <a href="0697.html">J. R. Molloy: "Re: How to prepare"</a>
<li><strong>In reply to:</strong> <a href="0684.html">Nick Bostrom: "Re: How To Live In A Simulation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0783.html">Nick Bostrom: "Re: How To Live In A Simulation"</a>
<li><strong>Reply:</strong> <a href="0783.html">Nick Bostrom: "Re: How To Live In A Simulation"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#698">[ date ]</a>
<a href="index.html#698">[ thread ]</a>
<a href="subject.html#698">[ subject ]</a>
<a href="author.html#698">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
On Thu, 15 Mar 2001, Nick Bostrom, commenting on my comments wrote:
<br>
<p><p><em>&gt; is the only way of preventing a black goo disaster.
</em><br>
<p>Actually, I think the standard term is &quot;gray goo&quot;, you need to search
<br>
Engines of Creation or Robert F.'s Ecophagy paper.
<br>
<p><em>&gt; Another is that we might understand that this degree of coordination
</em><br>
<em>&gt; is necessary to avoid Robin's Burning of the Cosmic Commons scenario.
</em><br>
<p>But from my perspective (and I know we haven't resolved this debate),
<br>
there is no point to interstellar colonization because you get no
<br>
benefit from it.
<br>
<p><em>&gt; A third reason is that if there is a singularity then the transcending
</em><br>
<em>&gt; power might well get enough power to become a singleton.
</em><br>
<p>Yes, I think this is a variant of the runaway first upload (which
<br>
may be part of Robin's &quot;If Uploads Come First&quot;, I can't remember.
<br>
<p><em>&gt; This requires a rather strong convergence-hypothesis 
</em><br>
<em>&gt; (&quot;all advanced civilization evolve in the same direction&quot;), but it is real 
</em><br>
<em>&gt; possibility in my opinion, and indeed is the scenario that we should hope 
</em><br>
<em>&gt; is correct.
</em><br>
<p>I'm pretty sure that we all do evolve in the same direction that
<br>
gets very close to the limits of known physical laws.  However
<br>
because of the huge variants in starting conditions (# planets,
<br>
element abundances, star size, history of encounters as ones
<br>
system travels through the galaxy, disruptions caused by galactic
<br>
collisions, etc. we may all end up with somewhat different final
<br>
configurations (we know for example Dyson Nets, JBrain'ed systems,
<br>
Matrioshka Brains, Ander's more fanciful end-points that require
<br>
sub-atomic engineering, etc.).  I think convergent evolution is
<br>
going to drive entities into specific architectures that are
<br>
optimal for specific purposes.  But I don't see anything that
<br>
suggests you don't get one type of civilization/architecture
<br>
in which ancestor simulations are allowed and another type
<br>
of civilization/architecture in which they are not.
<br>
<p>Now, there may be a general rule that says that running sub-SI
<br>
simulations (at least at our level) produces no-net gain in knowledge
<br>
and is a waste of computronium and energy.  Or there may
<br>
be a general rule that says that once you have fully optimized
<br>
all the local matter and energy all you can do in terms of
<br>
allowing evolution to continue is run sub-SI simulations in
<br>
which virtual evolution occurs.
<br>
<p>So either of those seem possible to me but I don't see how to
<br>
decide between them.
<br>
<p><em>&gt; It would be a fundamental mistake to think of simulations as not fully 
</em><br>
<em>&gt; real.
</em><br>
<p>But they aren't *real* to me!  Humanity has got a huge range of
<br>
standards regarding what can be morally justified in terms of
<br>
the treatment of other sub-&quot;conscious&quot; entities.  Look at eating
<br>
beef, or eating dog-meat or eliminating cochroaches, etc.
<br>
To an SI we are *way* below the cockroach level in terms of
<br>
its consciousness.
<br>
<p><em>&gt; If the beings in the simulations are conscious then their well-being is as 
</em><br>
<em>&gt; ethically important as that of those who are implemented directly in 
</em><br>
<em>&gt; biological brains in the basement universe.
</em><br>
<p>Hmmmm, this seems to be the &quot;conscious&quot; = valuable, &quot;non-conscious&quot;
<br>
equals not valuable (e.g. a binary state).  I'm fairly sure that
<br>
many neuroscientists would argue there is a consciousness scale
<br>
going from the lowest to the highest animals.  (This goes back to those
<br>
animals that we can perceive recognizing themselves in the mirror.)  
<br>
There is probably also a &quot;pain&quot; scale as well.   Say the SI evolved
<br>
from a culture that valued information content over self-awareness.
<br>
In that situation, you eliminate the sub-SI simulations as soon as
<br>
they have reached the end of their useful lifetime and can be
<br>
reallocated to something that generates a greater amount of information.
<br>
<p><em>&gt; You wouldn't be allowed to think certain thoughts for the same reason that
</em><br>
<em>&gt; a human is not allowed to have a child and maltreat it. Thinking is not a 
</em><br>
<em>&gt; private matter when it directly affects somebody else.
</em><br>
<p>So I can't just go thinking my girlfriend-de-jour into the replicator
<br>
for a fun night on the town and then recycle her atoms the next morning???
<br>
<p>That just sucks!
<br>
<p>Robert
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0699.html">J. R. Molloy: "Re: Kurzweil's new Singularity/AI page"</a>
<li><strong>Previous message:</strong> <a href="0697.html">J. R. Molloy: "Re: How to prepare"</a>
<li><strong>In reply to:</strong> <a href="0684.html">Nick Bostrom: "Re: How To Live In A Simulation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0783.html">Nick Bostrom: "Re: How To Live In A Simulation"</a>
<li><strong>Reply:</strong> <a href="0783.html">Nick Bostrom: "Re: How To Live In A Simulation"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#698">[ date ]</a>
<a href="index.html#698">[ thread ]</a>
<a href="subject.html#698">[ subject ]</a>
<a href="author.html#698">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:59:40 MDT</em>
</em>
</small>
</body>
</html>
