<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Maximizing results of efforts Re: Mainstreaming</title>
<meta name="Author" content="J. R. Molloy (jr@shasta.com)">
<meta name="Subject" content="Re: Maximizing results of efforts Re: Mainstreaming">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Maximizing results of efforts Re: Mainstreaming</h1>
<!-- received="Sun Apr 29 00:17:12 2001" -->
<!-- isoreceived="20010429061712" -->
<!-- sent="Sat, 28 Apr 2001 23:15:52 -0700" -->
<!-- isosent="20010429061552" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: Maximizing results of efforts Re: Mainstreaming" -->
<!-- id="0fd901c0d073$dc6e3380$6c5d2a42@jrmolloy" -->
<!-- inreplyto="64.d84133f.281cd7e8@aol.com" -->
<strong>From:</strong> J. R. Molloy (<a href="mailto:jr@shasta.com?Subject=Re:%20Maximizing%20results%20of%20efforts%20Re:%20Mainstreaming&In-Reply-To=&lt;0fd901c0d073$dc6e3380$6c5d2a42@jrmolloy&gt;"><em>jr@shasta.com</em></a>)<br>
<strong>Date:</strong> Sun Apr 29 2001 - 00:15:52 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3373.html">James J. Hughes: "Re: Origins of Political Beliefs"</a>
<li><strong>Previous message:</strong> <a href="3371.html">Ben Goertzel: "RE: Maximizing results of efforts Re: Mainstreaming"</a>
<li><strong>In reply to:</strong> <a href="3359.html">Spudboy100@aol.com: "Re: Maximizing results of efforts Re: Mainstreaming"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3551.html">Samantha Atkins: "Re: Maximizing results of efforts Re: Mainstreaming"</a>
<li><strong>Reply:</strong> <a href="3551.html">Samantha Atkins: "Re: Maximizing results of efforts Re: Mainstreaming"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3372">[ date ]</a>
<a href="index.html#3372">[ thread ]</a>
<a href="subject.html#3372">[ subject ]</a>
<a href="author.html#3372">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
<em>&gt; ...it seems to scary because it (as of this
</em><br>
<em>&gt; point in time) because it's untried; like aviation was during the early 20th
</em><br>
<em>&gt; century.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Mitch
</em><br>
<p>That analogy works well enough for me.
<br>
The fact that AI may (or may not) share our fears doesn't make it less scary
<br>
does it?
<br>
Oh, well. Optimizing the search for synthetic sentience requires the capacity
<br>
to detect sentience, obviously. Does it take super sentience to detect super
<br>
sentience, or is that the name of some stupid song... by &quot;Rabbi Loew and The
<br>
Emotionally Peculiar Prague Puppets&quot; ô¿ô
<br>
<p>Human-competitive machine intelligence, as several experts have noted, shall
<br>
become the holy grail of the next few years (or decades, whatever), and it
<br>
shall become the most expensive commodity of the last ten minutes of the next
<br>
few years (or...). So, to maximize the results of extropian effort, put as
<br>
much mainstream money into machine intelligence as you can manage, and put as
<br>
much machine intelligence into mainstreaming extropian effort as you can
<br>
afford. Of course if we had a human-competitive AI, it could help solve this
<br>
problem for us, and we wouldn't need to worry about it quite as much. So, from
<br>
a circular logic standpoint, the first and most important task is to evolve
<br>
human-competitive machine intelligence. Not everyone favors circular logic.
<br>
Outside this box biotechies make magic with neural models. Dozens of teams all
<br>
over the planet are working night and day to pop out a real live
<br>
human-competitive machine intelligence. The stakes are so high, it's hard to
<br>
remember that this is very stale news.
<br>
<p>From: &quot;Ben Goertzel&quot; &lt;<a href="mailto:ben@goertzel.org?Subject=Re:%20Maximizing%20results%20of%20efforts%20Re:%20Mainstreaming&In-Reply-To=&lt;0fd901c0d073$dc6e3380$6c5d2a42@jrmolloy&gt;">ben@goertzel.org</a>&gt;
<br>
<em>&gt; It may well wind up that we want to build computers with a kind of
</em><br>
<em>&gt; &quot;intelligence ceiling&quot;, computers that ~don't~ become vastly
</em><br>
<em>&gt; superintelligent precisely because they're more useful to us when their
</em><br>
<em>&gt; intelligence is at a level that our problems are still interesting to them.
</em><br>
<p>Religion has served this purpose (preventing children from thinking for
<br>
themselves) for thousands of years, Ben. What if the Mormons (or worse yet,
<br>
the Scientologists) build the first AI... will they indoctrinate it with their
<br>
intelligence ceiling memes? Why not, they hobble their own kids with
<br>
superstitious memes.
<br>
<p><em>&gt; This is reminiscent of the situation in &quot;A Fire Upon the Deep&quot;, in which not
</em><br>
<em>&gt; all civilizations choose to transcend and become  vastly superintelligent...
</em><br>
<p>Yeah, I get it. Sort of like Democrats, huh? (SI ain't democratic.) ô¿ô
<br>
<p><em>&gt; Or one can imagine &quot;bodhisattva AI's&quot; that become superintelligent and then
</em><br>
<em>&gt; stupidify themselves so they can help us better.
</em><br>
<p>Or one can imagine Moses stupified himself when he got the ten commandments.
<br>
(God only wanted to give him one commandment, but since they didn't cost
<br>
anything... OK, dumb joke.) A real bodhisattva would tell you to become one
<br>
yourself instead of trying to build an artificial one. Does it make sense to
<br>
build a super sentient instead of experiencing superlative sentience? Buddha
<br>
reportedly told his brother Ananda, &quot;Be a light unto yourself.&quot; That was
<br>
twenty-five centuries ago, and it's still the best advice.
<br>
<p><em>&gt; When I was younger and even more foolish, I used to have a theory that I was
</em><br>
<em>&gt; an all knowing all powerful God who had intentionally blotted out most of
</em><br>
<em>&gt; his mind and made himself into a mere mortal, just because he'd gotten tired
</em><br>
<em>&gt; of being so damn powerful  ;&gt;
</em><br>
<p>I had exactly the same theory. I'll bet many of us did. Of course sooner or
<br>
later you'll remember who you are, and then we'll all know the power of
<br>
awakening a foolish god.
<br>
<p>BTW, some folks are still trying to get the Net to transcend.
<br>
<p>&quot;The web knows. It knows everything. The web is god.&quot;
<br>
--Spike
<br>
<p>--J. R.
<br>
<p>Useless hypotheses:
<br>
&nbsp;consciousness, phlogiston, philosophy, vitalism, mind, free will, qualia,
<br>
analog computing, cultural relativism
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Everything that can happen has already happened, not just once,
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;but an infinite number of times, and will continue to do so forever.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Everything that can happen = more than anyone can imagine.)
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3373.html">James J. Hughes: "Re: Origins of Political Beliefs"</a>
<li><strong>Previous message:</strong> <a href="3371.html">Ben Goertzel: "RE: Maximizing results of efforts Re: Mainstreaming"</a>
<li><strong>In reply to:</strong> <a href="3359.html">Spudboy100@aol.com: "Re: Maximizing results of efforts Re: Mainstreaming"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3551.html">Samantha Atkins: "Re: Maximizing results of efforts Re: Mainstreaming"</a>
<li><strong>Reply:</strong> <a href="3551.html">Samantha Atkins: "Re: Maximizing results of efforts Re: Mainstreaming"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3372">[ date ]</a>
<a href="index.html#3372">[ thread ]</a>
<a href="subject.html#3372">[ subject ]</a>
<a href="author.html#3372">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 10:00:00 MDT</em>
</em>
</small>
</body>
</html>
