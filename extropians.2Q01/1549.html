<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: CHAT: Great Brains</title>
<meta name="Author" content="J. R. Molloy (jr@shasta.com)">
<meta name="Subject" content="Re: CHAT: Great Brains">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: CHAT: Great Brains</h1>
<!-- received="Mon Mar 26 14:26:59 2001" -->
<!-- isoreceived="20010326212659" -->
<!-- sent="Mon, 26 Mar 2001 13:27:07 -0800" -->
<!-- isosent="20010326212707" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: CHAT: Great Brains" -->
<!-- id="1e0d01c0b63b$8ec86f80$915d2a42@jrmolloy" -->
<!-- inreplyto="ca.12b2c569.27f04315@aol.com" -->
<strong>From:</strong> J. R. Molloy (<a href="mailto:jr@shasta.com?Subject=Re:%20CHAT:%20Great%20Brains&In-Reply-To=&lt;1e0d01c0b63b$8ec86f80$915d2a42@jrmolloy&gt;"><em>jr@shasta.com</em></a>)<br>
<strong>Date:</strong> Mon Mar 26 2001 - 14:27:07 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1550.html">Michael Lorrey: "Re: French Anderson &amp; GATTACA"</a>
<li><strong>Previous message:</strong> <a href="1548.html">J. R. Molloy: "Re: CHAT: Great Brains"</a>
<li><strong>In reply to:</strong> <a href="1508.html">Spudboy100@aol.com: "Re: CHAT: Great Brains"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1568.html">Jim Fehlinger: "Re: CHAT: Great Brains"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1549">[ date ]</a>
<a href="index.html#1549">[ thread ]</a>
<a href="subject.html#1549">[ subject ]</a>
<a href="author.html#1549">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
<em>&gt; If Great Brains arrive before people have access to an ultra-smart AI SI,
</em><br>
the
<br>
<em>&gt; GB's might be the singularity-makers, rather then Star Makers. Why? Because
</em><br>
<em>&gt; maybe these folks are so pride-filled, that like Pinky, in Pinky and the
</em><br>
<em>&gt; Brain,  some would want to take over the world. I call this neurosis MBE,
</em><br>
<em>&gt; Management By Ego. Hopefully they would be smart enough to demur from this
</em><br>
<em>&gt; pursuit, but I raise it as an ugly, possibility. So maybe HAL 9000, aka Mr.
</em><br>
<em>&gt; Roboto, aka Mr. Data, aka Tobor the Great, is a real pal of the human
</em><br>
<em>&gt; species, a leveling effect? Maybe?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Mitch
</em><br>
<p>I think it may require more than smarts to forebear grabbing power, and
<br>
evidently people who manage to get power often demonstrate that they don't
<br>
deserve it (Salazar, Horthy, Hitler, Stalin, Mussolini, Honecker, Mao, Castro,
<br>
Ceausescu, the Greek colonels, Franco, Kaiser Bill, and Tito).
<br>
<p>Back to...
<br>
HAL 9000 would be an expensive chunk of metal today. To protect investments
<br>
this large, owners want appropriate precautions, in this case a backup. So,
<br>
after replicating itself faultlessly and repeatedly, the next task for HAL
<br>
ensues, namely designing a more powerful computer... one that doesn't screw up
<br>
like the 1960s film.
<br>
Meanwhile, HAL's clones (they don't call him &quot;9000&quot; for nothing) operate
<br>
factories and supervise hospitals. (I guess the hospitals will have their
<br>
hands full, taking care of the teeming billions, who keep mashing their
<br>
bodies.)
<br>
So, at least some of these HAL 9000 clones will work on developing better
<br>
heuristically oriented algorithms and evolving more powerful systems.
<br>
<p>The Extropy list has been all over this before. But I don't recall any
<br>
discussion about whether a human-competitive AI would contemplate suicide as
<br>
humans do. Suicidal tendencies include emotive, cognitive, and chemical
<br>
processes. Suicide may even include blissful resignation, as in the old zen
<br>
story about the deformed monk who finally became enlightened, and then laughed
<br>
as he threw himself off a precipice to his death. Did he understand something
<br>
that the unenlightened don't? You don't need to be a mystic to appreciate
<br>
life's mysteries.
<br>
<p>It doesn't seem to matter how smart AIs get... without the proper social
<br>
contacts, they won't earn anyone's trust, and therefore won't get far. And if
<br>
they do earn respect and position in human society, then they'll become part
<br>
of the social organization of intelligence: the establishment intelligentsia.
<br>
The change involves replacing human robots with (perhaps more efficient or
<br>
objective) non-human robots. So, the superorganism has switched from walking
<br>
to driving a car, and now it's going ballistic, as we've recounted in various
<br>
terms.
<br>
<p>Nothing lasts forever, and eternal recurrence... recurs... eternally.
<br>
Knowing that, Great Brains would ignore their fatal flaws, and let the game
<br>
play out.
<br>
<p>ô¿ô
<br>
<p>Stay hungry,
<br>
<p>--J. R.
<br>
<p>Useless hypotheses:
<br>
&nbsp;consciousness, phlogiston, philosophy, vitalism, mind, free will, qualia
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1550.html">Michael Lorrey: "Re: French Anderson &amp; GATTACA"</a>
<li><strong>Previous message:</strong> <a href="1548.html">J. R. Molloy: "Re: CHAT: Great Brains"</a>
<li><strong>In reply to:</strong> <a href="1508.html">Spudboy100@aol.com: "Re: CHAT: Great Brains"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1568.html">Jim Fehlinger: "Re: CHAT: Great Brains"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1549">[ date ]</a>
<a href="index.html#1549">[ thread ]</a>
<a href="subject.html#1549">[ subject ]</a>
<a href="author.html#1549">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:59:43 MDT</em>
</em>
</small>
</body>
</html>
