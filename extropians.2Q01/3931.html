<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>extropians: Opinions as Evidence: Should Rational Bayesian Agen</title>
<meta name="Author" content="CurtAdams@aol.com (CurtAdams@aol.com)">
<meta name="Subject" content="Opinions as Evidence: Should Rational Bayesian Agents Commonize Priors?">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Opinions as Evidence: Should Rational Bayesian Agents Commonize Priors?</h1>
<!-- received="Sun May  6 22:06:18 2001" -->
<!-- isoreceived="20010507040618" -->
<!-- sent="Mon, 7 May 2001 00:05:43 EDT" -->
<!-- isosent="20010507040543" -->
<!-- name="CurtAdams@aol.com" -->
<!-- email="CurtAdams@aol.com" -->
<!-- subject="Opinions as Evidence: Should Rational Bayesian Agents Commonize Priors?" -->
<!-- id="ac.14a8b919.28277917@aol.com" -->
<!-- charset="UTF-8" -->
<strong>From:</strong> <a href="mailto:CurtAdams@aol.com?Subject=Re:%20Opinions%20as%20Evidence:%20Should%20Rational%20Bayesian%20Agents%20Commonize%20Priors?&In-Reply-To=&lt;ac.14a8b919.28277917@aol.com&gt;"><em>CurtAdams@aol.com</em></a><br>
<strong>Date:</strong> Sun May 06 2001 - 22:05:43 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3932.html">Max More: "NEWS: Information Week on Ellison, anti-aging, ExI mention"</a>
<li><strong>Previous message:</strong> <a href="3930.html">J. R. Molloy: "Re: Fuel Cell House, was Re: TECH: fuel cell car"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3948.html">Eliezer S. Yudkowsky: "Re: Opinions as Evidence: Should Rational Bayesian Agents Commonize  Priors?"</a>
<li><strong>Reply:</strong> <a href="3948.html">Eliezer S. Yudkowsky: "Re: Opinions as Evidence: Should Rational Bayesian Agents Commonize  Priors?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3931">[ date ]</a>
<a href="index.html#3931">[ thread ]</a>
<a href="subject.html#3931">[ subject ]</a>
<a href="author.html#3931">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
I consider the situation of two uninformed Bayesian agents becoming aware of 
<br>
differences in priors about a binary world-state.  I derive their confidence 
<br>
functions in the binary world-state upon the information of the other’s 
<br>
priors and show the confidence functions coincide only if priors are drawn 
<br>
from one particular distribution strongly informative on the world-state.  I 
<br>
non-rigorously prove the result generalizes to a world with any finite number 
<br>
of possible states and to Bayesian agents with access to common public 
<br>
information.  I present arguments for the conjecture that the result further 
<br>
generalizes to worlds with infinite states and to Bayesians with private 
<br>
information.  Hence, rational Bayesians with initially differing priors 
<br>
should continue to disagree even when fully informed of each other’s beliefs.
<br>
<p>Take a world with two possible states, Q and ~Q.  Assume two Bayesian agents 
<br>
with prior degrees of belief in Q, denoted as A and B, and degrees of belief 
<br>
in ~Q of (1-A) and (1-B).  For notational convenience in text transmission, I 
<br>
denote the degree of belief of a Bayesian agent with initial degree of belief 
<br>
X, given information Y, as (X)Y.  Priors provide information in that they 
<br>
depend on the state of the world; for any given prior P there is a 
<br>
probability P|Q of that prior in world with Q and P|~Q in worlds with ~Q.  
<br>
This defines a function f(P) = (P|Q)/(P|~Q) which eases notation of 
<br>
confidence functions upon the information of a new agent with prior P.
<br>
<p>On learning of B, A should now have a degree of belief in Q of A*B|Q and a 
<br>
degree of belief in ~Q of (1-A)*(B|~Q).  Normalizing A’s total degree of 
<br>
belief to 1 and using f(B) to simplify notation, I derive:
<br>
(A)B = A*f(B)/( A*f( B)+(1-A))  (1)
<br>
<p>And by symmetry
<br>
<p>(B)A = B*f(B)/(B*f(A) + (1-B))   (2)
<br>
<p>If A and B commonize their priors on learning of each other, (A)B = (B)A
<br>
Solving for f:
<br>
<p>A*f(B)/(A*f(B) + 1-A) = B*f(A)/(B*f(A) + 1-B)
<br>
A*f(B)(B*f(A) + 1-B) = B*f(A)( A*f(B) + 1-A) 
<br>
AB*f(B)*f(A) + A(1-B)f(B) = AB*f(B)*f(A) + B(1-A)f(A)
<br>
A(1-B)f(B) = B(1-A)f(A)
<br>
F(A)/f(B) = A(1-B)/B(1-A)  (3)
<br>
<p>This requires f(A) = cA/(1-A) (4), with some arbitrary constant c.  Hence 
<br>
priors must be highly dependent on the world-state.  In particular, 
<br>
completely uninformed individuals are almost never profoundly wrong: given 
<br>
world-state Q, the chance of a prior with low A (relatively strong disbelief 
<br>
in the actual world-state) goes to 0 as A goes to zero, and does so rather 
<br>
rapidly.  This disagrees markedly with actual experience, which shows most 
<br>
completely uninformed people have very incorrect beliefs.
<br>
<p>Let us suppose the agents with priors A and B have access to additional 
<br>
public information E prior to learning each other’s priors.  If they concur 
<br>
at this stage, then ((A)E)B = ((B)E)A.  By standard Bayesian inference, 
<br>
((A)E)B = ((A)B)E  so ((A)B)E = ((B)A)E.  Hence, by Bayesian rules (A)B) = 
<br>
(B)A.  Hence two Bayesian agents with access to public information will 
<br>
concur only under exactly the same restrictive conditions required to concur 
<br>
in the absence of public information.
<br>
<p>A world with three states Q, R, and S can be described with the binary 
<br>
beliefs Q/~Q and R/~R, given that Q implies ~R.  In order for two agent to 
<br>
agree on degrees of belief to all three states, they must concur on both Q 
<br>
and R.  The requirements to concur on binary beliefs Q and R are as above.  
<br>
By induction, two agents will concur on finite multi-state worlds only if the 
<br>
probability of a degree of belief AN in each particular state N follows the 
<br>
condition p(AN) = cAN(1-AN).
<br>
<p>Even if the world consist of an infinite set of possible states, these can be 
<br>
partitioned into two sets, each of which can be partitioned into two sets, 
<br>
etc., leading to a sequence of binary possibilities Q1, Q2, Q3,…  For two 
<br>
agents to concur on the confidence function over all sets, it seems intuitive 
<br>
they must concur on Q1, Q2, Q3 …, with each concurrence requiring the 
<br>
distribution of priors on each Q follow condition (4).
<br>
<p>Finally, if Bayesian agents with private information meet, the conclusion 
<br>
above follows by replacing prior A with private-informed degree of belief A.  
<br>
The requirement in (4) still holds, except that now private-informed beliefs, 
<br>
rather than priors, must follow the distribution.  I conjecture that if, in 
<br>
some world, (4) holds for private-informed beliefs, gain or loss of private 
<br>
information would in general cause (4) to no longer hold.  
<br>
<p>My personal experience is that priors and private information are only weakly 
<br>
informative; i.e., even given world-state Q obtains, it isn’t particularly 
<br>
difficult to find uninformed individuals with strong disbelief in Q.  Given 
<br>
this, the probability of a given degree of belief A in Q varies only mildly 
<br>
with whether Q obtains.  Hence the information derived from a given person 
<br>
holding a degree of belief Q is small and a rational Bayesian should have 
<br>
only a small change in belief on learning another’s opinions.  Rational 
<br>
Bayesians, then, generally should maintain differences of opinion due to 
<br>
differences in priors.  Under most circumstances, for two agents to commonize 
<br>
priors requires a violation of Bayesian inference.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3932.html">Max More: "NEWS: Information Week on Ellison, anti-aging, ExI mention"</a>
<li><strong>Previous message:</strong> <a href="3930.html">J. R. Molloy: "Re: Fuel Cell House, was Re: TECH: fuel cell car"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3948.html">Eliezer S. Yudkowsky: "Re: Opinions as Evidence: Should Rational Bayesian Agents Commonize  Priors?"</a>
<li><strong>Reply:</strong> <a href="3948.html">Eliezer S. Yudkowsky: "Re: Opinions as Evidence: Should Rational Bayesian Agents Commonize  Priors?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3931">[ date ]</a>
<a href="index.html#3931">[ thread ]</a>
<a href="subject.html#3931">[ subject ]</a>
<a href="author.html#3931">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 10:00:03 MDT</em>
</em>
</small>
</body>
</html>
