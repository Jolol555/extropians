<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Keeping AI at bay (was: How to help create a si</title>
<meta name="Author" content="Eugene Leitl (Eugene.Leitl@lrz.uni-muenchen.de)">
<meta name="Subject" content="Re: Keeping AI at bay (was: How to help create a singularity)">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Keeping AI at bay (was: How to help create a singularity)</h1>
<!-- received="Wed May  2 15:40:58 2001" -->
<!-- isoreceived="20010502214058" -->
<!-- sent="Wed, 2 May 2001 19:46:02 +0200 (MET DST)" -->
<!-- isosent="20010502174602" -->
<!-- name="Eugene Leitl" -->
<!-- email="Eugene.Leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: Keeping AI at bay (was: How to help create a singularity)" -->
<!-- id="Pine.SOL.4.31.0105021839250.10562-100000@sun1.lrz-muenchen.de" -->
<!-- inreplyto="01050217065200.07834@akira" -->
<strong>From:</strong> Eugene Leitl (<a href="mailto:Eugene.Leitl@lrz.uni-muenchen.de?Subject=Re:%20Keeping%20AI%20at%20bay%20(was:%20How%20to%20help%20create%20a%20singularity)&In-Reply-To=&lt;Pine.SOL.4.31.0105021839250.10562-100000@sun1.lrz-muenchen.de&gt;"><em>Eugene.Leitl@lrz.uni-muenchen.de</em></a>)<br>
<strong>Date:</strong> Wed May 02 2001 - 11:46:02 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3691.html">Spike Jones: "Re: META: To my fellow Extropians, calc your ERA"</a>
<li><strong>Previous message:</strong> <a href="3689.html">Natasha Vita-More: "Re: BIOTECH: Farners are not shunning biotech this season"</a>
<li><strong>In reply to:</strong> <a href="3680.html">Anders Sandberg: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3725.html">Damien Sullivan: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3690">[ date ]</a>
<a href="index.html#3690">[ thread ]</a>
<a href="subject.html#3690">[ subject ]</a>
<a href="author.html#3690">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
On Wed, 2 May 2001, Anders Sandberg wrote:
<br>
<p><em>&gt; No problems with that. But I think some of the macros you use are
</em><br>
<em>&gt; non-trivial and in a discussion like this they ought to be on the
</em><br>
<em>&gt; visible level.
</em><br>
<p>You're no fun to argue with. If you instantly spot even the subtler
<br>
subterfuges, how am I supposed to weasel myself out of half-baked
<br>
arguments?
<br>
<p><em>&gt; People are very aware about the problem, IMHO. It is just that so far
</em><br>
<p>Most IT people I know are pretty much into heavy self-delusion, and
<br>
usually interpret you criticizing the methods they adhere to as a personal
<br>
attack. I think the field could use a liberal sprinkling of professional
<br>
humility.
<br>
<p><em>&gt; many of the solutions have not panned out, making people rather
</em><br>
<em>&gt; risk-aversive when it comes to new approaches. But given how quickly
</em><br>
<p>Of course it's the economy (whether the grant-determining paper output, or
<br>
products delivered on schedule, i.e. sufficient quality before the
<br>
competitors') which causes people to become risk-averse. Industry does no
<br>
longer seem to be able to afford bluesky type of research. Modern variants
<br>
of Xerox PARC are quite scarce. Our only hope is smart mecenate.
<br>
<p><em>&gt; people also hang on to new computing trends when they become
</em><br>
<em>&gt; fashionable and have enough mass, I don't think a method of better
</em><br>
<p>You mention the correct word: &quot;fashion&quot;. Contrary to the claims the field
<br>
is not nearly as rational as claimed.
<br>
<p><em>&gt; software efficiency would be ignored if it could demonstrate a
</em><br>
<em>&gt; measurable improvement.
</em><br>
<p>It is difficult to warrant considerable investment with a ROI latency of
<br>
decades, particularly at this day and age. People flock quickly to a new
<br>
field curently emerged as hot, and desert it as quickly as it fails to
<br>
deliver on (notoriously overhyped) promises. Some fields stick around long
<br>
enough to even see several waves of above behaviour.
<br>
<p><em>&gt; Evolutionary algorithms are great for specialised modules, but lousy
</em><br>
<em>&gt; at less well defined problems or when the complexity of the problem
</em><br>
<em>&gt; makes the evolutionary search space too nasty. I don't think we will
</em><br>
<p>I'm arguing that we don't have real evolutionary algorithms. First, we
<br>
have toy population sizes, and not enough generations, because our
<br>
hardware is so very lousy. Noise generation is easy, but the fitness
<br>
function usually takes its sweet time to evaluate. Mapping bottleneck
<br>
stages to reconfigurable hardware should somewhat ameliorate that.
<br>
<p>More importantly, the investigators seem to somehow assume that
<br>
evolutionary algorithms are simple. They're that only superficially. Imo,
<br>
biology has only been able to accomplish what it did because it uses
<br>
something else. During long rounds of coevolution both the substrate and
<br>
the mutation function have changed in mutual adaptation, creating the
<br>
usual benign form of the mapping of sequence to fitness space: long
<br>
neutral-fitness filaments, allowing mutants to percolate through wide
<br>
areas of sequence space at very little costs, and maximum diversity in a
<br>
small ball once you leave the filaments.
<br>
<p>So what happens, is that investigators pick up a framework they deem nifty
<br>
(vogues+personal bias), hard-code a mutation function, let it run on
<br>
trivial population sizes for a few rounds and then complain the approach
<br>
doesn't scale not much beyond the size of trivial.
<br>
<p>If you pick a few ad hoc numbers from the parameter space, you almost
<br>
certainly will wind up with a suboptimal, nonadaptive system. The
<br>
substrate does not show above properties, and the mutation function is
<br>
held fixed, so it can't adapt itself by learning the limitations of the
<br>
substrate (ideally, you can't leave all the work just to the mutation
<br>
function, but coevolve the substrate as well -- reconfigurable hardware
<br>
gives us the most basic capability for the first time), and understand the
<br>
emerged code that maps higher-order genomic elements into shape features.
<br>
The mutation rate over genome is not constant. The genome uses an evolved
<br>
modular representation, where there's e.g. a morphological code, which
<br>
allows you to shift a limb from one place of the body to another, without
<br>
turning it into a random mass of tissue. There are probably
<br>
adaptive-response metalibraries stored in the biological genome, adaptive
<br>
mutation rate being the simplest example.
<br>
<p>I have a strong feeling we're stuck in what is essentially a weakly
<br>
biased random search in gene space, and performance will as long scale
<br>
poorly as long as we don't realize that something more intelligent
<br>
is going on in there.
<br>
<p>To use evolution to produce solutions, we must first learn to evolve.
<br>
There might be several phase transitions along the way, in which the
<br>
system uses more and more advanced coding mechanisms (which it will
<br>
hopefully discover, given enough resources to search the space), and
<br>
becomes better and better at the task.
<br>
<p>Now this was pure speculation, but I'm missing investigations to check
<br>
whether there is something hidden operating there. We're still proceeding
<br>
on doing minor variations of the scheme produced by the few seminal works,
<br>
and are slowly getting discouraged by the fact that the systems seem to
<br>
remain stuck at the digital analogon of autocatalytic sets. Evolutionary
<br>
algorithms have been having a deteriorating reputation for a while now.
<br>
Sounds familiar?
<br>
<p><em>&gt; get a dramatic jump in software abilities through stringing together
</em><br>
<em>&gt; efficient small modules since as you say the glueing is the hard part
</em><br>
<em>&gt; and not easy to evolve itself.  On the other hand, it seems to be a
</em><br>
<p>I was actually thinking about procedural glue, which lumps interface
<br>
vectors, to catch the noise. You'll need some fancy ultrawide ALUs
<br>
to compute that efficiently.
<br>
<p><em>&gt; way to help improve software and hardware a bit by making the hardware
</em><br>
<em>&gt; adaptable to the software, which is after all a nice thing.
</em><br>
<em>&gt;
</em><br>
<em>&gt; My experience with evolving mutation functions and fitness landscapes
</em><br>
<p>Can you describe us the state of the art in making mutation function a
<br>
part of the population?
<br>
<p><em>&gt; suggest that this is a very hard problem. Computing cycles help, but I
</em><br>
<em>&gt; am not that optimistic about positive feedback scenarios. The problem
</em><br>
<em>&gt; is likely that the mutation function is problem-dependent, and for
</em><br>
<p>Of course.
<br>
<p><em>&gt; arbitrary, ill-defined and complex problems there are no good
</em><br>
<em>&gt; muctation functions. Life only had to solve the problem of adapting to
</em><br>
<p>The first ones will perform poorly, surely.
<br>
<p><em>&gt; staying alive, the rest was largely random experimentation (with
</em><br>
<em>&gt; sudden bangs like Cambrium when occasional tricks became available).
</em><br>
<p>I'm very interested in the shape of these tricks. I cannot currently think
<br>
of any easy solutions as to flush out these ever-elusive higher order
<br>
mechanisms from the sea of flat numbers constituting the genome.
<br>
<p><em>&gt; As for taking over computational resources, that implies that
</em><br>
<em>&gt; intelligence is just a question of sufficient resrources and not much
</em><br>
<em>&gt; of an algorithmic problem. But so far all evidence seem to point
</em><br>
<p>The rate of discovery of new algorithms is certainly powered by the search
<br>
resources available to us. It is both the question of smarts (hitting the
<br>
right needle in the stack of the initial parameter space in a rather
<br>
bruteforceish, but positive-autofeedbacking manner), and to have enough
<br>
firepower to keep going, puncturing subsequent kinetic bareers along the
<br>
way.
<br>
<p>It is a demanding mental work, to keep zooming out from the problem so
<br>
that you can become increasibly flexible as you eliminate hidden crippling
<br>
built-in assumptions, of course making the search space larger, and thus
<br>
needing these rather absurd hardware requirements I mentioned.
<br>
<p>It is certainly hard to get going. There seems to be a bareer in there,
<br>
which might take several decades of hard work to pierce. It looks like
<br>
bootstrap a lot.
<br>
<p><em>&gt; towards algorithms being very important; having more resources speed
</em><br>
<em>&gt; up research, but I have not seen any evidence (yes, not even Moravec's
</em><br>
<em>&gt; _Robot_) that suggest that if we could just use a lot of computing
</em><br>
<em>&gt; power we would get smarter behavior. Besides, hostile takeovers of net
</em><br>
<p>I'm sorry if I came over as a brute force magalomaniac. You can keep
<br>
cooking random bitsoup in circumstellar computronium clouds until the
<br>
galactic cows come home, and not producing any solutions if the initial
<br>
set of parameters is not right.
<br>
<p><em>&gt; resources are rather uncertain operations, highly dependent on the
</em><br>
<em>&gt; security culture at the time, which is hard to predict.
</em><br>
<p>Code reviews do help, but we're trapped within the constraints of the
<br>
system: very low variation across population, not smooth degradation but
<br>
sudden failure (imagine a dog having a sudden coredump if it runs over a
<br>
certain tile pattern while the sun illuminating it in a very specific
<br>
angle, and doing perfectly fine under any other conditions).
<br>
<p><em>&gt; &gt; This is a very favourable scenario, especially if it occurs
</em><br>
<em>&gt; &gt; relatively early, because it highly hardens the network layer
</em><br>
<em>&gt; &gt; against future perversion attempts by virtue of establishing
</em><br>
<em>&gt; &gt; a baseline diversity and response adaptiveness.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Sounds like a good defense in court :-)
</em><br>
<p>I'm rooting for the (hopefully coming) Microsoft Net worm, of orders
<br>
magnitude bigger proportions than Morris' one. You don't even need true
<br>
polymorphism, just enough variation to escape pattern-matcher vaccines,
<br>
and library of canned exploits, preferably few of them undocumented ones.
<br>
It would wake up people as to the damage potential, and would readjust the
<br>
attitude in regards to holistic system security.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3691.html">Spike Jones: "Re: META: To my fellow Extropians, calc your ERA"</a>
<li><strong>Previous message:</strong> <a href="3689.html">Natasha Vita-More: "Re: BIOTECH: Farners are not shunning biotech this season"</a>
<li><strong>In reply to:</strong> <a href="3680.html">Anders Sandberg: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3725.html">Damien Sullivan: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3690">[ date ]</a>
<a href="index.html#3690">[ thread ]</a>
<a href="subject.html#3690">[ subject ]</a>
<a href="author.html#3690">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 10:00:02 MDT</em>
</em>
</small>
</body>
</html>
