<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Lagrangian multipliers and neural learning?</title>
<meta name="Author" content="Christian Szegedy (szegedy@or.uni-bonn.de)">
<meta name="Subject" content="Lagrangian multipliers and neural learning?">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Lagrangian multipliers and neural learning?</h1>
<!-- received="Mon May 14 11:55:42 2001" -->
<!-- isoreceived="20010514175542" -->
<!-- sent="Mon, 14 May 2001 19:55:37 +0200" -->
<!-- isosent="20010514175537" -->
<!-- name="Christian Szegedy" -->
<!-- email="szegedy@or.uni-bonn.de" -->
<!-- subject="Lagrangian multipliers and neural learning?" -->
<!-- id="200105141755.TAA86321@mailserv.or.uni-bonn.de" -->
<strong>From:</strong> Christian Szegedy (<a href="mailto:szegedy@or.uni-bonn.de?Subject=Re:%20Lagrangian%20multipliers%20and%20neural%20learning?&In-Reply-To=&lt;200105141755.TAA86321@mailserv.or.uni-bonn.de&gt;"><em>szegedy@or.uni-bonn.de</em></a>)<br>
<strong>Date:</strong> Mon May 14 2001 - 11:55:37 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4328.html">Robin Hanson: "Re: Opinions as Evidence: Should Rational Bayesian Agents Commonize Priors"</a>
<li><strong>Previous message:</strong> <a href="4326.html">Adrian Tymes: "Re: I strongly disagree with Lee's answer"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4341.html">Anders Sandberg: "Re: Lagrangian multipliers and neural learning?"</a>
<li><strong>Reply:</strong> <a href="4341.html">Anders Sandberg: "Re: Lagrangian multipliers and neural learning?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4327">[ date ]</a>
<a href="index.html#4327">[ thread ]</a>
<a href="subject.html#4327">[ subject ]</a>
<a href="author.html#4327">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
I am not an expert of neural networks and AI, but I am
<br>
interested in them know some of the most basic ideas there.
<br>
I am curious about the opinion and thoughts of the gurus
<br>
in this list on the (possible) connection between neural
<br>
learning and Lagrangian optimization.
<br>
<p>Recently, worked on some very new and cute
<br>
optimization algorithms using sophisticated Lagrangian 
<br>
relaxation techniques, which seems to be a topic 
<br>
becoming more and more recognized in areas outside 
<br>
the classical nonlinear optimization to which they belongs 
<br>
historically. These methods based on a duality between
<br>
two types of variables: the original ones and the 
<br>
so called &quot;Lagrangian multipliers&quot; which may be familiar
<br>
to some of you from elementary analysis courses.
<br>
Generally they are much more useful than a simple 
<br>
optimization method learnt there. There is huge theory
<br>
about them with deep results and a lot of applications.
<br>
<p>An interesting thing about them (to me) is that they seem
<br>
to be closely related to neural learning algorithms, since
<br>
this lagrangian multipliers can be interpreted as a 
<br>
weighting of some components of the dual of the objective
<br>
function. So during the algorithms, this dual variables
<br>
punish the &quot;bad&quot; variables by increase their weight, so
<br>
the algorithm can learn that they are &quot;dangerous&quot; and
<br>
should not increase (or decrease) them.
<br>
<p>Of course this description was highly surficial and far
<br>
from being scientific.
<br>
<p>I also know that AI consists of much more than neural 
<br>
learning, but this is also considered to be a possible 
<br>
component of a future AI.
<br>
<p>My question: is this connection recognized and exploited
<br>
by the researchs of AI?
<br>
Do they use the results and strength of the theory and 
<br>
methods of Lagrangian relaxation  explicitly or implicitely?
<br>
<p>(It is well possible that this connections are already 
<br>
discovered and heavily used by AI experts: I am a newbie
<br>
in this terrain.)
<br>
<p>Thanks, Christian Szegedy
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4328.html">Robin Hanson: "Re: Opinions as Evidence: Should Rational Bayesian Agents Commonize Priors"</a>
<li><strong>Previous message:</strong> <a href="4326.html">Adrian Tymes: "Re: I strongly disagree with Lee's answer"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4341.html">Anders Sandberg: "Re: Lagrangian multipliers and neural learning?"</a>
<li><strong>Reply:</strong> <a href="4341.html">Anders Sandberg: "Re: Lagrangian multipliers and neural learning?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4327">[ date ]</a>
<a href="index.html#4327">[ thread ]</a>
<a href="subject.html#4327">[ subject ]</a>
<a href="author.html#4327">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 10:00:05 MDT</em>
</em>
</small>
</body>
</html>
