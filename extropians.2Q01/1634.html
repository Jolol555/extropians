<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Emulation vs. Simulation</title>
<meta name="Author" content="Jim Fehlinger (fehlinger@home.com)">
<meta name="Subject" content="Re: Emulation vs. Simulation">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Emulation vs. Simulation</h1>
<!-- received="Tue Mar 27 21:19:55 2001" -->
<!-- isoreceived="20010328041955" -->
<!-- sent="Tue, 27 Mar 2001 22:54:50 -0500" -->
<!-- isosent="20010328035450" -->
<!-- name="Jim Fehlinger" -->
<!-- email="fehlinger@home.com" -->
<!-- subject="Re: Emulation vs. Simulation" -->
<!-- id="3AC1608A.35242D6F@home.com" -->
<!-- inreplyto="200103270737.XAA18796@finney.org" -->
<strong>From:</strong> Jim Fehlinger (<a href="mailto:fehlinger@home.com?Subject=Re:%20Emulation%20vs.%20Simulation&In-Reply-To=&lt;3AC1608A.35242D6F@home.com&gt;"><em>fehlinger@home.com</em></a>)<br>
<strong>Date:</strong> Tue Mar 27 2001 - 20:54:50 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1635.html">Mark Walker: "Re: WAS: Re: Economic (ignorance) Nativism and me"</a>
<li><strong>Previous message:</strong> <a href="1633.html">Robert J. Bradbury: "Re: Emulation vs. Simulation"</a>
<li><strong>In reply to:</strong> <a href="1579.html">hal@finney.org: "Re: Emulation vs. Simulation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1630.html">Lee Corbin: "Re: Emulation vs. Simulation"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1634">[ date ]</a>
<a href="index.html#1634">[ thread ]</a>
<a href="subject.html#1634">[ subject ]</a>
<a href="author.html#1634">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
<a href="mailto:hal@finney.org?Subject=Re:%20Emulation%20vs.%20Simulation&In-Reply-To=&lt;3AC1608A.35242D6F@home.com&gt;">hal@finney.org</a> wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Jim Fehlinger, &lt;<a href="mailto:fehlinger@home.com?Subject=Re:%20Emulation%20vs.%20Simulation&In-Reply-To=&lt;3AC1608A.35242D6F@home.com&gt;">fehlinger@home.com</a>&gt;, writes:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; In George Lakoff's words &quot;Functionalism... is the theory that
</em><br>
<em>&gt; &gt; all aspects of mind can be characterized adequately without
</em><br>
<em>&gt; &gt; looking at the brain...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This seems to be a somewhat contradictory definition.  The point is
</em><br>
<em>&gt; that the manner in which you &quot;program&quot; a brain to run an algorithm is
</em><br>
<em>&gt; by arranging the details of neural connection and tissue organization.
</em><br>
<p>Well, the cognitivists of the 70's and 80's didn't bother about &quot;the
<br>
details of neural connection and tissue organization&quot;.  They were
<br>
concerned with abstract information-processing models which, as
<br>
far as they were concerned, might just as well be executing on a
<br>
digital computer as have anything to do with action potentials,
<br>
ion channels, and that grey and white glob of jello between the ears.
<br>
<p>The progression of &quot;hard&quot;, scientific psychology during the 20th
<br>
century (after the great days of William James) is:
<br>
<p>1.  Behaviorism.  Study observable stimulus and measurable behavior.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;What's inside the organism's skin is strictly off limits --
<br>
&nbsp;&nbsp;&nbsp;&nbsp;the organism is a black box.
<br>
<p>2.  Cognitivism.  You can theorize about what's inside the skin, but
<br>
&nbsp;&nbsp;&nbsp;&nbsp;you needn't actually bother about all that slimy stuff inside.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;Just treat the organism as if it could be replaced
<br>
&nbsp;&nbsp;&nbsp;&nbsp;by a computer hooked up to equivalent input and output devices.
<br>
<p>3.  The 90's thing (what's it called, anyway?)  Whatever science-
<br>
&nbsp;&nbsp;&nbsp;&nbsp;fictional Gedanken experiments can be cooked up about using
<br>
&nbsp;&nbsp;&nbsp;&nbsp;computronium to simulate physical reality at arbitrary levels
<br>
&nbsp;&nbsp;&nbsp;&nbsp;of detail, you aren't going to get anywhere figuring out how
<br>
&nbsp;&nbsp;&nbsp;&nbsp;brains work unless you actually look at the brains in detail.
<br>
<p><em>&gt; You can't be concerned about the algorithm without being concerned about
</em><br>
<em>&gt; the structural details.
</em><br>
<p>Yes, but those can be abstracted into a formal, high-level description
<br>
of the program.  As long as you've got a compiler or interpreter (a.k.a
<br>
&quot;virtual machine&quot;) on your computer for that high-level formal language,
<br>
then whether the physical computer's a Mac or a PC, the program will **function**
<br>
the same.
<br>
<p><em>&gt; &gt; &quot;In the functionalist view, what is ultimately important...
</em><br>
<em>&gt; &gt; are the algorithms, not the hardware on which they are executed...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is like saying that we care in a computer about the program it is
</em><br>
<em>&gt; running, but the contents of its memory and storage shouldn't concern us.
</em><br>
<p>It's like saying we don't care whether the program is running on a
<br>
Univac I, a PDP-10, or a Pentium (apart from questions of performance);
<br>
whether the memory is magnetic drum, acoustic delay line, vacuum tube,
<br>
or CMOS.  For that matter, the program could be run by a person
<br>
reading the code off sheets of paper, and using a pencil (and an
<br>
eraser) to write, erase, and re-write paper &quot;storage&quot; (I have worked
<br>
with people who learned to program that way, in places and at times
<br>
where real computers were scarce -- like in India 20 years ago).
<br>
<p><em>&gt; The problem is, it is the contents of its memory which determine what
</em><br>
<em>&gt; program it runs.  So if we are concerned about the program, we must
</em><br>
<em>&gt; therefore be concerned about the contents of memory.
</em><br>
<p>But the physical implementation of that memory is of no concern,
<br>
in the functionalist view.
<br>
<p>Here's what McCrone has to say about this in _Going Inside_:
<br>
<p>&quot;With hindsight, it seems odder and odder that mainstream psychologists
<br>
were so intent on studying the mind without also studying the brain.
<br>
Even if they could not do actual experiments, there was already
<br>
enough known about neurology to help frame their theories.  However,
<br>
a generation of researchers had grown up in the belief that
<br>
information processing was all about programs.  It did not really
<br>
matter what kind of hardware a program was running on -- whether it
<br>
was flesh and blood or silicon -- so long as the underlying logic
<br>
was preserved.  The brain was simply a particular implementation
<br>
of something more general.  So how the brain might choose to arrange
<br>
its circuits was of marginal interest at best.
<br>
<p>The justification for this view was put forward in 1936 by the
<br>
spiritual father of computing, the British mathematician Alan Turing.
<br>
Turing's proof was famously simple.  He created an imaginary device,
<br>
later dubbed the Turing machine, which was nothing more than a
<br>
long strip of paper tape and a processing gate which could carry out
<br>
four operations.  The gate could move the tape a step to the left
<br>
or the right and then it could either print or erase a mark.  Given
<br>
an infinite amount of time and an infinite length of tape, Turing
<br>
demonstrated that this most rudimentary of computers could
<br>
crunch its way through any problem that could be reduced to a
<br>
string of 0's and 1's.  Using a binary code to represent both
<br>
the data and the instructions which told the gate how to manipulate
<br>
the data, a Turing machine had all it needed to get the job
<br>
done.
<br>
<p>For computer science, this proof was enormously important because
<br>
it said that all computers were basically the same...  Whether
<br>
a machine used a single gate, or millions, or trillions;
<br>
whether it was built of paper tape, silicon chips, or something
<br>
really exotic like beams of laser light, the principles of its
<br>
operation would be identical...
<br>
<p>In 1960, one of the founders of cognitive science, the Princeton
<br>
philosopher Hilary Putnam, seized on Turing's proof to argue
<br>
that it meant brains did not matter.  If the processes of the
<br>
human mind could be expressed in computational form, then any
<br>
old contraption could be used to recreate what brains did.
<br>
The brain might be an incredibly complicated system, involving
<br>
billions of nerve cells all chattering at once -- not to
<br>
mention the biochemical reactions taking place within each
<br>
cell -- but, in the end, everything boiled down to a shifting
<br>
pattern of information.  It was the logic of what the brain was
<br>
trying to do that counted.  So given enough time, even the
<br>
simplest Turing machine could recreate these flows...
<br>
<p>...[T]here was a noticeable difference between the way
<br>
computer scientists and psychologists talked about the issue.
<br>
Those on the computer side of the divide could be as bullish
<br>
as they liked.  Many seemed convinced their creations were
<br>
practically conscious already; certainly, artificial
<br>
intelligence was only a matter of decades away.  The
<br>
psychologists had to choose their words more carefully.
<br>
Yet what Turing's proof did mean was that they never need
<br>
feel guilty about failing to take a neuroscience class
<br>
or open a volume on neuroanatomy.  During the 1970s and for
<br>
most of the 1980s, it was information theory which was
<br>
the future of mind science.  So while a psychologist might be
<br>
embarrassed by not being up to date with the latest
<br>
programming tricks or computer jargon, a complete
<br>
ignorance of the brain was no bar to a successful career.&quot;
<br>
<p>--  _Going Inside: A Tour Round a Single Moment of Consciousness_
<br>
&nbsp;&nbsp;&nbsp;Chapter 2, &quot;Disturbing the Surface&quot;, pp. 22-24
<br>
<p>And here's what Edelman has to say about functionalism:
<br>
<p>&quot;A persuasive set of arguments states that if I can describe
<br>
an effective mathematical procedure (technically called an
<br>
algorithm...), then that procedure can be carried out by a
<br>
Turing machine.  More generally, we know that **any**
<br>
algorithm or effective procedure may be executed by any
<br>
universal Turing machine.  The existence of universal
<br>
machines implies that the **mechanism** of operation of
<br>
any one of them is unimportant.  This can be shown in the
<br>
real world by running a given program on two digital computers
<br>
of radically different construction or hardware design and
<br>
successfully obtaining identical results...
<br>
<p>On the basis of these properties, the workings of the brain
<br>
have been considered to be the result of a &quot;functional&quot;
<br>
process, one held to be describable in a fashion similar
<br>
to that used for algorithms.  This point of view is
<br>
called functionalism (and in one of its more trenchant forms,
<br>
Turing machine functionalism).  Functionalism assumes
<br>
psychology can be adequately described in terms of the
<br>
&quot;functional organization of the brain&quot; -- much in the way
<br>
that software determines the performance of computer
<br>
hardware...
<br>
<p>This &quot;liberal&quot; position affirming the absence of any need
<br>
for particular kinds of brain tissue suffuses much of
<br>
present-day cognitive psychology...
<br>
<p>For problems that can be solved consistently in a finite
<br>
amount of time, a Turing machine is as powerful as any
<br>
other entity for solving the problem, **including the brain**.
<br>
According to this analysis, either the brain is a computer,
<br>
or the computer is an adequate model or analogue for
<br>
the interesting things that the brain does.
<br>
<p>This kind of analysis underlies what has become known
<br>
as the physical symbol system hypothesis, which provides
<br>
the basis for most research in artificial intelligence.
<br>
This hypothesis holds that cognitive functions are carried
<br>
out by the manipulation of symbols according to rules.
<br>
In physical symbol systems, symbols are instantiated in
<br>
a program as states of physical objects.  Strings of
<br>
symbols are used to represent sensory inputs, categories,
<br>
behaviors, memories, logical propositions, and indeed
<br>
all the information that the system deals with...
<br>
<p>If any of the forms of functionalism is a correct
<br>
theory of the mind, then the brain is truly analogous
<br>
to a Turing machine.  And in that case, the relevant
<br>
level of description for both is the level of symbolic
<br>
representation and of algorithms, not of biology...
<br>
<p>Why won't this position do?  The reasons are many...
<br>
<p>An analysis of the evolution, development, and structure
<br>
of brains makes it highly unlikely that they are
<br>
Turing machines.  As we saw..., brains possess enormous
<br>
individual structural variation at a variety of
<br>
organizational levels.  An examination of the means
<br>
by which brains develop indicates that each brain is
<br>
highly variable...  [E]ach organism's behavior is
<br>
biologically individual and enormously diverse...
<br>
<p>More damaging is the fact that an analysis of ecological
<br>
and environmental variation and of the categorization
<br>
procedures of animals and humans... makes it unlikely
<br>
that the world (physical and social) could function
<br>
as a tape for a Turing machine...  The brain and
<br>
nervous system cannot be considered in isolation from
<br>
states of the world and social interactions.  But
<br>
such states, both environmental and social, are
<br>
indeterminate and open-ended.  They cannot be simply
<br>
identified by any software description...
<br>
<p>What is at stake here is the notion of meaning.
<br>
Meaning, as Putnam puts it, 'is interactional.
<br>
The environment itself plays a role in determining
<br>
what a speaker's words, or a community's words,
<br>
refer to.'  Because such an environment is open-ended,
<br>
it admits of no a priori inclusive description in
<br>
terms of effective procedures...
<br>
<p>Now we begin to see why digital computers are a false
<br>
analogue to the brain.  The facile analogy with
<br>
digital computers breaks down for several reasons.
<br>
The tape read by a Turing machine is marked unambiguously
<br>
with symbols chosen from a finite set; in contrast,
<br>
the sensory signals available to nervous systems are
<br>
truly analogue in nature and therefore are neither
<br>
unambiguous nor finite in number.  Turing machines
<br>
have by definition a finite number of internal states,
<br>
while there are no apparent limits on the number of
<br>
states the human nervous system can assume (for example,
<br>
by analog modulation of large numbers of synaptic
<br>
strengths in neuronal connections).  The transitions
<br>
of Turing machines between states are entirely
<br>
deterministic, while those of humans give ample appearance
<br>
of indeterminacy.  Human experience is not based on
<br>
so simple an abstraction as a Turing machine; to get
<br>
our 'meanings' we have to grow and communicate in
<br>
a society.&quot;
<br>
<p>-- _Bright Air, Brilliant File_,
<br>
&nbsp;&nbsp;&nbsp;&quot;Mind Without Biology: A Critical Postscript&quot;, pp. 220-225
<br>
<p>Jim F.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1635.html">Mark Walker: "Re: WAS: Re: Economic (ignorance) Nativism and me"</a>
<li><strong>Previous message:</strong> <a href="1633.html">Robert J. Bradbury: "Re: Emulation vs. Simulation"</a>
<li><strong>In reply to:</strong> <a href="1579.html">hal@finney.org: "Re: Emulation vs. Simulation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1630.html">Lee Corbin: "Re: Emulation vs. Simulation"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1634">[ date ]</a>
<a href="index.html#1634">[ thread ]</a>
<a href="subject.html#1634">[ subject ]</a>
<a href="author.html#1634">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:59:43 MDT</em>
</em>
</small>
</body>
</html>
