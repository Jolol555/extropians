<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Keeping AI at bay (was: How to help create a si</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Keeping AI at bay (was: How to help create a singularity)">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Keeping AI at bay (was: How to help create a singularity)</h1>
<!-- received="Mon Apr 30 15:32:19 2001" -->
<!-- isoreceived="20010430213219" -->
<!-- sent="Mon, 30 Apr 2001 17:30:30 -0400" -->
<!-- isosent="20010430213030" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Keeping AI at bay (was: How to help create a singularity)" -->
<!-- id="3AEDD976.A86A8A1A@pobox.com" -->
<!-- inreplyto="3AEDE7CC.B69B47A0@lrz.uni-muenchen.de" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Keeping%20AI%20at%20bay%20(was:%20How%20to%20help%20create%20a%20singularity)&In-Reply-To=&lt;3AEDD976.A86A8A1A@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Apr 30 2001 - 15:30:30 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3519.html">Eliezer S. Yudkowsky: "Re: Maximizing results of efforts Re: Mainstreaming"</a>
<li><strong>Previous message:</strong> <a href="3517.html">R.: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>In reply to:</strong> <a href="3514.html">Eugene.Leitl@lrz.uni-muenchen.de: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3755.html">Nick Bostrom: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Reply:</strong> <a href="3755.html">Nick Bostrom: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3518">[ date ]</a>
<a href="index.html#3518">[ thread ]</a>
<a href="subject.html#3518">[ subject ]</a>
<a href="author.html#3518">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
<a href="mailto:Eugene.Leitl@lrz.uni-muenchen.de?Subject=Re:%20Keeping%20AI%20at%20bay%20(was:%20How%20to%20help%20create%20a%20singularity)&In-Reply-To=&lt;3AEDD976.A86A8A1A@pobox.com&gt;">Eugene.Leitl@lrz.uni-muenchen.de</a> wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I do not care what the AIs want, as long as they're something
</em><br>
<em>&gt; really big, fast, powerful, and growing exponentially, and having
</em><br>
<em>&gt; -- even unbiased -- agendas of their own. And they will be that, very
</em><br>
<em>&gt; soon, if they're subject to evolutionary  constraints. Which seems
</em><br>
<em>&gt; about the only bit of prejudice I have.
</em><br>
<p><a href="http://singinst.org/CaTAI/friendly/info/indexfaq.html#q_2.1">http://singinst.org/CaTAI/friendly/info/indexfaq.html#q_2.1</a>
<br>
<a href="http://singinst.org/CaTAI/friendly/anthro.html#observer">http://singinst.org/CaTAI/friendly/anthro.html#observer</a>
<br>
<a href="http://singinst.org/CaTAI/friendly/design/seed.html#directed">http://singinst.org/CaTAI/friendly/design/seed.html#directed</a>
<br>
<p><em>&gt; What is it with you people, are you just generically unreasonable,
</em><br>
<em>&gt; or do you habitually let your kids play russian roulette; or
</em><br>
<em>&gt; soccer on a minefield?
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't even have to be absolutely 100.1% positive about it, as
</em><br>
<em>&gt; long as I think there's a nonnegligable probability of Ragnar0k,
</em><br>
<em>&gt; I just think it's a monumentally bad idea to attempt. Billie Joy
</em><br>
<em>&gt; is right on the money on this one. Relinquishment, shmerlinquishment,
</em><br>
<em>&gt; just hold the horses until we're ready.
</em><br>
<p>It is, I suppose, emotionally understandable that you should have an
<br>
allergy to planetary risks (what Nick Bostrom calls &quot;existential&quot; risks -
<br>
is that paper out yet, Nick?).  But the simple fact is that it is not
<br>
entirely possible to eliminate existential risks, no matter how hard we
<br>
try.  The question is how to minimize existential risk.  For example, it
<br>
is better to develop Friendly AI before nanotechnology than vice versa,
<br>
because success in Friendly AI increases the chance of success in
<br>
nanotechnology more than success in nanotechnology increases the chance of
<br>
success in Friendly AI.
<br>
<p>An emotional intolerance to existential risk leads to disaster; it means
<br>
that a 1% probability of a 0% risk and a 99% probability of a 60% risk
<br>
will be preferred to a 100% probability of a 20% risk.  This is the basic
<br>
emotional driver behind Bill Joy's theory of relinquishment - as far as
<br>
he's concerned, there's no such thing as a necessary existential risk, so
<br>
even a 1% chance of avoiding all existential risks must be preferable to
<br>
accepting a single existential risk - even if the actions required to
<br>
create that 1% probability result in a 99% probability of a *much* *worse*
<br>
existential risk, such as AI or nanotechnology being created in secrecy by
<br>
rogue factions.  This only falls out of the equation above if you, like
<br>
Bill Joy, treat the possibility of a 60% existential risk and the
<br>
possibility of a 20% existential risk as identical total disasters.  Which
<br>
is what I mean when I say than an emotional intolerance of existential
<br>
risk leads to disaster.
<br>
<p><em>&gt; Is it too much too ask?
</em><br>
<p>Unless you explain why slowing down decreases risk, in at least as much
<br>
detail as I've explained why slowing down increases risk, yes.
<br>
<p><a href="http://singinst.org/CaTAI/friendly/policy.html#comparative">http://singinst.org/CaTAI/friendly/policy.html#comparative</a>
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3519.html">Eliezer S. Yudkowsky: "Re: Maximizing results of efforts Re: Mainstreaming"</a>
<li><strong>Previous message:</strong> <a href="3517.html">R.: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>In reply to:</strong> <a href="3514.html">Eugene.Leitl@lrz.uni-muenchen.de: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3755.html">Nick Bostrom: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Reply:</strong> <a href="3755.html">Nick Bostrom: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3518">[ date ]</a>
<a href="index.html#3518">[ thread ]</a>
<a href="subject.html#3518">[ subject ]</a>
<a href="author.html#3518">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 10:00:01 MDT</em>
</em>
</small>
</body>
</html>
