<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Emulation vs. Simulation</title>
<meta name="Author" content="Robert J. Bradbury (bradbury@aeiveos.com)">
<meta name="Subject" content="Re: Emulation vs. Simulation">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Emulation vs. Simulation</h1>
<!-- received="Mon Mar 26 11:17:15 2001" -->
<!-- isoreceived="20010326181715" -->
<!-- sent="Mon, 26 Mar 2001 10:17:09 -0800 (PST)" -->
<!-- isosent="20010326181709" -->
<!-- name="Robert J. Bradbury" -->
<!-- email="bradbury@aeiveos.com" -->
<!-- subject="Re: Emulation vs. Simulation" -->
<!-- id="Pine.UW2.4.20.0103260925030.444-100000@www.aeiveos.com" -->
<!-- inreplyto="200103260728.BAA26872@dal-mail2.ricochet.net" -->
<strong>From:</strong> Robert J. Bradbury (<a href="mailto:bradbury@aeiveos.com?Subject=Re:%20Emulation%20vs.%20Simulation&In-Reply-To=&lt;Pine.UW2.4.20.0103260925030.444-100000@www.aeiveos.com&gt;"><em>bradbury@aeiveos.com</em></a>)<br>
<strong>Date:</strong> Mon Mar 26 2001 - 11:17:09 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1538.html">Brian D Williams: "Re: Economic (ignorance) Nativism and me"</a>
<li><strong>Previous message:</strong> <a href="1536.html">Michael Lorrey: "Re: LUDD: &quot;The left is to blame for scaremongering&quot;"</a>
<li><strong>In reply to:</strong> <a href="1510.html">Lee Corbin: "Re: Emulation vs. Simulation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1576.html">Lee Corbin: "Re: Emulation vs. Simulation"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1537">[ date ]</a>
<a href="index.html#1537">[ thread ]</a>
<a href="subject.html#1537">[ subject ]</a>
<a href="author.html#1537">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
On Sun, 25 Mar 2001, Lee Corbin wrote:
<br>
<p><em>&gt; The whole question is, how plausible really is the existence of a
</em><br>
<em>&gt; statistical response unit, and how well could it survive on its own?
</em><br>
<p>Ah ha, he says, rubbing his hands with an evil grin on his face...
<br>
(How I love it when a trap I didn't even know I had set snaps shut...)
<br>
<p>But Lee *you* ARE a &quot;statistical response unit&quot;.  Presumably you
<br>
survive quite well 'on your own'.  I'd say 90+% of human behaviors
<br>
are completely canned.  Do you actually consciously 'think' about
<br>
brushing your teeth, driving your car, what is being said by
<br>
someone you are being 'polite to' in listening to them, etc.
<br>
<p>We all have 'canned', 'mechanical' behaviors that we have learned
<br>
that get executed at a sub-conscious level.  We have no awareness
<br>
of them at all unless the toothpaste tastes funny, someone runs
<br>
in front of the car, or the person going 'blah, blah, blah' suddenly
<br>
says 'and then I'm going to take this knife and stab you with it...'.
<br>
In those situations our subprocessors bring those 'variances' to
<br>
our attention and we make decisions about them.  If we had enough
<br>
experiences with the exceptions to the rules, those too would become
<br>
rules.  Since humanity collectively 'survives', I would say that
<br>
the statistical behavior of humanity, programmed into a zombie
<br>
would similarly be likely to survive.  The world has gone from
<br>
being a very dangerous place to being for the most part a very
<br>
safe place.  That has allowed us to relax the amount of consciousness
<br>
we need for survival.
<br>
<p>Interesting to consider that the simulations may 'have' to run this
<br>
way because as the population increases, you want to make the
<br>
simulation a more zombie safe world so there are rarely those
<br>
situations in which the lookup table comes up empty.  We can
<br>
all tolerate wierd behaviors by people &quot;some&quot; of the time,
<br>
we just can't tolerate it &quot;all&quot; of the time.  By safety-izing
<br>
the simulation, you make it computationally less burdensome.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; I contend that these &quot;common responses&quot; are inadequate for survival in
</em><br>
<em>&gt; the real world.  To be very concrete, let us ask what is the minimal
</em><br>
<em>&gt; programmable unit that could (a) hold down a job at Microsoft or Sun,
</em><br>
<em>&gt; (b) drive to work every day, (c) shop and do all the mundane things
</em><br>
<em>&gt; (e.g., fill out drivers license applications) necessary to 21st century
</em><br>
<em>&gt; existence?
</em><br>
<p>(b) &amp; (c) are largely statistical. (a) I'm going to have to think
<br>
about.  How do zombies handle 'creativity'.  Or does this get
<br>
swept aside by the fact that zombies don't have to really be creative?
<br>
Is the SI is the one really writing the code they produce, not the zombie?
<br>
<p><em>&gt; Forget Silicon Valley: even if it was possible in *ancient Sumeria*
</em><br>
<em>&gt; for someone to do what it took to survive back there, why wasn't
</em><br>
<em>&gt; there a series of natural mutations that got rid of all the excess
</em><br>
<em>&gt; baggage like consciousness, feelings, etc.?
</em><br>
<p>Ancient Sumeria was much harder to survive in that SV most likely.
<br>
(The average lifespan *was* much less.)  &quot;Consciousness&quot; confers
<br>
survival advantages by allowing you to practice future behaviors.
<br>
It just doesn't get used once the behaviors have been learned and
<br>
proven successful.
<br>
<p><em>&gt; The answer is that it's not possible.  It, like so many other
</em><br>
<em>&gt; programming projects, only seems feasible.  The first AI that
</em><br>
<em>&gt; would be capable of getting along in Silicon Valley (or ancient
</em><br>
<em>&gt; Sumeria) would be almost exactly as conscious and feeling as
</em><br>
<em>&gt; humans are.
</em><br>
<p>Aw, now don't go putting consciousness &amp; feeling back together
<br>
again when I'd worked so hard to separate them.  'Feelings' are
<br>
the feedback loops that inform about or the irritants that drive
<br>
behaviors that work or don't work.  They are our hardware
<br>
(genetic) and firmware (learned at a very young age)
<br>
'strings' that tell us what needs is a threat (that should be
<br>
avoided or something of benefit to be sought after.  They can
<br>
be very nice and wonderful at times (and painful at others)
<br>
but they shouldn't be considered at the level of consciousness.
<br>
<p>I think the answer is that the zombie, like humans will have
<br>
to have a default subroutine that manufactures a behavior when
<br>
the correct one is not known.  This doesn't have to be very
<br>
sophisticated.  You can say &quot;here I laugh&quot; or &quot;here I cry&quot;
<br>
or &quot;here I sit and hug myself&quot;.  What is so complex about
<br>
that?
<br>
<p><em>&gt; Okay, suppose that we have a creature (that I still don't want
</em><br>
<em>&gt; to call a zombie) which has 10^x times as much storage capability
</em><br>
<em>&gt; as a human being, and it merely does a lookup for everything that
</em><br>
<em>&gt; could conceivably happen to it.  It never really calculates
</em><br>
<em>&gt; anything.  I will officially concede that if x is a big enough
</em><br>
<em>&gt; number, then the entity is not conscious, and therefore is a
</em><br>
<em>&gt; zombie.
</em><br>
<p>Oh ho, point taken, thank you very much. (Not that I'm really
<br>
winning anything here, but we are learning how to make zombies.)
<br>
<p><em>&gt; A. A puppet or apparition run by a remote SI
</em><br>
<em>&gt; B. A self-contained entity with a fantastically large lookup table
</em><br>
<em>&gt; C. A self-contained entity with capabilities of human or above,
</em><br>
<em>&gt;    but which calculates its behavior (doesn't look it up)
</em><br>
<em>&gt; D. A self-contained entity that is like C, but isn't conscious
</em><br>
<em>&gt; 
</em><br>
Good summary.
<br>
<p><em>&gt; If D, then it's not smart enough to survive in a challenging
</em><br>
<em>&gt; environment (unlike a dog).
</em><br>
<p>Hmmmm, but it is generally thought that the only 'conscious'
<br>
animals are from the ape-level to humans.  Go back in the
<br>
archives and look up the mirror (self-recognition) test
<br>
discussions (in fall of '99 I think).  That means all other
<br>
animals down to the level of fish survive *quite* will with
<br>
little or no consciosness.  So for your statement above
<br>
to be true you seem to be saying that the natural environment
<br>
that these animals live in is not 'challenging'.
<br>
<p>For you to link 'consciousness' with 'survival' you are
<br>
going to have to link all K-strategy animals (long lived)
<br>
with consciousness!
<br>
<p><p><em>&gt; Only case B might be considered a zombie, but that case is not
</em><br>
<em>&gt; what people are talking about on this list (until your post).
</em><br>
<p>I'm considering a zombie to be something that looks like a human,
<br>
walks like a human, talks like a human a human, behaves in
<br>
general like a human and survives like a human but has no
<br>
self-consciousness or self-awareness.  Its a means to running
<br>
a simulation with fewer resources, because you can use one
<br>
giant behavioral lookup table for *all* the zombies, you don't
<br>
have to actually simulate them as conscious processes.
<br>
<p>(But now that I've thought about this a bit more, see some of
<br>
my other posts, I don't think consciousness is that sophisticated,
<br>
so there is going to be an interesting tradeoff between the
<br>
memory required for behavioral lookup tables and actual
<br>
computational requirements for 'consciousness'.)
<br>
<p><em>&gt; 
</em><br>
<em>&gt; So therefore, any creature in your immediate vicinity is either
</em><br>
<em>&gt; a creature with a big lookup table that, because the table isn't
</em><br>
<em>&gt; big enough, still cannot survive on its own---or, it engages in
</em><br>
<em>&gt; calculation, and is really no more efficient about it than we
</em><br>
<em>&gt; are, and so therefore is conscious.
</em><br>
<p>So, now we are back to -- &quot;If it uses an equivalent number of
<br>
CPU cycles it is conscious?&quot;  I don't buy that at all.  Deep Blue
<br>
didn't have anywhere near the number of CPU cycles as Kasparov,
<br>
but it still managed to behave quite 'surprisingly' at times.
<br>
It did not however have an internal model of 'itself' to think
<br>
about -- It did however have an internal model of an external
<br>
reality which it could simulate.  Presumably it had 'feelings'
<br>
as well because it evaluated whether a position in that
<br>
external reality was 'good' or 'bad'.
<br>
<p>What do you call an entity with an internal model of an external
<br>
reality, but no concept of 'itself' in that external reality?
<br>
[Note -- its probably possible to argue to some limited extent
<br>
that Deep Blue pictured itself as the chess board state in
<br>
the external reality, but this gets very swampy.]
<br>
<p><em>&gt; (P.S.  I'm not even sure that
</em><br>
<em>&gt; 10^21 actions would be enough for a human---remember, it has to
</em><br>
<em>&gt; ask human IN EVERY POSSIBLE situation, which might mean millions
</em><br>
<em>&gt; of separate emulations had to go into the lookup table.)
</em><br>
<p>So, they go there very fast (at GHz rates) while humans don't
<br>
notice things occuring faster than about 100 Hz.  They can
<br>
have replicated copies of the table (you have got 10^40+ bits
<br>
of memory.  You can multi-port the memory accesses.  You
<br>
can do hashed, nested tables that minimize collisions to
<br>
identical locations.  There are a host of ways to solve
<br>
problems like this.
<br>
<p><em>&gt; As a slighly relevant aside, I hope you believe that Searle's
</em><br>
<em>&gt; Chinese Room is conscious, intelligent, and has feelings!?
</em><br>
<p>To be honest Lee, I can't remember precisely what this is
<br>
at this point (I know I've read it).   But taking a page
<br>
from Spike's book, I'm too lazy right now to go find it again.
<br>
Remember I'm a computer scientist and/or molecular biologist --
<br>
not a philsopher.  For that you have to go to Max or Nick.
<br>
<p>Robert
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1538.html">Brian D Williams: "Re: Economic (ignorance) Nativism and me"</a>
<li><strong>Previous message:</strong> <a href="1536.html">Michael Lorrey: "Re: LUDD: &quot;The left is to blame for scaremongering&quot;"</a>
<li><strong>In reply to:</strong> <a href="1510.html">Lee Corbin: "Re: Emulation vs. Simulation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1576.html">Lee Corbin: "Re: Emulation vs. Simulation"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1537">[ date ]</a>
<a href="index.html#1537">[ thread ]</a>
<a href="subject.html#1537">[ subject ]</a>
<a href="author.html#1537">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:59:43 MDT</em>
</em>
</small>
</body>
</html>
