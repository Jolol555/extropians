<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Contextualizing seed-AI proposals</title>
<meta name="Author" content="Jim Fehlinger (fehlinger@home.com)">
<meta name="Subject" content="Contextualizing seed-AI proposals">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Contextualizing seed-AI proposals</h1>
<!-- received="Thu Apr 12 17:33:31 2001" -->
<!-- isoreceived="20010412233331" -->
<!-- sent="Thu, 12 Apr 2001 19:06:23 -0400" -->
<!-- isosent="20010412230623" -->
<!-- name="Jim Fehlinger" -->
<!-- email="fehlinger@home.com" -->
<!-- subject="Contextualizing seed-AI proposals" -->
<!-- id="3AD634EF.A64C39B7@home.com" -->
<strong>From:</strong> Jim Fehlinger (<a href="mailto:fehlinger@home.com?Subject=Re:%20Contextualizing%20seed-AI%20proposals&In-Reply-To=&lt;3AD634EF.A64C39B7@home.com&gt;"><em>fehlinger@home.com</em></a>)<br>
<strong>Date:</strong> Thu Apr 12 2001 - 17:06:23 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2260.html">Travas Gunnell: "Modeling God (was Re: The pool we're trying to paddle in)"</a>
<li><strong>Previous message:</strong> <a href="2258.html">Technotranscendence: "OBJ: The Atlas Society"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2263.html">Damien Broderick: "Re: Contextualizing seed-AI proposals"</a>
<li><strong>Reply:</strong> <a href="2263.html">Damien Broderick: "Re: Contextualizing seed-AI proposals"</a>
<li><strong>Reply:</strong> <a href="2265.html">Eliezer S. Yudkowsky: "Re: Contextualizing seed-AI proposals"</a>
<li><strong>Maybe reply:</strong> <a href="2268.html">hal@finney.org: "Re: Contextualizing seed-AI proposals"</a>
<li><strong>Reply:</strong> <a href="2639.html">Eliezer S. Yudkowsky: "Re: Contextualizing seed-AI proposals"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2259">[ date ]</a>
<a href="index.html#2259">[ thread ]</a>
<a href="subject.html#2259">[ subject ]</a>
<a href="author.html#2259">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
I've been on the lookout, ever since I first stumbled across the
<br>
transhumanist and Extropian community on the Web three years ago,
<br>
for any sign that a consensus might be emerging as to how
<br>
quasi-human artificial intelligence is likely to come into
<br>
existence (never mind, for the moment, SI, Sysops, computronium,
<br>
or the Singularity), and how such speculations might tie in with
<br>
the larger context of the cognitive science / neuroscience
<br>
interplay that's been simmering in recent years.  I've been aware
<br>
of the latter ever since running into Edelman's _Bright Air,
<br>
Brilliant Fire_ in 1992, long before I knew about the Extropians,
<br>
and that book definitely caused me to prick up my ears (I had
<br>
gotten bored with the perennially-unfulfilled promise of AI, a
<br>
boredom only momentarily relieved by the appearance of Moravec's
<br>
_Mind Children_ in 1988).
<br>
<p>It has been my impression of ca. 1970-1990 cognitive-science-era
<br>
AI prototypes of the like of Eurisko, Automated Mathematician,
<br>
Copycat, and so on, that these efforts were all primarily or
<br>
exclusively **linguistic** in intent -- that both their input and
<br>
output domains were sets of sentences in a natural language, or
<br>
in a restricted subset of a natural language, or strings of
<br>
symbols or expressions in a formal language, or mathematical
<br>
expressions.
<br>
<p>It strikes me that a late-90's view of the relationship between
<br>
&quot;knowledge&quot; and language (as espoused, for instance, by Hilary
<br>
Putnam, George Lakoff, Gerald Edelman and others) would likely
<br>
view the latter as far too coarse a net in which to completely
<br>
capture the former.  Rather, language functions more like a sort
<br>
of shorthand which relies, for its usefulness, on the fact that
<br>
two interlocutors have biologically similar bodies and brains,
<br>
and already share a great many similar experiences.
<br>
<p>Thus, in the view of neurobiologist Walter Freeman, as reported
<br>
in -- you guessed it! -- _Going Inside: A Tour Round a Single
<br>
Moment of Consciousness_ by John McCrone, for each interlocutor,
<br>
&quot;the brain [brings] the full weight of a lifetime's experience to
<br>
bear on each moment as it [is] being lived.  Every second of
<br>
awareness as a child or as an adolescent [will] in some measure
<br>
be part of [vis] consciousness of the present.  That [is] what
<br>
being a memory landscape really [means].&quot;  McCrone goes on with a
<br>
direct quote of Freeman: &quot;The cognitive guys think it's just
<br>
impossible to keep throwing everything you've got into the
<br>
computation every time.  But that is exactly what the brain does.
<br>
Consciousness is about bringing your entire history to bear on
<br>
your next step, your next breath, your next moment.&quot;  (p. 268).
<br>
A linguistic exchange between two such &quot;memory landscapes&quot; relies
<br>
for its power on the fact that the receiver can **recreate** a
<br>
conscious state similar (enough) to that of the sender, rather
<br>
than on the unlikely interpretation that the message somehow
<br>
encodes all the complexity of the conscious state of the sender:
<br>
&quot;A word is no more than a puff of air, a growl in the throat.  It
<br>
is a token.  But saying a word has the effect of grabbing the
<br>
mind of a listener and taking it to some specific spot within
<br>
[vis] memories.&quot; (p. 293).
<br>
<p>Gerald Edelman and Giulio Tononi, in _A Universe of
<br>
Consciousness_, call this phenomenon &quot;complexity matching&quot;:
<br>
<p>&quot;For a small value of the **extrinsic** mutual information
<br>
between a stimulus and a neural system, there is generally a
<br>
large change in the **intrinsic** mutual information among
<br>
subsets of units within the neural system.  This change can be
<br>
measured by a quantity, called complexity matching..., which is
<br>
the change in neural complexity that occurs as a result of the
<br>
encounter with external stimuli.
<br>
<p>According to this analysis, extrinsic signals convey information
<br>
not so much in themselves, but by virtue of how they modulate the
<br>
intrinsic signals exchanged within a previously experienced
<br>
nervous system.  In other words, a stimulus acts not so much by
<br>
adding large amounts of extrinsic information that need to be
<br>
processed as it does by amplifying the intrinsic information
<br>
resulting from neural interactions selected and stabilized by
<br>
memory through previous encounters with the environment...  At
<br>
every instant, the brain goes far 'beyond the information given,'
<br>
and in conscious animals its response to an incoming stimulus is
<br>
therefore a 'remembered present.'
<br>
<p>...
<br>
<p>This conclusion is consistent with an everyday observation: The
<br>
same stimulus, say, a Chinese character, can be meaningful to
<br>
Chinese speakers and meaningless to English speakers even if the
<br>
extrinsic information conveyed to the retina is the same.
<br>
Attempts to explain this difference that are based solely on the
<br>
processing of a previously coded message in an information
<br>
channel beg the question of where this information comes from.
<br>
The concept of matching in a selectional system easily resolves
<br>
the issue.&quot; (_A Universe of Consciousness_, pp. 137-138).
<br>
<p>This newer neurobiologically-based analysis (which, for lack of a
<br>
better term, I'm going to call the &quot;post-cognitivist&quot; view [*]),
<br>
suggests that both the external world and the brain are much
<br>
&quot;denser&quot;, in some sense, than networks of logic-linked sentences
<br>
in a formal or natural language can be.  On the other hand, the
<br>
1970-1990 cognitive-science-era view was based on the assumption
<br>
that the whole world **can** be captured in a net of words:
<br>
<p>&quot;Cognitivism is the view that reasoning is based solely on
<br>
manipulation of semantic representations. Cognitivism is based on
<br>
objectivism ('that an unequivocal description of reality can be
<br>
given by science') and classical categories (that objects and
<br>
events can be 'defined by sets of singly necessary and jointly
<br>
sufficient conditions')([_Bright Air, Brilliant Fire_] p. 14).
<br>
This conception is manifest in expert systems, for example, or
<br>
any cognitive model that supposes that human memory consists
<br>
only of stored linguistic descriptions (e.g., scripts, frames,
<br>
rules, grammars)...
<br>
<p>Edelman characterizes these computer programs as 'axiomatic
<br>
systems' because they contain the designer's symbolic categories
<br>
and rules of combination, from which all the program's subsequent
<br>
world models and sensorimotor procedures will be
<br>
derived. Paralleling the claims of many other theorists, from
<br>
Collingwood and Dewey to Garfinkel and Bateson, he asserts that
<br>
such linguistic models 'are social constructions that are the
<br>
results of thought, not the basis of thought.' ([BABF] p. 153) He
<br>
draws a basic distinction between what people do or experience and
<br>
their linguistic descriptions (names, laws, scripts): 'Laws do
<br>
not and cannot exhaust experience or replace history or the
<br>
events that occur in the actual courses of individual
<br>
lives. Events are denser than any possible scientific
<br>
description. They are also microscopically indeterminate, and,
<br>
given our theory, they are even to some extent macroscopically
<br>
so.' (_Bright Air, Brilliant Fire_ pp. 162-163)...  'By taking
<br>
the position of a biologically-based epistemology, we are in some
<br>
sense realists' (recognizing the inherent 'density' of objects
<br>
and events) 'and also sophisticated materialists' ([BABF] p. 161)&quot;.
<br>
[from an on-line review of _Bright Air, Brilliant Fire_ at
<br>
<a href="http://cogprints.soton.ac.uk/documents/disk0/00/00/03/35/cog00000335-00/123.htm">http://cogprints.soton.ac.uk/documents/disk0/00/00/03/35/cog00000335-00/123.htm</a> ]
<br>
<p>If sentences in a natural language &quot;[grab] the mind of a listener
<br>
and [take] it to some specific spot within [vis] memories&quot; (as
<br>
John McCrone quotes Walter Freeman as saying), then, to take a
<br>
culinary analogy, if we imagine these &quot;spots&quot; as corresponding to
<br>
fuzzy regions embedded the space of consciousness like raisins
<br>
baked into a muffin batter, then there are many more points in
<br>
the muffin-space of consciousness than correspond to the regions
<br>
occupied by the raisins, and the spatial relationships among the
<br>
raisins themselves are only maintained by the supporting muffin.
<br>
This brings to mind Wittgenstein's famous and enigmatic
<br>
proposition in the _Tractatus_, &quot;What we cannot speak about we
<br>
must pass over in silence&quot;.
<br>
<p>To digress into the personal realm, the times when I have felt
<br>
most painfully the inadequacy of language to capture the
<br>
subtleties of the flow of consciousness have been in the context
<br>
of intensely emotional interpersonal relationships that have gone
<br>
off the rails -- when such relationships are working, the
<br>
synchronization between minds takes place effortlessly, and the
<br>
consonance of linguistic interchanges is just another
<br>
manifestation of the underlying consonance of outlook; but when
<br>
they go sour, meta-discussions about where things went wrong are
<br>
usually pointless, and language, whether spoken or written, is
<br>
far too crude an instrument for the job.  At such times, one
<br>
feels like an angst-ridden character in a French New Wave film,
<br>
puffing meaningfully on a cigarette and staring silently into
<br>
space.
<br>
<p>This leads me to ask: In the light of the shifts in theories
<br>
about the mind which seem to have been taking place in the
<br>
1990's, in which neuroscience-based views of the human brain (as
<br>
exemplified, for instance, by the theories of Gerald M. Edelman)
<br>
seem to be eclipsing the symbolic modelling of cognitive science,
<br>
does **anyone** here still believe that an AI could actually
<br>
operate in a purely linguistic domain?  I suppose the last gasp
<br>
of the purely inferential approach to AI, in which the sentences
<br>
which the AI inputs and outputs are part of, and at the same
<br>
level as, the web of sentences which the AI is **made of**, was
<br>
Douglas Lenat's Cyc, and I haven't heard much encouraging news
<br>
from that direction lately.
<br>
<p>Not to be coy, I will admit that I'm thinking in particular about
<br>
the philosophy of seed AI sketched in Eliezer Yudkowsky's CaTAI
<br>
2.2[.0] ( <a href="http://www.singinst.org/CaTAI.html">http://www.singinst.org/CaTAI.html</a> ).  I've never quite
<br>
been able to figure out which side of the cognitive
<br>
vs. post-cognitive or language-as-stuff-of-intelligence vs.
<br>
language-as-epiphenomenon-of-intelligence fence this document
<br>
comes down on.  There are frustratingly vague hints of **both**
<br>
positions.
<br>
<p>-------------------------
<br>
<p>FOR EXAMPLE, the following opinions, statements and passages seem
<br>
to come down on the side of the cognitivists (I started by just
<br>
scanning down the document from the beginning, but then begin
<br>
skipping faster long before I got to the end; there are, no
<br>
doubt, many more examples than reproduced here):
<br>
<p>&quot;The task is not to build an AI with some astronomical level of
<br>
intelligence; the task is building an AI which is capable of
<br>
improving itself, of understanding and rewriting its own source
<br>
code.  The task is not to build a mighty oak tree, but a humble
<br>
seed.&quot;
<br>
<p>&quot;Intelligence, from a design perspective, is a goal with many,
<br>
many subgoals.&quot;
<br>
<p>&quot;...the functionality of evolution itself must be replaced -
<br>
either by the seed AI's self-tweaking of those algorithms, or by
<br>
replacing processes that are autonomic in humans with the
<br>
deliberate decisions of the seed AI.&quot;
<br>
<p>&quot;A seed AI could have a &quot;codic cortex&quot;, a sensory modality
<br>
devoted to code, with intuitions and instincts devoted to code,
<br>
and the ability to abstract higher-level concepts from code and
<br>
intuitively visualize complete models detailed in code.&quot;
<br>
<p>The &quot;world-model&quot; for an AI living in [a] microworld [of billiard
<br>
balls] consists of everything the AI knows about that world - the
<br>
positions, velocities, radii, and masses of the billiard
<br>
balls... The &quot;world-model&quot; is a cognitive concept; it refers to
<br>
the content of all beliefs...&quot;
<br>
<p>&quot;I mention that list of features to illustrate what will probably
<br>
be one of the major headaches for AI designers: If you design a
<br>
system and forget to allow for the possibility of expectation,
<br>
comparision, subjunctivity, visualization, or whatever, then
<br>
you'll either have to go back and redesign every single component
<br>
to open up space for the new possibilities, or start all over
<br>
from scratch.  Actualities can always be written in later, but
<br>
the potential has to be there from the beginning, and that means
<br>
a designer who knows the requirements spec in advance.&quot;
<br>
<p>&quot;[T]he possession of a codic modality may improve the AI's
<br>
understanding of source code, at least until the AI is smart
<br>
enough to make its own decisions about the balance between
<br>
slow-conscious and fast-autonomic thought.&quot;
<br>
<p>&quot;There's an AI called &quot;Copycat&quot;, written by Melanie Mitchell and
<br>
conceived by Douglas R. Hofstadter, that tries to solve analogy
<br>
problems in the microdomain of letter-strings...  Without going
<br>
too far into the details of Copycat, I believe that some of the
<br>
mental objects in Copycat are primitive enough to lie very close
<br>
to the foundations of cognition.&quot;
<br>
<p>&quot;Most of the time, the associational, similarity-based
<br>
architecture of biological neural structures is a terrible
<br>
inconvenience.  Human evolution always works with neural
<br>
structures - no other type of computational substrate is
<br>
available - but some computational tasks are so ill-suited to the
<br>
architecture that one must turn incredible hoops to encode them
<br>
neurally.  (This is why I tend to be instinctively suspicious of
<br>
someone who says, 'Let's solve this problem with a neural net!'
<br>
When the human mind comes up with a solution, it tends to phrase
<br>
it as code, not a neural network.  'If you really understood the
<br>
problem,' I think to myself, 'you wouldn't be using neural
<br>
nets.')&quot;
<br>
<p>&quot;Eurisko, designed by Douglas Lenat, is the best existing example
<br>
of a seed AI, or, for that matter, of any AI.&quot;  [OK, this is
<br>
actually from &quot;The Plan To Singularity&quot;, at
<br>
<a href="http://sysopmind.com/sing/PtS/vision/industry.html">http://sysopmind.com/sing/PtS/vision/industry.html</a> ]
<br>
<p>-------------------------
<br>
<p>ON THE OTHER HAND, there are many other statements that seem to
<br>
come down firmly on the side of the post-cognitivists:
<br>
<p>&quot;To think a single thought, it is necessary to duplicate far more
<br>
than the genetically programmed functionality of a single human
<br>
brain.  After all, even if the functionality of a human were
<br>
perfectly duplicated, the AI might do nothing but burble for the
<br>
first year - that's what human infants do.&quot;
<br>
<p>&quot;Self-improvement - the ubiquitous glue that holds a seed AI's
<br>
mind together; the means by which the AI moves from crystalline,
<br>
programmer-implemented skeleton functionality to rich and
<br>
flexible thoughts.  In the human mind, stochastic concepts -
<br>
combined answers made up of the average of many little answers -
<br>
leads to error tolerance; error tolerance lets concepts mutate
<br>
without breaking; mutation leads to evolutionary growth and rich
<br>
complexity.  An AI, by using probabilistic elements, can achieve
<br>
the same effect...&quot;
<br>
<p>&quot;AI has an embarassing tendency to predict success where none
<br>
materializes, to make mountains out of molehills, and to assert
<br>
that some simpleminded pattern of suggestively-named LISP tokens
<br>
completely explains some incredibly high-level thought process...
<br>
In the semantic net or Physical Symbol System of classical AI, a
<br>
light bulb would be represented by an atomic LISP token named
<br>
light-bulb.&quot;
<br>
<p>&quot;As always when trying to prove a desired result from a flawed
<br>
premise, the simplest path involves the Laws of Similarity and
<br>
Contagion.  For example, ... any instance of human deduction
<br>
which can be written down (after the fact) as a syllogism must be
<br>
explained by the blind operation of a ten-line-of-code process -
<br>
even if the human thoughts blatantly involve a rich visualization
<br>
of the subject matter, with the results yielded by direct
<br>
examination of the visualization rather than formal deductive
<br>
reasoning.&quot;
<br>
<p>&quot;There are several ways to avoid making this class of mistake.
<br>
One is to have the words &quot;Necessary, But Not Sufficient&quot; tattooed
<br>
on your forehead.  One is an intuition of causal analysis that
<br>
says 'This cause does not have sufficient complexity to explain
<br>
this effect.'  One is to be instinctively wary of attempts to
<br>
implement cognition on the token level.&quot;
<br>
<p>&quot;The Law of Pragmatism: Any form of cognition which can be
<br>
mathematically formalized, or which has a provably correct
<br>
implementation, is too simple to contribute materially to
<br>
intelligence.&quot;
<br>
<p>&quot;Classical AI programs, particularly &quot;expert systems&quot;, are often
<br>
partitioned into microtheories.  A microtheory is a body of
<br>
knowledge, i.e. a big semantic net, e.g.  propositional logic,
<br>
a.k.a. suggestively named LISP tokens...  Why did the microtheory
<br>
approach fail?  ...  First, microtheories attempt to embody
<br>
high-level rules of reasoning - heuristics that require a lot of
<br>
pre-existing content in the world-model...  We are not born with
<br>
experience of butterflies; we are born with the visual cortex
<br>
that gives us the capability to experience and remember
<br>
butterflies.&quot;
<br>
<p>&quot;We shouldn't be too harsh on the classical-AI researchers.
<br>
Building an AI that operates on &quot;pure logic&quot; - no sensory
<br>
modalities, no equivalent to the visual cortex - was worth
<br>
trying...  But it didn't work.  The recipe for intelligence
<br>
presented by CaTAI assumes an AI that possesses equivalents to
<br>
the visual cortex, auditory cortex, and so on...&quot;
<br>
<p>&quot;[T]houghts don't start out as abstract; they reach what we would
<br>
consider the &quot;abstract&quot; level by climbing a layer cake of ideas.
<br>
That layer cake starts with the non-abstract, autonomic
<br>
intuitions and perceptions of the world described by modalities.
<br>
The concrete world provided by modalities is what enables the AI
<br>
to learn its way up to tackling abstract problems.&quot;
<br>
<p>&quot;Many classical AIs lack even basic quantitative interactions
<br>
(such as fuzzy logic), rendering them incapable of using methods
<br>
such as holistic network relaxation, and lending all interactions
<br>
an even more crystalline feeling.  Still, there are classical AIs
<br>
that use fuzzy logic. What's missing is flexibility, mutability,
<br>
and above all richness; what's missing is the complexity that
<br>
comes from learning a concept.&quot;
<br>
<p>&quot;It turns out that [Douglas Lenat's] Eurisko's &quot;heuristics&quot; were
<br>
arbitrary pieces of LISP code.  Eurisko could modify heuristics
<br>
because it possessed &quot;heuristics&quot; which acted by splicing,
<br>
modifying, or composing - in short, mutating - pieces of LISP
<br>
code...  In a sense, Eurisko was the first attempt at a seed AI -
<br>
although it was far from truly self-swallowing, possessed no
<br>
general intelligence, and was created from crystalline
<br>
components.&quot;
<br>
<p>-------------------------
<br>
<p>SOMETIMES we get both points of view in the same sentence:
<br>
<p>&quot;[N]eural networks are very hard to understand, or debug, or
<br>
sensibly modify.  I believe in the ideal of mindstuff that both
<br>
human programmers and the AI can understand and manipulate.  To
<br>
expect direct human readability may be a little too much; that
<br>
goal, if taken literally, tends to promote fragile, crystalline,
<br>
simplistic code, like that of a classical AI.&quot;
<br>
<p>&quot;[D]efining concepts in terms of other concepts is what classical
<br>
AIs do...  I can't recall any classical AIs that constructed
<br>
explicitly multilevel models to ground reasoning using semantic
<br>
networks...&quot;  [**ground** reasoning using **semantic** networks?]
<br>
<p>&quot;[E]ven if the mind were deprived of its ultimate grounding and
<br>
left floating - the result wouldn't be a classical AI.  Abstract
<br>
concepts are learned, are grown in a world that's almost as rich
<br>
as a sensory modality - because the grounding definitions are
<br>
composed of slightly less abstract concepts with rich
<br>
interactions, and those less-abstract concepts are rich because
<br>
they grew up in a rich world composed of interactions between
<br>
even-less-abstract concepts, and so on, until you reach the level
<br>
of sensory modalities.&quot; [grounding **definitions**??]
<br>
<p>&quot;In a human, these features are complex functional adaptations,
<br>
generated by millions of years of evolution.  For an AI, that means
<br>
you sit down and write the code; that you change the design, or
<br>
add design elements (special-purpose low-level code that directly
<br>
implements a high-level case is usually a Bad Thing), specifically
<br>
to yield the needed result.&quot;
<br>
[some might comment that if a piece of code is the kind you
<br>
can &quot;sit down and write&quot;, then it is ipso facto too high-level
<br>
to constitute &quot;mindstuff&quot;]
<br>
<p>&quot;Mindstuff is the basic substrate from which the AI's permanently
<br>
stored cognitive objects (and particularly the AI's concepts) are
<br>
constructed.  If a cognitive architecture is a structure of pipes, then
<br>
mindstuff is the liquid flowing through the pipes.&quot;
<br>
[permanently stored cognitive objects?]
<br>
<p>&quot;Human scientific thought relies on millennia of accumulated
<br>
knowledge, the how-to-think heuristics discovered by hundreds
<br>
of geniuses.  While a seed AI may be able to absorb some of
<br>
this knowledge by surfing the 'Net...&quot;
<br>
[**knowledge** from surfing the 'Net?]
<br>
<p>-------------------------
<br>
<p>Anyway, you get the idea -- it's been really, really hard for me
<br>
to contextualize this document in terms of my other reading, and
<br>
I don't think this is entirely due to the limitations of my own
<br>
intellect ;-&gt; .
<br>
<p>At the risk of offering what might be interpreted as a gross
<br>
impertinence (but isn't intended that way at all), here is my
<br>
take on CaTAI.  I believe that Eliezer has absorbed enough of
<br>
what's going on to have realized at some level that the symbolic
<br>
logic, computer-programming, classical AI approach (what he calls
<br>
the fragile, crystalline, simplistic approach somewhere quoted
<br>
above), is in trouble.  However, I think he thinks that he has to
<br>
cling to this approach, to some degree, in order to see his way
<br>
clear to a self-improving AI -- it needs **source code** that can
<br>
be raked over by that codic cortex (telling quote: &quot;I believe in
<br>
the ideal of mindstuff that both human programmers and the AI
<br>
can understand and manipulate.&quot;).  The self-improvement, of
<br>
course, is necessary in order to be able to get to his vision of
<br>
the Singularity, which is not a neutral goal in Eliezer's case --
<br>
he sees it as the salvation of the human race  (another telling
<br>
quote:  &quot;The leap to true understanding, when it happens, will
<br>
open up at least as many possibilities as would be available
<br>
to a human researcher with access to vis own neural source
<br>
code.&quot; [but what if there **is** no &quot;neural source code&quot;?]).  This has
<br>
led to the confused tone of his document, the mixing of levels, the
<br>
arguments being dragged toward the post-cognivitist point of view
<br>
while remaining stubbornly framed in the old cognitivist
<br>
language.
<br>
<p>In a sense, the emotional tone of this position is similar to
<br>
what I believe I was sensing a while ago in the discussion of
<br>
digital vs. analog computing.  I think there are folks who
<br>
believe an AI has **got** to be digital, or the party's over,
<br>
just as Eliezer seems to believe that an AI has **got** to have
<br>
source code, or the party's over.
<br>
<p>I'm a little dismayed to find these self-imposed blinders among
<br>
the bright lights of this list.  I think we've all been spending
<br>
too much time around computers, folks -- they're lots of fun, but
<br>
they're not the whole world, and in fact as far as future
<br>
ultratechnology is concerned, a veer **away** from the digital,
<br>
computational model of AI is **just** the sort of unsurprising
<br>
surprise we should all half expect, a paradigm shift we should all
<br>
be prepared for.
<br>
<p>As McCrone says in _Going Inside_ (Chapter 12, &quot;Getting It
<br>
Backwards&quot;): &quot;[P]ersonally speaking, the biggest change for me
<br>
was not how much new needed to be learnt, but how much that was
<br>
old and deeply buried needed to be unlearnt.  I thought my
<br>
roundabout route into the subject would leave me well prepared.
<br>
I spent most of the 1980s dividing my time between computer
<br>
science and anthropology.  Following at first-hand the attempts
<br>
of technologists to build intelligent machines would be a good
<br>
way of seeing where cognitive psychology fell short of the mark,
<br>
while taking in the bigger picture -- looking at what is known
<br>
about the human evolutionary story -- ought to highlight the
<br>
purposes for which brains are really designed [**].  It would be a
<br>
pincer movement that should result in the known facts about the
<br>
brain making more sense.
<br>
<p>Yet it took many years, many conversations, and many false starts
<br>
to discover that the real problem was not mastering a mass of
<br>
detail but making the right shift in viewpoint.  Despite
<br>
everything, a standard reductionist and computational outlook on
<br>
life had taken deep root in my thinking, shaping what I expected
<br>
to see and making it hard to appreciate anything or anyone who
<br>
was not coming from the same direction.  Getting the fundamental
<br>
of what dynamic systems were all about was easy enough, but then
<br>
moving on from there to find some sort of balance between
<br>
computational and dynamic thinking was extraordinarily difficult.
<br>
Getting used to the idea of plastic structure or guided
<br>
competitions needed plenty of mental gymnastics...
<br>
<p>[A]s I began to feel more at home with this more organic way of
<br>
thinking, it also became plain how many others were groping their
<br>
way to the same sort of accommodation -- psychologists and brain
<br>
researchers who, because of the lack of an established vocabulary
<br>
or stock of metaphors, had often sounded as if they were all
<br>
talking about completely different things when, in fact, the same
<br>
basic insights were driving their work.&quot;
<br>
<p>Jim F.
<br>
<p>[*] In _Bright Air, Brilliant Fire_, Edelman speaks of this
<br>
&quot;post-cognitivist&quot; view as characterizing a &quot;Realists Club&quot;:
<br>
<p>&quot;It appears that the majority of those working in cognitive
<br>
psychology hold to the views I attack here.  But there is a
<br>
minority who hold contrary views, in many ways similar to mine.
<br>
These thinkers come from many fields: cognitive psychology,
<br>
linguistics, philosophy, and neuroscience.  They include John
<br>
Searle, Hilary Putnam, Ruth Garret Millikan, George Lakoff,
<br>
Ronald Langacker, Alan Gauld, Benny Shanon, Claes von Hofsten,
<br>
Jerome Bruner, and no doubt others as well.  I like to think of
<br>
them as belonging to a Realists Club, a dispersed group whose
<br>
thoughts largely converge and whose hope it is that someday the
<br>
more vocal practitioners of cognitive psychology and the
<br>
frequently smug empricists of neuroscience will understand that
<br>
they have unknowingly subjected themselves to an intellectual
<br>
swindle.  The views of this minority will be reflected in what I
<br>
have to say, but obviously they vary from person to person.  The
<br>
reader is urged to consult these scholars' works directly for a
<br>
closer look at the diversity of their thoughts and
<br>
interpretations.&quot;
<br>
<p>-- _Bright Air, Brilliant Fire_, &quot;Mind Without Biology: A
<br>
&nbsp;&nbsp;&nbsp;Critical Postscript&quot;, section &quot;Some Vicious Circles in the
<br>
&nbsp;&nbsp;&nbsp;Cognitive Landscape&quot;, p. 229
<br>
<p>_Going Inside: A Tour Round a Single Moment of Consciousness_ by
<br>
John McCrone, which gives an overview of recent developments and
<br>
shifts of opinion at the border between neuroscience and
<br>
cognitive science, adds more names to this list
<br>
( <a href="http://www.btinternet.com/~neuronaut/webtwo_book_intro.html">http://www.btinternet.com/~neuronaut/webtwo_book_intro.html</a> ).
<br>
<p>[**]  Another trend in CaTAI, and in a lot of SF-ish and computerish
<br>
dreaming about AI, is the burning desire to jettison human &quot;emotional
<br>
weakness&quot; (remember Forbin's comment in _Colossus_, &quot;I wanted an
<br>
impartial, emotionless machine --  a paragon of reason...&quot;)
<br>
Telling quote:  &quot;Freedom from human failings, and especially human
<br>
politics...  A synthetic mind has no political instincts; a synthetic
<br>
mind could run the course of human civilization without politically-imposed
<br>
dead ends, without observer bias, without the tendency to rationalize.&quot;
<br>
Again, this seems profoundly out of sync with recent, post-cognitivist
<br>
thinking about human intelligence.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2260.html">Travas Gunnell: "Modeling God (was Re: The pool we're trying to paddle in)"</a>
<li><strong>Previous message:</strong> <a href="2258.html">Technotranscendence: "OBJ: The Atlas Society"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2263.html">Damien Broderick: "Re: Contextualizing seed-AI proposals"</a>
<li><strong>Reply:</strong> <a href="2263.html">Damien Broderick: "Re: Contextualizing seed-AI proposals"</a>
<li><strong>Reply:</strong> <a href="2265.html">Eliezer S. Yudkowsky: "Re: Contextualizing seed-AI proposals"</a>
<li><strong>Maybe reply:</strong> <a href="2268.html">hal@finney.org: "Re: Contextualizing seed-AI proposals"</a>
<li><strong>Reply:</strong> <a href="2639.html">Eliezer S. Yudkowsky: "Re: Contextualizing seed-AI proposals"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2259">[ date ]</a>
<a href="index.html#2259">[ thread ]</a>
<a href="subject.html#2259">[ subject ]</a>
<a href="author.html#2259">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:59:45 MDT</em>
</em>
</small>
</body>
</html>
