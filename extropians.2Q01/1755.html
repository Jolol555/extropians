<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Emulation vs. Simulation</title>
<meta name="Author" content="Jim Fehlinger (fehlinger@home.com)">
<meta name="Subject" content="Re: Emulation vs. Simulation">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Emulation vs. Simulation</h1>
<!-- received="Thu Mar 29 21:29:29 2001" -->
<!-- isoreceived="20010330042929" -->
<!-- sent="Thu, 29 Mar 2001 23:04:10 -0500" -->
<!-- isosent="20010330040410" -->
<!-- name="Jim Fehlinger" -->
<!-- email="fehlinger@home.com" -->
<!-- subject="Re: Emulation vs. Simulation" -->
<!-- id="3AC405BA.FFBA8A8B@home.com" -->
<!-- inreplyto="200103290807.AAA08085@finney.org" -->
<strong>From:</strong> Jim Fehlinger (<a href="mailto:fehlinger@home.com?Subject=Re:%20Emulation%20vs.%20Simulation&In-Reply-To=&lt;3AC405BA.FFBA8A8B@home.com&gt;"><em>fehlinger@home.com</em></a>)<br>
<strong>Date:</strong> Thu Mar 29 2001 - 21:04:10 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1756.html">Spike Jones: "Re: 70s trivia, was Re: Antidepressants: Happiness is only a drug?"</a>
<li><strong>Previous message:</strong> <a href="1754.html">Adrian Tymes: "Re: What does the US need to move ahead in Space?"</a>
<li><strong>In reply to:</strong> <a href="1718.html">hal@finney.org: "Re: Emulation vs. Simulation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1757.html">Spudboy100@aol.com: "Re: Emulation vs. Simulation"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1755">[ date ]</a>
<a href="index.html#1755">[ thread ]</a>
<a href="subject.html#1755">[ subject ]</a>
<a href="author.html#1755">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
<a href="mailto:hal@finney.org?Subject=Re:%20Emulation%20vs.%20Simulation&In-Reply-To=&lt;3AC405BA.FFBA8A8B@home.com&gt;">hal@finney.org</a> wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I, like most of us, adopt a position which is basically functionalism;
</em><br>
<em>&gt; anyone who believes that uploading is possible (even gradual uploading)
</em><br>
<em>&gt; believes in it.
</em><br>
<p>There are some exceedingly slippery issues here.  Even the trans-cognitivists
<br>
(that's my coinage) like Edelman still believe that a non-Turing system like
<br>
the brain can be **simulated** by a Turing machine operating at a low
<br>
enough level.  But that's **not** what the 1970s-1980s cognitivists were
<br>
talking about doing (even though it's what many folks on this list do talk about --
<br>
but that's because we read too much SF, dontcha know? :-&gt; ).
<br>
<p>It does seem likely to me that what Edelman objects to as a difference
<br>
in **kind** between his view of the &quot;hardware&quot; of consciousness and
<br>
the traditional cognitivist approach is really just a matter of degree.
<br>
For one thing, the traditional cog-sci/AI folks clung to the notion,
<br>
for a long time, that you could get genuine AI if you could just come
<br>
up with the right Lisp program to run on a 1 MIPS, 1 megabyte class
<br>
PDP-10.  That's a pretty forlorn hope, these days.
<br>
<p>Edelman argues, plausibly, that there are a number of special characteristics
<br>
of biological brains -- namely, their lavish, fine-grained structure,
<br>
the multiple levels of stochastic variability, and the variation-precedes-selection
<br>
dynamics of biological systems -- that are so different in degree from all
<br>
contemporary hardware and software as to constitute a major qualitative gap.
<br>
<p>When Edelman argues his points, he's looking back over his shoulder at the
<br>
MIT/PDP-10/Lisp crowd.  He's assuredly **not** looking forward to the sort of
<br>
science-fictional scenarios that color the discussions on this list (I'd be
<br>
really, really surprised if he's ever heard of Greg Egan).
<br>
<p><em>&gt; Yet do any of us agree that how a brain arranges its
</em><br>
<em>&gt; circuits is of only marginal interest?  I don't see how.  This is of
</em><br>
<em>&gt; crucial interest in understanding brain behavior.
</em><br>
<p>Again, **we** may not, but apparently the legit cognitivists did.  This
<br>
is a fact about the recent history of science, and I'm willing to take
<br>
the word of folks like Putnam, Lakoff, and Edelman that that's what
<br>
happened.
<br>
<p><em>&gt; &gt; And here's what Edelman has to say about functionalism:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; ...we know that **any** algorithm or effective procedure
</em><br>
<em>&gt; &gt; may be executed by any universal Turing machine.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Sure, keeping in mind that the details are unimportant only in the
</em><br>
<em>&gt; philosophical sense.
</em><br>
<p>Yes, of course.  It the real world, the practical questions would totally
<br>
swamp the philosophical ones.  For one thing, a &quot;universal Turing machine&quot;
<br>
has unlimited time and memory.  Real computers have hard physical
<br>
memory limits, which means that you probably **couldn't** shoehorn
<br>
Netscape onto a Univac I, even if you were crazy enough to want to try.
<br>
<p><em>&gt; I don't view this as of crucial importance, because the basic idea
</em><br>
<em>&gt; still holds.  Modern computers are open systems just like brains; they
</em><br>
<em>&gt; interact with their environments.  I don't know if anyone has formalized
</em><br>
<em>&gt; this notion of &quot;open&quot; computation.  But the general idea is still valid,
</em><br>
<em>&gt; that a computer interacting with an environment is every bit as powerful
</em><br>
<em>&gt; in its information-processing capabilities as a brain interacting with
</em><br>
<em>&gt; that environment.
</em><br>
<p>Well, again, most of the computers in the world today are **not** doing anything
<br>
meaningful to the computers themselves; their behavior only makes sense in the context
<br>
of the human systems they serve.  An exception to this is Edelman's own
<br>
prototype of what he calls a &quot;noetic system&quot;:  Darwin IV -- a simulation
<br>
with the sort of architecture that Edelman claims is necessary
<br>
infrastructure for what he calls &quot;primary consciousness&quot;.  These machines
<br>
**are** doing things meaningful to themselves, even if its just reaching
<br>
for or batting away objects of various colors and shapes.
<br>
<p><em>&gt; The basic point remains true, that information processing is a fundamental
</em><br>
<em>&gt; physical process which can be carried out by many kinds of systems,
</em><br>
<em>&gt; from brains to computer chips.
</em><br>
<p>&quot;Information&quot; is a slippery, slippery thing to define.  Edelman and
<br>
Tononi devote a great deal of discussion to the definition of &quot;information&quot;
<br>
in _A Universe of Consciousness_.
<br>
<p><em>&gt; &gt; The facile analogy with digital computers breaks down [because]
</em><br>
<em>&gt; &gt; the sensory signals available to nervous systems are
</em><br>
<em>&gt; &gt; truly analogue in nature and therefore are neither
</em><br>
<em>&gt; &gt; unambiguous nor finite in number.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Nonsense!  If sensory signals were truly analog they would have an
</em><br>
<em>&gt; infinite amount of precision and therefore carry an infinite amount
</em><br>
<em>&gt; of information.
</em><br>
<p>Yes, it struck me that Edelman was putting his argument badly here
<br>
even as I was typing it out.  But reading between his lines,
<br>
in light of arguments elsewhere in his books, he is making a
<br>
legitimate point about the lack of predetermined categorical
<br>
boundaries in the signals coming from real world, and the chaotic
<br>
indeterminacy in the nervous system's reaction to those signals.
<br>
<p>It was careless phrasing on his part, though.
<br>
<p><em>&gt; &gt; Turing machines have by definition a finite number of internal states,
</em><br>
<em>&gt; &gt; while there are no apparent limits on the number of
</em><br>
<em>&gt; &gt; states the human nervous system can assume (for example,
</em><br>
<em>&gt; &gt; by analog modulation of large numbers of synaptic
</em><br>
<em>&gt; &gt; strengths in neuronal connections).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Further nonsense!  Are brains immune to the Bekenstein Bound?  Does
</em><br>
<em>&gt; Edelman really think the information storage capacity of the human brain
</em><br>
<em>&gt; is INFINITE?
</em><br>
<p>Again, I cringed a bit when I was typing this.  I almost edited
<br>
it out, but I decided to leave Edelman's argument intact, warts and all.
<br>
<p>It crossed my mind that, since Edelman gives hints in his books
<br>
that he is an audiophile as well as a music lover, he should
<br>
have been better prepared for this sort of discussion by the
<br>
enormous amount of ink that's been spilled in the audio press
<br>
about the merits of analog vs. digital recordings ;-&gt; .
<br>
<p><em>&gt; I am surprised that these quotes (which I appreciate Jim taking the
</em><br>
<em>&gt; time to find and present) are what passes for intelligent commentary on
</em><br>
<em>&gt; these issues.  There are arguments against functionalism which are far
</em><br>
<em>&gt; more profound than what McCrone and Edelman offer.  They focus on one
</em><br>
<em>&gt; weak point, which is that there is no agreed-upon way to unambiguously
</em><br>
<em>&gt; describe what consitutes an implementation of a given computation.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Such arguments are much more difficult to deal with
</em><br>
<em>&gt; than claiming that brains have more power than TMs because they are
</em><br>
<em>&gt; analog, for Pete's sake.
</em><br>
<p>And that would be an oversimplification of Edelman's position, too,
<br>
I think.  I don't think you can throw out Edelman's whole argument
<br>
because of his infelicitous characterization of the resolution
<br>
of analog signals and processing elements as &quot;unlimited&quot;.
<br>
<p>However, since you seem to agree that it's a good idea to actually
<br>
look at brains in detail to see how they work, rather than ignoring
<br>
them completely and just trying to write programs to try to duplicate
<br>
their input/output functions at the highest level of abstraction,
<br>
I guess there's really no conflict, anyway.
<br>
<p>Jim F.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1756.html">Spike Jones: "Re: 70s trivia, was Re: Antidepressants: Happiness is only a drug?"</a>
<li><strong>Previous message:</strong> <a href="1754.html">Adrian Tymes: "Re: What does the US need to move ahead in Space?"</a>
<li><strong>In reply to:</strong> <a href="1718.html">hal@finney.org: "Re: Emulation vs. Simulation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1757.html">Spudboy100@aol.com: "Re: Emulation vs. Simulation"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1755">[ date ]</a>
<a href="index.html#1755">[ thread ]</a>
<a href="subject.html#1755">[ subject ]</a>
<a href="author.html#1755">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:59:44 MDT</em>
</em>
</small>
</body>
</html>
