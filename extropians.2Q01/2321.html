<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Contextualizing seed-AI proposals</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Contextualizing seed-AI proposals">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Contextualizing seed-AI proposals</h1>
<!-- received="Sat Apr 14 12:16:18 2001" -->
<!-- isoreceived="20010414181618" -->
<!-- sent="Sat, 14 Apr 2001 14:14:32 -0400" -->
<!-- isosent="20010414181432" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Contextualizing seed-AI proposals" -->
<!-- id="3AD89388.12405A80@pobox.com" -->
<!-- inreplyto="3AD86BD7.24B9DDB6@home.com" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Contextualizing%20seed-AI%20proposals&In-Reply-To=&lt;3AD89388.12405A80@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Apr 14 2001 - 12:14:32 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2322.html">Eliezer S. Yudkowsky: "Re: The Human Touch"</a>
<li><strong>Previous message:</strong> <a href="2320.html">CYMM: "...help needed on quercetin &amp; resveratrol containing plants..."</a>
<li><strong>In reply to:</strong> <a href="2311.html">Jim Fehlinger: "Re: Contextualizing seed-AI proposals"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2268.html">hal@finney.org: "Re: Contextualizing seed-AI proposals"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2321">[ date ]</a>
<a href="index.html#2321">[ thread ]</a>
<a href="subject.html#2321">[ subject ]</a>
<a href="author.html#2321">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Jim Fehlinger wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &quot;Eliezer S. Yudkowsky&quot; wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; I wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; &gt; I've never quite been able to figure out which side of the
</em><br>
<em>&gt; &gt; &gt; cognitive vs. post-cognitive or language-as-stuff-of-intelligence
</em><br>
<em>&gt; &gt; &gt; vs. language-as-epiphenomenon-of-intelligence fence this document
</em><br>
<em>&gt; &gt; &gt; [CaTAI 2.2, <a href="http://www.singinst.org/CaTAI.html">http://www.singinst.org/CaTAI.html</a> ] comes down on...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; &quot;Hm, the old debate about symbols, representational power,
</em><br>
<em>&gt; &gt; and so on.  I'm well out of it.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Sigh.  I freely admit to being an amateur (and worse than that --
</em><br>
<em>&gt; a dilettante) in this field, as in all others, but my impression
</em><br>
<em>&gt; is that this &quot;old debate&quot; is far from settled, and that you can't
</em><br>
<em>&gt; afford to be &quot;well out of it&quot; if you want to make a serious
</em><br>
<em>&gt; contribution to AI.
</em><br>
<p>I know it's far from settled.  My point is that I *grew up* - as an AI guy
<br>
- with this argument resounding in my ears, and my sheer *annoyance* with
<br>
the massive *sloppiness* of the arguments in this area is part of what
<br>
powered my entrance into AI.  People seemed to get emotionally attached to
<br>
their positions, for reasons that I understand but still can't sympathize
<br>
with.  If it turns out that yet another layer of functional complexity is
<br>
necessary over and above the CaTAI model, I'll deal with it!  My annoyance
<br>
is with what I named the Physicist's Paradigm - the idea that one major
<br>
trick has to be THE ANSWER; or, almost as annoying, the idea that a bunch
<br>
of little tricks must be the answer and that there are no major tricks. 
<br>
It takes lots and lots of major tricks, and even if you have a bunch of
<br>
major tricks you can't be sure you've found them all.  I think I know the
<br>
role of symbols in humans, and how to generalize that role to generic
<br>
minds, but it's really not the most important thing I know about AI.  It's
<br>
in the top ten, but not the top five.
<br>
<p><em>&gt; The precise role of &quot;symbols&quot; in the functional hierarchy of an
</em><br>
<em>&gt; intelligent organism is a wickedly-beckoning trap for human
</em><br>
<em>&gt; philosophers, mathematicians, and AI researchers **precisely** because
</em><br>
<em>&gt; they're so impressed by, and enamored of, human language.
</em><br>
<p>Is it a wickedly-beckoning trap for neuroanatomists, evolutionary
<br>
psychologists, and cognitive scientists?
<br>
<p><em>&gt; They
</em><br>
<em>&gt; have, historically, succumbed to the temptation to stick it in
</em><br>
<em>&gt; where it doesn't belong.  That tendency is now being successfully
</em><br>
<em>&gt; combatted, I gather.
</em><br>
<p>I can only speak for myself, but I do not feel compelled to attach moral
<br>
significance to language or the lack of it.  I am not impressed by, nor
<br>
enamored of, either language, or those theories that hold language to be
<br>
irrelevant.  I know what language is, what it does, what functionality it
<br>
serves on both the individual and social scales, the role it plays in the
<br>
mind, and the size of that role.  And if I turn out to be wrong, I'll
<br>
deal.
<br>
<p>The tendency to attach moral significance to theories of cognition is the
<br>
absolute scourge of AI.  Even I got zapped in the one instance I tried,
<br>
but by and large I don't do it all, and that accounts for at least a third
<br>
of my lead over the rest of the pack.  My theories are not symbolic of
<br>
human society, or logic, or capitalism, or communism, or individualism, or
<br>
anything else, except insofar as these theories enable the construction of
<br>
artificial minds and that act is morally valent.  The theories themselves
<br>
are simply theories.
<br>
<p><em>&gt; In particular, you need to armor yourself, in your pursuit
</em><br>
<em>&gt; of the truth of these matters, against what I perceive is
</em><br>
<em>&gt; your own particular weakness -- your burning desire to find
</em><br>
<em>&gt; a rocket fuel to power the runaway Singularity you do desperately
</em><br>
<em>&gt; long for.  Your motives for that may be laudable -- saving the
</em><br>
<em>&gt; human race, and all that -- but the truth needs to come
</em><br>
<em>&gt; first.
</em><br>
<p>Now there's a sword that truly and beautifully cuts both ways.  Since I
<br>
already believe that successfully constructing an AI would save the human
<br>
race, I have no need whatsoever to believe that the theories themselves
<br>
are morally valent.  They're just theories and can be modified at will.  I
<br>
get my kicks from imagining the outcome.  And of course, I try to
<br>
structure those &quot;kicks&quot; in such a way that I don't have an emotional
<br>
attachment to the hypotheses that predict that outcome, or that tell me
<br>
how to create that outcome, or even the hypotheses that say the outcome is
<br>
possible; I try to be very careful to regard those things as hidden
<br>
variables.
<br>
<p><em>&gt; No, no, a thousand times no!!!  Symbolic vs. &quot;connectionist&quot; isn't
</em><br>
<em>&gt; the point (or at least it's far from being the whole story).
</em><br>
<p>Of course it's not the whole story.  The amount of philosophizing that's
<br>
gone into this is tremendous; it ranges from Searle's Chinese Room to the
<br>
Eliza Effect to &quot;Artificial Intelligence Meets Natural Stupidity&quot; to
<br>
&quot;Godel Escher Bach&quot; to &quot;The Emperor's New Mind&quot; to the entire &quot;The Mind's
<br>
I&quot;.  My point is that I read all this stuff when I was, say, fifteen or
<br>
so.  And, having moved on, I would now say that fighting about the role of
<br>
language happens because people are hanging enormous weights of
<br>
functionality from the tiny little bits of the mind they know about,
<br>
whether those tiny little bits are LISP tokens or Hopfield networks. 
<br>
Hence the Physicist's Paradigm.
<br>
<p><em>&gt; &gt; Intelligence (&quot;problem-solving&quot;, &quot;stream of consciousness&quot;)
</em><br>
<em>&gt; &gt; is built from thoughts.  Thoughts are built from structures
</em><br>
<em>&gt; &gt; of concepts (&quot;categories&quot;, &quot;symbols&quot;).  Concepts are built from
</em><br>
<em>&gt; &gt; sensory modalities.  Sensory modalities are built from the
</em><br>
<em>&gt; &gt; actual code.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Too static, I fear.  Also, too dangerously perched on
</em><br>
<em>&gt; the edge of what you have already dismissed as the &quot;suggestively-
</em><br>
<em>&gt; named Lisp token&quot; fallacy.
</em><br>
<p>Static?!  There are four layers of behavior here!  Some phenomena will be
<br>
emergent, some will be deliberate, but in both cases they will be holistic
<br>
and not just copies of the lower-level behaviors.  *Four layers* of
<br>
different behaviors at different levels, not counting the &quot;code&quot; layer. 
<br>
That may not be chaos but it ain't exactly crystalline either.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2322.html">Eliezer S. Yudkowsky: "Re: The Human Touch"</a>
<li><strong>Previous message:</strong> <a href="2320.html">CYMM: "...help needed on quercetin &amp; resveratrol containing plants..."</a>
<li><strong>In reply to:</strong> <a href="2311.html">Jim Fehlinger: "Re: Contextualizing seed-AI proposals"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2268.html">hal@finney.org: "Re: Contextualizing seed-AI proposals"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2321">[ date ]</a>
<a href="index.html#2321">[ thread ]</a>
<a href="subject.html#2321">[ subject ]</a>
<a href="author.html#2321">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:59:46 MDT</em>
</em>
</small>
</body>
</html>
