<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Keeping AI at bay (was: How to help create a si</title>
<meta name="Author" content="Eugene.Leitl@lrz.uni-muenchen.de (Eugene.Leitl@lrz.uni-muenchen.de)">
<meta name="Subject" content="Re: Keeping AI at bay (was: How to help create a singularity)">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Keeping AI at bay (was: How to help create a singularity)</h1>
<!-- received="Tue May  1 04:41:14 2001" -->
<!-- isoreceived="20010501104114" -->
<!-- sent="Tue, 01 May 2001 14:38:22 +0200" -->
<!-- isosent="20010501123822" -->
<!-- name="Eugene.Leitl@lrz.uni-muenchen.de" -->
<!-- email="Eugene.Leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: Keeping AI at bay (was: How to help create a singularity)" -->
<!-- id="3AEEAE3E.7D1C1D8A@lrz.uni-muenchen.de" -->
<!-- inreplyto="3.0.5.32.20010501174116.00812eb0@ariel.unimelb.edu.au" -->
<strong>From:</strong> <a href="mailto:Eugene.Leitl@lrz.uni-muenchen.de?Subject=Re:%20Keeping%20AI%20at%20bay%20(was:%20How%20to%20help%20create%20a%20singularity)&In-Reply-To=&lt;3AEEAE3E.7D1C1D8A@lrz.uni-muenchen.de&gt;"><em>Eugene.Leitl@lrz.uni-muenchen.de</em></a><br>
<strong>Date:</strong> Tue May 01 2001 - 06:38:22 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3580.html">scerir: "Re: META: To my fellow Extropians"</a>
<li><strong>Previous message:</strong> <a href="3578.html">Eugene.Leitl@lrz.uni-muenchen.de: "Re: Genetic transition to posthumanism"</a>
<li><strong>In reply to:</strong> <a href="3569.html">Damien Broderick: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3581.html">Anders Sandberg: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Reply:</strong> <a href="3581.html">Anders Sandberg: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3579">[ date ]</a>
<a href="index.html#3579">[ thread ]</a>
<a href="subject.html#3579">[ subject ]</a>
<a href="author.html#3579">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Damien Broderick wrote:
<br>
<p><em>&gt; How will we know when we're ready, 'gene? What does it *mean* to be `ready'
</em><br>
<em>&gt; in this case?
</em><br>
<p>What I mean is to reduce the vulnerability.
<br>
<p>There are reasons to suspect that AI is easier to do than an upload,
<br>
and that it will emerge relatively explosively, maybe even in a
<br>
catastrophic fashion -- the manmade Blight scenario.
<br>
<p>The reasoning goes as follows: the capabilities of hardware progress
<br>
much faster than capabilities of orthodox (human) software design.
<br>
This causes a growing underutilization of hardware, particularly 
<br>
exacerbated the impending restructuring of computer architectures 
<br>
towards fine-grain reconfigurable computers, and the impending advent of
<br>
molecular electronics, and world-wide deployment of such hardware,
<br>
well interconnected by photonically switched networking, while
<br>
running buggy, bloated pieces of man-made code. A brain the size
<br>
of a planet, potentially.
<br>
<p>The moment somebody creates a darwin-in machine method to utilize
<br>
above infrastructure much, much better, and uses it to breed 
<br>
an AI core, and -- either deliberately, or accidentally releases 
<br>
the thing into the network, we've got a large problem on our
<br>
hands. Very soon, we're no longer in control. Soon after, we
<br>
might be dead, as a simple side effect of the new ecology's 
<br>
metabolism. It probably happened before, it's just the molecular 
<br>
moieties bicking the bucket were incapable of reflecting on their 
<br>
fate while busily being restructured by the new kids on the block.
<br>
<p>In contrast to this, an upload is hard. The brain doesn't have
<br>
a clean, abstractable architecture from a human point of view.
<br>
So you're forced to do neuronal emulation, plugging in 
<br>
neuronanatomy and computing its dynamics at a rather low 
<br>
level of theory. This makes it slow, and power hungry. Many 
<br>
orders of magnitude separate such a first target upload from 
<br>
an AI running on the same substrate, because the AI has evolved 
<br>
a highly efficient encoding and means of processing on that 
<br>
substrate. It hugs the substrate very close, instead of 
<br>
something cloaked in layers over layers of legacy virtual 
<br>
machines, which go back to what people designed in the middle 
<br>
of last century.
<br>
<p>The situation is not hopeless, because a better encoding can be
<br>
developed, using more abstract means such as statespace dynamics
<br>
instead of numerical crunching electrochemical spikes propagation 
<br>
along pieces of squishy stuff, which some large, complicated 
<br>
digitized by disassembling your frozen noggin. The problem is 
<br>
it, we have very little idea how to do it, and it will take a 
<br>
while before we'll figure it out, and by the time it will become 
<br>
an option for people to converted into this after their death, 
<br>
or before, if we thoroughly debug and streamline it. As I said,
<br>
uploads are hard. It will take a while.
<br>
<p>This doesn't mean everybody is going to jump the train, some
<br>
will choose to remain on the platform, but eventually the train
<br>
will start to move, unfortunately dramatically restructuring
<br>
everything in its wake, the platform included. Unless we chose
<br>
to drag the remaining dawdlers and renitents in against their 
<br>
will, kicking and screaming, they'll probably going to die. 
<br>
It's a tough choice: is their choice informed? Do we have to
<br>
respect their wishes, or assume they're just being difficult,
<br>
and upload the heck out of them (of course, you can still leave
<br>
them the choice; telling them: you're a virtual model now.
<br>
You've got a day or two to decide whether you like it, then
<br>
we're giving you the choice to terminate yourself).
<br>
<p>Convergent evolution says at this stage we'll become indistinguishable 
<br>
from the AIs, so we're removed the threat of mass extinction by 
<br>
becoming indistinguishable from the threat in a more or less smooth
<br>
course of development.
<br>
<p>This is iffy, because the uploaders who're ahead in the conversion
<br>
process might grow impatient, and unwilling to wait for all the 
<br>
dawdlers, in which case we're dead meat, too. All hail to the new
<br>
master race, the stupid buggers.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3580.html">scerir: "Re: META: To my fellow Extropians"</a>
<li><strong>Previous message:</strong> <a href="3578.html">Eugene.Leitl@lrz.uni-muenchen.de: "Re: Genetic transition to posthumanism"</a>
<li><strong>In reply to:</strong> <a href="3569.html">Damien Broderick: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3581.html">Anders Sandberg: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Reply:</strong> <a href="3581.html">Anders Sandberg: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3579">[ date ]</a>
<a href="index.html#3579">[ thread ]</a>
<a href="subject.html#3579">[ subject ]</a>
<a href="author.html#3579">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 10:00:01 MDT</em>
</em>
</small>
</body>
</html>
