<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Friendly AI (was: Maximizing results of efforts)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Friendly AI (was: Maximizing results of efforts)">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Friendly AI (was: Maximizing results of efforts)</h1>
<!-- received="Mon Apr 30 13:25:23 2001" -->
<!-- isoreceived="20010430192523" -->
<!-- sent="Mon, 30 Apr 2001 15:23:29 -0400" -->
<!-- isosent="20010430192329" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Friendly AI (was: Maximizing results of efforts)" -->
<!-- id="3AEDBBB1.AD017C01@pobox.com" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMOEMNFHAA.ben@goertzel.org" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Friendly%20AI%20(was:%20Maximizing%20results%20of%20efforts)&In-Reply-To=&lt;3AEDBBB1.AD017C01@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Apr 30 2001 - 13:23:29 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3505.html">J. R. Molloy: "Re: Map of Extropy"</a>
<li><strong>Previous message:</strong> <a href="3503.html">BillK: "Re: META: GET LOST Re: Common Extropian Errors"</a>
<li><strong>In reply to:</strong> <a href="3482.html">Ben Goertzel: "RE: Maximizing results of efforts Re: Mainstreaming"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3516.html">Ben Goertzel: "RE: Friendly AI (was: Maximizing results of efforts)"</a>
<li><strong>Reply:</strong> <a href="3516.html">Ben Goertzel: "RE: Friendly AI (was: Maximizing results of efforts)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3504">[ date ]</a>
<a href="index.html#3504">[ thread ]</a>
<a href="subject.html#3504">[ subject ]</a>
<a href="author.html#3504">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt; you can only spend your life on one
</em><br>
<em>&gt; &gt; impossibility, after all.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; well, no.  I mean, I am married, after all ;D
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Anyway, are two impossibilities more impossible than one?  Isn't the
</em><br>
<em>&gt; siutation sort of like infinity+infinity=infinity?
</em><br>
<p>I realize that you're joking, but no, it's not.  Two impossibilities are a
<br>
*lot* more impossible than one.  Impossible things have happened, but only
<br>
one at a time.
<br>
<p><em>&gt; Yes.  My guess is that organic integration will occur, and on a slower
</em><br>
<em>&gt; time-scale than the development of nonhuman superintelligence.  I am not so
</em><br>
<em>&gt; certain as you that the development of superhuman superintelligence is going
</em><br>
<em>&gt; to obsolete human life as we know it... just as we have not obsoleted ants
</em><br>
<em>&gt; and cockroaches, though we've changed their lives in many ways.
</em><br>
<p>And that analogy might hold between the posthumans in space and the
<br>
Pedestrians on Old Earth.  (Wonderful term, &quot;pedestrian&quot;; it so neatly
<br>
conveys the idea of someone who chooses to go slowly, but still has rights
<br>
and isn't automatically run over.  Thanks to Debbie the Roboteer for
<br>
suggesting it.)
<br>
<p>But for those who choose to race ahead at full speed, or for the seed AI
<br>
itself, the governing speed is likely to run at a time rate far faster
<br>
than the rate of change in human civilization.  AIs are not humans and
<br>
they are not going to run at the same speed.
<br>
<p><em>&gt; I tend to think that once the system has gotten smart enough to rewrite its
</em><br>
<em>&gt; own code in ways that we can't understand, it's likely to morph its initial
</em><br>
<em>&gt; goal system into something rather different.  So I'm just not as confident
</em><br>
<em>&gt; in you that explicitly programming in Friendliness as a goal is the magic
</em><br>
<em>&gt; solution.  It's certainly worth doing, but I don't see it as being **as**
</em><br>
<em>&gt; important as positive social integration of the young AI.
</em><br>
<p>Isn't &quot;positive social integration&quot; something that, in humans, relies on
<br>
complex functional adaptation and built-in brainware support?  I've always
<br>
acknowledged that goals might morph into something different.  What counts
<br>
is conveying what humans would count as &quot;common sense&quot; in the domain of
<br>
goals; decisions require causes, supergoals can be uncertain, there is
<br>
such a thing as a &quot;transmission error&quot;, and so on.  What I fear is not
<br>
*different* (but magical and wonderful) goals, but goals that a human
<br>
being would regard as blatantly worthless and stupid.  There's a certain
<br>
threshold level of complexity required to reach out for any magical and
<br>
wonderful goals that do exist.
<br>
<p>A positive social environment is one of the factors that is known to
<br>
determine the difference between social and unsocial humans.  But humans
<br>
come with an awful lot of built-in functionality.  We should therefore be
<br>
very suspicious of the suggestion that a positive social environment is a
<br>
*sufficient* condition for the positive social integration of AIs.
<br>
<p>An animal deprived of visual stimulation during formative years will
<br>
become blind - will fail to develop the necessary neural organization of
<br>
the retina, LGN, visual cortex and so on.  But this is because the
<br>
adaptations for vision occurred in a total environment in which incoming
<br>
visual stimulation was a reliable constant.  Thus, the adaptations have
<br>
occurred in reaction to this constant incoming stimulation; are evolved to
<br>
use that incoming stimulation as a source of organizing information.  Not
<br>
because it's evolutionarily *necessary*, but because it's evolutionarily
<br>
*possible*.  It does not follow that exposing an arbitrary computer
<br>
program to the input stream of a digital camera will result in the
<br>
development of functional organization equivalent to that of the visual
<br>
cortex.  Nor does it follow that a visual computer program would require
<br>
visual stimulation to self-wire.  We do not have enough data to conclude
<br>
that self-wiring in reaction to visual stimulation is the *only* way to
<br>
get a mature visual system; just that, in the presence of visual
<br>
stimulation, it is evolutionarily easier to use it than to not use it.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3505.html">J. R. Molloy: "Re: Map of Extropy"</a>
<li><strong>Previous message:</strong> <a href="3503.html">BillK: "Re: META: GET LOST Re: Common Extropian Errors"</a>
<li><strong>In reply to:</strong> <a href="3482.html">Ben Goertzel: "RE: Maximizing results of efforts Re: Mainstreaming"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3516.html">Ben Goertzel: "RE: Friendly AI (was: Maximizing results of efforts)"</a>
<li><strong>Reply:</strong> <a href="3516.html">Ben Goertzel: "RE: Friendly AI (was: Maximizing results of efforts)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3504">[ date ]</a>
<a href="index.html#3504">[ thread ]</a>
<a href="subject.html#3504">[ subject ]</a>
<a href="author.html#3504">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 10:00:01 MDT</em>
</em>
</small>
</body>
</html>
