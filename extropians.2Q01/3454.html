<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Maximizing results of efforts Re: Mainstreaming</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Maximizing results of efforts Re: Mainstreaming">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Maximizing results of efforts Re: Mainstreaming</h1>
<!-- received="Mon Apr 30 02:01:59 2001" -->
<!-- isoreceived="20010430080159" -->
<!-- sent="Mon, 30 Apr 2001 04:00:11 -0400" -->
<!-- isosent="20010430080011" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Maximizing results of efforts Re: Mainstreaming" -->
<!-- id="3AED1B8B.368B265D@pobox.com" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMIEMCFHAA.ben@goertzel.org" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Maximizing%20results%20of%20efforts%20Re:%20Mainstreaming&In-Reply-To=&lt;3AED1B8B.368B265D@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Apr 30 2001 - 02:00:11 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3455.html">Samantha Atkins: "Re: Relinquish JXTA"</a>
<li><strong>Previous message:</strong> <a href="3453.html">Eliezer S. Yudkowsky: "Re: Singurapture"</a>
<li><strong>In reply to:</strong> <a href="3437.html">Ben Goertzel: "RE: Maximizing results of efforts Re: Mainstreaming"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3482.html">Ben Goertzel: "RE: Maximizing results of efforts Re: Mainstreaming"</a>
<li><strong>Reply:</strong> <a href="3482.html">Ben Goertzel: "RE: Maximizing results of efforts Re: Mainstreaming"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3454">[ date ]</a>
<a href="index.html#3454">[ thread ]</a>
<a href="subject.html#3454">[ subject ]</a>
<a href="author.html#3454">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I like your categorization of plans:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Plans can be divided into three types.  There are plans like Bill Joy's,
</em><br>
<em>&gt; &gt; that work only if everyone on the planet signs on, and which get hosed if
</em><br>
<em>&gt; &gt; even 1% disagree.  Such plans are unworkable.  There are plans like the
</em><br>
<em>&gt; &gt; thirteen colonies' War for Independence, which work only if a *lot* of
</em><br>
<em>&gt; &gt; people - i.e., 30% or 70% or whatever - sign on.  Such plans require
</em><br>
<em>&gt; &gt; tremendous effort, and pre-existing momentum, to build up to the requisite
</em><br>
<em>&gt; &gt; number of people.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; And there are plans like building a seed AI, which require only a finite
</em><br>
<em>&gt; &gt; number of people to sign on, but which benefit the whole world.  The third
</em><br>
<em>&gt; &gt; class of plan requires only that a majority *not* get ticked off enough to
</em><br>
<em>&gt; &gt; shut you down, which is a more achievable goal than proselytizing a
</em><br>
<em>&gt; &gt; majority of the entire planet.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; For the seed AI to be useful to humans rather than indifferent or hostile to
</em><br>
<em>&gt; them, what we need in my view is NOT an artificially-rigged Friendliness
</em><br>
<em>&gt; goal system,
</em><br>
<p>I protest thy slander.  Friendliness is about duplicating really deep
<br>
*structural* cognitive properties that are present in human minds but
<br>
which are not automatically present in AIs.  The actual content is nearly
<br>
icing on the cake by comparison.  Friendly AI is a self-sustaining funnel
<br>
through which we can pour certain types of human complexity into the AI,
<br>
such that the pouring is seen by the AI as desirable at any given point in
<br>
time.  An &quot;artificial&quot; system would be one that let you get away with
<br>
making statements or alterations in bad faith.  I am sharing a piece of
<br>
myself with the AI, not commanding or coercing or dominating or otherwise
<br>
diminishing.
<br>
<p><em>&gt; but rather, an organic integration of the seed AI with the
</em><br>
<em>&gt; global brain.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And this, I suspect, is a plan of the second type, according to your
</em><br>
<em>&gt; categorization...
</em><br>
<p>Yep!  Sure is!  Plans of the second type aren't impossible, just
<br>
difficult.  Even when all the existing momentum is already there, people
<br>
still spend themselves and their lives in the course of actualizing it -
<br>
civil rights may have been an idea whose time had come, but a lot of
<br>
caring people still broke themselves in the course of making it real.  It
<br>
seems like rather a harsh requirement to load onto a plan that *already*
<br>
requires the creation of true AI... you can only spend your life on one
<br>
impossibility, after all.
<br>
<p>Besides which, your visualization of organic integration implies growth
<br>
occurring on a timescale comparable to the rate of change in human
<br>
civilizations, which is not realistic for an entity that can absorb
<br>
arbitrarily large multiples of its initial hardware, that has a serial
<br>
element speed millions of times faster than neurons, and that can
<br>
recursively self-improve.  But we've already been through that.
<br>
<p><em>&gt; My belief is that one will work toward Friendly AI better if one spends a
</em><br>
<em>&gt; bit of one's time actually engaged in directly Friendly (compassionate,
</em><br>
<em>&gt; helpful) activities toward humans in real-time.  This is because such
</em><br>
<em>&gt; activities help give one a much richer intuition for the nuances of what
</em><br>
<em>&gt; helping people really means.
</em><br>
<p>If this was a method that relied on the programmer being Mother Theresa,
<br>
we might as well shoot ourselves now and be done with it, because humans
<br>
are by nature imperfect.  Programmer-wise, FAI is a plan of the second
<br>
kind - it only requires a mostly altruistic programmer to get started. 
<br>
Actually, it's possible that FAI will work even if you use Saddam Hussein
<br>
as the exclusive source of content, which would make FAI a plan of the
<br>
third kind.  But I can't prove that, and it's not the conservative
<br>
assumption, so I'm assuming that the programmer's surface decisions need
<br>
to be mostly altruistic, and more importantly, mostly in favor of
<br>
correcting errors.  Provide that, and the system can renormalize itself to
<br>
what it *would* have been if you *had* been Mother Theresa.
<br>
<p><em>&gt; This is an age-old philosophical dispute, of course.  Your lifestyle and
</em><br>
<em>&gt; approach to work are what Nietzsche called &quot;ascetic&quot;, and he railed against
</em><br>
<em>&gt; ascetisicm mercilessly while practicing it himself.
</em><br>
<p>&quot;Ascetism&quot; is orthogonal to &quot;unity of purpose&quot;.  I spend an appropriate
<br>
amount of time in relaxation... maybe a little less than I would if I were
<br>
working for myself, but not much less, and certainly more while writing
<br>
and thinking than when I'm performing relatively automatic tasks such as
<br>
clearly defined coding goals.  The difference is in the goals and the
<br>
purpose and the means by which decisions are made.  I've resolved the
<br>
traditional tormented self-conflict simply by moving entirely to one end
<br>
of the spectrum, which is not exactly the Eastern-philosophy solution, but
<br>
which works pretty well if you can get away with it.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3455.html">Samantha Atkins: "Re: Relinquish JXTA"</a>
<li><strong>Previous message:</strong> <a href="3453.html">Eliezer S. Yudkowsky: "Re: Singurapture"</a>
<li><strong>In reply to:</strong> <a href="3437.html">Ben Goertzel: "RE: Maximizing results of efforts Re: Mainstreaming"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3482.html">Ben Goertzel: "RE: Maximizing results of efforts Re: Mainstreaming"</a>
<li><strong>Reply:</strong> <a href="3482.html">Ben Goertzel: "RE: Maximizing results of efforts Re: Mainstreaming"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3454">[ date ]</a>
<a href="index.html#3454">[ thread ]</a>
<a href="subject.html#3454">[ subject ]</a>
<a href="author.html#3454">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 10:00:00 MDT</em>
</em>
</small>
</body>
</html>
