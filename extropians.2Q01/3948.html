<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Opinions as Evidence: Should Rational Bayesian</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Opinions as Evidence: Should Rational Bayesian Agents Commonize  Priors?">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Opinions as Evidence: Should Rational Bayesian Agents Commonize  Priors?</h1>
<!-- received="Mon May  7 11:06:07 2001" -->
<!-- isoreceived="20010507170607" -->
<!-- sent="Mon, 07 May 2001 13:04:13 -0400" -->
<!-- isosent="20010507170413" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Opinions as Evidence: Should Rational Bayesian Agents Commonize  Priors?" -->
<!-- id="3AF6D58D.24AE4558@pobox.com" -->
<!-- inreplyto="ac.14a8b919.28277917@aol.com" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Opinions%20as%20Evidence:%20Should%20Rational%20Bayesian%20Agents%20Commonize%20%20Priors?&In-Reply-To=&lt;3AF6D58D.24AE4558@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon May 07 2001 - 11:04:13 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3949.html">Adrian Tymes: "Re: Extro-5: Suggestions for lunch networking facilitation?"</a>
<li><strong>Previous message:</strong> <a href="3947.html">Eugene Leitl: "Re: Fuel Cell House, was Re: TECH: fuel cell car"</a>
<li><strong>In reply to:</strong> <a href="3931.html">CurtAdams@aol.com: "Opinions as Evidence: Should Rational Bayesian Agents Commonize Priors?"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3948">[ date ]</a>
<a href="index.html#3948">[ thread ]</a>
<a href="subject.html#3948">[ subject ]</a>
<a href="author.html#3948">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
<a href="mailto:CurtAdams@aol.com?Subject=Re:%20Opinions%20as%20Evidence:%20Should%20Rational%20Bayesian%20Agents%20Commonize%20%20Priors?&In-Reply-To=&lt;3AF6D58D.24AE4558@pobox.com&gt;">CurtAdams@aol.com</a> wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; My personal experience is that priors and private information are only weakly
</em><br>
<em>&gt; informative; i.e., even given world-state Q obtains, it isn't particularly
</em><br>
<em>&gt; difficult to find uninformed individuals with strong disbelief in Q.  Given
</em><br>
<em>&gt; this, the probability of a given degree of belief A in Q varies only mildly
</em><br>
<em>&gt; with whether Q obtains.  Hence the information derived from a given person
</em><br>
<em>&gt; holding a degree of belief Q is small and a rational Bayesian should have
</em><br>
<em>&gt; only a small change in belief on learning another's opinions.  Rational
</em><br>
<em>&gt; Bayesians, then, generally should maintain differences of opinion due to
</em><br>
<em>&gt; differences in priors.  Under most circumstances, for two agents to commonize
</em><br>
<em>&gt; priors requires a violation of Bayesian inference.
</em><br>
<p>What your analysis leaves out (I think) is the possibility of symmetry
<br>
between the observers.  Of course it isn't rational for a perfect Bayesian
<br>
reasoner to adjust vis opinions based on what humans think - at most, vis
<br>
opinions should be adjusted when ve encounters a human who could plausibly
<br>
be making vis conclusions based on novel but correct information. 
<br>
However, a human, encountering another human, must consider the
<br>
possibility of an internal mistake as well as an external mistake.  That
<br>
all humans evaluate themselves as having a considerably above-average
<br>
meta-rationality, and hence a considerably lower-than-average possibility
<br>
of underestimating how likely an internal mistake is, is not compatible
<br>
with rationality on the part of all observers; it implies an evolved bias
<br>
to overestimate rationality or meta-rationality.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3949.html">Adrian Tymes: "Re: Extro-5: Suggestions for lunch networking facilitation?"</a>
<li><strong>Previous message:</strong> <a href="3947.html">Eugene Leitl: "Re: Fuel Cell House, was Re: TECH: fuel cell car"</a>
<li><strong>In reply to:</strong> <a href="3931.html">CurtAdams@aol.com: "Opinions as Evidence: Should Rational Bayesian Agents Commonize Priors?"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3948">[ date ]</a>
<a href="index.html#3948">[ thread ]</a>
<a href="subject.html#3948">[ subject ]</a>
<a href="author.html#3948">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 10:00:03 MDT</em>
</em>
</small>
</body>
</html>
