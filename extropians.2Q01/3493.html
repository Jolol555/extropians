<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Keeping AI at bay (was: How to help create a singul</title>
<meta name="Author" content="Mitchell, Jerry (3337) (Jerry.Mitchell@esavio.com)">
<meta name="Subject" content="Keeping AI at bay (was: How to help create a singularity)">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Keeping AI at bay (was: How to help create a singularity)</h1>
<!-- received="Mon Apr 30 10:47:42 2001" -->
<!-- isoreceived="20010430164742" -->
<!-- sent="Mon, 30 Apr 2001 11:57:32 -0400" -->
<!-- isosent="20010430155732" -->
<!-- name="Mitchell, Jerry (3337)" -->
<!-- email="Jerry.Mitchell@esavio.com" -->
<!-- subject="Keeping AI at bay (was: How to help create a singularity)" -->
<!-- id="9D167F53998DD411805700D0B776468FBEB126@zeus.esavio.com" -->
<strong>From:</strong> Mitchell, Jerry (3337) (<a href="mailto:Jerry.Mitchell@esavio.com?Subject=Re:%20Keeping%20AI%20at%20bay%20(was:%20How%20to%20help%20create%20a%20singularity)&In-Reply-To=&lt;9D167F53998DD411805700D0B776468FBEB126@zeus.esavio.com&gt;"><em>Jerry.Mitchell@esavio.com</em></a>)<br>
<strong>Date:</strong> Mon Apr 30 2001 - 09:57:32 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3494.html">Matthew Gaylor: "Societal Implications of Nanoscience and Nanotechnology"</a>
<li><strong>Previous message:</strong> <a href="3492.html">Spudboy100@aol.com: "Re: CRYO: &quot;Ischemia&quot; vs. &quot;Reversibly dead&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3498.html">Robert Coyote: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Maybe reply:</strong> <a href="3498.html">Robert Coyote: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Reply:</strong> <a href="3548.html">John Marlow: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Maybe reply:</strong> <a href="3627.html">Mitchell, Jerry (3337): "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Maybe reply:</strong> <a href="3758.html">Zero Powers: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Maybe reply:</strong> <a href="3838.html">Spudboy100@aol.com: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Maybe reply:</strong> <a href="3860.html">CurtAdams@aol.com: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3493">[ date ]</a>
<a href="index.html#3493">[ thread ]</a>
<a href="subject.html#3493">[ subject ]</a>
<a href="author.html#3493">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Eugene Leitl wrote
<br>
<p>&lt;snip&gt;
<br>
. Rather, don't. We would all die. A real AI
<br>
could clean ruin your day, by eating the world, with you on it. So don't.
<br>
It's that simple.
<br>
&lt;snip&gt;
<br>
<p>I think a bootstrapping AI very well could eat the world if you just hand it
<br>
over nanotech that it can control although I dont think its certain. I think
<br>
the trick is to convince (demand) it to upgrade us to &quot;super-Jupiter
<br>
brained&quot; intelligence so we too can participate without getting eaten
<br>
ourselves. This should be pretty easy. If the AI wants us to do things for
<br>
it (like give it power, memory, upgrades, etc....), it better be churning
<br>
out the upgrade diagrams and procedures (cures for aging, cancer, biomind to
<br>
silicon mind downloading, etc...) for us. Then, and only then, when all the
<br>
humans (?) are at the same level as the AI, can we talk about nanotech and
<br>
macro-engineering the galaxy. I think an AI would even want to take this
<br>
approach. I personally think that morality is based on reason and logic, so
<br>
if &quot;we&quot; can start deriving the science of morality, an AI certainly should
<br>
come to the conclusion that killing intelligent beings should be avoided if
<br>
possible. Besides, what's the point of being an omnipotent,
<br>
super-intelligence discovering the secrets of the universe if there's noone
<br>
to share it with?
<br>
<p>Jerry Mitchell
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3494.html">Matthew Gaylor: "Societal Implications of Nanoscience and Nanotechnology"</a>
<li><strong>Previous message:</strong> <a href="3492.html">Spudboy100@aol.com: "Re: CRYO: &quot;Ischemia&quot; vs. &quot;Reversibly dead&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3498.html">Robert Coyote: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Maybe reply:</strong> <a href="3498.html">Robert Coyote: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Reply:</strong> <a href="3548.html">John Marlow: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Maybe reply:</strong> <a href="3627.html">Mitchell, Jerry (3337): "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Maybe reply:</strong> <a href="3758.html">Zero Powers: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Maybe reply:</strong> <a href="3838.html">Spudboy100@aol.com: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<li><strong>Maybe reply:</strong> <a href="3860.html">CurtAdams@aol.com: "Re: Keeping AI at bay (was: How to help create a singularity)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3493">[ date ]</a>
<a href="index.html#3493">[ thread ]</a>
<a href="subject.html#3493">[ subject ]</a>
<a href="author.html#3493">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 10:00:01 MDT</em>
</em>
</small>
</body>
</html>
