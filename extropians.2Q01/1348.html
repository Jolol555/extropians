<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: FY;) see, it's still spreading</title>
<meta name="Author" content="J. R. Molloy (jr@shasta.com)">
<meta name="Subject" content="Re: FY;) see, it's still spreading">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: FY;) see, it's still spreading</h1>
<!-- received="Fri Mar 23 12:27:07 2001" -->
<!-- isoreceived="20010323192707" -->
<!-- sent="Fri, 23 Mar 2001 11:28:12 -0800" -->
<!-- isosent="20010323192812" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: FY;) see, it's still spreading" -->
<!-- id="12ad01c0b3cf$6b6ece80$915d2a42@jrmolloy" -->
<!-- inreplyto="Pine.GSO.4.03.10103231614140.132-100000@sun1.lrz-muenchen.de" -->
<strong>From:</strong> J. R. Molloy (<a href="mailto:jr@shasta.com?Subject=Re:%20FY;)%20see,%20it's%20still%20spreading&In-Reply-To=&lt;12ad01c0b3cf$6b6ece80$915d2a42@jrmolloy&gt;"><em>jr@shasta.com</em></a>)<br>
<strong>Date:</strong> Fri Mar 23 2001 - 12:28:12 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1349.html">Michael Lorrey: "LUDD: ELF calls for action April 19th"</a>
<li><strong>Previous message:</strong> <a href="1347.html">Robert J. Bradbury: "Re: Recycling solar sails"</a>
<li><strong>In reply to:</strong> <a href="1333.html">Eugene Leitl: "Re: FY;) see, it's still spreading"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1353.html">Robert J. Bradbury: "MPI and LogP [was: Re: FY;) see, it's still spreading]"</a>
<li><strong>Reply:</strong> <a href="1353.html">Robert J. Bradbury: "MPI and LogP [was: Re: FY;) see, it's still spreading]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1348">[ date ]</a>
<a href="index.html#1348">[ thread ]</a>
<a href="subject.html#1348">[ subject ]</a>
<a href="author.html#1348">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Eugene Leitl has come back to confer positively:
<br>
<em>&gt; On the more positive side, I've just come back from a 3 day tutorial on
</em><br>
<em>&gt; MPI, and I'm pretty much impressed with the thing. Any parallel processing
</em><br>
<em>&gt; wonk should take a very good look at it, as it's 1) clever 2) is here to
</em><br>
<em>&gt; stay for a long time.
</em><br>
<p>MPI... it's still spreading... 249, 000 hits
<br>
<p>MPI parallel processing = 36,300 hits
<br>
<p>Introduction to Message Passing Parallel Programming with MPI: Parallel Processing 
<br>
<a href="http://www.psu.edu/dept/cac/ait/nic_group/Edu_Train/VT/IntroMPI/VT-IntroMPI-pp.html">http://www.psu.edu/dept/cac/ait/nic_group/Edu_Train/VT/IntroMPI/VT-IntroMPI-pp.html</a>
<br>
As stated in the introduction, MPI stands for Message Passing Interface. Message passing is a form of parallel processing where data between individual cpu's is explicitly transmitted by the programmer. Before we get into the thick of message passing parallel computing, let us take a few steps back and scrutinize why program in parallel at all. In this section we will introduce you to a basic understanding of the important concepts in the field of parallel programming, including its goals. We'll take a quick glance at the history of parallel processing that led to the development of MPI. 
<br>
______________________________
<br>
<p>In conjunction with its partnership with the
<br>
National Computational Science Alliance, the
<br>
Scientific Computing and Visualization group
<br>
and the Center for Computational Science at
<br>
Boston University will be offering a two-day
<br>
workshop on high-performance computing on
<br>
December 2-3, 1999.  There is no fee for the
<br>
workshop, and it is open to anyone interested
<br>
in high-performance computing, including those
<br>
in industry as well as academia.  Topics will
<br>
include parallel processing with MPI, parallel
<br>
processing with OpenMP, Fortran 90, performance
<br>
tuning, and debugging.  Participants will be
<br>
awarded access to Boston University's SGI Power
<br>
Challenge and Origin2000 supercomputer systems
<br>
during and after the workshop.
<br>
<p>LAM / MPI Parallel Computing
<br>
<a href="http://www.mpi.nd.edu/lam/">http://www.mpi.nd.edu/lam/</a>
<br>
LAM (Local Area Multicomputer) is an MPI programming environment and development system for heterogeneous computers on a network. With LAM, a dedicated cluster or an existing network computing infrastructure can act as one parallel computer solving one problem. 
<br>
<p>Fwd. Mssg. Follows
<br>
--------------------------------------------------------------------------------
<br>
<a href="http://www.tlug.gr.jp/ML/0009/msg00008.html">http://www.tlug.gr.jp/ML/0009/msg00008.html</a>
<br>
Hi everybody.
<br>
<p>&nbsp;&nbsp;I'm building a simulator, which seems to work fine so far except that
<br>
it is too slow.  Since the algorithms themself cannot be significantly
<br>
more optimized, I decided with my boss to make a version that will work
<br>
on a Beowulf.
<br>
<p>&nbsp;&nbsp;I found out that LAM/MPI would probably be a very good mix of
<br>
efficiency and portability (the Beowulf I am building is a mix of PC,
<br>
DEC Alpha and Sun computers).
<br>
<p>&nbsp;&nbsp;There is a lot of documentation about using or setting up LAM/MPI, and
<br>
I find the library very easy to use.  But there is not so much good
<br>
documentation about how to make a good and efficient design for parallel
<br>
programs.  I already have some idea about how to make my simulator
<br>
working efficiently in parallel, but I'm sure that a book would be able
<br>
to give me many tips and making me saving time.
<br>
<p>&nbsp;&nbsp;So, I would really appreciate if someone can suggest me some good
<br>
books about designing efficient programs on a heterogeneous cluster of
<br>
computer.  I don't mind if the book use Fortran, C or any other commonly
<br>
used language (personnaly, I'm using C++), as long it help me making a
<br>
more efficient simulator, considering the available hardware.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;By the way, the structure using a master that just dispatch jobs to
<br>
slave computers are not suitable for me, because each nodes have to
<br>
communicate with some others nodes, and that the amount of data is too
<br>
big (say, easilly over one GB for a single simulation).
<br>
<p>&nbsp;&nbsp;Thank you in advance,
<br>
<p>Simon Valiquette
<br>
: <a href="mailto:simon@crl.fujixerox.co.jp?Subject=Re:%20FY;)%20see,%20it's%20still%20spreading&In-Reply-To=&lt;12ad01c0b3cf$6b6ece80$915d2a42@jrmolloy&gt;">simon@crl.fujixerox.co.jp</a>
<br>
<p>PS:   My boss is really amazed that I can produce better quality
<br>
software using only free GPL softwares, than in using for over 1000$ in
<br>
softwares running under the other OS.
<br>
<p><pre>
---
It took the computational power of three Commodore 64s to fly to the
moon.
It takes a 486 to run Windows 95.
Something is wrong here.
_________________________________
<p>Standard MPI Interprocess Communication
<a href="http://www.ptf.com/ptf/products/UNIX/current/0538.0.html">http://www.ptf.com/ptf/products/UNIX/current/0538.0.html</a>
    The MPI Forum, which included representatives from every end of the
    parallel computing community, has specified a complete interface for
    message based interprocess communication, named MPI (Message Passing
    Interface).  Widespread use of MPI will benefit the general advancement of
    parallel processing technology.  The specific semantics of the communica-
    tion interface, its perceived popular and future prospects, need no longer
    be a factor in choosing an environment or a vendor for developing message
    passing applications.  Users and buyers can focus on other factors such as
    efficiency of implementation and related development tools.
____________________________
<p>And so on...
<p>ô¿ô
<p>Stay hungry,
<p>--J. R.
Useless hypotheses:
 consciousness, phlogiston, philosophy, vitalism, mind, free will
Take off every 'Zig'!!
</pre>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1349.html">Michael Lorrey: "LUDD: ELF calls for action April 19th"</a>
<li><strong>Previous message:</strong> <a href="1347.html">Robert J. Bradbury: "Re: Recycling solar sails"</a>
<li><strong>In reply to:</strong> <a href="1333.html">Eugene Leitl: "Re: FY;) see, it's still spreading"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1353.html">Robert J. Bradbury: "MPI and LogP [was: Re: FY;) see, it's still spreading]"</a>
<li><strong>Reply:</strong> <a href="1353.html">Robert J. Bradbury: "MPI and LogP [was: Re: FY;) see, it's still spreading]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1348">[ date ]</a>
<a href="index.html#1348">[ thread ]</a>
<a href="subject.html#1348">[ subject ]</a>
<a href="author.html#1348">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:59:42 MDT</em>
</em>
</small>
</body>
</html>
