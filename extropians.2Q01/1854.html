<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: &quot;analog computer&quot; = useless hypothesi</title>
<meta name="Author" content="Jim Fehlinger (fehlinger@home.com)">
<meta name="Subject" content="Re: &quot;analog computer&quot; = useless hypothesis?">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: &quot;analog computer&quot; = useless hypothesis?</h1>
<!-- received="Mon Apr  2 19:14:21 2001" -->
<!-- isoreceived="20010403011421" -->
<!-- sent="Mon, 02 Apr 2001 20:48:06 -0400" -->
<!-- isosent="20010403004806" -->
<!-- name="Jim Fehlinger" -->
<!-- email="fehlinger@home.com" -->
<!-- subject="Re: &quot;analog computer&quot; = useless hypothesis?" -->
<!-- id="3AC91DC6.9AFDC59@home.com" -->
<!-- inreplyto="342c01c0bb15$fdadb160$915d2a42@jrmolloy" -->
<strong>From:</strong> Jim Fehlinger (<a href="mailto:fehlinger@home.com?Subject=Re:%20&quot;analog%20computer&quot;%20=%20useless%20hypothesis?&In-Reply-To=&lt;3AC91DC6.9AFDC59@home.com&gt;"><em>fehlinger@home.com</em></a>)<br>
<strong>Date:</strong> Mon Apr 02 2001 - 18:48:06 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1855.html">Eliezer S. Yudkowsky: "Re: &quot;analog computer&quot; = useless hypothesis?"</a>
<li><strong>Previous message:</strong> <a href="1853.html">CurtAdams@aol.com: "Re: MATH/COMP/PHIL: &quot;Omega Man&quot;"</a>
<li><strong>In reply to:</strong> <a href="1830.html">J. R. Molloy: "Re: &quot;analog computer&quot; = useless hypothesis?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1855.html">Eliezer S. Yudkowsky: "Re: &quot;analog computer&quot; = useless hypothesis?"</a>
<li><strong>Reply:</strong> <a href="1855.html">Eliezer S. Yudkowsky: "Re: &quot;analog computer&quot; = useless hypothesis?"</a>
<li><strong>Reply:</strong> <a href="1874.html">Harvey Newstrom: "RE: &quot;analog computer&quot; = useless hypothesis?"</a>
<li><strong>Reply:</strong> <a href="1886.html">J. R. Molloy: "Re: &quot;analog computer&quot; = useless hypothesis?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1854">[ date ]</a>
<a href="index.html#1854">[ thread ]</a>
<a href="subject.html#1854">[ subject ]</a>
<a href="author.html#1854">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
&quot;J. R. Molloy&quot; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Thank you for the input, Anders. Nice to hear from someone who keeps up to
</em><br>
<em>&gt; date on these matters. I'd thought it premature to write off analog computing
</em><br>
<em>&gt; as a useless hypothesis, but now it seems intelligence has a singular affinity
</em><br>
<em>&gt; for analysis of the digital computation kind (conveyance of this information
</em><br>
<em>&gt; is completely digital), so the tagline remains amended accordingly.
</em><br>
<p>I should probably stay out of this henceforward, but I can't resist the
<br>
opportunity to quote some more from the book I bought weekend before last,
<br>
_Going Inside..._ by John McCrone.  This will be what I would have marked
<br>
from this section in pink highlighter (if I were the sort of person who
<br>
did that to books).  At the end are some comments of my very own (!),
<br>
so you can skip there if you're not interested in the McCrone.
<br>
<p>&nbsp;From Chapter 3 &quot;Ugly Questions About Chaos&quot; (pp. 50-73):
<br>
<p>&quot;[T]he success of the digital computer is founded on precisely its ability
<br>
to squeeze out any uncertainty in its behaviour.  Computers are built to
<br>
follow the same steps and come to the same answers every time they run a
<br>
program.  The advantage of this is that once the design of the hardware or
<br>
software has been perfected, endless copies can be churned out, all with
<br>
identical characteristics and performance.  By freezing the logic, the logic
<br>
can be mass-produced.  But the flipside is that the smallest bug or mis-step
<br>
can bring the whole system crashing down.  For a computer to work, chance
<br>
events have to be ruled out right down to the level of the electrical
<br>
components from which the hardware is built.  A transistor is engineered
<br>
so that it can put up with things like slight fluctuations in the power
<br>
supply or changes in temperature without changing state.  Some kinds of
<br>
electronic devices are analogue -- they produce a continuously varying
<br>
output -- but a transistor is a binary switch [in a computer anyway; not
<br>
in most audio amplifiers!].  That is the meaning of digital.  It is
<br>
either on or off, a 1 or a 0.  There is no room for shades of grey, only
<br>
black and white.  A bit of information either exists, or it does not.
<br>
<p>The assumption [of the back-propagation neural network folks of the 1980's]
<br>
was that brain cells were also basically digital devices.  The brain might
<br>
be a pink handful of gloopy mush; brain cells themselves might be rather
<br>
unsightly tangles of protoplasm, no two ever shaped the same; but it was
<br>
believed that information processing in the brain must somehow rise above
<br>
this organic squalor.  There might be no engineer to draw neat circuit
<br>
diagrams, but something about neurons had to allow them to act together
<br>
with logic and precision.
<br>
<p>Brain cells certainly had a few suggestive features.  To start with... they
<br>
have a separate input and output end... there is a direction in which
<br>
information flows...  It is true that a few synapses... are found on
<br>
the cell body, and sometimes even on the axon itself, but generally
<br>
speaking dendrites collect the information and axons deliver the response...
<br>
Whether two cells are connected is a black and white issue...
<br>
<p>There is a physical logic in the wiring patterns of the brain.  Then, on
<br>
top of this, there is something quite plainly binary about the all-or-
<br>
nothing nature of a neuron's decision to fire...
<br>
<p>So, despite the brain being made of flesh and blood, the propagation
<br>
of signals looks to have a digital clarity.  But the question is whether
<br>
brains are exclusively digital in their operation.  A computer...
<br>
relies on its circuits being completely insulated from any source of
<br>
noise which might interfere with the clockwork progression of 0s and
<br>
1s.  But it is not so clear that brain cells are designed to be shielded
<br>
from the messy details of their biology.  Indeed, a closer look at a
<br>
neuron soon suggests the exact opposite:  it is alive to every small
<br>
fluctuation or nuance in its internal environment.  Where a
<br>
transistor is engineered for stability, a brain cell trades in the
<br>
almost overwhelming sensitivity of its response...
<br>
<p>[D]epending on what mix of pores [ion channels, pumps, and receptors]
<br>
is built into an area of membrane -- something which itself can be
<br>
changed in minutes or hours -- ... a neuron can show a trememdous
<br>
variety of responses.  A computer is made of standardised components.
<br>
One transistor is exactly like the next.  But every bit of
<br>
membrane in the brain is individual.  The blend of pores can be tailored
<br>
to do a particular job, and that blend can be fine-tuned at any time.
<br>
There is a plasticity that makes the outside of a neuron itself seem
<br>
like a learning surface, a landscape of competition and adaptation...
<br>
<p>[I]n practice, there is nothing certain about any of the steps in [the]
<br>
chain [of an action potential in one neuron leading to neurotransmitter
<br>
release and stimulation of target neuron].  An axon may often not
<br>
even release any neurotransmitter, despite being hit by a full-strength
<br>
spike.  The amount of neurotransmitter spilled into the gap can also
<br>
vary.  Plus, there is a whole cocktail of other substances that may
<br>
or may not be released at the same time...  So a spike may seem like
<br>
a digital event -- the all-or-nothing creation of a bit of information
<br>
guaranteed to reach a known destination -- but the same signal
<br>
might one moment be met with an instant and enthusiastic response,
<br>
the next only fizzle away into nothing, failing even to stir a cell's
<br>
own axon tip.
<br>
<p>Some of the variability in the behavior of a neuron could be just noise...
<br>
[b]ut while there is undoubtedly a degree of noise in the brain, much
<br>
of the variability looks deliberate...  [Neurons] appear to
<br>
thrive on being fluid.  By using competition and feedback to fine-tune
<br>
their workings, they can adapt their response to meet the needs of the
<br>
moment...
<br>
<p>[A] quite recently discovered fact is [that while it] had always been
<br>
assumed that it was the action of billions of synapses that added up
<br>
to make a state of consciousness...  consciousness -- at least,
<br>
levels of attention and alertness -- seems able to influence the response
<br>
of individual synapses.  In computer terms, the logic sounds alarmingly
<br>
backwards...
<br>
<p>One crucial discovery was that a cell's output spike actually travels
<br>
both ways: it runs down the axon, but also back over the cell itself and
<br>
through its own dendrites.  What this means is that synapses are told
<br>
whether or not they contributed to the last firing decision...
<br>
<p>A still more surprising discovery [of the 1990s] was that a stimulated
<br>
dendrite releases a small dose of... nitric oxide (NO)... back across
<br>
the synaptic junction...  [I]t appears to switch on certain enzymes
<br>
in an axon tip, prompting them to ramp up production of neurotransmitters...
<br>
Nitric oxide also has the secondary effect of relaxing the walls of
<br>
blood vessels, so its release probably increases the blood flow into
<br>
an active area of the brain [the brain's self-administered poppers ;-&gt; ]...
<br>
<p>[T]hen there are the feedback connections between cells...  Here,
<br>
[Donald O.] Hebb [author of _The Organization of Behavior_ (1949)]
<br>
turned out to be more right than he imagined...  [F]eedback connections
<br>
dominated the brain.  They were everywhere, from short loops linking
<br>
neighbouring cells to long chains in which a signal might bounce right
<br>
around the brain before feeding back, much modified, to its source...
<br>
Some came back as part of the wash of input hitting a cell's dendrites,
<br>
but others formed synapses on the cell body or close to the axon hillock
<br>
where -- being closer to the action -- they could exert a much more
<br>
powerful effect...
<br>
<p>As a biological organ, the brain could not help being a little noisy
<br>
and unpredictable in its workings...  The brain... [uses] feedback to
<br>
adjust its circuits and competition to evolve its answers, which again
<br>
introduced an element of unpredictability.  But ultimately, all this
<br>
feedback and competition appeared to be directed towards producing
<br>
a well-organized response...  To the computer-minded, the foundations
<br>
might look soggy, but there did seem to be something concrete going
<br>
on...
<br>
<p>The trouble with this charitable view was that there remained
<br>
something fundamentally different about brains and computers.  Any
<br>
digitalism in the brain was a weak, blurred-edge, pseudo kind of
<br>
digitalism...  Computers, on the other hand, were digital by nature...
<br>
So if a computer wanted to behave like a dynamic, feedback-tuned
<br>
system, it had to fake it...
<br>
<p>For example, to make the neurons in a backprop network seem more
<br>
realistic... they broadcast... some figure between the full-off
<br>
of a 0 and the full-on of a 1...  Surely, it would not take
<br>
too many... decimal places to render the problem of rounding...
<br>
completely irrelevant?  A simulated neuron should be able to
<br>
show all the rich variety of output of a real one...
<br>
<p>There is a lot of science that can be done by concentrating on
<br>
situations so close to being digital as not to make a difference.
<br>
Yet there are clearly also a great many areas... where the
<br>
blurring of boundaries and the fluid nature of the relationships
<br>
cannot be ignored.  The classic examples are the weather,
<br>
economics, social systems, condensed matter physics, quantum
<br>
mechanics, fluid dynamics, and anything to do with biology.
<br>
Such systems are not just accumulations of components, bits of
<br>
clockwork in which every gear is locked into a fixed relationship
<br>
with its fellows...
<br>
<p>[There follows a brief summary of the mathematics of chaos and
<br>
complexity; the former dealing in iterated applications of a function
<br>
whose output is fed back to itself (generating fractals and
<br>
Mandelbrot sets and so forth); the latter with the further wrinkle
<br>
that the chaos-generating functions evolve in time).  Description
<br>
of point, limit-cycle, and strange attractors.]
<br>
<p>What startled [meteorologist Edward] Lorenz... was that the maths was
<br>
both deterministic and utterly unpredictable.  It was deterministic
<br>
in that so long as the starting values were exactly the same,
<br>
his program would always crank out the same result...  Yet the
<br>
merest hint of a change in those values and immediately there
<br>
was no telling where the situation might go...
<br>
<p>Given that the real world is a continuous place, and so exact
<br>
starting points can never be measured, this means that it is
<br>
impossible -- as a matter of principle -- to predict the behavior
<br>
of a feedback-dependent system...
<br>
<p>This was dismal news for scientists wedded to a reductionist view
<br>
of the world.  It destroyed the belief that if you knew all the rules
<br>
governing a system, you could then predict its future.  Chaos
<br>
theory said you could know the laws and still not predict...
<br>
<p>Fortunately, chaos had an important saving grace.  While the path
<br>
of any particular system could not be predicted, outcomes had a
<br>
tendency to group...  A truly random system would be equally
<br>
likely to visit every point in the space of all possible
<br>
outcomes.  But a chaotic system would have some kind of
<br>
attractor -- a region it preferred to inhabit...
<br>
<p>Computer designers wondered whether they could harness a chaotic
<br>
attractor to drive a new kind of neural network.  A network
<br>
might be able to represent its memories or programs as an
<br>
attractor state distributed across the strength of its
<br>
connections.  So rather than following a rigid step-by-step
<br>
summation of weights to produce an answer, the system would
<br>
be like a Hebbian feedback network in which input would
<br>
wander about a bit before it eventually fell into a basin
<br>
of attraction...  Neuroscientists saw the same link...
<br>
<p>There is much more that could be said about chaos and complexity;
<br>
they are huge subjects in their own right.  But for mind
<br>
science, the point is that their mathematics must shake the
<br>
common conviction that computers and brains are fundamentally
<br>
the same.  Chaos theory says that being digital matters.
<br>
There are consequences when decimal places get rounded off
<br>
as small errors can soon develop into large differences.  But
<br>
much more importantly, there is a hidden energy in a feedback-
<br>
driven, chaos-harnessing system.  There is both the push
<br>
of its competitions and the pull of its underlying dynamics --
<br>
the places its attractors want it to go.  So brains and
<br>
computers might both process information, but as technology
<br>
stands -- even with the glamorous new field of artificial
<br>
neural networks -- they do it in a deeply different way...
<br>
<p>Even then, complexity cannot be the whole story.  The brain
<br>
still has a digital-like side.  There is no escaping the
<br>
all-or-nothing nature of cell firing, or the precision with
<br>
which neurons make their connections...  Human brains
<br>
can also -- with perhaps a bit of a struggle -- think logically.
<br>
We can reason in a sequential, linear fashion which appears
<br>
not unlike a computer program.
<br>
<p>------------------------
<br>
<p>It crossed my mind today that the reason for some
<br>
of the &quot;tetchiness&quot; surrounding this whole subject of
<br>
digital vs. analog may be the unspoken (and mistaken, I
<br>
think) assumption that if the brain **does** rely in some
<br>
fundamental way on analog processing, then that would mean
<br>
the party's over in terms of AI, SI, the Singularity, and
<br>
all the other fun stuff the Extropians have planned for
<br>
the coming century.  It's certainly true that most of
<br>
the talk about AI on this list has been in terms of
<br>
software running on some sort of digital substrate
<br>
(&quot;computronium&quot;, or whatever), but that's not the only
<br>
way AI or SI could happen.
<br>
&nbsp;
<br>
While digital integrated circuits might be the most
<br>
glamorous electronics on the market these days, don't
<br>
forget that there are still linear ICs being manufactured!
<br>
It's altogether conceivable that a non-biological AI or
<br>
upload could use molecular or nano-scale **linear** devices
<br>
as processing elements!  Such a contraption might not be
<br>
quite as tame inside as we probably visualize -- most of us
<br>
probably think of some sort of 3D crystalline lattice of
<br>
nanotubes with nothing but electrons flashing around, and with
<br>
all the action happening in software, rather than something that
<br>
might look more like Babbage's (or Gibson and Sterling's)
<br>
Difference Engine, with gears and whirlygigs making and
<br>
breaking connections or aiming little laser beams around,
<br>
or scurrying nanobots.  Or what about an AI/SI made
<br>
out of honest-to-God biological tissue, but freed from the
<br>
confines of a human skull and serviced by nanobots (yes,
<br>
that idea gives me the creeps, too.  Too _Last and First
<br>
Men_).
<br>
<p>I get the impression that's the way Kurzweil thinks AI
<br>
will happen -- he talks about the brain as a &quot;digitally-
<br>
controlled analog system&quot;, and I think he thinks we're
<br>
going to get to AI by reverse-engineering the human
<br>
brain.
<br>
<p>Maybe a partly analog phase will be a necessary **transition**
<br>
on the way to all-digital AI and/or uploads.   It's certainly
<br>
true that all-digital does have its attractions:  if nothing
<br>
else, the idea of being able to halt the processor between
<br>
one clock cycle and the next, and being able read out, save,
<br>
transmit, reload, and restart an AI with no interruption
<br>
in the flow of consciousness does sound sort of appealing.
<br>
But even a self-enhancing AI such as Eliezer imagines doesn't
<br>
have to be a digital computer.  True, a &quot;codic cortex&quot; might
<br>
not be of much use in that case, but maybe the AI will have
<br>
to major in EE instead of in CS ;-&gt; .
<br>
<p>Here's a deep question:  would an analog AI constructed out
<br>
of vacuum tubes have a &quot;mellower&quot; personality than one
<br>
made out of transistors?  ;-&gt;    ;-&gt;    ;-&gt;
<br>
<p>Jim F.
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1855.html">Eliezer S. Yudkowsky: "Re: &quot;analog computer&quot; = useless hypothesis?"</a>
<li><strong>Previous message:</strong> <a href="1853.html">CurtAdams@aol.com: "Re: MATH/COMP/PHIL: &quot;Omega Man&quot;"</a>
<li><strong>In reply to:</strong> <a href="1830.html">J. R. Molloy: "Re: &quot;analog computer&quot; = useless hypothesis?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1855.html">Eliezer S. Yudkowsky: "Re: &quot;analog computer&quot; = useless hypothesis?"</a>
<li><strong>Reply:</strong> <a href="1855.html">Eliezer S. Yudkowsky: "Re: &quot;analog computer&quot; = useless hypothesis?"</a>
<li><strong>Reply:</strong> <a href="1874.html">Harvey Newstrom: "RE: &quot;analog computer&quot; = useless hypothesis?"</a>
<li><strong>Reply:</strong> <a href="1886.html">J. R. Molloy: "Re: &quot;analog computer&quot; = useless hypothesis?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1854">[ date ]</a>
<a href="index.html#1854">[ thread ]</a>
<a href="subject.html#1854">[ subject ]</a>
<a href="author.html#1854">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:59:44 MDT</em>
</em>
</small>
</body>
</html>
