<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: [isml] Making HAL Your Pal (fwd)</title>
<meta name="Author" content="Eugene Leitl (Eugene.Leitl@lrz.uni-muenchen.de)">
<meta name="Subject" content="[isml] Making HAL Your Pal (fwd)">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>[isml] Making HAL Your Pal (fwd)</h1>
<!-- received="Thu Apr 19 07:12:52 2001" -->
<!-- isoreceived="20010419131252" -->
<!-- sent="Thu, 19 Apr 2001 15:11:27 +0200 (MET DST)" -->
<!-- isosent="20010419131127" -->
<!-- name="Eugene Leitl" -->
<!-- email="Eugene.Leitl@lrz.uni-muenchen.de" -->
<!-- subject="[isml] Making HAL Your Pal (fwd)" -->
<!-- id="Pine.SOL.4.31.0104191511180.22296-100000@sun1.lrz-muenchen.de" -->
<strong>From:</strong> Eugene Leitl (<a href="mailto:Eugene.Leitl@lrz.uni-muenchen.de?Subject=Re:%20[isml]%20Making%20HAL%20Your%20Pal%20(fwd)&In-Reply-To=&lt;Pine.SOL.4.31.0104191511180.22296-100000@sun1.lrz-muenchen.de&gt;"><em>Eugene.Leitl@lrz.uni-muenchen.de</em></a>)<br>
<strong>Date:</strong> Thu Apr 19 2001 - 07:11:27 MDT
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2692.html">Michael Lorrey: "Re: Anti-Capitalism"</a>
<li><strong>Previous message:</strong> <a href="2690.html">Alejandro Dubrovsky: "Re: Anti-Capitalism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2697.html">Eliezer S. Yudkowsky: "Re: [isml] Making HAL Your Pal (fwd)"</a>
<li><strong>Reply:</strong> <a href="2697.html">Eliezer S. Yudkowsky: "Re: [isml] Making HAL Your Pal (fwd)"</a>
<li><strong>Maybe reply:</strong> <a href="2751.html">Mitchell Porter: "Re: [isml] Making HAL Your Pal (fwd)"</a>
<li><strong>Maybe reply:</strong> <a href="2797.html">Brian Phillips: "Re: Re: [isml] Making HAL Your Pal (fwd)"</a>
<li><strong>Maybe reply:</strong> <a href="2824.html">Billy Brown: "RE: [isml] Making HAL Your Pal (fwd)"</a>
<li><strong>Maybe reply:</strong> <a href="3062.html">J Corbally: "Re: [isml] Making HAL Your Pal (fwd)"</a>
<li><strong>Maybe reply:</strong> <a href="3166.html">Billy Brown: "RE: [isml] Making HAL Your Pal (fwd)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2691">[ date ]</a>
<a href="index.html#2691">[ thread ]</a>
<a href="subject.html#2691">[ subject ]</a>
<a href="author.html#2691">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
______________________________________________________________
<br>
ICBMTO  : N48 10'07'' E011 33'53'' <a href="http://www.lrz.de/~ui22204">http://www.lrz.de/~ui22204</a>
<br>
57F9CFD3: ED90 0433 EB74 E4A9 537F CFF5 86E7 629B 57F9 CFD3
<br>
<p>---------- Forwarded message ----------
<br>
Date: Thu, 19 Apr 2001 09:06:33 -0700
<br>
From: DS2000 &lt;<a href="mailto:ds2000@mediaone.net?Subject=Re:%20[isml]%20Making%20HAL%20Your%20Pal%20(fwd)&In-Reply-To=&lt;Pine.SOL.4.31.0104191511180.22296-100000@sun1.lrz-muenchen.de&gt;">ds2000@mediaone.net</a>&gt;
<br>
Reply-To: <a href="mailto:isml@yahoogroups.com?Subject=Re:%20[isml]%20Making%20HAL%20Your%20Pal%20(fwd)&In-Reply-To=&lt;Pine.SOL.4.31.0104191511180.22296-100000@sun1.lrz-muenchen.de&gt;">isml@yahoogroups.com</a>
<br>
To: isml &lt;<a href="mailto:isml@yahoogroups.com?Subject=Re:%20[isml]%20Making%20HAL%20Your%20Pal%20(fwd)&In-Reply-To=&lt;Pine.SOL.4.31.0104191511180.22296-100000@sun1.lrz-muenchen.de&gt;">isml@yahoogroups.com</a>&gt;
<br>
Subject: [isml] Making HAL Your Pal
<br>
<p><em>&gt;From Wired,
</em><br>
<a href="http://www.wired.com/news/technology/1,1282,43080,00.html">http://www.wired.com/news/technology/1,1282,43080,00.html</a>
<br>
-
<br>
Making HAL Your Pal
<br>
by Declan McCullagh
<br>
<p>Eliezer Yudkowsky
<br>
Photo by Brian Atkins.
<br>
<p>2:00 a.m. Apr. 19, 2001 PDT
<br>
Eliezer Yudkowsky has devoted his young life to an undeniably unusual
<br>
pursuit: planning for what happens when computers become far smarter than
<br>
us.
<br>
<p>Yudkowsky, a 21-year-old researcher at the Singularity Institute, has spent
<br>
the last eight months writing an essay that's half precaution, half thought
<br>
exercise, and entirely in earnest.
<br>
<p>This 750 KB treatise, released Wednesday, is not as much speculative as
<br>
predictive. If a computer becomes sufficiently smart, the argument goes, and
<br>
if it gains the ability to harm humans through nanotechnology or some means
<br>
we don't expect, it may decide it doesn't need us or want us around.
<br>
<p>One solution: Unconditional &quot;friendliness,&quot; built into the AI as surely as
<br>
our genes are coded into us.
<br>
<p>&quot;I've devoted my life to this,&quot; says Yudkowsky, a self-proclaimed &quot;genius&quot;
<br>
who lives in Atlanta and opted out of attending high school and college.
<br>
<p>It's not for lack of smarts. He's a skilled, if verbose, writer and an avid
<br>
science-fiction reader who reports he scored 1410 on his SATs, not far below
<br>
the average score for Stanford or MIT students.
<br>
<p>Yudkowsky's reason for shunning formal education is that he believes the
<br>
danger of unfriendly AI to be so near -- as early as tomorrow -- that there
<br>
was no time for a traditional adolescence. &quot;If you take the Singularity
<br>
seriously, you tend to live out your life on a shorter time scale,&quot; he said.
<br>
<p>Mind you, that's &quot;Singularity&quot; in capital letters. Even so-called
<br>
Singularitians like Yudkowsky admit that the term has no precise meaning,
<br>
but a commonly accepted definition is a point when human progress,
<br>
particularly technological progress, accelerates so dramatically that
<br>
predicting what will happen next is futile.
<br>
<p>The term appears to have been coined by John von Neumann, the great
<br>
mathematician and computer scientist who used it not to refer to superhuman
<br>
intelligence, but to the everyday pace of science and technology.
<br>
<p>Science-fiction author Vernor Vinge popularized the concept in the 1980s,
<br>
capitalizing the word and writing about whether mankind would approach
<br>
Singularity by way of machine intelligence alone or through augmented mental
<br>
processes. Predictions vary wildly about what happens at the Singularity,
<br>
but the consensus seems to be that life as humanity currently knows it will
<br>
come to a sudden end.
<br>
<p>Vinge is the closest thing Singularitians have to a thought leader,
<br>
spokesman and hero. He offers predictions based on measures of technological
<br>
progress such as Moore's Law, and sees the Singularity as arriving between
<br>
2005 and 2030 -- though some Vinge aficionados hope the possibility of
<br>
uploading their brains into an immortal computer is just around the corner.
<br>
<p>One of them is Yudkowsky, who credits Vinge for turning him onto the
<br>
Singularity at age 11. &quot;I read True Names,&quot; he said, referring to a Vinge
<br>
novel. &quot;I got to page 47 and found out what I was going to be doing for the
<br>
rest of my life.&quot;
<br>
<p>Since then, Yudkowsky has become not just someone who predicts the
<br>
Singularity, but a committed activist trying to speed its arrival. &quot;My first
<br>
allegiance is to the Singularity, not humanity,&quot; he writes in one essay. &quot;I
<br>
don't know what the Singularity will do with us. I don't know whether
<br>
Singularities upgrade mortal races, or disassemble us for spare atoms.... If
<br>
it comes down to Us or Them, I'm with Them.&quot;
<br>
<p>His life has included endless theorizing -- little programming, though --
<br>
about friendly AI.
<br>
<p>&quot;Any damn fool can design an AI that's friendly if nothing goes wrong,&quot;
<br>
Yudkowsky says. &quot;This is an AI in theory that should be friendly if
<br>
everything goes wrong.&quot;
<br>
<p>Of course, some of the brightest people in the world, including Nobel
<br>
laureates, have spent decades researching AI -- friendly or not -- and have
<br>
failed to realize their dreams. AI, it seems, is always just a decade or
<br>
less away from becoming reality -- and has been for the last 40 years.
<br>
<p>The difference today? The Singularitian movement. Back when the late Herb
<br>
Simon co-invented the humble General Problem Solver in the mid-1950s, there
<br>
wasn't a crowd of eager geeks cheering his efforts and hoping to dump their
<br>
brains into his technology.
<br>
<p>Yudkowsky says he hopes to show his essay to the AI community &quot;and maybe
<br>
even branch out into the cognitive science community and maybe get some
<br>
useful comments that can be incorporated into the document.&quot;
<br>
<p>The only problem is that academics don't seem interested. When asked for
<br>
comment, one well-known researcher said in response to the essay: &quot;Worthless
<br>
speculation. Call me when you have running code.&quot;
<br>
<p>Alon Halevy, a faculty member in the University of Washington's computer
<br>
science department and an editor at the Journal of Artificial Intelligence
<br>
Research, said he's not worried about friendliness.
<br>
<p>&quot;As a practical matter, I'm not concerned at all about AI being friendly or
<br>
not,&quot; Halevy said. &quot;The challenges we face are so enormous to even get to
<br>
the point where we can call a system reasonably intelligent, that whether
<br>
they are friendly or not will be an issue that is relatively easy to solve.&quot;
<br>
<p>Rules limiting smart computers to human-approved limits, of course, are
<br>
nothing new. The most famous example is the Laws of Robotics, written by
<br>
Isaac Asimov, one of the fathers of science fiction.
<br>
<p>To Asimov, only three laws were necessary: (1) A robot may not injure a
<br>
human being, or, through inaction, allow a human being to come to harm; (2)
<br>
A robot must obey orders given it by human beings, except where such orders
<br>
would conflict with the First Law; (3) A robot must protect its own
<br>
existence as long as such protection does not conflict with the First or
<br>
Second Laws.
<br>
<p>In 1993, Roger Clarke, a fellow at the Australian National University, wrote
<br>
an essay wondering how Asimov's laws applied to information technology.
<br>
<p>One conclusion: &quot;Existing codes of ethics need to be re-examined in the
<br>
light of developing technology. Codes generally fail to reflect the
<br>
potential effects of computer-enhanced machines and the inadequacy of
<br>
existing managerial, institutional and legal processes for coping with
<br>
inherent risks.&quot;
<br>
<p>Yudkowsky takes it a step further, writing that he believes AI &quot;will be
<br>
developed on symmetric-multiprocessing hardware, at least initially.&quot; He
<br>
said he expects Singularity could happen in the very near future: &quot;I
<br>
wouldn't be surprised if tomorrow was the Final Dawn, the last sunrise
<br>
before the Earth and Sun are reshaped into computing elements.&quot;
<br>
<p>When one researcher booted up a program he hoped would be AI-like, Yudkowsky
<br>
said he believed there was a 5 percent chance the Singularity was about to
<br>
happen and human existence would be forever changed.
<br>
<p>After another firm announced it might pull the plug on advanced search
<br>
software it created, Yudkowsky wrote on Sunday to a Singularity mailing
<br>
list: &quot;Did anyone try, just by way of experimentation, explaining to the
<br>
current Webmind instantiation that it's about to die?&quot;
<br>
<p>That kind of earnest hopefulness comes more from science fiction than
<br>
computer science, and in fact some researchers don't even think the
<br>
traditional geek-does-programming AI field is that interesting nowadays. The
<br>
interesting advances, the thinking goes, are taking place in cognitive and
<br>
computational neuroscience.
<br>
<p>Yudkowsky seems undeterred, saying he wants the Singularity Institute's
<br>
friendly AI guidelines eventually to become the equivalent of the Foresight
<br>
Institute's nanotechnology guidelines. Released last year, they include,
<br>
among others, principles saying nanobots should not be allowed to replicate
<br>
outside a laboratory, and only companies that agree to follow these rules
<br>
should receive nanotech hardware.
<br>
<p>&quot;The Singularity Institute is not just in the business of predicting it, but
<br>
creating it and reacting to it,&quot; he says. &quot;If AI doesn't come for another 50
<br>
years, then one way of looking at it would be that we have 50 years to plan
<br>
in advance about friendly AI.&quot;
<br>
<p>Given humanity's drive to better itself, it seems likely that eventually --
<br>
even if the date is hundreds or thousands of years off -- some form of AI
<br>
will exist.
<br>
<p>Though even then, some skeptics like John Searle, a professor at the
<br>
University of California at Berkeley, argue that a machine will merely be
<br>
manipulating symbols -- and will lack any true understanding of their
<br>
meaning.
<br>
<p>Yudkowsky, on the other hand, sees reason for urgency in developing Friendly
<br>
AI guidelines.
<br>
<p>&quot;We don't have the code yet but we have something that is pretty near the
<br>
code level,&quot; Yudkowsky said, talking about the Institute's work. &quot;We have
<br>
something that could be readily implemented by any fairly advanced AI
<br>
project.&quot;
<br>
<p>Like a character from science fiction, Yudkowsky sees his efforts as
<br>
humanity's only hope.
<br>
<p>In an autobiographical essay, he writes: &quot;I think my efforts could spell the
<br>
difference between life and death for most of humanity, or even the
<br>
difference between a Singularity and a lifeless, sterilized planet... I
<br>
think that I can save the world, not just because I'm the one who happens to
<br>
be making the effort, but because I'm the only one who can make the effort.&quot;
<br>
<p><pre>
--
Dan S
<p><p>[ISML] Insane Science Mailing List
<p>- To subscribe: <a href="http://www.onelist.com/subscribe.cgi/isml">http://www.onelist.com/subscribe.cgi/isml</a>
<p><p><p>Your use of Yahoo! Groups is subject to <a href="http://docs.yahoo.com/info/terms/">http://docs.yahoo.com/info/terms/</a>
</pre>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2692.html">Michael Lorrey: "Re: Anti-Capitalism"</a>
<li><strong>Previous message:</strong> <a href="2690.html">Alejandro Dubrovsky: "Re: Anti-Capitalism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2697.html">Eliezer S. Yudkowsky: "Re: [isml] Making HAL Your Pal (fwd)"</a>
<li><strong>Reply:</strong> <a href="2697.html">Eliezer S. Yudkowsky: "Re: [isml] Making HAL Your Pal (fwd)"</a>
<li><strong>Maybe reply:</strong> <a href="2751.html">Mitchell Porter: "Re: [isml] Making HAL Your Pal (fwd)"</a>
<li><strong>Maybe reply:</strong> <a href="2797.html">Brian Phillips: "Re: Re: [isml] Making HAL Your Pal (fwd)"</a>
<li><strong>Maybe reply:</strong> <a href="2824.html">Billy Brown: "RE: [isml] Making HAL Your Pal (fwd)"</a>
<li><strong>Maybe reply:</strong> <a href="3062.html">J Corbally: "Re: [isml] Making HAL Your Pal (fwd)"</a>
<li><strong>Maybe reply:</strong> <a href="3166.html">Billy Brown: "RE: [isml] Making HAL Your Pal (fwd)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2691">[ date ]</a>
<a href="index.html#2691">[ thread ]</a>
<a href="subject.html#2691">[ subject ]</a>
<a href="author.html#2691">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:59:47 MDT</em>
</em>
</small>
</body>
</html>
