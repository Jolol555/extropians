<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Emulation vs. Simulation</title>
<meta name="Author" content="Lee Corbin (lcorbin@ricochet.net)">
<meta name="Subject" content="Re: Emulation vs. Simulation">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Emulation vs. Simulation</h1>
<!-- received="Mon Mar 26 00:31:13 2001" -->
<!-- isoreceived="20010326073113" -->
<!-- sent="Sun, 25 Mar 2001 23:38:38 -0800" -->
<!-- isosent="20010326073838" -->
<!-- name="Lee Corbin" -->
<!-- email="lcorbin@ricochet.net" -->
<!-- subject="Re: Emulation vs. Simulation" -->
<!-- id="200103260728.BAA26872@dal-mail2.ricochet.net" -->
<!-- inreplyto="Emulation vs. Simulation" -->
<strong>From:</strong> Lee Corbin (<a href="mailto:lcorbin@ricochet.net?Subject=Re:%20Emulation%20vs.%20Simulation&In-Reply-To=&lt;200103260728.BAA26872@dal-mail2.ricochet.net&gt;"><em>lcorbin@ricochet.net</em></a>)<br>
<strong>Date:</strong> Mon Mar 26 2001 - 00:38:38 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1511.html">Damien Broderick: "Re: Emulation vs. Simulation"</a>
<li><strong>Previous message:</strong> <a href="1509.html">scerir: "Re: a little meditation on literature and math"</a>
<li><strong>Maybe in reply to:</strong> <a href="1120.html">Lee Corbin: "Emulation vs. Simulation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1511.html">Damien Broderick: "Re: Emulation vs. Simulation"</a>
<li><strong>Reply:</strong> <a href="1511.html">Damien Broderick: "Re: Emulation vs. Simulation"</a>
<li><strong>Reply:</strong> <a href="1537.html">Robert J. Bradbury: "Re: Emulation vs. Simulation"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1510">[ date ]</a>
<a href="index.html#1510">[ thread ]</a>
<a href="subject.html#1510">[ subject ]</a>
<a href="author.html#1510">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Robert J. Bradbury wrote
<br>
<p><em>&gt; You will have to split hairs here -- is a statistical response behavioral
</em><br>
<em>&gt; unit an 'emulation'?  If you say yes, then I would argue that it does not
</em><br>
<em>&gt; have everything we do.  I would argue a statistical response behavioral
</em><br>
<em>&gt; unit is much closer to a 'portrayal'.  I would also argue that few humans
</em><br>
have
<br>
<em>&gt; the necessary tools to distinguish between an emulation and a portrayal.
</em><br>
<p>The whole question is, how plausible really is the existence of a
<br>
statistical response unit, and how well could it survive on its own?
<br>
<p><em>&gt; At some level of technological sophistication, I can statistically 'emulate'
</em><br>
<em>&gt; human behaviors (i.e. given these inputs produce those outputs) and the
</em><br>
humans
<br>
<em>&gt; would not have a clue that they were dealing with a zombie vs. a real
</em><br>
person.
<br>
<em>&gt; You *DO NOT* need consciousness or feelings.  You simply need to
</em><br>
replicate the
<br>
<em>&gt; most common responses in those situations.  That is something that is
</em><br>
entirely
<br>
<em>&gt; programmable.
</em><br>
<p>I contend that these &quot;common responses&quot; are inadequate for survival in
<br>
the real world.  To be very concrete, let us ask what is the minimal
<br>
programmable unit that could (a) hold down a job at Microsoft or Sun,
<br>
(b) drive to work every day, (c) shop and do all the mundane things
<br>
(e.g., fill out drivers license applications) necessary to 21st century
<br>
existence?
<br>
<p>Forget Silicon Valley: even if it was possible in *ancient Sumeria*
<br>
for someone to do what it took to survive back there, why wasn't
<br>
there a series of natural mutations that got rid of all the excess
<br>
baggage like consciousness, feelings, etc.?
<br>
<p>The answer is that it's not possible.  It, like so many other
<br>
programming projects, only seems feasible.  The first AI that
<br>
would be capable of getting along in Silicon Valley (or ancient
<br>
Sumeria) would be almost exactly as conscious and feeling as
<br>
humans are.  There just aren't any shortcuts (or nature, for one,
<br>
would have found them).
<br>
<p>(By the way, before I forget, by &quot;portrayal&quot; I mean &quot;a puppet&quot;, ...if
<br>
you get the analogy.  Picture the smug SI with his metaphorical hand
<br>
running a (metaphorical) puppet in front of some people, and being
<br>
very mildly amused at how easily they are fooled, and the way that they
<br>
think his puppet to be the greatest human being since Jesus Christ.)
<br>
<p>Now we get down to the deep part (in my opinion):
<br>
<p><em>&gt; You can be a zombie with zettabytes (10^21) of code that
</em><br>
<em>&gt; says &quot;In this situation, I say or do X&quot;.  [For reference,
</em><br>
<em>&gt; human memories appear to be able to retrieve several hundred
</em><br>
<em>&gt; megabytes (~10^8) of information, though their 'recognition'
</em><br>
<em>&gt; capacity may be greater.]  That 'program' has no conscious
</em><br>
<em>&gt; ('feelings'?) that say 'I am self-aware').  It doesn't have
</em><br>
<em>&gt; to run a self-rehersal program (which is what consciousness
</em><br>
<em>&gt; is if you follow Calvin).  It simply 'knows' the preprogrammed
</em><br>
<em>&gt; responses to a huge number of situations.
</em><br>
<p>Okay, suppose that we have a creature (that I still don't want
<br>
to call a zombie) which has 10^x times as much storage capability
<br>
as a human being, and it merely does a lookup for everything that
<br>
could conceivably happen to it.  It never really calculates
<br>
anything.  I will officially concede that if x is a big enough
<br>
number, then the entity is not conscious, and therefore is a
<br>
zombie.
<br>
<p>BUT!!!  I think that 10^x is so big that it is completely out
<br>
of the question that anyone would ever want to build a creature
<br>
with 10^x times as much storage as a human being, merely to
<br>
imitate a trivial human being.  Moreover, you might have to
<br>
run a human being or emulation of one (there's no difference)
<br>
in order to generate the table.  And in that case, it can be
<br>
argued that when you ask the &quot;zombie&quot; a question, you are in
<br>
effect asking the original emulation the question.
<br>
<p>(Incidently, I stopped calling myself a functionalist a few
<br>
years back exactly because of this question of lookups: for,
<br>
just as you say, technically speaking there can be (absurdly
<br>
large) lookup tables that indeed act conscious, but are not.)
<br>
<p>But where does this leave us?  In my opinion, we have these
<br>
<p>cases:
<br>
<p>A. A puppet or apparition run by a remote SI
<br>
B. A self-contained entity with a fantastically large lookup table
<br>
C. A self-contained entity with capabilities of human or above,
<br>
&nbsp;&nbsp;&nbsp;but which calculates its behavior (doesn't look it up)
<br>
D. A self-contained entity that is like C, but isn't conscious
<br>
<p>If A, then the question is moot, since the entity doesn't
<br>
really exist.  If B, the entity is not conscious, but is
<br>
probably just the standing record of an earlier run of a
<br>
conscious entity.  If C, then the entity is conscious.  If
<br>
D, then it's not smart enough to survive in a challenging
<br>
environment (unlike a dog).  Only case B might be considered
<br>
a zombie, but that case is not what people are talking about
<br>
on this list (until your post).
<br>
<p>So therefore, any creature in your immediate vicinity is either
<br>
a creature with a big lookup table that, because the table isn't
<br>
big enough, still cannot survive on its own---or, it engages in
<br>
calculation, and is really no more efficient about it than we
<br>
are, and so therefore is conscious.  Either way, in a realistic
<br>
scenario, we don't have &quot;zombies&quot;.  (P.S.  I'm not even sure that
<br>
10^21 actions would be enough for a human---remember, it has to
<br>
ask human IN EVERY POSSIBLE situation, which might mean millions
<br>
of separate emulations had to go into the lookup table.)
<br>
<p>As a slighly relevant aside, I hope you believe that Searle's
<br>
Chinese Room is conscious, intelligent, and has feelings!?
<br>
<p>Lee Corbin
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1511.html">Damien Broderick: "Re: Emulation vs. Simulation"</a>
<li><strong>Previous message:</strong> <a href="1509.html">scerir: "Re: a little meditation on literature and math"</a>
<li><strong>Maybe in reply to:</strong> <a href="1120.html">Lee Corbin: "Emulation vs. Simulation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1511.html">Damien Broderick: "Re: Emulation vs. Simulation"</a>
<li><strong>Reply:</strong> <a href="1511.html">Damien Broderick: "Re: Emulation vs. Simulation"</a>
<li><strong>Reply:</strong> <a href="1537.html">Robert J. Bradbury: "Re: Emulation vs. Simulation"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1510">[ date ]</a>
<a href="index.html#1510">[ thread ]</a>
<a href="subject.html#1510">[ subject ]</a>
<a href="author.html#1510">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:59:43 MDT</em>
</em>
</small>
</body>
</html>
