<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Emulation vs. Simulation</title>
<meta name="Author" content="Robert J. Bradbury (bradbury@aeiveos.com)">
<meta name="Subject" content="Re: Emulation vs. Simulation">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Emulation vs. Simulation</h1>
<!-- received="Sat Mar 24 07:47:24 2001" -->
<!-- isoreceived="20010324144724" -->
<!-- sent="Sat, 24 Mar 2001 06:47:23 -0800 (PST)" -->
<!-- isosent="20010324144723" -->
<!-- name="Robert J. Bradbury" -->
<!-- email="bradbury@aeiveos.com" -->
<!-- subject="Re: Emulation vs. Simulation" -->
<!-- id="Pine.UW2.4.20.0103240558310.15316-100000@www.aeiveos.com" -->
<!-- inreplyto="200103240418.WAA23096@dal-mail2.ricochet.net" -->
<strong>From:</strong> Robert J. Bradbury (<a href="mailto:bradbury@aeiveos.com?Subject=Re:%20Emulation%20vs.%20Simulation&In-Reply-To=&lt;Pine.UW2.4.20.0103240558310.15316-100000@www.aeiveos.com&gt;"><em>bradbury@aeiveos.com</em></a>)<br>
<strong>Date:</strong> Sat Mar 24 2001 - 07:47:23 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1406.html">GBurch1@aol.com: "ECON/POWER: Excellent Article About Cal. Power Fiasco"</a>
<li><strong>Previous message:</strong> <a href="1404.html">GBurch1@aol.com: "Re: SOC: The Challenge of &quot;The Second World&quot;"</a>
<li><strong>In reply to:</strong> <a href="1377.html">Lee Corbin: "Re: Emulation vs. Simulation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1486.html">Jim Fehlinger: "Re: Emulation vs. Simulation"</a>
<li><strong>Reply:</strong> <a href="1486.html">Jim Fehlinger: "Re: Emulation vs. Simulation"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1405">[ date ]</a>
<a href="index.html#1405">[ thread ]</a>
<a href="subject.html#1405">[ subject ]</a>
<a href="author.html#1405">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
On Fri, 23 Mar 2001, Lee Corbin wrote:
<br>
<p><em>&gt; Here is what is happening:  The Holodeck, like any good SI, mocks
</em><br>
<em>&gt; up a portrayal of Professor Moriarty for the benefit of the
</em><br>
<em>&gt; actual sapients watching.  At this point, by definition, Moriarty
</em><br>
<em>&gt; has no consciousness or or feelings.  You could, for example,
</em><br>
<em>&gt; attempt to do anything whatsoever to the Professor without
</em><br>
<em>&gt; even coming close to harming anyone.
</em><br>
<p>Here is an interesting twist, based on some of my comments
<br>
regarding the need for robotic junk retreival systems and
<br>
the telepresence robots -- say the zombie 'body' or 'hologram'
<br>
is being remotely operated by a real human being (or an SI).
<br>
<p>Now in this case you can do whatever you want to the body/holo
<br>
and you are not harming anyone.  However humans are designed
<br>
to have empathy for things that look, walk, and talk like
<br>
humans (they are also designed to be wary of them until
<br>
a trust-bond is developed).  So the tele-operator is going
<br>
to have to be 'tightly' wired to the remote body, such that
<br>
pain caused to the remote body will generate natural reactions
<br>
in the operator.  [Presumably sensory safety devices cut
<br>
in at extreme levels.]  Now, the operator is getting paid
<br>
quite well for performing this service and have freely
<br>
entered into this situation.
<br>
<p>In this case is the remotely piloted body/holo a zombie?
<br>
<p><em>&gt; But then comes the transformation that you describe: the SI
</em><br>
<em>&gt; spawns the Professor Moriarty process, and now there is a
</em><br>
<em>&gt; separate, human-level sentience, with separate consciousness,
</em><br>
<em>&gt; feelings, etc.
</em><br>
<p>Given that the SI is writing the code that 'operates' the
<br>
telepresence body/holo, I would argue that it can prevent
<br>
it from ever becoming self-conscious.  You simply execute the
<br>
common standard behaviors based on similar situations and random
<br>
behavior selector strategies for the rarer situations (not too
<br>
different from your standard issue human as far as I can tell).
<br>
<p><em>&gt; As to whether the actual Moriarty consciousness
</em><br>
<em>&gt; (if we feel justified in talking about such a thing), or the
</em><br>
<em>&gt; actual Moriarty feelings (ditto) existed isomorphically in
</em><br>
<em>&gt; the complex SI is a question too deep to pursue in this email.
</em><br>
<em>&gt; (I believe the answer is, for all practical purposes, &quot;no&quot;.)
</em><br>
<p>But 'feelings' are gentico-socio-'thingys' (insert a word
<br>
here to represent a neurocomputational 'pattern') that are
<br>
designed to promote survival.  There is no reason to elevate
<br>
them to significance (if that is what you are doing).  They
<br>
are subroutines designed to promote behaviors that have
<br>
specific goal seeking strategies in the framework of the system.
<br>
<p><em>&gt; But why dignify that earlier mere portrayal with the term
</em><br>
<em>&gt; &quot;zombie&quot;, which HAS ALWAYS MEANT an independent creature
</em><br>
<em>&gt; (not a puppet) whose behavior is identical but who lacks
</em><br>
<em>&gt; consciousness (which is impossible or nonsensical)?
</em><br>
<p>Ah ha, so here a zombie cannot be 'tele-operated'.  But
<br>
if a non-tele-operated creature does not have 'feelings' that
<br>
promote its survival, then its very rapidly a dead zombie.
<br>
The zombie movies would be much less interesting if they
<br>
were not trying to 'kill' humans (presumably motivated
<br>
by feelings).  If they were just stumbling around in the
<br>
world, its simple -- &quot;'Thunk', you no longer function.&quot;
<br>
<p>But I don't buy the impossible/nonsensical part.  With the
<br>
statistical correlation and fuzzy logic capabilities that
<br>
we now have, do you not think we could produce fully
<br>
functional unrecognizable zombies with no consciousness?
<br>
I think the AOL-Eliza example demonstrates that this
<br>
is feasible based on much simpler principles.  The
<br>
interesting figure-of-merit is the time it takes
<br>
individuals of various IQs or educations to recognize
<br>
they are talking to a non-conscious entity.
<br>
<p>You can be a zombie with zettabytes (10^21) of code that
<br>
says &quot;In this situation, I say or do X&quot;.  [For reference,
<br>
human memories appear to be able to retrieve several hundred
<br>
megabytes (~10^8) of information, though their 'recognition'
<br>
capacity may be greater.]  That 'program' has no conscious
<br>
('feelings'?) that say 'I am self-aware').  It doesn't have
<br>
to run a self-rehersal program (which is what consciousness
<br>
is if you follow Calvin).  It simply 'knows' the preprogrammed
<br>
responses to a huge number of situations.
<br>
<p>Now, I've kind of lost track of the definitions of 'zombie'
<br>
in this message, but it seems clear to me that in the
<br>
classical sense -- a non-self-conscious or self-aware, but
<br>
Turing-test equivalent, human should be feasible.  This
<br>
becomes further complicated by the tele-operation scenario.
<br>
There you are 'relating' to a real consciousness but one
<br>
that may be arbitrarily disconnected from the pleasures or
<br>
pain experienced in life.  I.e. they can dial up or down
<br>
the motivational 'feelings'.  The degree to which you
<br>
perceive them as a 'zombie' depends in large part on
<br>
their acting abilities.  They may dial down the stimulation
<br>
vectors, but know how they should react in specific
<br>
circumstances and so you have no way of knowing whether
<br>
or not its 'real' or a remotely directed experience.
<br>
<p>Robert
<br>
<p>Robert
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1406.html">GBurch1@aol.com: "ECON/POWER: Excellent Article About Cal. Power Fiasco"</a>
<li><strong>Previous message:</strong> <a href="1404.html">GBurch1@aol.com: "Re: SOC: The Challenge of &quot;The Second World&quot;"</a>
<li><strong>In reply to:</strong> <a href="1377.html">Lee Corbin: "Re: Emulation vs. Simulation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1486.html">Jim Fehlinger: "Re: Emulation vs. Simulation"</a>
<li><strong>Reply:</strong> <a href="1486.html">Jim Fehlinger: "Re: Emulation vs. Simulation"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1405">[ date ]</a>
<a href="index.html#1405">[ thread ]</a>
<a href="subject.html#1405">[ subject ]</a>
<a href="author.html#1405">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:59:42 MDT</em>
</em>
</small>
</body>
</html>
