<!-- received="Sun Jul 25 23:06:02 1999 MDT" -->
<!-- sent="Mon, 26 Jul 1999 00:05:46 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: IA vs. AI was: longevity vs singularity" -->
<!-- id="379BECA9.1AFD2B5B@pobox.com" -->
<!-- inreplyto="IA vs. AI was: longevity vs singularity" -->
<!-- version=1.10, linesinbody=107 -->
<html><head><title>extropians: Re: IA vs. AI was: longevity vs singularity</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: IA vs. AI was: longevity vs singularity</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Mon, 26 Jul 1999 00:05:46 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1121">[ date ]</a><a href="index.html#1121">[ thread ]</a><a href="subject.html#1121">[ subject ]</a><a href="author.html#1121">[ author ]</a>
<!-- next="start" -->
<li><a href="1122.html">[ Next ]</a><a href="1120.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1090.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
den Otter wrote:
<br>
<i>&gt; </i><br>
<a href="1090.html#1121qlink1">&gt; &gt; A suggestion bordering on the absurd.  Uploading becomes possible at</a><br>
<i>&gt; &gt; 2040 CRNS.  It becomes available to the average person at 2060 CRNS.</i><br>
<i>&gt; &gt; Transhuman AI becomes possible at 2020 CRNS.  Nanotechnology becomes</i><br>
<i>&gt; &gt; possible at 2015 CRNS.</i><br>
<i>&gt; </i><br>
<i>&gt; -there's still a gap of 5 years between nanotech and AI, ample time</i><br>
<i>&gt; to wipe out civilization if nanotech is as dangerous as you seem to</i><br>
<i>&gt; assume.</i><br>

<p>
Yes, I've noticed that.  I stay up at night worrying about it.

<p>
<a href="1090.html#1121qlink2">&gt; -I'd have to stop nanowar, assuming I'd want to do that, for 25 and</a><br>
<i>&gt; not 45 years (because the Singularity comes immediately after</i><br>
<i>&gt; the first wave of uploads. Anyone who's serious about survival,</i><br>
<i>&gt; let alone ascension, must be in the first wave. Needless to say,</i><br>
<i>&gt; be sure to get rich). Consequently, I'd have to hold back AI for</i><br>
<i>&gt; 20 years, not 40. Still very hard, but feasible with the proper</i><br>
<i>&gt; support.</i><br>

<p>
<a name="1245qlink2">Has it occurred to you that if the first uploads *are* charitably
inclined, then it's *much* safer to be in the second wave?  The first
uploads are likely to be more in the nature of suicide volunteers,</a>
<a name="1245qlink3">especially when you consider that a rough, destructive, but adequate
scan is likely to come before a perfect, nondestructive scan.</a>

<p>
<a name="1245qlink4">You're staking an awful lot on the selfishness of superintelligences.</a> 
<a name="1245qlink5">Maybe you don't have the faintest speck of charity in your soul, but if
uploading and upgrading inevitably wipes out enough of your personality
that anyone would stop being cooperative - well, does it really make
that much of a difference who this new intelligence "started out" as? 
It's not you.  I know that you might identify with a selfish SI, but my
point is that if SIs are *inevitably* selfish, if *anyone* would
converge to selfishness, that probably involves enough of a personality
change in other departments that even you wouldn't call</a> it you.

<p>
<a name="1245qlink6"><a href="1090.html#1121qlink3">&gt; -Stopping you from writing an AI wouldn't be all that hard, if I really</a><br>
<i>&gt; wanted to. ;-)</i><br>

<p>
Sure.  One bullet, no more Specialist.  Except that that just means it
takes a few more years.  You can't stop it forever.</a>  All you can do is
<a name="1245qlink7">speed up the development of nanotech...relatively speaking.  We both
know you can't steer a car by selectively shooting out the tires.</a>

<p>
<a href="1090.html#1121qlink4">&gt; -If nanotech is advanced enough to destroy the world, it can</a><br>
<i>&gt; surely also be used to move to space and live there long enough</i><br>
<i>&gt; to transcend.</i><br>

<p>
More sophisticated nanotech, and it's nanotech in your own personal
hands.  I don't need personal nanotech to die, but I need it to run. 
Realistically, it's obvious that the probabilities are not in my favor. 
Unless I'm so caught up in wishful thinking that I think I can get, and
retain exclusive control of, nanotechnology AND uploading AND AI AND
intelligence enhancement... but why continue?

<p>
<a name="1245qlink8"><a href="1090.html#1121qlink5">&gt; You can run and/or hide hide from nanotech, even</a><br>
<i>&gt; fight it successfully, but you can't do that with a superhuman</i><br>
<i>&gt; AI, i.e. nanotech leaves some room for error, while AI doesn't (or</i><br>
<i>&gt; much less in any case). As I've said before, intelligence is the</i><br>
<i>&gt; ultimate weapon, infinitely more dangerous than stupid nanites.</i><br>

<p>
Quite.  And an inescapable one.<a name="1172qlink1">  See, what *you* want is unrealistic
<a name="1210qlink1">because you want yourself to be the first one to upload, which excludes
</a>
you from cooperation with more than a small group and limits your
<a name="1245qlink10">ability to rely on things like open-source projects and charitable
foundations.</a>  What *they* want is unrealistic because they want to
</a>
</a>
freeze progress.

<p>
<a name="1245qlink11">Both of you are imposing all kinds of extra constraints.  You're always
going to be at a competitive disadvantage relative to a pure
</a>Singularitarian<a name="1245qlink12"> or the classic "reckless researcher", who doesn't demand
that the AI be loaded down with coercions, or that nanotechnology not be
unleashed until it can be used for space travel, or that nobody uploads
until everyone can do it simultaneously, or that nobody has access to
the project except eight people, and so on ad nauseam.  The open-source
free-willed AI project is going to be twenty million miles ahead while
you're still dotting your "i"s and crossing your "t"s.</a>

<p>
<a href="1090.html#1121qlink6">&gt; -Synchronized Singularity *is* feasible for a limited number of people</a><br>
<i>&gt; (some of which may choose to uplift the rest later). Yes, it's extremely</i><br>
<i>&gt; hard, but not impossible. More importantly, it's the only real option</i><br>
<i>&gt; someone who values his existence has. The AI should only be used</i><br>
<i>&gt; after all else has failed and the world is going to hell in a handbasket.</i><br>

<p>
I don't understand your calculations.

<p>
<a name="1245qlink13"><a name="1210qlink2"><a name="1172qlink2">A-priori chance that you, personally, can be in the first 6 people to
upload:  1e-9.</a></a>
<br>
<a name="1210qlink3"><a name="1172qlink3">Extremely optimistic chance:  1%</a></a></a>
<br>
<a name="1245qlink14"><a name="1210qlink4"><a name="1172qlink4">Extremely pessimistic chance that AIs are benevolent: 10%</a></a>

<p>
<a name="1210qlink5"><a name="1172qlink5">Therefore it's 10 times better to concentrate on AI.</a></a></a>

<p>
<a href="1090.html#1121qlink7">&gt; &gt; after all, I've openly declared that</a><br>
<i>&gt; &gt; my first allegiance is not to humanity.</i><br>
<i>&gt; </i><br>
<i>&gt; No, it should be to yourself, of course. Anyway, so you're willing to</i><br>
<i>&gt; kill everyone on earth, including yourself, to achieve...what, exactly?</i><br>

<p>
Sorry, I'm not on the need-to-know list for that information.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1122.html">[ Next ]</a><a href="1120.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1090.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
</body></html>
