<!-- received="Wed Sep 15 18:21:18 1999 MDT" -->
<!-- sent="Wed, 15 Sep 1999 19:23:06 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Artilects &amp; stuff" -->
<!-- id="37E03853.230B4C13@pobox.com" -->
<!-- inreplyto="Artilects &amp; stuff" -->
<!-- version=1.10, linesinbody=144 -->
<html><head><title>extropians: Re: Artilects &amp; stuff</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Artilects &amp; stuff</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Wed, 15 Sep 1999 19:23:06 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4231">[ date ]</a><a href="index.html#4231">[ thread ]</a><a href="subject.html#4231">[ subject ]</a><a href="author.html#4231">[ author ]</a>
<!-- next="start" -->
<li><a href="4232.html">[ Next ]</a><a href="4230.html">[ Previous ]</a>
<b>In reply to:</b> <a href="4222.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
den Otter wrote:
<br>
<i>&gt; </i><br>
<a href="4222.html#4231qlink1">&gt; Ok, let me explain...again.</a><br>

<p>
Please do.  I think your position is very peculiar; you've gotten
through most of the logic of causality and then, as far as I can tell,
stopped halfway.

<p>
<a name="4379qlink2"><a href="4222.html#4231qlink2">&gt; Emotions may be "arbitrary evolved adaptations", but they're</a><br>
<i>&gt; also *the* meaning of life (or: give life meaning, but that's</i><br>
<i>&gt; essentially the same thing). That's how we humans work.</i><br>

<p>
I disagree.  Let's suppose I accept your basic logic, as a premise for
discussion.  Fine.  Emotions are still only part of the mind, not all of
it.  For as long as I continue doing things, making choices, instead of
committing suicide, then my life is as meaningful as yours.</a>  It doesn't
<a name="4379qlink3">matter whether the particular set of cognitive contents that causes me
to eat a slice of pizza, or the cognitive contents that make me happy as
a result, are in the cortex or the limbic system - whether they're
built-in "emotions" or software "thoughts".</a>  How can you draw a hard
line between the two?  They're both made of neurons.

<p>
<a name="4379qlink4"><a href="4222.html#4231qlink3">&gt; The only reason why logic matters is because it can be a useful</a><br>
<i>&gt; tool to achieve "random" emotional goals. Rationality is practical</i><br>
<i>&gt; ...in an emotional context. It has no inherent meaning or value.</i><br>
<i>&gt; NOTHING has. Even the most superb SI would (by definition) be</i><br>
<i>&gt; _meaningless_ if there wasn't someone or something to *appreciate*</i><br>
<i>&gt; it. Value is observer-dependent. Subjective.</i><br>

<p>
Okay, suppose I accept this.  I still don't see why any SI will
automatically drop off all its emotions *except* selfishness and then
you think this is a good thing.  That's really where you lost me.</a>

<p>
<a name="4379qlink5"><a href="4222.html#4231qlink4">&gt; At least, that's what I suspect. I could be wrong, but I seriously</a><br>
<i>&gt; doubt it. So what's the logical thing to do? Risk my life because</i><br>
<i>&gt; of some emotional whim ("must find the truth")? No, obviously not.</i><br>

<p>
But why not?  There's one emotional whim, "must find the truth".  And
another emotional whim, "stay alive".  And another, "be happy".  In me
the first one is stronger than the other two.  In you the two are
stronger than the one.  I don't get the part where your way is more
rational than mine, especially if all this is observer-dependent.</a>

<p>
<a name="4379qlink6">I particularly don't get the part where an SI converges to your point of
view.  And if it doesn't converge, why not build an SI whose innate
desires are to make you happy, and everyone else who helped out on the
project happy, and everyone on the planet happy, thus hopscotching the
whole uploading technological impossibility and guaranteeing yourself a
much larger support base?</a>

<p>
<a href="4222.html#4231qlink5">&gt; The rational thing to do is to stay alive indefinitely, sticking</a><br>
<i>&gt; to the default meaning of life (pleasant emotions) until, if ever [*]</i><br>
<i>&gt; , you find something better. So maybe you'll just lead a "meaningful",</i><br>
<i>&gt; happy life for all eternity. How horrible!</i><br>

<p>
<a name="4379qlink7">Okay, now I spot a level confusion.  First you say "the rational thing
to do" - that is, the rational way to make choices - and then you follow
it up with the words "meaning of life".  Aren't these the same thing? 
Either you have an emotion impelling you to do rational things, or you
don't.  If you don't, why are you rationally serving any other emotions?
 It's not clear to me what kind of cognitive architecture you're
proposing.  However an AI makes choices, that, to you, I think, is its
"meaning of life".  Same thing goes for humans.</a>

<p>
Trying to divide this "meaning of life" into a set of declarative
"goals" and a set of procedural "rationality" strikes as being
artificial and subject to all sorts of challenge.  Maybe the
"rationality" is another set of goals - "Goal:  If A and A implies B,
conclude B." - and the new procedural meta-rationality is "follow all
these goal rules".  Like taking a Turing machine with "rational
cognition" and "goals", and turning the state transition diagram into
goals, and making the new rational rules the Universal Turing machine
rules.  Has anything really changed?  No.  The "meaning of life" is the
entire causal matrix that produces choices and behaviors; you can't
really subdivide it any further.  The only reason we call something a
"goal" in the first place is because it produces goal-seeking behavior. 
There's nothing magical about the LISP atom labeled "goal" that turns it
into an actual goal.

<p>
<a name="4379qlink8"><a href="4222.html#4231qlink6">&gt; [*] Actually, as I've pointed out before, uncertainty is</a><br>
<i>&gt; "eternal"; you can never, for example, know 100% sure that</i><br>
<i>&gt; killing yourself is the "right" thing to do, even if you're</i><br>
<i>&gt; a SI^9. Likewise, the nonexistence of (a) God will never be</i><br>
<i>&gt; proven conclusively, or that our "reality" isn't some superior</i><br>
<i>&gt; entity's pet simulation etc. You can be "fairly sure", but</i><br>
<i>&gt; never *completely* sure. This isn't "defeatism", but pure,</i><br>
<i>&gt; hard logic. But that aside.</i><br>

<p>
And I repeat:  "You know this for certain?"  But that aside.</a>

<p>
<a name="4379qlink9"><a href="4222.html#4231qlink7">&gt; So stay alive, evolve and be happy. Don't get your ass killed</a><br>
<i>&gt; because of some silly chimera. Once you've abolished suffering</i><br>
<i>&gt; and gained immortality, you have forever to find out whether</i><br>
<i>&gt; there is such a thing as an "external(ist)" meaning of life.</i><br>
<i>&gt; There's no rush.</i><br>

<p>
I'll certainly agree with that part.  Like I said, the part I don't get
is where you object to an AI.</a>  And I also don't entirely understand what
you think the difference is between human and AI cognitive architectures
that makes humans so much more stable and trustworthy.  Do you have any
idea what kind of muck is down there?

<p>
<i>&gt; Or:</i><br>
<i>&gt; </i><br>
<i>&gt; #1</i><br>
<a href="4222.html#4231qlink8">&gt; Your goal of creating superhuman AI and causing a</a><br>
<i>&gt; Singularity is worth 10 emotional points. You get killed</i><br>
<i>&gt; by the Singularity, so you have 10 points (+any previously</i><br>
<i>&gt; earned points, obviously) total. A finite amount.</i><br>
<i>&gt; </i><br>
<i>&gt; #2</i><br>
<i>&gt; You upload, transcend and live forever. You gain an infinite</i><br>
<i>&gt; amount of points.</i><br>
<i>&gt; </i><br>
<i>&gt; Who wins?</i><br>

<p>
Nobody, unless you say that "getting points" is the objective, provable,
rational meaning of life, in which case obviously the rational thing to
do is to create a superhuman AI, which will naturally follow the same
logic and attempt to earn an infinite number of points.

<p>
Or:

<p>
#1
<br>
"Your goal of creating superhuman AI and causing a
Singularity is worth 10 emotional points. You get killed
by the Singularity, so you have 10 points (+any previously
earned points, obviously) total. A finite amount."

<p>
This scenario is worth 20 emotional points to me.

<p>
#2
<br>
"You upload, transcend and live forever. You gain an infinite
amount of points."

<p>
This scenario is worth 5 emotional points to me.

<p>
Who wins?
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="4232.html">[ Next ]</a><a href="4230.html">[ Previous ]</a>
<b>In reply to:</b> <a href="4222.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
</body></html>
