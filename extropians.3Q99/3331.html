<!-- received="Wed Sep  1 23:41:58 1999 MDT" -->
<!-- sent="Wed, 1 Sep 1999 22:39:04 -0700 (PDT)" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="RE: Singularity?" -->
<!-- id="14286.3448.64390.688982@lrz.de" -->
<!-- inreplyto="NDBBLBGGEJLACCFCPNINGEKBCCAA.ewbrownv@mindspring.com" -->
<!-- version=1.10, linesinbody=56 -->
<html><head><title>extropians: RE: Singularity?</title>
<meta name=author content="Eugene Leitl">
<link rel=author rev=made href="mailto:eugene.leitl@lrz.uni-muenchen.de" title ="Eugene Leitl">
</head><body>
<h1>RE: Singularity?</h1>
Eugene Leitl (<i>eugene.leitl@lrz.uni-muenchen.de</i>)<br>
<i>Wed, 1 Sep 1999 22:39:04 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3331">[ date ]</a><a href="index.html#3331">[ thread ]</a><a href="subject.html#3331">[ subject ]</a><a href="author.html#3331">[ author ]</a>
<!-- next="start" -->
<li><a href="3332.html">[ Next ]</a><a href="3330.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3325.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3355.html">Robert J. Bradbury</a>
</ul>
<!-- body="start" -->

<p>
Billy Brown writes:

<p>
<a href="3325.html#3331qlink1"> &gt; The determining factor on this issue is how hard AI and intelligence</a><br>
<i> &gt; enhancement turn out to be.  If intelligence is staggeringly complex, and</i><br>

<p>
<a name="3355qlink1">I think intelligence enhancement (i.e. beyond the trivial: as in
augmented reality/wearable) will be very very hard. AI, provided the
</a>
hardware is moving into the right direction (people have recently
started buying the memory-on-die/ultra-wide-buses/massively parallel
pile of chips as well as started looking into molecular electronics)
will be comparatively easy, if you do it the ALife way. Contrary to
<a name="3355qlink2">what Eliezer profusely professes, human-coded AI will never become
relevant.
</a>

<p>
Providing boundary conditions for emergence, yes. Coding it all by
hand, never.

<p>
<a href="3325.html#3331qlink2"> &gt; requires opaque data structures that are generally inscrutable to beings of</a><br>
<i> &gt; human intelligence, we get one kind of future (nanotech and human</i><br>

<p>
You don't need to understand the physics of your mental process to be
intelligent. Evolution is not sentient, yet it apparently produces
sentience. Providing a good evolvable nucleus in a dramatically sped
up coevolution sure does sound like a winner to me. Once you get this
started you don't have to worry about optimizing initial conditions,
because these don't matter for the final result.

<p>
<a href="3325.html#3331qlink3"> &gt; enhancement come online relatively slowly, AI creeps along at a modest rate,</a><br>
<i> &gt; and enhanced humans have a good shot at staying in charge).  If intelligence</i><br>

<p>
<a name="3355qlink3">Uh, don't think so. The threshold for enhancing humans is terribly
high: essentially you'll need to be able to do uploads. Anything else
is comparatively insignificant. AI might be here much sooner than
</a>
uploads. Being nonhuman and subject to positive autofeedback makes it
<a name="3355qlink4">a very dangerous thing to build indeed. Maybe we need another
</a>
Kaczynski...

<p>
<a href="3325.html#3331qlink4"> &gt; can be achieved using relatively comprehensible programming techniques, such</a><br>
<i> &gt; that a sentient AI can understand its own operation, we get a very different</i><br>
<i> &gt; kind of future (very fast AI progresss leading to a rapid Singularity, with</i><br>
<i> &gt; essentially no chance for humanity to keep up).  Either way, the kind of</i><br>
<i> &gt; future we end up in has absolutely nothing to do with the decisions we make.</i><br>
<i> &gt; </i><br>
<i> &gt; Personally, I feel that the first scenario is somewhat more likely than the</i><br>
<i> &gt; second.  However, I can't know for sure until we get a lot closer to</i><br>
<i> &gt; actually having sentient AI.  It therefore pays to take out some insurance</i><br>
<i> &gt; by doing what I can to make sure that if we do end up in the second kind of</i><br>
<i> &gt; future we won't screw things up.  So far, the best way I can see to</i><br>
<i> &gt; influence events is to try to end up being one of the people doing the work</i><br>
<i> &gt; (although I'm working on being one of the funding sources, which could</i><br>
<i> &gt; potentially be a better angle).</i><br>

<p>
<a name="3355qlink5">I think one of the best projects for funding is brain vitrification
which does not require fractal cooling channel plumbing in vivo.
</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3332.html">[ Next ]</a><a href="3330.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3325.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3355.html">Robert J. Bradbury</a>
</ul>
</body></html>
