<!-- received="Tue Aug  3 04:20:08 1999 MDT" -->
<!-- sent="Tue, 03 Aug 1999 05:20:06 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Major Public AI Backlash Inevitable." -->
<!-- id="37A6C24B.5444DCD5@pobox.com" -->
<!-- inreplyto="Major Public AI Backlash Inevitable." -->
<!-- version=1.10, linesinbody=60 -->
<html><head><title>extropians: Re: Major Public AI Backlash Inevitable.</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Major Public AI Backlash Inevitable.</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Tue, 03 Aug 1999 05:20:06 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1451">[ date ]</a><a href="index.html#1451">[ thread ]</a><a href="subject.html#1451">[ subject ]</a><a href="author.html#1451">[ author ]</a>
<!-- next="start" -->
<li><a href="1452.html">[ Next ]</a><a href="1450.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1447.html">paul@i2.to</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
paul@i2.to wrote:
<br>
<i>&gt; </i><br>
<a href="1447.html#1451qlink1">&gt; For those following the IA vs. AI thread, consider the</a><br>
<i>&gt; backlash now escalating against genetic engineering</i><br>
<i>&gt; (bioenhanced food, cloning).  And this hysterical backlash</i><br>
<i>&gt; is happening despite the most ethical intentions and</i><br>
<i>&gt; carefully crafted rhetoric of working biotechnologists.</i><br>

<p>
<a name="1459qlink1">I hate relying on human stupidity, but in this case I think we're fairly
safe.  Unless AI starts depriving people of jobs, or becomes available
at the local supermarket, there isn't going to be a GM-type scare.</a> 
Remember, there haven't been any instances of AIs going Frankenstein,
and there probably never will be.  That does make it harder to advance
what-if scenarios of the type used by anti-GM advocates.  You can burn a
crop to the ground, you can rail against devil foods, but it's going to
be hard to get your average villager angry at a computer program.  It's
<a name="1459qlink2">actually a lot harder to get people excited over the end of the world
then it is to get them excited about an evil hamburger.</a>

<p>
<a name="1528qlink1">The people who'd be most opposed to us will dismiss the entire thing as
a figment of the imagination.  In fact, I'd suggest deliberately
<a name="1585qlink1">planting memes among New Agers to the effect that transhumanists are
stupid, misguided, comical, and harmless.</a>  When some Congresscritter
</a>
gets up to make a speech about evil AIs taking over the world, we want
half the floor laughing and the other half saying that he's buying into
the whole evil transhumanist worldview by suggesting the possibility.

<p>
<a name="1459qlink3">Or at least, I *would* suggest that, if it weren't lying.  From an
ethical standpoint, I would feel better if people took an interest in
their own destiny, even if it was the wrong interest.  It's your world
too, humanity!  If you believe AI is wrong, then stand up and fight! 
I'm tired of being the only one who cares!</a>

<p>
<a name="1459qlink4">In the end, humanity's strength of will and mind may be more important
than whose side anyone is on.  If there are going to be anti-AI
arguments, then let's do everything we can to supply them with the
factual information they need to develop those arguments; raise the
level of debate so that facts win out.</a>  I'm not as idealistic - in the
social sense - as I used to be, but I still like to think of myself as
serving intelligence, and I would rather not deliberately encourage stupidity.

<p>
<a href="1447.html#1451qlink2">&gt; Now imagine the public reaction when they discover the</a><br>
<i>&gt; *real* agenda of most AI researchers who joyfully look</i><br>
<i>&gt; forward to the day their creations make us all extinct!</i><br>

<p>
When the public "discovers" our "real" agenda?  I don't recall
pretending for one second that my agenda was anything other than the
reality, here or anywhere else.  When I thought AIs would be benevolent,
I said so.  When I realized I didn't have the vaguest notion, I said so.
 When I realized it wasn't my job to care, I said so.  From Moravec to
Yudkowsky, we've been square with you.  We have nothing to hide.  And I
think the public will appreciate that.  Once you've admitted you're out
to destroy Life As We Know It, anything your opponent accuses you of is
going to be an anticlimax.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1452.html">[ Next ]</a><a href="1450.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1447.html">paul@i2.to</a>
<!-- nextthread="start" -->
</ul>
</body></html>
