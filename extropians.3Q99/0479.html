<!-- received="Sat Jul 10 09:00:54 1999 MDT" -->
<!-- sent="Sat, 10 Jul 1999 11:00:11 -0400" -->
<!-- name="Chris Fedeli" -->
<!-- email="fedeli@email.msn.com" -->
<!-- subject="RE: Robots in Social Positions (Plus Intelligent Environments)" -->
<!-- id="000301becae4$fd0d15c0$6f032599@fedeli" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=98 -->
<html><head><title>extropians: RE: Robots in Social Positions (Plus Intelligent Environments)</title>
<meta name=author content="Chris Fedeli">
<link rel=author rev=made href="mailto:fedeli@email.msn.com" title ="Chris Fedeli">
</head><body>
<h1>RE: Robots in Social Positions (Plus Intelligent Environments)</h1>
Chris Fedeli (<i>fedeli@email.msn.com</i>)<br>
<i>Sat, 10 Jul 1999 11:00:11 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#479">[ date ]</a><a href="index.html#479">[ thread ]</a><a href="subject.html#479">[ subject ]</a><a href="author.html#479">[ author ]</a>
<!-- next="start" -->
<li><a href="0480.html">[ Next ]</a><a href="0478.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0423.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->


<p>
Mark Phillips wrote:

<p>
<a href="0397.html#0479qlink1">&gt;&gt;My intuition though, goes for a kind of</a><br>
<i>&gt;&gt;modified combination of "Asimov's Laws</i><br>
<i>&gt;&gt;of Robotics" plus something closely resembling</i><br>
<i>&gt;&gt;a robust set of liberal/civil libertarian protocols for</i><br>
<i>&gt;&gt;humans vis-a-vis such systems.</i><br>

<p>
Billy Brown replied:

<p>
<a href="0423.html#0479qlink2">&gt;It seems to me that you are suggesting that we</a><br>
<i>&gt;implant such "Laws" into their code as moral</i><br>
<i>&gt;coercions.  We've argued over that before on this list,</i><br>
<i>&gt;and the summary position of my camp is as follows:</i><br>

<p>
I'm sorry I missed this earlier discussion, but when I
see the term "coercion" I have to chime in.  All of law is
coercion, the point being if behavior didn't need to be
coerced there would be no need for laws, and we would
have none.  Whether we are coerced by external threats
or deeply ingrained moral inclinations, there is no behavior
that isn't influenced by rules which take into account the
the world outside our own minds.

<p>
<a name="0492qlink1"><a href="0423.html#0479qlink3">&gt;1) It won't work.  You can only use this sort of mind</a><br>
<i>&gt;control on something that is less intelligent than</a> you are.</i><br>

<p>
Well...  Evolution by natural selection has ingrained scores
of complicated rules for social interaction into human
brains.
<br>
True, the advent of human culture has created some elbow
room whereby those rules are revised and amended in ways
the selfish genes never intended (called learning), but this
<a name="0521qlink1"><a name="0492qlink2">recent development has not amounted to a wholesale rewriting
of our moral programming.</a>  As culture and experience are
</a>
absorbed by our newly conscious<a name="0492qlink3"> minds, these memes interact
in a formulaic way with the tons of evolutionary garbage
</a>
which still occupies most of our lobe space.

<p>
<a name="0492qlink4"><a href="0423.html#0479qlink4">&gt;2) It is immoral to try.</a></a><br>

<p>
<a name="0521qlink3"><a name="0492qlink5">To give robots Asimov-type laws would be a planned effort to
do to them what evolution has done to us - make them capable
of co-existing with others.</a>  We are already finding portions
of our evolutionary moral heritage distasteful, and when we
</a>
do we
<br>
revise it using the tools of culture such as law (ie,
outlawing rape
<br>
and murder, two perfectly respectable moral choices from an
evolutionary perspective).  In the future we'll continue to
do
<br>
this using the more powerful tools of neuroscience and the
like.

<p>
<a name="0492qlink6">When we develop robots that become members of society,</a> they
will need the kind of moral frame of reference that we take
for
<br>
granted.<a name="0492qlink7">  If they are to have the capacity of self
</a>
awareness,
<br>
<a name="0492qlink8">then we recognize that their learning and experience will
enable
<br>
them to revise their internal programming just as modern
</a>
humans
<br>
have.

<p>
<a href="0423.html#0479qlink5">&gt;3) It is likely to be unnecesary.  If they are really that</a><br>
<i>&gt;smart, they will know more about morality than we do.</i><br>
<a name="0492qlink9"><i>&gt;The best course of action is simply to start them off with</i><br>
<i>&gt;a firm knowledge of human morality, without trying to</i><br>
<i>&gt;prevent them from evolving their own (superior) versions</i><br>
<i>&gt;as they learn.</a></i><br>

<p>
Superior in the Nietzschean sense?  Set a default option for
moral nihilism on our future AI's and thats just what we'll
get.
<br>
Nothing is more superior (from an individual's perspective)
than the ability to be caluculating and wholly without
compunction.

<p>
I guess I'm really not too far from you on this.  I agree
that we
<br>
can't prevent intelligent beings from evolving their own
moral
<br>
ideas, not because we shouldn't but just because it probably
<a name="0492qlink10">isn't possible.</a>

<p>
Anyway, social systems can be buit for amoral sentients -
modern police states alredy exist to accomade an
increasingly
<br>
cultured and "morally flexible" populace.


<p>
Chris Fedeli
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0480.html">[ Next ]</a><a href="0478.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0423.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
</body></html>
