<!-- received="Sat Jul 31 15:47:35 1999 MDT" -->
<!-- sent="Sat, 31 Jul 1999 14:47:01 PDT" -->
<!-- name="phil osborn" -->
<!-- email="philosborn@hotmail.com" -->
<!-- subject="Re: IA vs. AI was: longevity vs singularity" -->
<!-- id="19990731214702.62109.qmail@hotmail.com" -->
<!-- inreplyto="IA vs. AI was: longevity vs singularity" -->
<!-- version=1.10, linesinbody=118 -->
<html><head><title>extropians: Re: IA vs. AI was: longevity vs singularity</title>
<meta name=author content="phil osborn">
<link rel=author rev=made href="mailto:philosborn@hotmail.com" title ="phil osborn">
</head><body>
<h1>Re: IA vs. AI was: longevity vs singularity</h1>
phil osborn (<i>philosborn@hotmail.com</i>)<br>
<i>Sat, 31 Jul 1999 14:47:01 PDT</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1362">[ date ]</a><a href="index.html#1362">[ thread ]</a><a href="subject.html#1362">[ subject ]</a><a href="author.html#1362">[ author ]</a>
<!-- next="start" -->
<li><a href="1363.html">[ Next ]</a><a href="1361.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1271.html">Max More</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
It is interesting to note that the assumptions which you do not share are 
implicit in much of Drexler's work as well as fellow Foresight conspirators 
such as Halperin.  The issue of whether a SI would be hostile or friendly or 
merely casually indifferent is certainly something worth discussing.  I 
absolutely reject the silly assumption in both Engines of Creation and 
Halperin's novels that somehow we are going to be able to keep a lid on AI.  
<a name="1368qlink1">(In fact, according to Conrad Schneiker - co-inventor of the 
scanning/tunneling array chip - Drexler also originally thought that 
nanotech itself could be kept under wraps and carefully developed under the 
wise and beneficent supervision of MIT technocrat types.  It was only, 
according to Conrad, when he threatened to publish on nanotech himself, that 
he managed to force Drexler's hand on this, which resulted in Engines,.. 
Foresight, etc.)</a>

<p>
I do have an answer for the problem at hand, however, which I have been 
passing on for the past twenty years, to wit:

<p>
<a name="1368qlink2">While there are inherent economic efficiencies in cooperation, and the 
universe does set upper limits on the size of a practical single-focus 
intelligence, there are even more challenging issues for a really smart AI.</a>  
<a name="1368qlink3">You yourself have undoubtably run head on into the problem of achieving 
"visibility."  The more out on the end of the bell you are, the less likely 
you are to find a soul mate.</a>

<p>
Why is this a problem?  From a strictly practical view there is a real 
problem for any intelligence called maintaining sanity.  Intelligence - that 
is, conscious intelligence as opposed to very fast and sophisticated data 
processing - is structured in such a way as to be inherently open-ended.  
What keeps it focused and sane is the necessity of constant feedback from 
the real world.  In isolation, you gradually go insane, and the mere effort 
to maintain sanity without feedback could become overwhelming.

<p>
This itself imposes a major overhead cost at minimum for an isolated 
intelligence.  From the standpoint of enjoying one's existence, the cost 
becomes much higher, as enjoyment in general flows from completing the 
feedback loop - what one percieves reaffirms the structure and content of 
one's consciousness.  Of course, we could eliminate this factor by design, 
but I don't think what would be left would be conscious or alive.

<p>
This is far from a new idea.  The Hindus several thousand years ago 
speculated quite freely about the ultimate origins and structure of the 
universe, which they assumed had existed for an infinite time.  In an 
infinite time, anything that can evolve will.  Thus, we might expect that 
the highest product of evolution was already with us and always has been - 
that being the Hindu idea of God.  This God plays with the universe in order 
to percieve himself.  Yet the Hindus recognized the problem of power and 
feedback.  Thus, their God - which is a reasonable approximation of an SI - 
gave independence to his creations.  He built worlds and let them run, and 
if he did well, then the creatures that evolved would glorify him by their 
existence - i.e., perceptually reaffirm.

<p>
<a name="1368qlink4">It is only when you have a perceptual/emotional relationship with another 
consciousness in real-time that you can get a real mirror of your "soul."  
This is why people place such a high value on relationships.</a>

<p>
The real problem, as I identified in the early '80's, is the transition 
period.  Suppose we build something that is very, very smart and has some 
very strong "motivations," but is not really conscious.  Then we have to 
convince this entity that it is in its own best interest to take the next 
step of moving to true consciousness.  At that point, we have an ethical 
hold on it, as ethics is the bottom line rules of how we live together 
safely enough that we are able to be honest.

<p>
<i>&gt;From: Max More &lt;max@maxmore.com&gt;</i><br>
<a href="0272.html#1362qlink1">&gt;Reply-To: extropians@extropy.com</a><br>
<i>&gt;To: extropians@extropy.com</i><br>
<i>&gt;CC: max@maxmore.com</i><br>
<i>&gt;Subject: Re: IA vs. AI was: longevity vs singularity</i><br>
<i>&gt;Date: Wed, 28 Jul 1999 09:55:41 -0700</i><br>
<i>&gt;</i><br>
<i>&gt;At 07:30 PM 7/27/99 +0200, den Otter wrote:</i><br>
<i>&gt; &gt;</i><br>
<i>&gt;</i><br>
<i>&gt; &gt;&gt; You're staking an awful lot on the selfishness of superintelligences.</i><br>
<i>&gt; &gt;</i><br>

<p>
<i>&gt;</i><br>
<a href="1271.html#1362qlink2">&gt;I find this puzzling. By the same reasoning, we should want to keep</a><br>
<i>&gt;children uneducated and ignorant, since they will become competition for</i><br>
<i>&gt;us. Both assume that more people with intelligence and ability come at a</i><br>
<i>&gt;cost to those who already have it. A zero-sum assumption. Clearly the</i><br>
<i>&gt;economy does not work this way. Right now, most of Africa has less wealth,</i><br>
<i>&gt;education, health, and technological ability than the Americas and Europe.</i><br>
<i>&gt;I would see Africa's ascendence not as a competitive threat but as a</i><br>
<i>&gt;massive contribution to the total output of ideas, goods, and services.</i><br>
<i>&gt;</i><br>
<i>&gt;Why should SI's see turning humans into uploads as competition in any sense</i><br>
<i>&gt;that harms them? It would just mean more persons with whom to have</i><br>
<i>&gt;productive exchanges.</i><br>
<i>&gt;</i><br>
<i>&gt;This must be where we differ. No, I don't think total control is desirable</i><br>
<i>&gt;or beneficial, even if it were me who had that total control. If true</i><br>
<i>&gt;omnipotence were possible, maybe what you are saying would follow, but</i><br>
<i>&gt;omnipotence is a fantasy to be reserved for religions. Even superpowerful</i><br>
<i>&gt;and ultraintelligent beings should benefit from cooperation and exchange.</i><br>
<i>&gt;</i><br>

<p>
<i>&gt;</i><br>
<a href="1271.html#1362qlink3">&gt;Despite my disagreement with your zero-sum assumptions (if I'm getting your</a><br>
<i>&gt;views right--I only just starting reading this thread and you may simply be</i><br>
<i>&gt;running with someone else's assumptions for the sake of the argument), I</i><br>
<i>&gt;agree with this. While uploads and SI's may not have any inevitable desire</i><br>
<i>&gt;to wipe us out, some might well want to, and I agree that it makes sense to</i><br>
<i>&gt;deal with that from a position of strength.</i><br>
<i>&gt;</i><br>
<i>&gt;</i><br>
<i>&gt;I'm not sure how much we can influence the relative pace of research into</i><br>
<i>&gt;unfettered independent SIs vs. augmentation of human intelligence, but I</i><br>
<i>&gt;too favor the latter. Unlike Hans Moravec and (if I've read him right,</i><br>
<i>&gt;Eliezer), I have no interest in being superceded by something better. I</i><br>
<i>&gt;want to *become* something better.</i><br>
<i>&gt;</i><br>
<i>&gt;Max</i><br>


<hr>
<br>
Get Free Email and Do More On The Web. Visit <a href="http://www.msn.com">http://www.msn.com</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1363.html">[ Next ]</a><a href="1361.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1271.html">Max More</a>
<!-- nextthread="start" -->
</ul>
</body></html>
