<!-- received="Thu Sep  2 11:46:21 1999 MDT" -->
<!-- sent="Thu, 02 Sep 1999 19:41:20 +0200" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: Principle of Nonsuppression" -->
<!-- id="37CEB6C0.240D@geocities.com" -->
<!-- inreplyto="Principle of Nonsuppression" -->
<!-- version=1.10, linesinbody=126 -->
<html><head><title>extropians: Re: Principle of Nonsuppression</title>
<meta name=author content="den Otter">
<link rel=author rev=made href="mailto:neosapient@geocities.com" title ="den Otter">
</head><body>
<h1>Re: Principle of Nonsuppression</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Thu, 02 Sep 1999 19:41:20 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3374">[ date ]</a><a href="index.html#3374">[ thread ]</a><a href="subject.html#3374">[ subject ]</a><a href="author.html#3374">[ author ]</a>
<!-- next="start" -->
<li><a href="3375.html">[ Next ]</a><a href="3373.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3292.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<hr>
<br>
<a href="3201.html#3374qlink1">&gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</a><br>

<p>
<a href="3292.html#3374qlink2">&gt; &gt; &gt; 1)  We're so busy trying to sabotage each other's efforts that we all</a><br>
<i>&gt; &gt; &gt; wind up getting eaten by goo / insane Powers.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Let's hope we can somehow prevent that...</i><br>
<i>&gt; </i><br>
<i>&gt; Ooh!  Yeah!  Great plan!</i><br>

<p>
See, we have a little problem: sabotaging eachother's efforts would
likely get everyone killed, but so would letting you do your AI thing.
Damned if you do, damned if you don't. Oh dear.
 
<p>
<a href="3292.html#3374qlink3">&gt; &gt; &gt; 2)  Iraq gets nanotechnology instead of the US / the AI project has to</a><br>
<i>&gt; &gt; &gt; be run in secret and is not subject to public supervision and error correction.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Note that "curbing" AI (or any other dangerous technology) by no means</i><br>
<i>&gt; &gt; has to involve government-imposed bans. There are other (better) ways to</i><br>
<i>&gt; &gt; do this.</i><br>
<i>&gt; </i><br>
<i>&gt; Such as...</i><br>

<p>
Dissuade individual scientists from going ahead with dangerous research.
Many of those involved in building the bomb regretted their actions
afterwards, afaik. With AI, there may be no "afterwards", so the people
working in this field should be made to understand what they're dealing
with. Same goes, though to a lesser extent, for nanotech. Of course,
*some* people (like the suicidal nihilists or hopelessly naive
technophiles) will be immune to even the harshest psychological warfare,
and might require more drastic measures (though actually killing them
may not be necessary. That would be rather barbaric and inhumane, after
all. No, a blast from the Zombie Gun (TM) will do just fine. Or...no,
that's a surprise).
 
<p>
<a href="3292.html#3374qlink4">&gt; &gt; No, scaring the</a><br>
<i>&gt; &gt; public and the government would more likely result in a tightening of</i><br>
<i>&gt; &gt; project security, which is quite good because it would buy us some time.</i><br>
<i>&gt; </i><br>
<i>&gt; Buy *who* some time?</i><br>

<p>
Everyone. Mankind.
 
<p>
<a href="3292.html#3374qlink5">&gt; &gt; &gt; "Trying to suppress a dangerous technology only inflicts more damage."</a><br>
<i>&gt; &gt; &gt; (Yudkowsky's Threats #2.)</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; How defeatist. I'd say that suppressing the proliferation of nukes, for</i><br>
<i>&gt; &gt; example, was a *great* idea. Otherwise we probably wouldn't be here</i><br>
<i>&gt; &gt; right now. Stupid as they may be, big governments do offer fairly good</i><br>
<i>&gt; &gt; stability, on average.</i><br>
<i>&gt; </i><br>
<i>&gt; Yes, nuclear weapons are an interesting case.  I should say that trying</i><br>
<i>&gt; to suppress the *creation* of a technology - research and development -</i><br>
<i>&gt; only inflicts more damage.  I'm fully in favor of suppressing the</i><br>
<i>&gt; *proliferation* of dangerous technology.  </i><br>

<p>
The best way to prevent proliferation (nanotech and AI are *much* harder
to contain than nukes) is to hold back the technology for as long as
possible. 

<p>
But, contrary to that you may think, I'm not all that interested in
"stopping" any technology, not even AI. I'd rather just keep up with
developments, and stimulate the "good" technologies. The problem is,
that doing (just) the latter may simply not be good enough. We'll see...

<p>
<i>&gt; Once Zyvex has nanotechnology,</i><br>
<a href="3292.html#3374qlink6">&gt; I'd be fully in favor of their immediately conquering the world to</a><br>
<i>&gt; prevent anyone else from getting it.  </i><br>

<p>
&lt;bitter sarcasm&gt; Excellent idea! &lt;/bitter sarcasm&gt;

<p>
&gt; That's what should've been done<br>
&gt; with nuclear weapons.  <br>

<p>
No, that would have been a bad idea, IMHO. Most likely outcome: nuclear
holocaust, as the US wasn't *that* much ahead of the USSR, and their
supply of bombs was depleted after the barbeque in Japan. The Soviets,
duly outraged, would have fought the US with the same vigor they
displayed against the Nazis, and inflict horrible damage. Their massive,
battle-hardened armies could easily hold back Allied (or maybe not so
allied) forces until they had their own nukes. Apart from this, the
American (and European) public probably wouldn't have tolerated an
attack on the Soviets, then still officially allies, directly after the
end of WW2.

<p>
Apart from that, I wouldn't trust the US with absolute power. They may
be *relatively* "good", but unchecked they'd probably develop into a
very nasty dictatorship. And of course, there would still be plenty of
conflicts (Vietnam on a global scale), as the occupied world would be
united in its hate against the aggressors. Sooner or later, they'd start
nuking local uprisings. There's your brave new world.

<p>
<a href="3292.html#3374qlink7">&gt; If the "good guys"  refuse to conquer the world</a><br>
<i>&gt; each time a powerful new weapon is developed, sooner or later a bad guy</i><br>
<i>&gt; is going to get to the crux point first.  </i><br>

<p>
"Power corrupts, and absolute power corrupts absolutely" and "the road
to hell is paved with good intentions". What kind of dictatorship do you
propose anyway? Destroying the world is relatively easy, but setting up
a stable empire is a rather different ballgame.

<p>
&gt; Alas, I don't think Zyvex's<br>
<a href="3292.html#3374qlink8">&gt; resources will suffice for the "matter programming" needed.</a><br>

<p>
No, unless Zyvex use the nanotech to build an AI, they wouldn't stand
much of a chance. And of course if that were the case, it would be the
AI, and not Zyvex, that would "conquer the world". 

<p>
<a href="3292.html#3374qlink9">&gt; &gt; So what? Laws can be broken, twisted, evaded. Like we were waiting for</a><br>
<i>&gt; &gt; the government's blessing in the first place.</i><br>
<i>&gt; </i><br>
<i>&gt; Okay, now I don't get it.  Are you under the impression you'll find it</i><br>
<i>&gt; easier to evade nanotechnology laws than I'll find it to evade AI laws?</i><br>

<p>
As I've pointed out before, I'm not counting on governments and their
laws to stop dangerous research. 
 
<p>
<a href="3292.html#3374qlink10">&gt; &gt; The writings are mine, obviously. Anyone who agrees with the principles</a><br>
<i>&gt; &gt; can call himself a "Transtopian". And yes, there are actually</i><br>
<i>&gt; &gt; like-minded people out there, strangely enough. Of course, as this is</i><br>
<i>&gt; &gt; the fringe of a fringe movement, you can't expect it to be very big.</i><br>
<i>&gt; </i><br>
<i>&gt; An... interesting... perspective</i><br>

<p>
I see. So, have you found any millionaires to fund your project yet?
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3375.html">[ Next ]</a><a href="3373.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3292.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
