<!-- received="Thu Aug  5 08:18:04 1999 MDT" -->
<!-- sent="Thu, 05 Aug 1999 16:13:33 +0200" -->
<!-- name="den Otter" -->
<!-- email="otter@globalxs.nl" -->
<!-- subject="Re: IA vs. AI was: longevity vs singularity" -->
<!-- id="37A99C0D.2AE7@globalxs.nl" -->
<!-- inreplyto="IA vs. AI was: longevity vs singularity" -->
<!-- version=1.10, linesinbody=635 -->
<html><head><title>extropians: Re: IA vs. AI was: longevity vs singularity</title>
<meta name=author content="den Otter">
<link rel=author rev=made href="mailto:otter@globalxs.nl" title ="den Otter">
</head><body>
<h1>Re: IA vs. AI was: longevity vs singularity</h1>
den Otter (<i>otter@globalxs.nl</i>)<br>
<i>Thu, 05 Aug 1999 16:13:33 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1639">[ date ]</a><a href="index.html#1639">[ thread ]</a><a href="subject.html#1639">[ subject ]</a><a href="author.html#1639">[ author ]</a>
<!-- next="start" -->
<li><a href="1640.html">[ Next ]</a><a href="1638.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1386.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<hr>
<br>
<a name="1677qlink1"><a name="1657qlink1"><a href="0817.html#1639qlink1">&gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</a><br>

<p>
<a href="1386.html#1639qlink2">&gt; I think we have a serious technological disagreement on the costs and</a><br>
<i>&gt; sophistication of uploading.  My uploading-in-2040 estimate is based on</i><br>
<i>&</a>gt; the document "Large Scale Analysis of Neural Structures" by Ralph Merkle</i><br>
<i>&gt; (<a href="http://www.merkle.com/merkleDir/brainAnalysis.html">http://www.merkle.com/merkleDir/brainAnalysis.html</a>) which says</i><br>
<i>&gt; "Manhattan Project, one person, 2040" - and, I believe, that's for</i><br>
<i>&gt; destructive uploading.  </i><br>

<p>
Then somewhere else you wrote:

<p>
<a href="1386.html#1639qlink3">&gt; I would</a><br>
<i>&gt; expect virtually everything Drexler ever wrote about to be developed</i><br>
<i>&gt; within a year of the first assembler, after which, if the planet is</i><br>
<i>&gt; still around, we'll start to get the really *interesting* technologies. </i><br>
<i>&gt; Nanotechnology is ten times as powerful and versatile as electricity. </i><br>
<i>&gt; What we forsee is only the tip of the iceberg, the immediate</i><br>
<i>&gt; possibilities.  Don't be fooled by their awesome raw power into</i><br>
<i>&gt; categorizing drextechs as "high" nanotechnology.  Drextechs are the</i><br>
<i>&gt; obvious stuff.  Like I said, I would expect almost all of it to follow</i><br>
<i>&gt; almost immediately.</i><br>

<p>
So you've said yourself that nanotech is to be expected around 2015, 
and that even today's most advanced Drextech designs would soon be 
obsolete. How does this jive with a 25(!!!) year gap between the first 
assemblers and uploading? I bet that if nanotech would be as potent as 
you assume, full uploading could be feasible in 2020, about the same
time as your AI. If time is no longer a factor, uploading becomes more 
than ever the superior option.

</a>
<p>
<a name="1657qlink2">Merkle's article is a conservative extrapolation of current technology,
*it does not include nanotech*. I didn't see that quote "Manhattan 
project, one person, 2040" either. He simply concludes that:  "If we 
use the [conventional]<a name="5076qlink1"> technology that will be available</a> in 10 to 
20 years, if we increase the budget to about one billion dollars, 
and if we use specially designed special purpose hardware -- 
then we can determine the structure of an organ that has long 
been of the greatest interest to all humanity, the human brain".</a> 

<p>
<a name="1657qlink3"><a href="1386.html#1639qlink4">&gt; You're talking about off-the-shelf uploading. </a><br>
<i>&gt; You're talking about nondestructive-uploading kiosks at the local</i><br>
<i>&gt; supermarket.  </i><br>

<p>
No, though these could very well be feasible with 2020 nanotech.</a>

<p>
<a name="1657qlink4">&gt; That's 2060 CRNS <br>

<p>
Yeah, right, 45 years after the first functional assembler. Wow,
progress will actually be *slowing down* in the next century.
Would this result in an anti-singularity or something?</a>

<p>
<a href="1386.html#1639qlink5">&gt; and you've committed yourself to making</a><br>
<i>&gt; sure that *nobody* upgrades themselves, or even runs themselves for a</i><br>
<i>&gt; few million years subjective time, until that technology is available.  </i><br>

<p>
Huh, what makes you think that? I welcome *any* kind of technology
that can upgrade a human, because with this there's at least the
theoretical chance that you can use it yourself. 
 
<p>
<a href="1386.html#1639qlink6">&gt; Let me know if you need any help.  I don't think that neurohack-IA is</a><br>
<i>&gt; going to change the odds in the slightest, though.  Actually, it'll</i><br>
<i>&gt; change the odds in my direction.  I'll have friends.</i><br>

<p>
Ah yes, you assume that all, or at least most, people of enhanced
intelligence would share your goals. You could very well be right 
with regard to natural neurohacks (geniuses do seem to have a
lot in common), but would upgrading a "normal" person have the
same effect? 

<p>
 &gt; &gt; Or the Luddites may finally "discover" AI  and do some serious
damage.
<br>
<i>&gt; </i><br>
<a name="1657qlink5"><a href="1386.html#1639qlink7">&gt; Then they'd damage uploading more.  It's a lot harder to run an</a><br>
<i>&gt; uploading project using PGP.</i><br>

<p>
Why would this be harder for IA than for AI?</a>

<p>
<a name="1657qlink6"><a href="1386.html#1639qlink8">&gt; Do I think nanotechnology is going to blow up the world?  Yes.  </a><br>

<p>
.......but probably not before we (most likely in a substantially
augmented form) can escape to space.</a>

<p>
<i>&gt; Do I</i><br>
<a name="1657qlink7"><a href="1386.html#1639qlink9">&gt; lift the smallest finger against it?  No.</a><br>

<p>
Unless nanotech is absolutely crucial to build your AI, this statement
doesn't make any sense. This race is way too important to worry
about good sportsmanship.</a> 
 
<p>
<a href="1386.html#1639qlink10">&gt; Okay.  You don't mind killing off copies of yourself?  No, wait, wrong</a><br>
<i>&gt; question.  Of course den Otter doesn't mind.  Your *copies* don't mind</i><br>
<i>&gt; your killing them off?</i><br>

<p>
As I've already mentioned, perhaps they could be tweaked in such 
a way that they're still useful as test subjects but lack "free will",
or
<br>
you merge with your copy (only the good ones, of course) afterwards, 
and no-one dies. Not that the copy could do that much if it disagreed,
btw; after all the original is much faster and has control over the test
setting.
 
<p>
<a href="1386.html#1639qlink11">&gt; &gt; &gt; *You* try to think like an SI.  Why do you keep on insisting on the</a><br>
<i>&gt; &gt; &gt; preservation of the wholly human emotion of selfishness?</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Because a minimal amount of "selfishness" is necessary to</i><br>
<i>&gt; &gt; survive. Survival is the basic prerequisite for everything else.</i><br>
<i>&gt; </i><br>
<i>&gt; First of all, you are wrong.  If I may formalize your statement, you're</i><br>
<i>&gt; saying:  "For all goals G: Survival is a subgoal of G."  Well, if my</i><br>
<i>&gt; goal is maximizing the sum of human pleasure, it may make perfect sense</i><br>
<i>&gt; to die in a raid on the Luddite Anti-Wireheading Headquarters.  What</i><br>
<i>&gt; you've done is establish "survival" as an interim subgoal of most goals.</i><br>
<i>&gt;  Then you perform a mental slight-of-hand and say it's an interim</i><br>
<i>&gt; subgoal of all goals.  Then you perform another slight-of-hand and say</i><br>
<i>&gt; this universality means survival is an end in itself.  Then you say it's</i><br>
<i>&gt; the only thing that can be an end in itself.  Then you say everything</i><br>
<i>&gt; else is irrational.  I think you're skipping a few steps here.</i><br>

<p>
Ok, different approach: first of all, what is the most basic reason why 
we pursue goals? Because it gives us "fulfilment" (pleasure) when 
we succeed. That's how we're wired (maybe some severe mental
cases aside). Do you agree? If so, then if you cut away all the fluff 
we're essentially pleasure-seeking agents. Now, when "pleasure" is
good, then the more of it one can get, the better. Obviously a subgoal
like "survive" and *its* subgoals like "evolve as far as you can",
"gather
<br>
knowledge" etc. are a more efficient approach to maximize the duration
and intensity of "pleasure" than the "altruistic" subgoal of "create a 
superior intelligence no matter what the costs (to yourself)". 

<p>
If your ASI kills you, you have cheated yourself out of a potential
infinity of pleasure for a brief moment of satisfaction. This is very
short-sighted, not what I'd call "intelligent hedonism" (and yes,
deep down you're a hedonist, whether you like it or not). Afaik,
your whole agrument rests on the denial of your true nature. The
fluff has gotten in your eyes, so to speak. 
 
<p>
<a href="1386.html#1639qlink12">&gt; For that matter, selfishness isn't a 100%-certain prerequisite for</a><br>
<i>&gt; survival.  Suppose I point a gun to your head and say "Suppress all</i><br>
<i>&gt; selfishness as indicated by this here fMRI display or I blow your brains out."</i><br>

<p>
If successful, that action (trick) would still be *motivated* by 
"selfishness". Without that motivation, one might not even try.
Anyway, this is just silly hair-splitting. The "selfish" survival
drive is simply a bloody useful tool and a solid fundament to
build a mental structure on. It's practical, as being alive gives
you a near(?)-infinite number possible goals to pursue, while 
being dead gives you exactly zero choice. 
 
<p>
Emotions are the (only) meaning of life. To have emotions you 
need to be alive, so be sure to include survival as a subgoal.
That's logic.

<p>
<a href="1386.html#1639qlink13">&gt; You're arguing in certainties - that is, you think your arguments are</a><br>
<i>&gt; certain.  Actually, I don't know what you *think*, but that's what</i><br>
<i>&gt; you've been saying.  Survival is a *necessary* prerequisite for</i><br>
<i>&gt; *everything* else.  Selfishness is the *only* rational goal.  When</i><br>
<i>&gt; anyone thinks they have a 100% certainty, it is 99% probable that their</i><br>
<i>&gt; arguments are wrong.  Not necessarily the conclusion, it doesn't say</i><br>
<i>&gt; anything about the conclusion, but it does say that their arguments</i><br>
<i>&gt; aren't based in messy ol' reality.</i><br>

<p>
A pleasure-seeking goal system which includes survival and all of
its little helpers (you know, science, technology etc.) is a *very
safe bet*. It may not be the Ultimate Truth, but it's a most practical
and user-friendly interim goal, and one which allows you to look
for your Truth, whatever it may be, indefinitely. "God's waiting room".

<p>
Can you offer me something more practical than this? If not, this
wil be the "ultimate" interim goal until further notice.
 
<p>
<a href="1386.html#1639qlink14">&gt; &gt; What you describe is basically common sense, not altruism (well,</a><br>
<i>&gt; &gt; not according to *my* dictionary, anyway). Perhaps you should</i><br>
<i>&gt; &gt; use some other term to avoid confusion.</i><br>
<i>&gt; </i><br>
<i>&gt; Okay.  I'm talking about a reasoning system that chooses between</i><br>
<i>&gt; options, plus the probabilistic assertion that one choice is superior to</i><br>
<i>&gt; others, will yield choices without any initial goals.  The assumption</i><br>
<i>&gt; that differentials exist is enough to produce them.</i><br>

<p>
Oh, that's much better (though a bit long, perhaps).
 
<p>
<a href="1386.html#1639qlink15">&gt; &gt; &gt; Acting so as to increase personal power</a><br>
<i>&gt; &gt; &gt; doesn't make rational sense except in terms of a greater goal.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Yes, and the "greater goal" is survival (well, strictly speaking it's</i><br>
<i>&gt; &gt; an auxiliary goal, but one that must always be included).</i><br>
<i>&gt; </i><br>
<i>&gt; YES!  *Think* about that!  Survival is only an auxiliary goal - what I</i><br>
<i>&gt; would call a "subgoal".  To make the system go, you need supergoals. </i><br>
<i>&gt; What are they?  Are they observer-dependent?  </i><br>

<p>
You bet your ass they are! The universe may exist "objectively", but
goals (mental states) are by definition observer-dependent. 

<p>
<i>&gt; I think Occam's Razor</i><br>
<a href="1386.html#1639qlink16">&gt; would tend to rule this out until specific evidence is produced to the</a><br>
<i>&gt; contrary.  Are the supergoals arbitrary, depending on the initial state</i><br>
<i>&gt; of the system?  If so, my supergoals are as rational as yours.</i><br>

<p>
Since your supergoals are in fact nothing but fluffy pleasure
actuators (or discomfort avoiders), it follows that my goals are 
(much) better. See above.
 
<p>
<a href="1386.html#1639qlink17">&gt; If I share the supergoals of the Powers, then wouldn't the same</a><br>
<i>&gt; inexorable reasoning take over and make the survival of Powers a subgoal</i><br>
<i>&gt; of *mine*?  </i><br>

<p>
Not automatically, if that's what you mean. You could choose to
adopt the survival of Powers as a subgoal, of course, but in the
end it's all about the satisfaction *you* (and only you) can feel
when the Powers do you proud.

<p>
<a href="1386.html#1639qlink18">&gt; In essence that's exactly what happened.  I don't insist on</a><br>
<i>&gt; personal continuity with the Powers because I don't have a supergoal</i><br>
<i>&gt; that requires it.  Asserting that I can't trust the Powers is foolish;</i><br>
<i>&gt; you don't know what *you'll* do tomorrow, either - trusting yourself</i><br>
<i>&gt; more than others is a lesson learned from the mortal environment.</i><br>

<p>
Powers might be an awful lot smarter than we are, but that doesn't
mean that they can't be flawed, far from it. They could kill everyone
on earth only to find out that they'd been flat wrong about it "five
minutes" later. Or they could kill themselves and never find out.
 
<p>
<a href="1386.html#1639qlink19">&gt; I have supergoals.  Survival is only an auxiliary goal, compared to</a><br>
<i>&gt; that.  </i><br>

<p>
But an oh-so-necessary one. Without it you can't chase your
chimeras for very long.

<p>
<a href="1386.html#1639qlink20">&gt; My projection of our relative mental architectures indicates that</a><br>
<i>&gt; a de-novo AI programmed to serve those supergoals will be more</i><br>
<i>&gt; efficient, more reliable, and above all *less expensive* than a</i><br>
<i>&gt; hacked-up upload/upgrade of myself.  It is therefore rational to serve</i><br>
<i>&gt; my supergoals through the survival of AI.</i><br>

<p>
You're being way to clinical about this. Sure, the AI could be a much
more efficient, elegantly structured being, but its existence only
matters in relation to *you*. If you cease to exist, the AI loses its
meaning. 

<p>
 &gt; You're arguing that I should let my subgoal of survival get in the
way
<br>
&gt; of AI.  <br>

<p>
I say you should pick a safer way to get your kicks.

<p>
<a href="1386.html#1639qlink21">&gt; Let's think about that.  Why are the AIs, hypothetically,</a><br>
<i>&gt; exterminating me?  Because I'm not needed to serve their - and my -</i><br>
<i>&gt; supergoals.  So in this case, I don't mind that the survival subgoal is</i><br>
<i>&gt; violated, because the value has been reduced to zero; the causal link</i><br>
<i>&gt; between survival and accomplishment has been broken.  No, I won't get to</i><br>
<i>&gt; see my accomplishments, but my goal is not "seeing X" but simply "X";</i><br>
<i>&gt; which, by Occam's Razor, is simpler.</i><br>

<p>
Well, see above I guess. Or better yet, check out this link. Now *this*
is enlightened stuff...
<br>
<a href="http://pierce.ee.washington.edu/~davisd/egoist/articles/Egoism.Robinson.html">http://pierce.ee.washington.edu/~davisd/egoist/articles/Egoism.Robinson.html</a>
 
<p>
<a href="1386.html#1639qlink22">&gt; You can see the evolution in my old posts on Extropians</a><br>
<i>&gt; and the old version of "Staring Into the Singularity" - I went from "The</i><br>
<i>&gt; Singularity is absolute good, no question about it, and They'll be nice</i><br>
<i>&gt; to us" to "I don't have the vaguest idea of what absolute good is, and</i><br>
<i>&gt; the whole thing is extremely tentative, but if there is one I suppose</i><br>
<i>&gt; the Singularity is the best way to serve it; and I don't care about</i><br>
<i>&gt; anything else, including my own survival or the survival of humanity,</i><br>
<i>&gt; because that's not in the absolute minimal set of assumptions I need to</i><br>
<i>&gt; generate choice differentials."</i><br>

<p>
So essentially you've evolved from a naive state to misguided state.
You say you serve Truth or the Absolute Good, but these are empty
religious terms; you might as well choose to serve "God" and it
would be just as pointless. Enligtenment is realizing that *you*
are the center of your universe, that there's nothing more important
than your existence. When you die, the universe might as well
cease to exist, nothing matters anymore from your *subjective*
point of view, which is the only one you have, and always will
have. So select "supergoals" that don't conflict with your survival, 
and be happy. It's the best thing one can do with an otherwise
meaningless existence. Forget objective goals, they're bullshit.

<p>
Btw, you have changed your views (radically) before, so the
same might happen in the future (you're still young so this
isn't exactly unrealistic). How can you be so sure that
you're right *now* when apparently you were "wrong" 
before? You think that you can't lose with your approach,
but you're dead wrong (pun intented); there's no guarantee
whatsoever that your ASI will do "the right thing", if it
exists at all. It's a blind gamble, nothing more. My 
approach (subgoal: stay alive, interim goals: seek 
pleasure, expand sphere of influence, seek knowledge,
evolve until you find a better goal system) may seem a 
bit mundane in comparison, but it *is* a safe bet.

<p>
<a href="1386.html#1639qlink23">&gt; &gt; Well, IMHO there's no ultimate</a><br>
<i>&gt; &gt; answer. It's an illusion, an ever receding horizon. Even the smartest</i><br>
<i>&gt; &gt; SI can only guess at what's right and true, but it will never ever</i><br>
<i>&gt; &gt; know for sure.</i><br>
<i>&gt; </i><br>
<i>&gt; How can you know this?  I do not see any way in which you would have</i><br>
<i>&gt; access to that piece of information.</i><br>

<p>
Really, I can't understand how an apparently intelligent guy like you
can belief in absolute certainty. 
 
<p>
<a href="1386.html#1639qlink24">&gt; Happiness is what we experience when we achieve goals.  </a><br>

<p>
Yep. This is the real drive behind your, mine and everyone else's
goals.

<p>
<a href="1386.html#1639qlink25">&gt; Are you going to</a><br>
<i>&gt; spend your life trying to be happy?  </i><br>

<p>
You, I and everyone else is already doing that, with varying
success, obviously...Nothing wrong with this system, IMHO,
certainly not when you can satisfy all your needs with future
tech, or adapt your goals for maximal happiness.

<p>
&gt; Well, then how do you know you're<br>
<a href="1386.html#1639qlink26">&gt; happy?  Because you think you're happy, right?  </a><br>

<p>
Yes. I think (I'm happy) therefore I am (happy). Good enough
for me!

<p>
&gt; So thinking you're happy<br>
&gt; is the indicator of happiness?  <br>

<p>
Yep.

<p>
<a href="1386.html#1639qlink27">&gt; Maybe you should actually try to spend</a><br>
<i>&gt; your life thinking you're happy, instead of being happy.</i><br>

<p>
It's all the same thing.
 
<p>
<a href="1386.html#1639qlink28">&gt; What this is is one of those meta/data confusions, like "the class of</a><br>
<i>&gt; all classes".  Once you place the indicator of success on the same</i><br>
<i>&gt; logical level as the goal, you've opened the gates of chaos.</i><br>

<p>
If you call the above chaos, then chaos (apparently) isn't so bad
after all.
 
<p>
<a href="1386.html#1639qlink29">&gt; &gt; It is, together with the auxiliary goals of survival and</a><br>
<i>&gt; &gt; the drive to increase one's knowledge and sphere of influence, the</i><br>
<i>&gt; &gt; most logical default option. You don't have to be a Power to figure</i><br>
<i>&gt; &gt; that one out.</i><br>
<i>&gt; </i><br>
<i>&gt; Like I said - and like you admit - "auxiliary".  Once we've dismissed</i><br>
<i>&gt; inflating an indicator as the supergoal, and admit that knowledge,</i><br>
<i>&gt; power, and survival are simply means to an end, we once again confront</i><br>
<i>&gt; the question of what that end is.  </i><br>

<p>
The end could be anything you want it to be, though the logically
consistent approach would be to seek pleasure, a safe bet, until
something better comes along.

<p>
<a href="1386.html#1639qlink30">&gt; I am not joking.  Selfishness as an end in itself is insane.</a><br>

<p>
Aha, "as an end in itself". Well, first of all it isn't any more sane
or insane than any other supergoal (supergoals are all arbitrary 
in the end), and as a subgoal or auxiliary goal it is one of the 
most practical, if not *the most practical*, around.
 
<p>
<a href="1386.html#1639qlink31">&gt; &gt; Reason dictates survival, simply because pleasure is better than death.</a><br>
<i>&gt; &gt; If "symetry" or whatever says I should kill myself, it can kiss my ass.</i><br>
<i>&gt; </i><br>
<i>&gt; See, there you go.  You've just put your evolved, hormonal, emotional</i><br>
<i>&gt; impulses on a level above logic and reason.  </i><br>

<p>
Logic and reason are just tools, auxiliary like survival. Emotions are
our (only) motivator, they give meaning to our meaningless existence.
We are both motivated by emotions, the only difference is that I'm
being frank about it. 

<p>
&gt; And you're willing to<br>
<a href="1386.html#1639qlink32">&gt; sacrifice whatever parts of yourself are needed to become an SI?  </a><br>

<p>
Becoming a SI isn't, or at least shouldn't be, about sacrifice, but
about gain. Becoming more than you are, but keeping all options
open.

<p>
<i>&gt; &gt; "I" want to feel</i><br>
<a href="1386.html#1639qlink33">&gt; &gt; the glory of ascension *personally*. That it wouldn't matter to an</a><br>
<i>&gt; &gt; outside observer is irrelevant from the individual's point of view.</i><br>
<i>&gt; </i><br>
<i>&gt; Well, that kind of emotional desire is exactly what you have to be</i><br>
<i>&gt; willing to give up, if you want to grow your mind.  Not "give up",</i><br>
<i>&gt; actually, but you do have to be willing to say that logic occupies a</i><br>
<i>&gt; level above it.  </i><br>

<p>
This is completely arbitrary. Your emotions cause you to
worship logic, a tool which was developed to aid survival. 
You want to switch the tool and the actuator? Fine, but 
there's no logic in that unless you have supergoals
which require a practical, logical approach (like survival)
But wait, those supergoals are the result of emotions
too, so you still end up serving emotions like everyone
else. 

<p>
When logic, and proportion, have fallen sloppy
dead...ha, ha. Go with the flow, man.

<p>
<a href="1386.html#1639qlink34">&gt; For your mind to grow strong, reason has to have</a><br>
<i>&gt; logical priority over everything else.  I'm not talking about the wimpy</i><br>
<i>&gt; heart-vs-head or intuition-vs-skill cultural myth.  Intuition and skills</i><br>
<i>&gt; are simply different ways to approximate a perfect rationality that, as</i><br>
<i>&gt; humans, we can never be isomorphic with.  But you do have to admit that</i><br>
<i>&gt; rationality takes priority over not-rational things.  Otherwise you'll</i><br>
<i>&gt; find it difficult to hone your skill because you'll have an excuse to</i><br>
<i>&gt; keep ugly messes around in your head.</i><br>

<p>
Like I said, rationality is a very practical tool, and I value it
greatly for
<br>
it, but ultimately it's just that, a tool, and not an end in itself.
Emotions
<br>
(of the positive kind) are the (only) end in itself.
 
<p>
<a href="1386.html#1639qlink35">&gt; Hey, do the math.  Figure that humanity has been around for, what,</a><br>
<i>&gt; 50,000 years with an average population of 100,000,000?  So that's a</i><br>
<i>&gt; total of five trillion experience-years.  Now there are six billion</i><br>
<i>&gt; people, so if they last an extra decade... that's a 1% increase.  Not</i><br>
<i>&gt; really significant except as a minor fluctuation.</i><br>
<i>&gt; </i><br>
<i>&gt; Oh, wait.  I forgot.  You're only doing the calculations for den Otter.</i><br>

<p>
If the calculations apply to me, they apply to everyone else who
is alive *presently* (what is the relevance of previous generations?
Oh wait, it's that silly objectivism again, isn't it?) A decade extra 
per person does indeed matter *to that person*.
 
<p>
<a name="1657qlink8"><a href="1386.html#1639qlink36">&gt; I really don't see that much of a difference between vaporizing the</a><br>
<i>&gt; Earth and just toasting it to charcoal.  Considered as weapons, AIs and</i><br>
<i>&gt; nanotech have equal destructive power; the difference is that an AI can</i><br>
<i>&gt; have a conscience.</i><br>

<p>
It has a *will*, an intelligence (but not necessarily a conscience in
the sense that it feels "guilt"). An ASI is an infinitely more
formidable weapon than nanotech because it can come up with new ways to
crush your defences and kill you at a truly astronomical speed.
Like the Borg, who adjust their shielding after you've shot a couple
of them, only *a lot* more efficient. Nanotech is just stupid goo
that will try to disassemble anything it comes into contact with it
(unless it's another goo nanite -- hey, you could base your defenses
on that). So...avoid contact. Unless the goo is controlled by a
SI (not that it would bother with such hideously primitive 
technology), it can be tricked, avoided and destroyed. Try that
</a>with a SI...
 
<p>
<a href="1386.html#1639qlink37">&gt; I mean, *I* care about a Perversion, or the possibility of a malevolent</a><br>
<i>&gt; Power wreaking true evil on an intergalactic scale.  </i><br>

<p>
I wonder, what's "evil" to someone who doesn't mind wiping out
humanity for the abstract concept of "truth" (or whatever)? Does
"evil" equal "irrational" in your view?

<p>
<i>&gt; I don't care about</i><br>
<a href="1386.html#1639qlink38">&gt; it much because if it can happen, it undoubtedly has already, so our</a><br>
<i>&gt; doing it wouldn't make *too* much of a difference, plus it's a pretty</i><br>
<i>&gt; small possibility to begin with.  I don't see why *you* would care at all.</i><br>

<p>
Well, maybe because I care about my existence? 
 
<p>
<a href="1386.html#1639qlink39">&gt; I mean, look at all the rationalizing you have to do just to justify the</a><br>
<i>&gt; idea that *you* somehow know whether or not your own survival is right</i><br>
<i>&gt; in ultimate terms.  </i><br>

<p>
But it's the *relative* terms that matter...

<p>
<a href="1386.html#1639qlink40">&gt; &gt; Tough luck, I'm The One.</a><br>
<i>&gt; </i><br>
<i>&gt; I'll be cheering your efforts on, as long as you don't interfere with</i><br>
<i>&gt; mine.  After all, my success counts as your failure but your success</i><br>
<i>&gt; counts as my success.  </i><br>

<p>
Does this mean that if I'd repeat what I've written so far after having
ascended, you'd belief me unconditionally?

<p>
&gt; See how much easier navigation is once you drop<br>
<a href="1386.html#1639qlink41">&gt; all the unnecessary preconditions?</a><br>

<p>
Yes, and if you stop caring about anything, you'll be enlightened,
right?
 
<p>
<a href="1386.html#1639qlink42">&gt; Then we have nanotech.  Once diamond drextech becomes common, one guy,</a><br>
<i>&gt; in a lab, can get a complete arsenal that goes from zero to sixty in</i><br>
<i>&gt; hours once the basic unit is developed.  There's no anti-first-strike</i><br>
<i>&gt; restraint.  There's no balance of power.  And there are too many</i><br>
<i>&gt; players.  I think I posted this whole argument earlier - were you around?</i><br>

<p>
Well then, looks like we should have that debate again and again
until we *do* find a solution, or are you giving up already? For
example, would goo nanites eat eachother too? If not, you
could make defenses by surrounding your base with nanites
that look like the enemy, but are inactive. Or you surround
yourself with something that kills nanites, like molten rock
or massive radiation(?). 

<p>
If nukes start flying, having a relatively simple bunker in the
middle of nowhere like (no offense, guys from Down Under) 
Australia for example could do the trick. You'll only have to 
hold out until you've made a spaceship, after all (now there's 
a nice nano@home project: how to swiftly grow a space craft 
with Drextech). 

<p>
<a href="1386.html#1639qlink43">&gt; &gt; Many would be killed, but humanity probably</a><br>
<i>&gt; &gt; woudn't be wiped out, far from it. A malevolent AI would</i><br>
<i>&gt; &gt; kill *everyone*. See my point?</i><br>
<i>&gt; </i><br>
<i>&gt; Yes.  You're wrong.</i><br>

<p>
No, *you' re* wrong. Or could we both be wrong? Within 30
years, we'll know for sure...
 
<p>
<a href="1386.html#1639qlink44">&gt; &gt; &gt; Second:  I know how to program altruism into</a><br>
<i>&gt; &gt; &gt; an AI; how would you program selfishness?</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Make survival the Prime Directive.</i><br>
<i>&gt; </i><br>
<i>&gt; You did read the section on Interim Goal Systems and the Prime Directive</i><br>
<i>&gt; from _Coding a Transhuman AI_, right?  You're aware that initial-value</i><br>
<i>&gt; goals are both unnecessary and unstable?</i><br>

<p>
Could be, but you asked me to program selfishness, not to make the 
most stable system. Not that I assume that the "selfish" AI would be 
somehow disfunctional, far from it. Like I said, it's a very solid
fundament 
<br>
to expand your consciousness on.
 
<p>
<a href="1386.html#1639qlink45">&gt; &gt; Ah well, I guess I'll try to find eternal</a><br>
<i>&gt; &gt; bliss then, until (if ever) I come up with something better. But</i><br>
<i>&gt; &gt; feel free to kill yourself if you disagree.</i><br>
<i>&gt; </i><br>
<i>&gt; I certainly will.  Please *don't* feel free to interfere with my efforts</i><br>
<i>&gt; to create an AI because *you* think that just because *you* can't figure</i><br>
<i>&gt; out the answer no possible entity can.</i><br>

<p>
I maintain, The Answer is a chimera. What kind of arrogant fool could
ever think that he holds All The Answers? The very moment he'd say
that, God or some badass alien Ultrapower could pop up and laugh 
in his face.

<p>
<a href="1386.html#1639qlink46">&gt; I really object to your intimation that "only God" can figure out the</a><br>
<i>&gt; ultimate meaning of life.  That sounds defeatist, </i><br>

<p>
What a joke, this coming from someone who assumes that 
mankind will kill itself the very moment it has nanotech, and
that our only hope is some deus ex machina.

<p>
&gt; passivist, <br>

<p>
Huh?

<p>
<i>&gt; and</i><br>
&gt; Luddite.  <br>

<p>
The term "Luddite" isn't what it used to be, I guess.

<p>
<a href="1386.html#1639qlink47">&gt; Me, I figure that any garden-variety superintelligence should</a><br>
<i>&gt; be able to handle the job. </i><br>

<p>
Oh right, it will say "X is the meaning of life. *Of course* I'm 100% 
sure, I'm a *SI* for cryin' out loud, so you better listen to me, dumb 
humans. Oh and btw, "X" demands that you die. Resistance is
futile, of course". Along comes a SSI. "You damn stupid moron SI,
the meaning of life is "Y". *Anyone* knows *that*, now look what
you did, killing all those humans for nothing. _Of course_ I'm 100% 
sure, I'm a SSI. But wait! There's a SSSI! "You both suck -- the 
meaning of life is "Z". Of course I'm 100% sure"...ad infinitum.

<p>
<a href="1386.html#1639qlink48">&gt; &gt; &gt; And I think an IA Transcend has a definite probability of being less</a><br>
<i>&gt; &gt; &gt; desirable than an AI Transcend.  Even from a human perspective.  In</i><br>

<p>
<a href="1386.html#1639qlink49">&gt; Oh, yeah, sure, evolution is WAY more trusty than a human designer. </a><br>

<p>
The human designer is a mere product of evolution.

<p>
<a href="1386.html#1639qlink50">&gt; Even, no, ESPECIALLY if you're trying to accomplish something that was</a><br>
<i>&gt; never even REMOTELY INCLUDED in the original design goals, like an</i><br>
<i>&gt; upgradable architecture.  And of course NO creation can be superior to</i><br>
<i>&gt; its Creator.  </i><br>

<p>
There's a BIG difference between something being "superior" and it
being a near-perfect Harbinger of Reason, Prophet of Truth. There's 
no reason to belief that SIs can't fuck up, and when they do they'll 
likely do it big (because they are big, think big and act on a cosmic 
scale). This is assuming that they are "pure", textbook Yudkowskian 
AIs; if there's a bug in the original program (and there *will* be
bugs), 
<br>
it  could be amplified or mutated in totally unpredictable and
potentially disastrous ways as the AI starts to play with itself.
If you or your team screw up, or someone tries to sneak in some 
Asimov laws or even a virus, you'll have a "bad seed".

<p>
<a name="1657qlink9">These possibilities are non-trivial, certainly when the military
start fooling around with AI, in which case they're likely to
be the first to have one up and running. Hell, those guys might
even try to stuff the thing into a cruise missile. So no, I don't
see why I should trust an AI more than myself.</a>

<p>
<a href="1386.html#1639qlink51">&gt; I'd call you a Luddite... but oh, wait, I already did.  </a><br>

<p>
Go right ahead...Yep, I'm a Luddite, sure thing! To hell with all
that disgusting technology (oh shit, I'm using it right now).

<p>
<a href="1386.html#1639qlink52">&gt; &gt; It may be messy, but it's all I have so it will have to do.</a><br>
<i>&gt; </i><br>
<i>&gt; It is NOT all you have.  The only reason it's all you have is because</i><br>
<i>&gt; you THINK it's all you have.  Get hip to the transhumanist meme, man! </i><br>
<i>&gt; If you want a better mind, program one!  Unless, of course, you're so</i><br>
<i>&gt; set on being The One that you refuse to do so.</i><br>

<p>
I thought that the transhumanist meme was more about *upgrading*
what we have. It was when I last checked, anyway. 

<p>
<a name="1659qlink1"><a href="1386.html#1639qlink53">&gt; Yes, I do have my "own" agenda, which involves using IA as a means to</a><br>
<i>&gt; AI.  And if you'd like to start your own IA project, I'll be more than</i><br>
<i>&gt; happy to contribute advice, brain scans, genetic material, or anything</i><br>
<i>&gt; else you need.</a></i><br>

<p>
<a name="1659qlink2">Are you serious about this? Well, ideally we'd have a project that
aims to develop practical nanotech designs for things like escape
craft, space habitats, food replicators, neurological enhancements,
weapons and, last but not least, mind uploading. If you test it
now in VR, that could save a lot of valuable time when the shit
hits the fan.</a> Apart from this, we need to focus on non-nanotech
<a name="1659qlink3">(contemporary) means to achieve IA, and ways to get some 
serious funding. If you have any suggestions, I'd love to hear
them.</a>

<p>
<a name="1659qlink4">As a matter of fact, you've mentioned on several occasions that 
you have "thousands" of potentially lucrative ideas. I'm certainly 
interested in a relatively easy, low budget way to make lots of 
money. So, how about this: if you can give me something that 
works, I'll give you 50% of all profits.
</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1640.html">[ Next ]</a><a href="1638.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1386.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
