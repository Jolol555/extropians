<!-- received="Thu Jul 22 09:44:10 1999 MDT" -->
<!-- sent="Thu, 22 Jul 1999 16:07:05 +0200" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: longevity vs singularity" -->
<!-- id="199907221544.IAA22088@geocities.com" -->
<!-- inreplyto="longevity vs singularity" -->
<!-- version=1.10, linesinbody=43 -->
<html><head><title>extropians: Re: longevity vs singularity</title>
<meta name=author content="den Otter">
<link rel=author rev=made href="mailto:neosapient@geocities.com" title ="den Otter">
</head><body>
<h1>Re: longevity vs singularity</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Thu, 22 Jul 1999 16:07:05 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#978">[ date ]</a><a href="index.html#978">[ thread ]</a><a href="subject.html#978">[ subject ]</a><a href="author.html#978">[ author ]</a>
<!-- next="start" -->
<li><a href="0979.html">[ Next ]</a><a href="0977.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0953.html">Spike Jones</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->



<hr>
<br>
&gt; From: Spike Jones &lt;spike66@ibm.net&gt;<br>

<p>
<a href="0953.html#0978qlink1">&gt; Eliezer, why whats the big hurry with this singularity thing?</a><br>
<i>&gt; Arent you a little curious to see how far we humans could</i><br>
<i>&gt; go without *that* coming along and doing...who knows what?</i><br>
<i>&gt; Maybe the new software really wont be interested in keeping</i><br>
<i>&gt; us carbon units around.  I *like* us.  spike</i><br>

<p>
Well, Eliezer is right about the Singularity being virtually 
inevitable (the only way to stop it is by destroying or at
least massively retarding civilization, which is of course
unacceptable because we need technological progress 
to survive), so we might as well accept it. The real issue 
is *how* we should cause a Singularity.<a name="1042qlink1"> Eliezer seems to
favor the AI approach (create a seed superintelligence and
hope that it will be benevolent towards humans instead of
using our atoms for something more useful), which is
IMHO reckless to the point of being suicidal. 

<p>
<a name="0987qlink1">A much better, though still far from ideal, way would be 
to focus on human uploading, and when the technology 
is operational upload everyone involved in the project 
</a>simultaneously. That way at least some people would
have a fighting chance to become posthuman. In
fact, I'm very surprised that so many otherwise fiercely
individualistic/libertarian people are so eager to throw
themselves at the mercy of some machine. It doesn't
compute.</a>

<p>
IMHO, the "mission" of transhumanism should be to 
develop uploading (and general intelligence amplification)
technology asap, while at the same time trying to curb the
development of "conscious" AI and other technologies
which may threaten our existence. We don't have to hold
them back forever; we just need to buy us some time to 
level the playing field.

<p>
Let's not forget that (technological) progress is just a tool;
the goal is our continued survival and well-being.
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0979.html">[ Next ]</a><a href="0977.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0953.html">Spike Jones</a>
<!-- nextthread="start" -->
</ul>
</body></html>
