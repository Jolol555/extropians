<!-- received="Sun Aug  1 22:27:47 1999 MDT" -->
<!-- sent="Sun, 01 Aug 1999 23:27:44 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: IA vs. AI was: longevity vs singularity" -->
<!-- id="37A51E3A.6AE7529B@pobox.com" -->
<!-- inreplyto="IA vs. AI was: longevity vs singularity" -->
<!-- version=1.10, linesinbody=692 -->
<html><head><title>extropians: Re: IA vs. AI was: longevity vs singularity</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: IA vs. AI was: longevity vs singularity</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sun, 01 Aug 1999 23:27:44 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1386">[ date ]</a><a href="index.html#1386">[ thread ]</a><a href="subject.html#1386">[ subject ]</a><a href="author.html#1386">[ author ]</a>
<!-- next="start" -->
<li><a href="1387.html">[ Next ]</a><a href="1385.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1375.html">den Otter</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1434.html">Xiaoguang Li</a>
</ul>
<!-- body="start" -->

<p>
den Otter wrote:
<br>
<i>&gt; </i><br>
<a href="1375.html#1386qlink1">&gt; ----------</a><br>
<i>&gt; &gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; &gt; The first uploads would no doubt be animals (of increasing complexity),</i><br>
<i>&gt; &gt; &gt; followed by tests with humans (preferably people who don't grasp</i><br>
<i>&gt; &gt; &gt; the full potential of being uploaded, for obvious reasons).</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; You have to be kidding.  Short of grabbing random derelicts off the</i><br>
<i>&gt; &gt; street, there's simply no way you could do that.  Or were you planning</i><br>
<i>&gt; &gt; to grab random derelicts off the street?</i><br>
<i>&gt; </i><br>
<i>&gt; The animal tests alone would make the procedure considerably</i><br>
<i>&gt; safer (if the procedure works for an ape, it will most likely work</i><br>
<i>&gt; for a human too), and it's really no problem to find relatively</i><br>
<i>&gt; "clueless" yet eager volunteers; I could very well imagine that</i><br>
<i>&gt; serious gamers would be lining up to get uploaded into the</i><br>
<i>&gt; "ultimate game", ExistenZ style, for example. If you make the</i><br>
<i>&gt; procedure reversible, i.e. leave the brain intact and switch the</i><br>
<i>&gt; consciousness between the brain and the machine, there</i><br>
<i>&gt; doesn't have to be much risk involved.</i><br>

<p>
<a name="1639qlink2">I think we have a serious technological disagreement on the costs and
sophistication of uploading.  My uploading-in-2040 estimate is based on
the document "Large Scale Analysis of Neural Structures" by Ralph Merkle
(<a href="http://www.merkle.com/merkleDir/brainAnalysis.html">http://www.merkle.com/merkleDir/brainAnalysis.html</a>) which says
"Manhattan Project, one person, 2040" - and, I believe, that's for
destructive uploading.<a name="1639qlink4">  You're talking about off-the-shelf uploading. 
</a>
You're talking about nondestructive-uploading kiosks at the local
</a>
supermarket.  That's 2060 CRNS<a name="1639qlink5"> and you've committed yourself to making
sure that *nobody* upgrades themselves, or even runs themselves for a
few million years subjective time, until that technology is available.</a>  

<p>
<a href="1375.html#1386qlink2">&gt; &gt; And did I mention that by the</a><br>
<i>&gt; &gt; time you can do something like that, in secret, on one supercomputer,</i><br>
<i>&gt; &gt; much less run 6000 people on one computer and then upgrade them in</i><br>
<i>&gt; &gt; synchronization, what *I* will be doing with distributed.net will -</i><br>
<i>&gt; </i><br>
<i>&gt; Yes, I know the odds are in your favor right now, but that could</i><br>
<i>&gt; easily change if, for example, there were a major breaktrough</i><br>
<i>&gt; in neurohacking within the next 10 years.</i><br>

<p>
<a name="1639qlink6">Let me know if you need any help.  I don't think that neurohack-IA is
going to change the odds in the slightest, though.  Actually, it'll
change the odds in my direction.  I'll have friends.</a>

<p>
<a href="1375.html#1386qlink3">&gt; Or programming a sentient AI may not be that easy after all.</a><br>

<p>
I'm not saying it's easy.  I'm saying it's easier than uploading.  Am I
certain?  Well, let's apply the David Gerrold criterion:  Can you rip my
arm off if I'm wrong?  Yes, absolutely.

<p>
&gt; Or the Luddites may finally "discover" AI  and do some serious damage.<br>

<p>
<a name="1639qlink7">Then they'd damage uploading more.  It's a lot harder to run an
uploading project using PGP.</a>

<p>
<a href="1375.html#1386qlink4">&gt; Or...Oh,</a><br>
<i>&gt; what the hell, the upload path is simply something that *has*</i><br>
<i>&gt; to be tried.</i><br>

<p>
Go right ahead, as long as you don't try to slow down other things so it
can be tried first.  That's the part I object to - you insist that your
technology be first and you're willing to deliberately slow others down
instead of doing like the rest of us and speeding your own efforts up. 
<a name="1639qlink8">Do I think nanotechnology is going to blow up the world?  Yes.</a>  Do I
<a name="1639qlink9">lift the smallest finger against it?  No.</a>

<p>
<a href="1375.html#1386qlink5">&gt; If it works, great! If it doesn't...well, nothing lost, eh?</a><br>

<p>
Nothing but time.  Oh, wait.  We don't *have* time.

<p>
<a href="1375.html#1386qlink6">&gt; &gt; Besides, there's more to uploading than scanning.  Like, the process of</a><br>
<i>&gt; &gt; upgrading to Powerdom.  How are you going to conduct those tests, hah?</i><br>
<i>&gt; </i><br>
<i>&gt; After having uploaded, you could, for example, run copies of</i><br>
<i>&gt; yourself at very slow speed (in relation to your "true" self),</i><br>
<i>&gt; and fool around with different settings for a while, before either</i><br>
<i>&gt; terminating, or merging with, the test copy. If the tests are</i><br>
<i>&gt; successful, you can upgrade and use copies of your new state,</i><br>
<i>&gt; again running at relatively slow speed, to conduct the next</i><br>
<i>&gt; series of tests. Ad infinitum.</i><br>

<p>
<a name="1639qlink10">Okay.  You don't mind killing off copies of yourself?  No, wait, wrong
question.  Of course den Otter doesn't mind.  Your *copies* don't mind
your killing them off?</a>

<p>
<a href="1375.html#1386qlink7">&gt; &gt; &gt; Forget PC and try to think like a SI (a superbly powerful</a><br>
<i>&gt; &gt; &gt; and exquisitely rational machine).</i><br>
<i>&gt; &gt;</i><br>
<a name="1639qlink11"><i>&gt; &gt; *You* try to think like an SI.  Why do you keep on insisting on the</i><br>
<i>&gt; &gt; preservation of the wholly human emotion of selfishness?</i><br>
<i>&gt; </i><br>
<i>&gt; Because a minimal amount of "selfishness" is necessary to</i><br>
<i>&gt; survive. Survival is the basic prerequisite for everything else.</i><br>

<p>
First of all, you are wrong.  If I may formalize your statement, you're
saying:  "For all goals G: Survival is a subgoal of G."  Well, if my
goal is maximizing the sum of human pleasure, it may make perfect sense
to die in a raid on the Luddite Anti-Wireheading Headquarters.  What
you've done is establish "survival" as an interim subgoal of most goals.
 Then you perform a mental slight-of-hand and say it's an interim
subgoal of all goals.  Then you perform another slight-of-hand and say
this universality means survival is an end in itself.  Then you say it's
the only thing that can be an end in itself.  Then you say everything
else is irrational.  I think you're skipping a few steps here.</a>

<p>
<a name="1639qlink12">For that matter, selfishness isn't a 100%-certain prerequisite for
survival.  Suppose I point a gun to your head and say "Suppress all
selfishness as indicated by this here fMRI display or I blow your brains out."</a>

<p>
<a name="1639qlink13">You're arguing in certainties - that is, you think your arguments are
certain.  Actually, I don't know what you *think*, but that's what
you've been saying.  Survival is a *necessary* prerequisite for
*everything* else.  Selfishness is the *only* rational goal.  When
anyone thinks they have a 100% certainty, it is 99% probable that their
arguments are wrong.  Not necessarily the conclusion, it doesn't say
anything about the conclusion, but it does say that their arguments
aren't based in messy ol' reality.</a>

<p>
<a href="1375.html#1386qlink8">&gt; &gt; I don't</a><br>
<i>&gt; &gt; understand how you can be so rational about everything except that!</i><br>
<i>&gt; &gt; Yes, we'll lose bonding, honor, love, all the romantic stuff.  But not</i><br>
<i>&gt; &gt; altruism.  Altruism is the default state of intelligence.  Selfishness</i><br>
<i>&gt; &gt; takes *work*, it's a far more complex emotion.</i><br>
<i>&gt; </i><br>
<i>&gt; Well, so what? *Existence* takes work, but that doesn't make it</i><br>
<i>&gt; irrational.</i><br>

<p>
So Occam's Razor.

<p>
<a href="1375.html#1386qlink9">&gt; &gt; I'm not talking about</a><br>
<i>&gt; &gt; feel-good altruism or working for the benefit of other subjectivities.</i><br>
<i>&gt; &gt; I'm talking about the idea of doing what's right, making the correct</i><br>
<i>&gt; &gt; choice, acting on truth.</i><br>
<i>&gt; </i><br>
<a name="1639qlink14"><i>&gt; What you describe is basically common sense, not altruism (well,</i><br>
<i>&gt; not according to *my* dictionary, anyway). Perhaps you should</i><br>
<i>&gt; use some other term to avoid confusion.</i><br>

<p>
Okay.  I'm talking about a reasoning system that chooses between
options, plus the probabilistic assertion that one choice is superior to
others, will yield choices without any initial goals.  The assumption
that differentials exist is enough to produce them.</a>

<p>
<a name="1639qlink15"><a href="1375.html#1386qlink10">&gt; &gt; Acting so as to increase personal power</a><br>
<i>&gt; &gt; doesn't make rational sense except in terms of a greater goal.</i><br>
<i>&gt; </i><br>
<i>&gt; Yes, and the "greater goal" is survival (well, strictly speaking it's</i><br>
<i>&gt; an auxiliary goal, but one that must always be included).</i><br>

<p>
YES!  *Think* about that!  Survival is only an auxiliary goal - what I
would call a "subgoal".  To make the system go, you need supergoals. 
What are they?  Are they observer-dependent?</a>  I think Occam's Razor
<a name="1639qlink16">would tend to rule this out until specific evidence is produced to the
contrary.  Are the supergoals arbitrary, depending on the initial state
of the system?  If so, my supergoals are as rational as yours.</a>

<p>
<a name="1639qlink17">If I share the supergoals of the Powers, then wouldn't the same
inexorable reasoning take over and make the survival of Powers a subgoal
<a name="1639qlink18">of *mine*?</a>  In essence that's exactly what happened.  I don't insist on
personal continuity with the Powers because I don't have a supergoal
that requires it.  Asserting that I can't trust the Powers is foolish;
you don't know what *you'll* do tomorrow, either - trusting yourself
more than others is a lesson learned from the mortal environment.</a>

<p>
<a name="1639qlink19">I have supergoals.  Survival is only an auxiliary goal, compared</a> to
<a name="1639qlink20">that.  My projection of our relative mental architectures indicates that
a de-novo AI programmed to serve those supergoals will be more
efficient, more reliable, and above all *less expensive* than a
hacked-up upload/upgrade of myself.  It is therefore rational to serve
my supergoals through the survival of AI.</a>

<p>
You're arguing that I should let my subgoal of survival get in the way
of AI.<a name="1639qlink21">  Let's think about that.  Why are the AIs, hypothetically,
exterminating me?  Because I'm not needed to serve their - and my -
supergoals.  So in this case, I don't mind that the survival subgoal is
violated, because the value has been reduced to zero; the causal link
between survival and accomplishment has been broken.  No, I won't get to
see my accomplishments, but my goal is not "seeing X" but simply "X";
which, by Occam's Razor, is simpler.</a>

<p>
<a href="1375.html#1386qlink11">&gt; &gt; I *know* that we'll be bugs to SIs.  I *know* they won't have any of the</a><br>
<i>&gt; &gt; emotions that are the source of cooperation in humans.  I *still* see so</i><br>
<i>&gt; &gt; many conflicting bits of reasoning and evidence that you might as well</i><br>
<i>&gt; &gt; flip a coin as ask me whether they'll be benevolent.  That's my</i><br>
<i>&gt; &gt; probability:  50%.</i><br>
<i>&gt; </i><br>
<i>&gt; Yes, ultimately it's all just unknowable. BUT, if we assume that the</i><br>
<i>&gt; SIs will act on reason, the benevolence-estimate should be a lot lower</i><br>
<i>&gt; than 50% (IMO).</i><br>

<p>
Reasonable, unreasonable - I don't know.  I give up.  Yes, it *is* 50%. 
Some days it's 30%, some days it's 10%, some days it's 70%; you might as
well split the difference and call it a coin-flip.  But even on the days
when it's 10%, it's still humanity's best chance for survival.  (It's
not my job to care whether or not humanity survives, of course, but it
is my job to know if I'm rationally opposed to the faction that wants
humanity to survive.  At present, the answer is no.)

<p>
<a href="1375.html#1386qlink12">&gt; &gt; If Powers are hungry, why haven't they expanded continually, at</a><br>
<i>&gt; &gt; lightspeed, starting with the very first intelligent race in this</i><br>
<i>&gt; &gt; Universe?  Why haven't they eaten Earth already?  Or if Powers upload</i><br>
<i>&gt; &gt; mortals because of a mortally understandable chain of logic, so as to</i><br>
<i>&gt; &gt; encourage Singularities, why don't they encourage Singularities by</i><br>
<i>&gt; &gt; sending helper robots?  We aren't the first ones.  There are visible</i><br>
<i>&gt; &gt; galaxies so much older that they've almost burned themselves out.  I</i><br>
<i>&gt; &gt; don't see *any* reasonable interpretation of SI motives that is</i><br>
<i>&gt; &gt; consistent with observed evidence.  Even assuming all Powers commit</i><br>
<i>&gt; &gt; suicide just gives you the same damn question with respect to mortal aliens.</i><br>
<i>&gt; </i><br>
<i>&gt; I dunno, maybe we're the first after all. Maybe intelligent life really</i><br>
<i>&gt; is extremely rare. I think Occam's Razor would agree.</i><br>

<p>
I've considered that, but I'm 95% certain that we're not first.  Of
course, that's because I have to invoke either the Anthropic Principle
or simulation-runners to explain the existence of qualia, a necessity
which you would probably regard as not.

<p>
<a href="1375.html#1386qlink13">&gt; &gt; And, speaking of IA motivations, look at *me*.  Are my motivations</a><br>
<i>&gt; &gt; human?</i><br>
<i>&gt; </i><br>
<i>&gt; Yes, your motivations are very human indeed. Like most people,</i><br>
<i>&gt; you seem to have an urge to find a higher truth or purpose in</i><br>
<i>&gt; life, some kind of objective answer (among other things, like</i><br>
<i>&gt; personal glory and procreation).</i><br>

<p>
That's how I got here.  But once I did have a fully logical
justification system, it sort of took over and blew away the
scaffolding.<a name="1639qlink22">  You can see the evolution in my old posts on Extropians
and the old version of "Staring Into the Singularity" - I went from "The
Singularity is absolute good, no question about it, and They'll be nice
to us" to "I don't have the vaguest idea of what absolute good is, and
the whole thing is extremely tentative, but if there is one I suppose
the Singularity is the best way to serve it; and I don't care about
anything else, including my own survival or the survival of humanity,
because that's not in the absolute minimal set of assumptions I need to
generate choice differentials."</a>

<p>
<a name="1639qlink23"><i>&gt; Well, IMHO there's no ultimate</i><br>
<a href="1375.html#1386qlink14">&gt; answer. It's an illusion, an ever receding horizon. Even the smartest</a><br>
<i>&gt; SI can only guess at what's right and true, but it will never ever</i><br>
<i>&gt; know for sure.</i><br>

<p>
How can you know this?  I do not see any way in which you would have
access to that piece of information.</a>

<p>
<a href="1375.html#1386qlink15">&gt; It will never know whether there's a God (etc.) either,</a><br>
<i>&gt; unless He reveals himself. And even then, is he *really* THE God?</i><br>
<i>&gt; Uncertainty is eternal. The interim meaning of life is all there is, and</i><br>
<i>&gt; all there ever will be.</i><br>

<p>
<a href="1375.html#1386qlink16">&gt; Out of all arbitrary options, the search for the</a><br>
<i>&gt; highest possible pleasure is the one that makes, by definition, the</i><br>
<i>&gt; most sense.</i><br>

<p>
If it's "by definition", then I assume you're defining it as the
achievement of supergoals.  And then you're performing a logical
sleight-of-hand and saying that the goal-achievement-indicator is itself
the goal.  That's the whole wireheading paradox.  I mean, if pleasure is
the goal, then maybe you need a pleasure-indicator so you know how happy
you are; and then maybe the AI will devote itself to increasing the
indicator, so it thinks it's infinitely happy when actually it's not.  I
mean, let's think this through, people.

<p>
<a name="1639qlink24">Happiness is what we experience when we achieve goals.</a>  Are you going to
<a name="1639qlink25">spend your life trying to be happy?</a>  Well, then how do you know you're
<a name="1639qlink26">happy?  Because you think you're happy, right?</a>  So thinking you're happy
is the indicator of happiness?<a name="1639qlink27">  Maybe you should actually try to spend
your life thinking you're happy, instead of being happy.</a>

<p>
<a name="1639qlink28">What this is is one of those meta/data confusions, like "the class of
all classes".  Once you place the indicator of success on the same
logical level as the goal, you've opened the gates of chaos.</a>

<p>
<a name="1639qlink29"><a href="1375.html#1386qlink17">&gt; It is, together with the auxiliary goals of survival and</a><br>
<i>&gt; the drive to increase one's knowledge and sphere of influence, the</i><br>
<i>&gt; most logical default option. You don't have to be a Power to figure</i><br>
<i>&gt; that one out.</i><br>

<p>
Like I said - and like you admit - "auxiliary".  Once we've dismissed
inflating an indicator as the supergoal, and admit that knowledge,
power, and survival are simply means to an end, we once again confront
the question of what that end</a> is.  And remember, knowledge, power, and
survival are not subgoals that apply only to myself.  The subgoal of
increase(power(me)) assumes more than the logical minimum required.  All
that's needed is increase(power(entity e: e.goal == me.goal)).

<p>
<a href="1375.html#1386qlink18">&gt; &gt; &gt; Or are you hoping for an insane Superpower? Not something you'd</a><br>
<i>&gt; &gt; &gt; want to be around, I reckon.</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; No, *you're* the one who wants an insane SI.  Selfishness is insane.</i><br>
<i>&gt; </i><br>
<i>&gt; You must be joking...</i><br>

<p>
<a name="1639qlink30">I am not joking.  Selfishness as an end in itself is insane.</a>

<p>
<a href="1375.html#1386qlink19">&gt; &gt; Anything is insane unless there's a rational reason for it, and there is</a><br>
<i>&gt; &gt; no rational reason I have ever heard of for an asymmetrical world-model.</i><br>
<i>&gt; &gt;  Using asymmetric reflective reasoning, what you would call</i><br>
<i>&gt; &gt; "subjectivity", violates Occam's Razor, the Principle of Mediocrity,</i><br>
<i>&gt; &gt; non-anthropocentrism, and... I don't think I *need* any "and" after that.</i><br>
<i>&gt; </i><br>
<a name="1639qlink31"><i>&gt; Reason dictates survival, simply because pleasure is better than death.</i><br>
<i>&gt; If "symetry" or whatever says I should kill myself, it can kiss my ass.</i><br>

<p>
See, there you go.  You've just put your evolved, hormonal, emotional
impulses on a level above logic and reason.</a>  And you're willing to
<a name="1639qlink32">sacrifice whatever parts of yourself are needed to become an SI?</a>  Oh,
sure you are.

<p>
<a href="1375.html#1386qlink20">&gt; &gt; I find it hard to believe you can be that reasonable about sacrificing</a><br>
<i>&gt; &gt; all the parts of yourself, and so unreasonable about insisting that the</i><br>
<i>&gt; &gt; end result start out as you.  If two computer programs converge to</i><br>
<i>&gt; &gt; exactly the same state, does it really make a difference to you whether</i><br>
<i>&gt; &gt; the one labeled "den Otter" or "Bill Gates" is chosen for the seed?</i><br>
<i>&gt; </i><br>
<i>&gt; Yes, it matters because the true "I", the raw consciousness, demands</i><br>
<i>&gt; continuity. There is no connection between Mr. Gates and me, so</i><br>
<i>&gt; it's of little use to me if he lives on after my death. "I" want to feel</i><br>
<a name="1639qlink33"><i>&gt; the glory of ascension *personally*. That it wouldn't matter to an</i><br>
<i>&gt; outside observer is irrelevant from the individual's point of view.</i><br>

<p>
Well, that kind of emotional desire is exactly what you have to be
willing to give up, if you want to grow your mind.  Not "give up",
actually, but you do have to be willing to say that logic occupies a
<a name="1639qlink34">level above</a> it.  For your mind to grow strong, reason has to have
logical priority over everything else.  I'm not talking about the wimpy
heart-vs-head or intuition-vs-skill cultural myth.  Intuition and skills
are simply different ways to approximate a perfect rationality that, as
humans, we can never be isomorphic with.  But you do have to admit that
rationality takes priority over not-rational things.  Otherwise you'll
find it difficult to hone your skill because you'll have an excuse to
keep ugly messes around in your head.</a>

<p>
<a href="1375.html#1386qlink21">&gt; Oh, if you assume that death is the most likely outcome, a decade</a><br>
<i>&gt; extra does indeed matter. It's better than nothing.</i><br>

<p>
<a name="1639qlink35">Hey, do the math.  Figure that humanity has been around for, what,
50,000 years with an average population of 100,000,000?  So that's a
total of five trillion experience-years.  Now there are six billion
people, so if they last an extra decade... that's a 1% increase.  Not
really significant except as a minor fluctuation.

<p>
Oh, wait.  I forgot.  You're only doing the calculations for den Otter.</a>

<p>
<a href="1375.html#1386qlink22">&gt; &gt; What</a><br>
<i>&gt; &gt; matters is the relative probabilities of the outcomes, and trying to</i><br>
<i>&gt; &gt; slow things down may increase the probability of *your* outcome relative</i><br>
<i>&gt; &gt; to *my* outcome, but it also increases the probability of planetary</i><br>
<i>&gt; &gt; destruction relative to *either* outcome... increases it by a lot more.</i><br>
<i>&gt; </i><br>
<i>&gt; Compared to a malevolent SI, all other (nano)disasters are peanuts,</i><br>
<i>&gt; so it's worth the risk IMO.</i><br>

<p>
<a name="1639qlink36">I really don't see that much of a difference between vaporizing the
Earth and just toasting it to charcoal.  Considered as weapons, AIs and
nanotech have equal destructive power; the difference is that an AI can
have a conscience.</a>

<p>
<a name="1639qlink37">I mean, *I* care about a Perversion, or the possibility of a malevolent
Power wreaking true evil on an intergalactic scale.</a>  I don't care about
<a name="1639qlink38">it much because if it can happen, it undoubtedly has already, so our
doing it wouldn't make *too* much of a difference, plus it's a pretty
small possibility to begin with.  I don't see why *you* would care at all.</a>

<p>
<a href="1375.html#1386qlink23">&gt; &gt; I think you overestimate the tendency of other people to be morons.</a><br>
<i>&gt; &gt; "Pitch in to help us develop an open-source nanotechnology package, and</i><br>
<i>&gt; &gt; we'll conquer the world, reduce you to serfdom, evolve into gods, and</i><br>
<i>&gt; &gt; crush you like bugs!"</i><br>
<i>&gt; </i><br>
<i>&gt; BS, by joining you get an equal chance to upload and become</i><br>
<i>&gt; posthuman.</i><br>

<p>
Oh, please.  This is like offering every Linux coder their own computer
manufacturing plant.  Like I said, you're assuming supermarket uploading kiosks.

<p>
<a href="1375.html#1386qlink24">&gt; If you let the others cheat you, you'll only have</a><br>
<i>&gt; yourself to blame. And looking at the world and its history,</i><br>
<i>&gt; it's hard to *underestimate* people's tendency to be morons,</i><br>
<i>&gt; btw. Damn!</i><br>

<p>
Oh, I quite disagree with you on that score.  Most idealists tend to
underestimate people's tendency to be morons.  But I'm glad that you
have such confidence in humanity.

<p>
Or did you perchance mean "overestimate"?  Or was that entire sentence a
little Hofstadterian joke?

<p>
<a href="1375.html#1386qlink25">&gt; &gt; &gt; What's a "pure" Singularitarian anyway, someone who wants a</a><br>
<i>&gt; &gt; &gt; Singularity asap at almost any cost? Someone who wants a</i><br>
<i>&gt; &gt; &gt; Singularity for its own sake?</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; Yep.</i><br>
<i>&gt; </i><br>
<i>&gt; (from another thread)</i><br>
<i>&gt; &gt; If this turns out to be true, I hereby award myself the "Be Careful What</i><br>
<i>&gt; &gt; You Wish For Award" for 1996.  Actually, make that the "Be Careful What</i><br>
<i>&gt; &gt; You Wish For, You Damned Moron Award",</i><br>
<i>&gt; </i><br>
<i>&gt; In case of a AI-driven Singularity, something like the above could</i><br>
<i>&gt; make a nice epitaph...</i><br>

<p>
Hey, it's not like I care.

<p>
<a name="1639qlink39">I mean, look at all the rationalizing you have to do just to justify the
idea that *you* somehow know whether or not your own survival is right
in ultimate terms.</a>  Just drop the baggage and let the SIs decide.  Your
mind will be a lot clearer once you make the commitment to minimalism.

<p>
<a href="1375.html#1386qlink26">&gt; &gt; Humanity *will* sink.  That is simply not something</a><br>
<i>&gt; &gt; subject to alteration.  Everything must either grow, or die.  If we</i><br>
<i>&gt; &gt; embrace the change, we stand the best chance of growing.  If not, we</i><br>
<i>&gt; &gt; die.  So let's punch those holes and hope we can breathe water.</i><br>
<i>&gt; </i><br>
<i>&gt; Ok, but don't forget to grow some gills before you sink the ship...</i><br>

<p>
No, no, and no!  I'm not a gill-grower!  I'm a ship-sinker!  Fish grow
gills!  That's not my job!

<p>
<a href="1375.html#1386qlink27">&gt; &gt; You may be high-percentile but you're still human, not human-plus-affector.</a><br>
<i>&gt; </i><br>
<a name="1639qlink40"><i>&gt; Tough luck, I'm The One.</i><br>

<p>
I'll be cheering your efforts on, as long as you don't interfere with
mine.  After all, my success counts as your failure but your success
counts as my success.</a>  See how much easier navigation is once you drop
<a name="1639qlink41">all the unnecessary preconditions?</a>

<p>
<a href="1375.html#1386qlink28">&gt; &gt; And, once again, you are being unrealistic about the way technologies</a><br>
<i>&gt; &gt; develop.  High-fidelity (much less identity-fidelity) uploading simply</i><br>
<i>&gt; &gt; isn't possible without a transhuman observer to help.</i><br>
<i>&gt; ....</i><br>
<i>&gt; &gt;  Any uploadee is</i><br>
<i>&gt; &gt; a suicide volunteer until there's an SI (whether IA or AI) to help.</i><br>
<i>&gt; &gt; There just isn't any realistic way of becoming the One Power because a</i><br>
<i>&gt; &gt; high-fidelity transition from human to Power requires a Power to help.</i><br>
<i>&gt; </i><br>
<i>&gt; Assumptions, assumptions. We'll never know for sure if we don't try.</i><br>
<i>&gt; Help from a Power would sure be nice, but since we [humans] can't</i><br>
<i>&gt; rely on that, we'll have to do it ourselves. If we can upload a dog, a</i><br>
<i>&gt; dolphin and a monkey successfully, we can probably do a human too.</i><br>

<p>
Big "if".  Again, go ahead and try, but don't ask *me* to wait.

<p>
<a href="1375.html#1386qlink29">&gt; &gt; &gt; Besides, a 90% chance of the AI killing us</a><br>
<i>&gt; &gt; &gt; isn't exactly an appealing situation. Would you get into a</i><br>
<i>&gt; &gt; &gt; machine that kills you 90% of the time, and gives total,</i><br>
<i>&gt; &gt; &gt; unprecedented bliss 10% of the time? The rational thing is</i><br>
<i>&gt; &gt; &gt; to look for something with better odds...</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; Yes, but you haven't offered me better odds.  You've asked me to accept</i><br>
<i>&gt; &gt; a 1% probability of success instead.</i><br>
<i>&gt; </i><br>
<i>&gt; I think that your 1% figure is rather pessimistic. In any case, don't</i><br>
<i>&gt; forget that AI researchers like yourself directly and disproportionately</i><br>
<i>&gt; influence the odds of AI vs IA. If some of the top names switched</i><br>
<i>&gt; sides, you could quite easily make IA the most likely path to</i><br>
<i>&gt; ascension. You demand better odds, but at the same time you</i><br>
<i>&gt; actively contribute to the discrepancy.</i><br>

<p>
Let's distinguish "uploading" from "IA".  I'm real big on IA.  I'm
certainly playing both sides of that coin.  I'm not an uploader, and I
don't think that any number of AI researchers switching will make
uploading the most likely path.

<p>
Remember, I demand better odds because you want me to switch my
criterion of success from "AI OR uploading" to "uploading", or in other
words, you want me to beat a deadline of 2015 CRNS with a 2040 CRNS
technology instead of a 2020 CRNS technology.  In fact, you want me to
classify the 2020 CRNS tech as "undesirable" so now I have to beat that
deadline too.  And to top it all off, you want to specify either that
the 2040 CRNS happens to *den Otter* out of all the people in the world,
or that there are 2060 CRNS uploading kiosks and six hundred thousand
people can upload simultaneously.

<p>
Speaking as a navigator, you'd have to offer me a DAMN HUGE differential
of desirability before I'd go within ten light-years of a plan with that
many extra constraints.  I expect ENOUGH goddamn trouble trying to
develop a 2020 CRNS technology before a 2015 CRNS technology, my success
percentile is dropping into the 30s, and you expect me to take on a gap
NINE TIMES AS LARGE?

<p>
<a href="1375.html#1386qlink30">&gt; Ultimately it always comes down to one thing: how likely is</a><br>
<i>&gt; a nuclear/nano war really within the next 30 years or so, and</i><br>
<i>&gt; how much damage would it do.</i><br>

<p>
Damn near inevitable given enough time; if it happens when the
technology is far enough along, it could easily wipe out all
multicellular life.

<p>
&gt; Does this threat justify the<br>
<a href="1375.html#1386qlink31">&gt; all-or-nothing approach of an AI Transcend?</a><br>

<p>
I bet your ass.

<p>
<i>&gt; Well, there hasn't</i><br>
<a href="1375.html#1386qlink32">&gt; been a nuclear war since the technology was developed more</a><br>
<i>&gt; than 50 years ago, I'd say that's a pretty good precedent. Also,</i><br>
<i>&gt; nukes and biological weapons haven't been used by terrorists</i><br>
<i>&gt; yet, which is another good precedent. Is this likely to change</i><br>
<i>&gt; in the first decades of the next century? If so, why?</i><br>

<p>
Yes.  Two nations have nuclear weapons - as in, enough to damage the
planet - so there's a balance of power.  It takes a long time and a lot
of resources to develop, so most other nations can't get a full arsenal,
and if they did we could see it coming.  The arsenals were developed
gradually, so at any given point an attack could be met with fairly
equal retaliation.  And finally, nuclear weapons aren't self-replicating.

<p>
<a name="1639qlink42">Then we have nanotech.  Once diamond drextech becomes common, one guy,
in a lab, can get a complete arsenal that goes from zero to sixty in
hours once the basic unit is developed.  There's no anti-first-strike
restraint.  There's no balance of power.  And there are too many
players.  I think I posted this whole argument earlier - were you around?</a>

<p>
<a href="1375.html#1386qlink33">&gt; Even if we had a full-scale nuclear conflict, this would by no</a><br>
<i>&gt; means kill everyone, in fact, most people would probably</i><br>
<i>&gt; survive, as would "civilization".</i><br>

<p>
I'm not worried about nuclear war, except insofar as it would shift the
balance of probabilities between nanotech and AI.  This is actually
enough to make me worry quite a bit.  A global computer network is
substantially harder to reconstruct than one Zyvex laboratory, so I'm
treating nuclear war as "losing" from my standpoint.

<p>
&gt; A "malevolent" AI would<br>
<a href="1375.html#1386qlink34">&gt; kill *everybody*. Is grey goo really that big a threat?</a><br>

<p>
Like I said, I bet your ass.

<p>
<a href="1375.html#1386qlink35">&gt; A fully autonomous replicator isn't exactly basic nanotech,</a><br>
<i>&gt; so wouldn't it be likely that people would already be</i><br>
<i>&gt; starting to move to space (due to the incresingly low</i><br>
<i>&gt; costs of spaceship/habitat etc. construction) before</i><br>
<i>&gt; actual "grey/black goo" could be developed?</i><br>

<p>
Oh, I wish.  But the law of these revolutions is that they hit harder
than anyone expects, although sometimes they take longer.  I would
<a name="1639qlink3">expect virtually everything Drexler ever wrote about to be developed
within a year of the first assembler, after which, if the planet is
still around, we'll start to get the really *interesting* technologies. 
Nanotechnology is ten times as powerful and versatile as electricity. 
What we forsee is only the tip of the iceberg, the immediate
possibilities.  Don't be fooled by their awesome raw power into
categorizing drextechs as "high" nanotechnology.  Drextechs are the
obvious stuff.  Like I said, I would expect almost all of it to follow
almost immediately.</a>

<p>
<a href="1375.html#1386qlink36">&gt; And even</a><br>
<i>&gt; on earth one could presumably make a stand against</i><br>
<i>&gt; goo using defender goo, small nukes and who knows</i><br>
<i>&gt; what else.</i><br>

<p>
No, I covered that during the "Goo Prophylaxis" debate on Extropians. 
Basically, we can't defend against nuclear weapons right now and
nanotechnology just makes it worse.

<p>
<a name="1639qlink43"><a href="1375.html#1386qlink37">&gt; Many would be killed, but humanity probably</a><br>
<i>&gt; woudn't be wiped out, far from it. A malevolent AI would</i><br>
<i>&gt; kill *everyone*. See my point?</i><br>

</a><p>
Yes.  You're wrong.

<p>
<a href="1375.html#1386qlink38">&gt; &gt; As far as I can tell, your evaluation of the desirability advantage is</a><br>
<i>&gt; &gt; based solely on your absolute conviction that rationality is equivalent</i><br>
<i>&gt; &gt; to selfishness.  I've got three questions for you on that one.  First:</i><br>
<i>&gt; &gt; Why is selfishness, an emotion implemented in the limbic system, any</i><br>
<i>&gt; &gt; less arbitrary than honor?</i><br>
<i>&gt; </i><br>
<i>&gt; Selfishness may be arbitrary, but it's also practical because it's</i><br>
<i>&gt; needed to keep you alive, and being alive is...etc. "Selfishness"</i><br>
<i>&gt; is a very sound fundament to build a personality on. Honor</i><br>
<i>&gt; often leads to insane forms of altruism which can result in</i><br>
<i>&gt; suffering and even death, and is therefore inferior as a meme</i><br>
<i>&gt; (assuming that pleasure is better than death and suffering).</i><br>

<p>
So, selfishness is arbitrary, but for some supergoals it's useful.  No
disagreement there.  Honor is also useful for some supergoals, since
other people tend to treat you better when you act honorably.  Many
emotions are useful.  They're still arbitrary.  And in a well-designed
system, they're just ordinary subgoals instead of special-purpose code,
entirely dependent on supergoals and having no existence apart from
them.  You seem to be according selfishness special treatment, placing
it above reason ("if symmetry [another word for Occam's Razor] wants me
to be unselfish it can kiss my ass"), which is what I object to.

<p>
<a name="1639qlink44"><a href="1375.html#1386qlink39">&gt; &gt; Second:  I know how to program altruism into</a><br>
<i>&gt; &gt; an AI; how would you program selfishness?</i><br>
<i>&gt; </i><br>
<i>&gt; Make survival the Prime Directive.</i><br>

<p>
You did read the section on Interim Goal Systems and the Prime Directive
from _Coding a Transhuman AI_, right?  You're aware that initial-value
goals are both unnecessary and unstable?</a>

<p>
<a href="1375.html#1386qlink40">&gt; &gt; Third:  What the hell makes</a><br>
<i>&gt; &gt; you think you know what rationality really is, mortal?</i><br>
<i>&gt; </i><br>
<i>&gt; What the hell makes you think you know what rationality really</i><br>
<i>&gt; is, Specialist</i><br>

<p>
I don't, that's why I hand it over too...

<p>
<a href="1375.html#1386qlink41">&gt; /AI/SI/Hivemind/PSE/Power/God etc., etc.? Oops,</a><br>
<i>&gt; I guess it's unknowable.</i><br>

<p>
Sheesh, give them a shot.  We know we can't do it because we tried.  You
can't conclude from our own pathetic failures that the answer is
"unknowable".  Me, I think they have a better chance than we do, and
that differential is all I need to make choices.

<p>
<a name="1639qlink45"><i>&gt; Ah well, I guess I'll try to find eternal</i><br>
<a href="1375.html#1386qlink42">&gt; bliss then, until (if ever) I come up with something better. But</a><br>
<i>&gt; feel free to kill yourself if you disagree.</i><br>

<p>
I certainly will.  Please *don't* feel free to interfere with my efforts
to create an AI because *you* think that just because *you* can't figure
out the answer no possible entity can.</a>

<p>
<a name="1639qlink46">I really object to your intimation that "only God" can figure out the
ultimate meaning of life.  That sounds defeatist,</a> passivist, and
Luddite.<a name="1639qlink47">  Me, I figure that any garden-variety superintelligence should
be able to handle the job.</a>

<p>
<a name="1639qlink48"><a href="1375.html#1386qlink43">&gt; &gt; And I think an IA Transcend has a definite probability of being less</a><br>
<i>&gt; &gt; desirable than an AI Transcend.  Even from a human perspective.</a>  In</i><br>
<i>&gt; &gt; fact, I'll go all the way and say that from a completely selfish</i><br>
<i>&gt; &gt; viewpoint, not only would I rather trust an AI than an upload, I'd</i><br>
<i>&gt; &gt; rather trust an AI than *me*.</i><br>
<i>&gt; </i><br>
<i>&gt; Well, speak for yourself. Since the AI has to start out as a</i><br>
<i>&gt; (no doubt flawed) human creation, I see no reason to trust it</i><br>
<i>&gt; more than the guy(s) who programmed it, let alone myself.</i><br>

<p>
<a name="1639qlink49">Oh, yeah, sure, evolution is WAY more trusty than a human designer.</a> 
<a name="1639qlink50">Even, no, ESPECIALLY if you're trying to accomplish something that was
never even REMOTELY INCLUDED in the original design goals, like an
upgradable architecture.  And of course NO creation can be superior to
<a name="1639qlink51">its Creator.</a>  I'd call you a Luddite... but oh, wait, I already did.  So
</a>
let me just say that I'll believe you can out-upgrade a seed AI on the
same day you outswim a nuclear submarine.

<p>
<a href="1375.html#1386qlink44">&gt; &gt; I think you're expressing a faith in the human mind</a><br>
<i>&gt; &gt; that borders on the absurd just because you happen to be human.</i><br>
<i>&gt; </i><br>
<a name="1639qlink52"><i>&gt; It may be messy, but it's all I have so it will have to do.</i><br>

<p>
It is NOT all you have.  The only reason it's all you have is because
you THINK it's all you have.  Get hip to the transhumanist meme, man! 
If you want a better mind, program one!  Unless, of course, you're so
set on being The One that you refuse to do so.</a>

<p>
<a href="1375.html#1386qlink45">&gt; &gt; &gt; What needs to be done: start a project with as many people as</a><br>
<i>&gt; &gt; &gt; possible to firgure out ways to a) enhance human intelligence</i><br>
<i>&gt; &gt; &gt; with available technology, using anything and everything that's</i><br>
<i>&gt; &gt; &gt; reasonably safe and effective</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; *Laugh*.  And he says this, of course, to the author of "Algernon's Law:</i><br>
<i>&gt; &gt;  A practical guide to intelligence enhancement using modern technology."</i><br>
<i>&gt; </i><br>
<i>&gt; Um no, actually this was meant for everyone out there; I know that</i><br>
<i>&gt; *you* have your own agenda, and that the chances of you abandoning</i><br>
<i>&gt; it are near-zero, but maybe someone else will follow my advice. Every</i><br>
<i>&gt; now and then, the supremacy of the AI meme needs to be challenged.</i><br>

<p>
<a name="1639qlink53">Yes, I do have my "own" agenda, which involves using IA as a means to
AI.  And if you'd like to start your own IA project, I'll be more than
happy to contribute advice, brain scans, genetic material, or anything
else you need.</a>

<p>
<a href="1375.html#1386qlink46">&gt; &gt;  Which is the *other* problem with steering a car by shooting out the</a><br>
<i>&gt; &gt; tires...  Taking potshots at me would do a lot more to cripple IA than</i><br>
<i>&gt; &gt; AI.  And, correspondingly, going on the available evidence, IAers will</i><br>
<i>&gt; &gt; tend to devote their lives to AI.</i><br>
<i>&gt; </i><br>
<i>&gt; Well, there certainly are a lot of misguided people in that field, no</i><br>
<i>&gt; doubt about that. Fortunately there also are plenty of people (in the</i><br>
<i>&gt; medical branches, for example) who, often unknowingly, will</i><br>
<i>&gt; help to advance the cause of IA, and ultimately uploading.</i><br>

<p>
Like I said, it doesn't make a difference.  AI is 2020 CRNS, and
uploading is 2040 CRNS.  No amount of optimism and plotting and
sucker-cheating and tire-shooting is going to change the ordering.  The
Soothsayer herself couldn't navigate a problem like that.  Gap's too large.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1387.html">[ Next ]</a><a href="1385.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1375.html">den Otter</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1434.html">Xiaoguang Li</a>
</ul>
</body></html>
