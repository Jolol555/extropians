<!-- received="Sat Aug 28 20:21:56 1999 MDT" -->
<!-- sent="Sat, 28 Aug 1999 21:22:55 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Present dangers to transhumanism" -->
<!-- id="37C8997D.31A1364F@pobox.com" -->
<!-- inreplyto="Present dangers to transhumanism" -->
<!-- version=1.10, linesinbody=86 -->
<html><head><title>extropians: Re: Present dangers to transhumanism</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Present dangers to transhumanism</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sat, 28 Aug 1999 21:22:55 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2975">[ date ]</a><a href="index.html#2975">[ thread ]</a><a href="subject.html#2975">[ subject ]</a><a href="author.html#2975">[ author ]</a>
<!-- next="start" -->
<li><a href="2976.html">[ Next ]</a><a href="2974.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2954.html">Anders Sandberg</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Anders Sandberg wrote:
<br>
<i>&gt; </i><br>
<a href="2954.html#2975qlink1">&gt; This of course doesn't mean we need to all dress up in suits and ties,</a><br>
<i>&gt; but we better get our act together. If there is anything that really</i><br>
<i>&gt; worries me it is uneducated transhumanists. Not in the sense of not</i><br>
<i>&gt; having academic degrees, but in the sense of people making claims</i><br>
<i>&gt; without any support, loudly assering various notions and claiming them</i><br>
<i>&gt; to be transhumanism without exploring how well they fit with reality,</i><br>
<i>&gt; transhumanist thinking and other known information. That kind of</i><br>
<i>&gt; behavior weakens transhumanism, and provides something that is easy</i><br>
<i>&gt; for opponents to attack ("Mr X, the transhumanist, said that the goal</i><br>
<i>&gt; of transhumanism was to wipe out anybody who won't become</i><br>
<i>&gt; superintelligent").</i><br>
<i>&gt; </i><br>
<i>&gt; Fortunately, I think this is curable. Both by holding "internal"</i><br>
<i>&gt; courses on our own subjects for transhumanists (this is a good</i><br>
<i>&gt; activity for our organisations - informal courses/seminars/discussions</i><br>
<i>&gt; on (say) how nanotech really would work or the realities of economics)</i><br>
<i>&gt; and by having us, as transhumanists, realise the need for</i><br>
<i>&gt; self-transformation and self improvement. After all, we want to grow</i><br>
<i>&gt; ourselves as people, and one of the best ways of doing that is</i><br>
<i>&gt; educating ourselves and becoming better communicators.</i><br>

<p>
Good intentions - check.  Good idea on general principles - check.  Will
solve stated problem - nope.

<p>
The problem is, well, gaussian.  The bell-curve thing.  You will never,
ever be able to eliminate the transhumanist kooks, or even educate them,
because there will always be a residuum of complete morons who combine
your Web pages with their private beliefs and go on to wreak havoc in
the media, immune to all reasoning or volume control.

<p>
I don't know if this problem has a transhumanist solution, but I do
think it has a Singularitarian solution.  You cannot out-extremist an
Externalist Singularitarian.  I can - training, experience, and
opportunity would be required in practice, but I'm talking about
possibilities - go on the Jerry Springer show with a raving, high-volume
lunatic who equates the Singularity with the Kali Yuga and overshadow
him without raising my voice.

<p>
"If computing power doubles every eighteen months, what happens when
computers are doing the research?  If technology is produced by
intelligence, what happens when intelligence is enhanced by technology?"
 In less then twenty years, all life as we know it will have come to an
end, for causes that lie completely within the laws of physics.  Or the
Q&amp;A sessions:  "What if your AI wipes out humanity?"  "If a
greater-than-human intelligence judges humanity to be unnecessary, I do
not consider myself competent to challenge it."  Et cetera.  I've found
that I can shock the living daylights out of most "mundanes" in a few
minutes.  I don't think a mock-Singularitarian could.

<p>
In the end, I don't think the lunatics will have the intelligence or the
comprehension-of-the-future to generate the alienness or the future
shock that surrounds the Singularitarian meme.  They can't compete on
shock value, and in the end, that's all junk television cares about.  I
believe that my rational discussion of the technological end of the
world will shock newscasters more than any apocalyptist cult can,
because cults can steal the terminology and the surface outcomes, but
they can't steal the alienness.

<p>
For you transhumanists, I would simply recommend putting up one person
as "leader"; I'd say Max More.  The tendency to give more media time to
loonies can be counteracted by the tendency to give more time to
leaders.  If there's a clear reference point that mediafolk can
understand - that means, "Max More big kahuna", not "The Extropian
Principles are available on the web at..." - then the principles tend to
get less distorted.  Same reasoning as with Eric S. Raymond.  He didn't
invent it, but if the open-source guys didn't have an archetypal
figurehead to wave in front of the media, ESR knew damn well that
Microsoft would be labeling Windows 2000 "open-source" inside of a
month.  So ESR elected himself as media spokesperson and did a fairly
good job.

<p>
I think that Singularitarianism should be able to get by without a
IAer-per-se as a *literally* archetypal figurehead; maybe Mitchell
Porter or Marc Steigler or someone else will wind up doing it.  If
someone else wants the limelight, and the byline on the Web pages, I'm
all for fading into the backlight and working on AI instead, saving my
mutant-supergenius status as a trump card.  Realistically, though, the
research guys often do wind up doing the PR.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2976.html">[ Next ]</a><a href="2974.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2954.html">Anders Sandberg</a>
<!-- nextthread="start" -->
</ul>
</body></html>
