<!-- received="Mon Jul 12 05:52:48 1999 MDT" -->
<!-- sent="Mon, 12 Jul 1999 11:57:52 +0100" -->
<!-- name="Rob Harris Cen-IT" -->
<!-- email="Rob.Harris@bournemouth.gov.uk" -->
<!-- subject="RE: Robots in Social Positions (Plus Intelligent Environments)" -->
<!-- id="31519CDE0AFAD11182BF00805F85F95D5D9F34@CS-EXCHANGE" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=42 -->
<html><head><title>extropians: RE: Robots in Social Positions (Plus Intelligent Environments)</title>
<meta name=author content="Rob Harris Cen-IT">
<link rel=author rev=made href="mailto:Rob.Harris@bournemouth.gov.uk" title ="Rob Harris Cen-IT">
</head><body>
<h1>RE: Robots in Social Positions (Plus Intelligent Environments)</h1>
Rob Harris Cen-IT (<i>Rob.Harris@bournemouth.gov.uk</i>)<br>
<i>Mon, 12 Jul 1999 11:57:52 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#547">[ date ]</a><a href="index.html#547">[ thread ]</a><a href="subject.html#547">[ subject ]</a><a href="author.html#547">[ author ]</a>
<!-- next="start" -->
<li><a href="0548.html">[ Next ]</a><a href="0546.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0492.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a href="0492.html#0547qlink1">&gt;    (g) The ten commandments (or another set of moral codes).</a><br>

<p>
I'm sorry to piss on your fire, folks, but it will not be necessary to
instill moral codes into our androids. You design a system for what you want
it to do, not what you want it not to do. You make the assumption that
"intelligence" (like humans) implies "free will" i.e. any possibility can be
selected in the solution of any self-selected goal for the fulfillment of an
equally self-selected motivation. Humans kill, rape and do all these
terrible things, because our base motivation is malevolent. That's
everyone's, not just the people who actually end up in the situations that
make these things necessary. Don't forget that we're genetic beings that
have evolved from a seething mass of competition and death for millions of
years. Robotic slaves made by humans will have a particular purpose, maybe
ironing, warcraft piloting etc...., none of which are similar domains to the
natural selection that humans and the other species have been through. Their
base motivation will be whatever we make it, and we're hardly likely to make
it "Survive at all costs, and try to rule the world". It will not
spontaneously decide to do anything that is not a solution to it's base
motivations (just like us). We humans just like to glorify ourselves, and in
doing this we pass on the abstract concept of our superiority and "free
will" to other "intelligences" that we create. Let's discuss this further -
I'm sure that some of you won't be happy with the above, so we can clarify
this most interesting point wiv some more discussion!

<p>
Rob.



<hr>
<br>
This email and any files transmitted with it are confidential and 
intended solely for the use of the individual or entity to whom they   
are addressed. If you have received this email in error please notify 
the system manager.

<p>
This footnote also confirms that this email message has been swept by 
MIMEsweeper for the presence of computer viruses.

<p>
www.bournemouth.gov.uk
<hr>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0548.html">[ Next ]</a><a href="0546.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0492.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
</ul>
</body></html>
