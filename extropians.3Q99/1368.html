<!-- received="Sun Aug  1 03:42:39 1999 MDT" -->
<!-- sent="Sun, 01 Aug 1999 04:42:32 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: IA vs. AI was: longevity vs singularity" -->
<!-- id="37A41688.10F5C6F5@pobox.com" -->
<!-- inreplyto="IA vs. AI was: longevity vs singularity" -->
<!-- version=1.10, linesinbody=57 -->
<html><head><title>extropians: Re: IA vs. AI was: longevity vs singularity</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: IA vs. AI was: longevity vs singularity</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sun, 01 Aug 1999 04:42:32 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1368">[ date ]</a><a href="index.html#1368">[ thread ]</a><a href="subject.html#1368">[ subject ]</a><a href="author.html#1368">[ author ]</a>
<!-- next="start" -->
<li><a href="1369.html">[ Next ]</a><a href="1367.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1362.html">phil osborn</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
phil osborn wrote:
<br>
<i>&gt; </i><br>
<a href="1362.html#1368qlink1">&gt; (In fact, according to Conrad Schneiker - co-inventor of the</a><br>
<i>&gt; scanning/tunneling array chip - Drexler also originally thought that</i><br>
<i>&gt; nanotech itself could be kept under wraps and carefully developed under the</i><br>
<i>&gt; wise and beneficent supervision of MIT technocrat types.  It was only,</i><br>
<i>&gt; according to Conrad, when he threatened to publish on nanotech himself, that</i><br>
<i>&gt; he managed to force Drexler's hand on this, which resulted in Engines,..</i><br>
<i>&gt; Foresight, etc.)</i><br>

<p>
Where are you getting this from?  I'd be extremely interested in any
biographical information about Drexler - he's the closest thing I have
to a role model.

<p>
<a href="1362.html#1368qlink2">&gt; While there are inherent economic efficiencies in cooperation, and the</a><br>
<i>&gt; universe does set upper limits on the size of a practical single-focus</i><br>
<i>&gt; intelligence, there are even more challenging issues for a really smart AI.</i><br>

<p>
No, there are not inherent economic efficiencies in cooperation.  Any
time you have two independent minds that are remotely similar, there's
going to be duplication of function and thus duplication of processing. 
Only silly human programmers with badly designed CPUs have to stoop to
that sort of thing.

<p>
Even if the lightspeed limit requires local duplication of processing,
the resultant spectrum of local/global consciousness would almost
certainly NOT spew up a recognizable individual.  Personalities do not
happen by coincidence.  Personalities are too complex to be forced by
optimized architectures.  Any deliberately designed system will not give
rise to individuals unless that is the deliberate intention of the designer.

<p>
<a href="1362.html#1368qlink3">&gt; You yourself have undoubtably run head on into the problem of achieving</a><br>
<i>&gt; "visibility."  The more out on the end of the bell you are, the less likely</i><br>
<i>&gt; you are to find a soul mate.</i><br>

<p>
Or someone to take over if you get hit by a truck.  But I suppose I echo
the general sentiment.

<p>
<a href="1362.html#1368qlink4">&gt; It is only when you have a perceptual/emotional relationship with another</a><br>
<i>&gt; consciousness in real-time that you can get a real mirror of your "soul."</i><br>
<i>&gt; This is why people place such a high value on relationships.</i><br>

<p>
I really think you're overestimating both the damage caused by lack of
relationships - nobody understands me, probably nobody ever will, big
deal - and the degree to which this damage is a necessary feature of
intelligence rather than a quirk of an evolved mind.  Besides, any
mind-plus-environment system can easily be converted to a standalone
computer simulation.  But the main thing is, I just don't see where the
cognitive problem is going to come from.  Minds don't require external
environments - human minds, yes; generic minds, no.  You can get
arbitrarily complex "sensory" inputs just through reflectivity.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1369.html">[ Next ]</a><a href="1367.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1362.html">phil osborn</a>
<!-- nextthread="start" -->
</ul>
</body></html>
