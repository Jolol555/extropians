<!-- received="Sat Aug  7 20:34:51 1999 MDT" -->
<!-- sent="Sat, 07 Aug 1999 21:35:04 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: IA vs. AI (vs. humanity)" -->
<!-- id="37ACECD7.AD018373@pobox.com" -->
<!-- inreplyto="IA vs. AI (vs. humanity)" -->
<!-- version=1.10, linesinbody=118 -->
<html><head><title>extropians: Re: IA vs. AI (vs. humanity)</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: IA vs. AI (vs. humanity)</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sat, 07 Aug 1999 21:35:04 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1751">[ date ]</a><a href="index.html#1751">[ thread ]</a><a href="subject.html#1751">[ subject ]</a><a href="author.html#1751">[ author ]</a>
<!-- next="start" -->
<li><a href="1752.html">[ Next ]</a><a href="1750.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1731.html">Jeff Davis</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Jeff Davis wrote:
<br>
<i>&gt; </i><br>
<a href="1731.html#1751qlink1">&gt; Ah, but if the proto-Transcendee has limited hardware resources to "run"</a><br>
<i>&gt; on, then it will be inherently limited.  Optimized self-evolutionary</i><br>
<i>&gt; enhancement capability, and optimized code-designing and writing capability</i><br>
<i>&gt; will run into this limit.  Every system has a size limit.  Whatever amount</i><br>
<i>&gt; of hardware is the minimum amount necessary to support the first-generation</i><br>
<i>&gt; pre-enhanced AI will also be the maximum amount of hardware available to</i><br>
<i>&gt; optimally-enhanced n'th generation "Transcendee".  The jump from minimum</i><br>
<i>&gt; efficiency to near optimal may be substantial, but how can it be unbounded?</i><br>

<p>
My current guess is that a first-stage seed AI is so massively
inefficient that the hardware it runs on should easily be sufficient for
transhuman intelligence, once the code itself is being written by
only-slightly-less-than-transhuman intelligence.  I don't know about
superintelligence, but transhumanity should be sufficient.

<p>
See <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>, the part about seed
AI trajectories.

<p>
<a href="1731.html#1751qlink2">&gt; So the AI development should be controllable (dare I say "simply"?) by the</a><br>
<i>&gt; rather conventional approach: experimenting with and coming to understand</i><br>
<i>&gt; the correlation between the size and quality of "the jump", the particular</i><br>
<i>&gt; version of AI seed programming, and the hardware size and architecture.</i><br>

<p>
I really don't see people running multiple seed-AI experiments.  The
very last seed AIs, as in the last programs humanity ever writes, are
likely to be too expensive.  By the time they would have become cheap...

<p>
<a href="1731.html#1751qlink3">&gt; &gt;Unfortunately, this is technically impossible. If you can't even get a</a><br>
<i>&gt; program to understand what</i><br>
<i>&gt; &gt;year it is, how do you expect complete control without an SI to do the</i><br>
<i>&gt; controlling?</i><br>
<i>&gt; </i><br>
<i>&gt; This is one of the problems.  If you have to give it self-control, then you</i><br>
<i>&gt; contain it and communicate with it.  If it says what you want to hear, then</i><br>
<i>&gt; you proceed.  If not you tweak the code till it does.  This way you develop</i><br>
<i>&gt; a controllable (perhaps "reliable" would be a better term) "personality".</i><br>
<i>&gt; Then you give it more hardware to work while watching for any signs of</i><br>
<i>&gt; "attitude".</i><br>

<p>
I'm beginning to understand human cognition, and I can see - dimly - the
level on which we're all deterministic mechanisms.  A transhuman - I
don't even say "SI" - would simply tell us the set of inputs that would
result in the behavioral output they wanted.  I can't do that yet, or
anything remotely like it, but I have hopes of learning a few parlor
tricks someday.

<p>
<a href="1731.html#1751qlink4">&gt; Then they will ask--no, they will require--you to do the impossible. (Which</a><br>
<i>&gt; of course is the greatest challenge, and--as the saying goes--takes a</i><br>
<i>&gt; little longer.)  All the really juicy bargains come with equally juicy</i><br>
<i>&gt; strings attached.</i><br>

<p>
Coding a transhuman AI is as close to impossibility as I care to flirt
with.  Anyone wants to attach strings to a job like that, they can take
their strings and shove them up the ass of whatever COBOL drone they con
into taking the job.  *I* am not going to lose *any* sleep worrying
about their device attaining a level of performance higher than pocket calculator.

<p>
<a href="1731.html#1751qlink5">&gt; When your passion faces off against your principles, then it will be your</a><br>
<i>&gt; turn to choose.</i><br>

<p>
I'm not presenting this a principled thing.  You *cannot* design a
controllable self-modifying system, and you *cannot* make a
non-self-modifying AI work.  It's a design fact.

<p>
<a href="1731.html#1751qlink6">&gt; Ideally, yes.  A controllable what-it-does which does what it does better</a><br>
<i>&gt; than a machine with a will is best.  If, however, a machine with a will</i><br>
<i>&gt; would be inherently better (which I warmly believe), then that's what they</i><br>
<i>&gt; will pursue, along with the means to control it.  More layers of</i><br>
<i>&gt; containment and a firm grip on the plug.</i><br>

<p>
I say again:  "Control" of a self-modifying AI is even harder than
controlling a human.  And non-self-modifying AI will be dumb as a brick;
it's the capacity for self-enhancement, and positive feedback in
self-enhancement, that replaces the millions of years of evolutionary
optimization that holds the human mind together.

<p>
<a href="1731.html#1751qlink7">&gt; &gt;Run an open-source project via anonymous PGP between participating</a><br>
<i>&gt; programmers.</i><br>
<i>&gt; </i><br>
<i>&gt; I'd really like to see that happen.  However, just as the powers that be</i><br>
<i>&gt; would flat out not let you build a nuke or a lethal virus (except under</i><br>
<i>&gt; contract to them and under conditions of strictist oversight), they're not</i><br>
<i>&gt; likely to sit idly by while you and your pals cobble together you own pet</i><br>
<i>&gt; SI.  (I saw the "anonymous PGP" part.  Since you know you need it, you know</i><br>
<i>&gt; why you need it.  Can you carry it off covertly?  No slip ups?  That's a</i><br>
<i>&gt; tough one.)</i><br>

<p>
The question wouldn't be preventing even one slip; the question would be
organizing a project that didn't care about slips, or even about FBI
agents masquerading as members.  The code would have to be self-checking
and modular to the extent of being able to function with sabotaged
components, and the project itself would have to be carried out with
complete anonymity on the part of all participants.  The project would
have to be mirrored on each individual's hard drive, and kept in sync
independently; the AI itself would have to run distributed with
untrusted communications and untrusted software modules.  It adds
another design problem to the AI, but it's a design problem that might
be good for the project.

<p>
<a href="1731.html#1751qlink8">&gt; Just the same, I say "Go for it!"  I suspect that a "good" AI may be the</a><br>
<i>&gt; only feasible defense against a "bad" one.</i><br>

<p>
Not really.  If I'm right, it should be fairly hard to design a "bad" AI
that's a serious threat - AIs with weirdwired motivations shouldn't be
able to get to the point of independence, though they might be used as
awful weapons.  Actually, AIs with weirdwired motivations aren't
*intrinsically* incapable of mildly transhuman intelligence - just
superintelligence.  But the kind of COBOL drones who would design AIs
with weirdwired motivations have basically no hope of even making much
of a weapon.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1752.html">[ Next ]</a><a href="1750.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1731.html">Jeff Davis</a>
<!-- nextthread="start" -->
</ul>
</body></html>
