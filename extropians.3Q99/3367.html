<!-- received="Thu Sep  2 10:56:39 1999 MDT" -->
<!-- sent="Thu, 2 Sep 1999 09:56:35 -0700 (PDT)" -->
<!-- name="mark@unicorn.com" -->
<!-- email="mark@unicorn.com" -->
<!-- subject="Re: Principle of Nonsuppression" -->
<!-- id="936291394.2281.193.133.230.33@unicorn.com" -->
<!-- inreplyto="Principle of Nonsuppression" -->
<!-- version=1.10, linesinbody=34 -->
<html><head><title>extropians: Re: Principle of Nonsuppression</title>
<meta name=author content="mark@unicorn.com">
<link rel=author rev=made href="mailto:mark@unicorn.com" title ="mark@unicorn.com">
</head><body>
<h1>Re: Principle of Nonsuppression</h1>
<i>mark@unicorn.com</i><br>
<i>Thu, 2 Sep 1999 09:56:35 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3367">[ date ]</a><a href="index.html#3367">[ thread ]</a><a href="subject.html#3367">[ subject ]</a><a href="author.html#3367">[ author ]</a>
<!-- next="start" -->
<li><a href="3368.html">[ Next ]</a><a href="3366.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3292.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="3386qlink1">Eliezer S. Yudkowsky [sentience@pobox.com] wrote:
<br>
<a href="3292.html#3367qlink1">&gt;Once Zyvex has nanotechnology,</a><br>
<i>&gt;I'd be fully in favor of their immediately conquering the world to</i><br>
<i>&gt;prevent anyone else from getting it.  That's what should've been done</i><br>
<i>&gt;with nuclear weapons.  If the "good guys" refuse to conquer the world</i><br>
<i>&gt;each time a powerful new weapon is developed, sooner or later a bad guy</i><br>
<i>&gt;is going to get to the crux point first. </i><br>

<p>
You know what scares me the most about the future? All these control freaks
and their desire to take over the world to protect themselves from the "bad
guys"; Eliezer and den Otter the most obvious proponents on this list. We
must all support the "good guys" in taking over the world and introducing
their global surveillance utopia while we await The Coming Of The Glorious
Singularity!</a>

<p>
<a name="3386qlink2">Look Eliezer, we know you're a rabid Singularitarian,</a> but to those of us who
<a name="3386qlink3">actually work on developing advanced hardware (my employer designs chips at
least as complicated as anything coming out of Intel) the idea that we'll
have this new technology appear and then in a few days we'll be surrounded 
by nanotech death machines and massively intelligent AIs is blatantly absurd.</a>
<a name="3386qlink4">Building hardware at nanoscales is difficult enough, but the software is
way, way, behind; there are features we've had in our chips for years which
are only just coming to be used by applications, and developers aren't even
beginning to use the computing power we're giving them in anything but the
most simplistic ways.<a name="3395qlink1"> No matter how powerful the hardware, the software will
be a long time coming, even with neural nets or genetic programming.</a></a>

<p>
<a name="3386qlink5">Maybe if you took a more realistic view of how technology is really developing
and how it's likely to develop in a nanotech future, you wouldn't be so
scared that you're willing to destroy the Earth in a nanotech war in order
to prevent one.
</a>

<p>
    Mark
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3368.html">[ Next ]</a><a href="3366.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3292.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
