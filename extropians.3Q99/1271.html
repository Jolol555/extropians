<!-- received="Wed Jul 28 10:57:07 1999 MDT" -->
<!-- sent="Wed, 28 Jul 1999 09:55:41 -0700" -->
<!-- name="Max More" -->
<!-- email="max@maxmore.com" -->
<!-- subject="Re: IA vs. AI was: longevity vs singularity" -->
<!-- id="199907281656.JAA16073@avocet.prod.itd.earthlink.net" -->
<!-- inreplyto="199907272359.QAA27448@geocities.com" -->
<!-- version=1.10, linesinbody=91 -->
<html><head><title>extropians: Re: IA vs. AI was: longevity vs singularity</title>
<meta name=author content="Max More">
<link rel=author rev=made href="mailto:max@maxmore.com" title ="Max More">
</head><body>
<h1>Re: IA vs. AI was: longevity vs singularity</h1>
Max More (<i>max@maxmore.com</i>)<br>
<i>Wed, 28 Jul 1999 09:55:41 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1271">[ date ]</a><a href="index.html#1271">[ thread ]</a><a href="subject.html#1271">[ subject ]</a><a href="author.html#1271">[ author ]</a>
<!-- next="start" -->
<li><a href="1272.html">[ Next ]</a><a href="1270.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1245.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
At 07:30 PM 7/27/99 +0200, den Otter wrote:
<br>
<i>&gt;</i><br>

<p>
<a href="1245.html#1271qlink1">&gt;&gt; You're staking an awful lot on the selfishness of superintelligences. </a><br>
<i>&gt;</i><br>
<i>&gt;I'm simply being realistic; when you realize how incredibly slow, </i><br>
<i>&gt;predictable and messy humans will be compared to even an early </i><br>
<i>&gt;SI, it is hard to imagine that it will bother helping us. Do we "respect"</i><br>
<i>&gt;ants? Hardly. Add to that the fact that the SI either won't have our</i><br>
<i>&gt;emotional (evolutionary) baggage to start with, or at least can modify </i><br>
<i>&gt;it at will, and it becomes harder still to believe that it would be </i><br>
<i>&gt;willing to keep humans around, let alone actively uplifting them.</i><br>
<i>&gt;</i><br>
<i>&gt;Why would it want to do that? There is no *rational* reason to </i><br>
<i>&gt;allow or create competition, and the SI would supposedly be the</i><br>
<i>&gt;very pinnacle of rationality. </i><br>

<p>
<a name="1362qlink2">I find this puzzling. By the same reasoning, we should want to keep
children uneducated and ignorant, since they will become competition for
us. Both assume that more people with intelligence and ability come at a
cost to those who already have it. A zero-sum assumption. Clearly the
economy does not work this way. Right now, most of Africa has less wealth,
education, health, and technological ability than the Americas and Europe.
I would see Africa's ascendence not as a competitive threat but as a
massive contribution to the total output of ideas, goods, and services.

<p>
<a name="1374qlink1"><a name="1323qlink1">Why should SI's see turning humans into uploads as competition in any sense
that harms them? It would just mean more persons with whom to have
productive exchanges.</a></a></a>

<p>
It's absurd to think that a true SI would
<br>
<a href="1245.html#1271qlink2">&gt;still run on the programming of tribal monkey-men, which are</a><br>
<i>&gt;weak and imperfect and therefore forced to cooperate. That's why</i><br>
<i>&gt;evolution has come up with things like altruism, bonding, honor </i><br>
<i>&gt;and all the rest. Nice for monkey-men, but utterly useless </i><br>
<i>&gt;for a supreme, near-omnipotent and fully self-contained SI. If</i><br>
<i>&gt;it has a shred of reason in its bloated head, it will shed those</i><br>
<i>&gt;vestigal handicaps asap, if it ever had them in the first place. </i><br>
<i>&gt;And of course, we'd be next as we'd just be annoying microbes </i><br>
<i>&gt;which can spawn competition. Competition means loss of control </i><br>
<i>&gt;over resources, and a potential threat. Not good. *Control* is good. </i><br>

<p>
<a name="1374qlink2"><a href="1245.html#1271qlink3">&gt;Total control is even better. The SI wouldn't rest before it had </a><br>
<i>&gt;brought "everything" under its control, or die trying. Logical, don't </i><br>
<i>&gt;you think? </i><br>

<p>
This must be where we differ. No, I don't think total control is desirable
or beneficial, even if it were me who had that total control. If true
omnipotence were possible, maybe what you are saying would follow, but
omnipotence is a fantasy to be reserved for religions. Even superpowerful
and ultraintelligent beings should benefit from cooperation and exchange.</a>

<p>
<a href="1245.html#1271qlink4">&gt;Well, yes of course I want that; after all, the alternative is to meekly</a><br>
<i>&gt;wait and hope that whoever/whatever turns SI first will have mercy on</i><br>
<i>&gt;your soul. If I had that kind of attitude I'd be a devout Christian, not</i><br>
<i>&gt;a transhumanist. Wanting to be among the first to upload is morally </i><br>
<i>&gt;right, if nothing else, just like signing up for suspension is morally</i><br>
<i>&gt;right, regardless whether it will work or not. It's man's duty (so to</i><br>
<i>&gt;speak) to reject oppression of any kind, which means spitting death</i><br>
<i>&gt;in the face, among other things. AI could very well be death/</i><br>
<i>&gt;oppression in sheep's clothing (which reminds me of the movie </i><br>
<i>&gt;"Screamers", btw, with the "cute" killer kid), so we should treat it </i><br>
<i>&gt;accordingly. </i><br>

<p>
<a name="1374qlink3"><a name="1362qlink3">Despite my disagreement with your zero-sum assumptions (if I'm getting your
views right--I only just starting reading this thread and you may simply be
running with someone else's assumptions for the sake of the argument), I
agree with this.<a name="1323qlink2"> While uploads and SI's may not have any inevitable desire
to wipe us out, some might well want to, and I agree that it makes sense to
deal with that from a position of strength.</a>
</a>


<p>
<a name="1374qlink4">I'm not sure how much we can influence the relative pace of research into
unfettered independent SIs vs. augmentation of human intelligence, but</a> I
<a name="1374qlink5">too favor the latter. Unlike Hans Moravec and (if I've read him right,
Eliezer), I have no interest in being superceded by something better. I
want to *become* something better.</a>

<p>
Max
</a>

<hr>
<br>
Max More, Ph.D.
<br>
&lt;max@maxmore.com&gt; or &lt;more@extropy.org&gt;

<p>
<a href="http://www.maxmore.com">http://www.maxmore.com</a>

<p>
Implications of Advanced Technologies
<br>
President, Extropy Institute: <a href="http://www.extropy.org">http://www.extropy.org</a>
EXTRO 4 Conference: Biotech Futures. See <a href="http://www.extropy.org/ex4/e4main.htm">http://www.extropy.org/ex4/e4main.htm</a>
<hr>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1272.html">[ Next ]</a><a href="1270.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1245.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
</body></html>
