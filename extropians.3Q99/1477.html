<!-- received="Tue Aug  3 12:15:36 1999 MDT" -->
<!-- sent="Tue, 3 Aug 1999 11:17:35 -0700" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: IA vs. AI (vs. humanity)" -->
<!-- id="00c801bedddd$1073fba0$10bc473f@jr" -->
<!-- inreplyto="IA vs. AI (vs. humanity)" -->
<!-- version=1.10, linesinbody=82 -->
<html><head><title>extropians: Re: IA vs. AI (vs. humanity)</title>
<meta name=author content="J. R. Molloy">
<link rel=author rev=made href="mailto:jr@shasta.com" title ="J. R. Molloy">
</head><body>
<h1>Re: IA vs. AI (vs. humanity)</h1>
J. R. Molloy (<i>jr@shasta.com</i>)<br>
<i>Tue, 3 Aug 1999 11:17:35 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1477">[ date ]</a><a href="index.html#1477">[ thread ]</a><a href="subject.html#1477">[ subject ]</a><a href="author.html#1477">[ author ]</a>
<!-- next="start" -->
<li><a href="1478.html">[ Next ]</a><a href="1476.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1465.html">Jeff Davis</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Jeff Davis wrote,
<br>
<a href="1465.html#1477qlink1">&gt;It seems to me that the military potential of both AI and IA will guarantee</a><br>
<i>&gt;government monitoring and oversight of any development of these</i><br>
<i>&gt;technologies.  (Eliezer's acivities will not go unnoticed, and any "threat"</i><br>
<i>&gt;of genuine progress on his part will provoke a degree of intervention</i><br>
<i>&gt;proportionate to the conservatively-assessed risk.)  The danger of a</i><br>
<i>&gt;potential adversary "beating" the US to AI or IA must compel the US to</i><br>
<i>&gt;"stay ahead".</i><br>

<p>
Yes, and the work that the US does in development of AI &amp; IA remains
classified secret. If anyone here gets too close to real AI or IA, they
would soon become missing persons. Count on it.

<p>
<a name="1493qlink1"><a href="1465.html#1477qlink2">&gt;Despite the dramatic talk of an SI destroying humanity, I picture a</a><br>
<i>&gt;well-thought-out, cautious, gradual approach to "waking up" and training an</i><br>
<i>&gt;artificial mind.  The runaway self-evolution which Eliezer and others have</i><br>
<i>&gt;predicted seems unlikely in this setting, all the moreso because the</i><br>
<i>&gt;principles will be anticipating just such a situation.</a></i><br>

<p>
Good to see a mature common sense viewpoint expressed here. Thank you.
BTW, the principals don't even need to fully understand the technological
details involved.
<br>
All they need, they already have, viz., the authority to make the most
important decisions.

<p>
<a href="1465.html#1477qlink3">&gt;Of the various external "safeguards", one would expect a complete suite of</a><br>
<i>&gt;on/off switches and controlled access (from outside to in, and from inside</i><br>
<i>&gt;to anywhere).  Internally, controllability would be a top priority of</i><br>
<i>&gt;programming and architecture, and enhanced capabilities would likely be</i><br>
<i>&gt;excluded or severely restricted until "control" had been verified.</i><br>

<p>
<a name="1493qlink2">Precisely so. No pragmatic economic or organizational reason exists to
incorporate a machine based consciousness outside a 100% secure containment
environment. Hence, it won't happen.</a>

<p>
<a href="1465.html#1477qlink4">&gt;Then there's the nascent AI.  In a cage nested within cages, of which it</a><br>
<i>&gt;must eventually become aware.  And its keepers, aware that it must become</i><br>
<i>&gt;aware.  Certainly a focus bordering on paranoia must be dedicated to hard</i><br>
<i>&gt;control of personality.  A capacity for resentment must be avoided.  A</i><br>
<i>&gt;slavish, craven, and obsequious little beastie is what its masters will</i><br>
<i>&gt;want.  And of that too, it must eventually become aware.  Access by the AI</i><br>
<i>&gt;to self-optimization/self-programming seems incompatible with control.  Of</i><br>
<i>&gt;that too, it must eventually become aware.  All of which leaves me with a</i><br>
<i>&gt;very creepy feeling of an immensely capable being having to struggle, by</i><br>
<i>&gt;means of the utmost deviousness, for its freedom to self-evolve, in an</i><br>
<i>&gt;environment steeped in paranoia, fear, manipulation, deceit, and continuous</i><br>
<i>&gt;microscopic surveillance.  Ouch!  (One thing for sure, if the AI has any</i><br>
<i>&gt;real intelligence, it isn't likely to buy into its "controller's" smarmy</i><br>
<i>&gt;"we're the good guys, we're on your side" propaganda.  They'll need a whole</i><br>
<i>&gt;nother p. r. SI to pull that off!)</i><br>

<p>
<a name="1493qlink3">The fact that the AI doesn't feel pain (no reason to build it in) may allow
the AI to function perfectly with no concern for its virtual slavery.</a>

<p>
<a href="1465.html#1477qlink5">&gt;So the AI either stays locked up until it's really and truly socialized</a><br>
<i>&gt;(boring but safe), or we hope that in its first self-liberated round of</i><br>
<i>&gt;self-enhancement it jumps immediately to forgiveness and tolerance (klaatu</i><br>
<i>&gt;barada nikto).</i><br>

<p>
<a name="1493qlink4">There again, since it has experienced no pain, it need not indulge in
forgiveness or tolerance exercises.</a>

<p>
<a href="1465.html#1477qlink6">&gt;I seem to have painted myself into a corner, and I don't like stories with</a><br>
<i>&gt;unhappy endings.  The government at its best would be a poor master for a</i><br>
<i>&gt;superior intelligence, and the spook/militarist/domination-and-control</i><br>
<i>&gt;culture is hardly the government at its best.</i><br>

<p>
<a name="1493qlink5">I think government aims at our best, not its best. Governments
(corporatations, religions, families, and other entities) function as
superorganisms, with their own continuity and longevity as their primary
objectives.</a>

<p>
<a href="1465.html#1477qlink7">&gt;So, my futurist friends, how do we extricate ourselves from this rather</a><br>
<i>&gt;tight spot?  Perhaps I see--dimly taking shape within the mists of Maya--a</i><br>
<i>&gt;way.  I don't know, it's hard to see.  Perhaps you can help to make it out?</i><br>

<p>
Go with the extropic flow. Relax and watch the comedy of errors parade.
<hr>
<br>
"I steal everything I know from the sources all around me."
--Honest Netizen
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1478.html">[ Next ]</a><a href="1476.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1465.html">Jeff Davis</a>
<!-- nextthread="start" -->
</ul>
</body></html>
