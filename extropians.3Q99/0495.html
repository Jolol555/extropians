<!-- received="Sat Jul 10 16:59:12 1999 MDT" -->
<!-- sent="Sat, 10 Jul 1999 18:56:13 -0400" -->
<!-- name="Michael S. Lorrey" -->
<!-- email="mike@lorrey.com" -->
<!-- subject="Re: SETSIs (was Re: seti@home WILL NOT WORK)" -->
<!-- id="3787CF8D.CEEEC37F@lorrey.com" -->
<!-- inreplyto="SETSIs (was Re: seti@home WILL NOT WORK)" -->
<!-- version=1.10, linesinbody=229 -->
<html><head><title>extropians: Re: SETSIs (was Re: seti@home WILL NOT WORK)</title>
<meta name=author content="Michael S. Lorrey">
<link rel=author rev=made href="mailto:mike@lorrey.com" title ="Michael S. Lorrey">
</head><body>
<h1>Re: SETSIs (was Re: seti@home WILL NOT WORK)</h1>
Michael S. Lorrey (<i>mike@lorrey.com</i>)<br>
<i>Sat, 10 Jul 1999 18:56:13 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#495">[ date ]</a><a href="index.html#495">[ thread ]</a><a href="subject.html#495">[ subject ]</a><a href="author.html#495">[ author ]</a>
<!-- next="start" -->
<li><a href="0496.html">[ Next ]</a><a href="0494.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0475.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
"Robert J. Bradbury" wrote:
<br>
<i>&gt; </i><br>
<a href="0475.html#0495qlink1">&gt; &gt; "Michael S. Lorrey" &lt;mike@lorrey.com&gt; wrote:</a><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; You are both wrongly assuming that all technological civilizations would</i><br>
<i>&gt; &gt; have similar exponent courves in technological/population development.</i><br>
<i>&gt; </i><br>
<i>&gt; I'm not making this assumption.  Dyson argued 40 years ago, even</i><br>
<i>&gt; if we slowed ourselves down to a 1% annual growth rate, we still</i><br>
<i>&gt; reach a power consumption level of the entire solar output</i><br>
<i>&gt; in only 3000 years!  Another SETI researcher made the point</i><br>
<i>&gt; it is difficult to imagine a society that could maintain a 0.000000...%</i><br>
<i>&gt; growth rate for thousands (or millions) of years (either you have an</i><br>
<i>&gt; accident or you decay away *or* you eventually evolve to your</i><br>
<i>&gt; environmental limits).</i><br>

<p>
A society may grow spatially and fill the volume of its solar system
with habitats, build a Niven ring or Dyson Sphere, may settle nearby
stars, and develop human equivalent artificial intelligence, yet not
NEED to develop computers beyond one or two magnitudes of human IQ.
<a name="0512qlink2">Technological advancement is not a goal in and of itself, it merely
serves to optimize human existence, and it does so at economically cost
effective rates. Dyson's major error in his calcuations was to assume no
increase in efficiency of utilization, which is not only crucial to make
further growth cost effective, but establishes resource re-utilization
at rates of greater efficiency as a cost effective strategy, eventually
concluding in a species being able to utilize resources at more and more
efficient rates.</a>

<p>
<a name="0512qlink3">Since our population growth rate is already at around 1%, if we increase
the education and intelligence of the population more, the growth rate
will fall even faster, possibly into the negative percentages much like
european countries (which is why they can sustain lower economic growth
rates and maintain standards of living with the US). As longevity
increases in a population, the desire to reproduce decreases, thus
reducing the necessary economic growth rates to maintain improving
standards of living.</a> 

<p>
<i>&gt; </i><br>
<a href="0475.html#0495qlink2">&gt; Growth is a fact of life.  It is built into Nature.  Population biologists</a><br>
<i>&gt; tell you that populations grow to the limits of the environment and then</i><br>
<i>&gt; crash when conditions change for the worse.  I would argue that it is</i><br>
<i>&gt; highly difficult for Nature to evolve "internal" limits.  The limits</i><br>
<i>&gt; are "imposed" by the environment.  Look what happens in situations</i><br>
<i>&gt; involving the introduction of a non-indigenous species (say rabbits</i><br>
<i>&gt; in Australia) -- if the environment is suitable the species expands to the</i><br>
<i>&gt; allowable limits.  In most other environments you don't see it</i><br>
<i>&gt; because everything is in balance between the predators and the prey</i><br>
<i>&gt; and the food resources and the reproduction rate.</i><br>

<p>
<a name="0512qlink4">Comparing the behavior of non-intelligent species to the long term
behavior of intelligent species if fallacious and fraudulent.</a>

<p>
<i>&gt; </i><br>
<a href="0475.html#0495qlink3">&gt; I would also argue that by definition "technological civilizations"</a><br>
<i>&gt; get on pretty exponential growth paths.  Humans didn't have exponential</i><br>
<i>&gt; growth (in fact we were barely surviving as a species) *until* we</i><br>
<i>&gt; developed the technologies that allowed us to manipulate the</i><br>
<i>&gt; environment in ways more sophisticated than our genetic program allowed.</i><br>

<p>
<a name="0512qlink5">And it is yet to be proven that some or all 'exponential growth paths'
are not paths of the third order, maxing out at some plateau, the
highest being that of light speed.</a>

<p>
<i>&gt; </i><br>
<a href="0475.html#0495qlink4">&gt; I believe you have to make a concrete case that a developing technological</a><br>
<i>&gt; species/civilization would consciously *choose* to terminate its growth.</i><br>
<i>&gt; That means that you have to negate the fundamental self-preservation</i><br>
<i>&gt; and/or reproductive instincts necessary for life.  As I've discussed</i><br>
<i>&gt; in other threads -- if you want to be immortal, you have to eliminate</i><br>
<i>&gt; reproduction -- if you want to reproduce, you have to choose to die</i><br>
<i>&gt; (or prevent the development of technologies that enable personal</i><br>
<i>&gt; immortality).  There *are* hard limits to growth.  There may be a few</i><br>
<i>&gt; examples of Vulcans in the galaxy, but they should not be in the majority</i><br>
<i>&gt; (the majority would seem to be those species that take as much as</i><br>
<i>&gt; they can and hold it the longest).   The exception to that would</i><br>
<i>&gt; appear to be cultures that follow a trans-humanist (trans-Natural-ist?)</i><br>
<i>&gt; path where they mentally/genetically engineer out the drives that</i><br>
<i>&gt; nature builds in.</i><br>

<p>
<a name="0512qlink6">Since greater intelligence and education result in lower population
growth rates, its not hard to imagine a future where the population
falls down to the levels of the previous hunter gatherer society (about
2 million per planet), yet each individual is of high intelligence, and
engages in much intellectual interaction outside of the feral/agronomist
lifestyle.</a> 

<p>
<i>&gt; </i><br>
<a href="0475.html#0495qlink5">&gt; As far as the exponential growth goes, we have a pretty good example</a><br>
<i>&gt; in the computer industry and Moore's law (before that it might</i><br>
<i>&gt; have been the industrial revolution and before that agriculture).</i><br>
<i>&gt; Can you make a good argument that any of these paths could have</i><br>
<i>&gt; been "consciously" arrested?  If you want to volunteer to stop</i><br>
<i>&gt; the $200 Billion/year+ electronics industry, I'll be happy to</i><br>
<i>&gt; sit back and watch.  If you can't stop it, then Moravec/Minsky</i><br>
<i>&gt; would seem to have a case -- we may not know how to create</i><br>
<i>&gt; intelligence (other than the good ole natural way), but if</i><br>
<i>&gt; we keep at it long enough we should figure it out.  Biotech</i><br>
<i>&gt; enabled super-longevity and nanotech enabled ultra-longevity</i><br>
<i>&gt; would seem to fall under the same development principles.</i><br>

<p>
<a name="0512qlink7">Moore's Law has yet to be shown to have no upper limit. Better yet,
light speed itself puts a limit on maximum growth of computational
technology. Making Moore's Law a Holy Mantra is an error of faith. Not
very scientific.</a>

<p>
<i>&gt; </i><br>
<a href="0475.html#0495qlink6">&gt; &gt; You are assuming that EVERY society will want to transcend, rather than</a><br>
<i>&gt; &gt; just staying at a comfortable early 21st century level.</i><br>
<i>&gt; </i><br>
<i>&gt; The environmental movement has been trying for 30-40 years to</i><br>
<i>&gt; "stop" our growth without much success.  The primary reason</i><br>
<i>&gt; is hasn't worked is that we can siphon off a fraction of our</i><br>
<i>&gt; productivity growth and technological capacities and</i><br>
<i>&gt; apply these to solving the environmental problems.  It is</i><br>
<i>&gt; pretty clear at this point that we can develop the</i><br>
<i>&gt; technologies to expand to the limits allowed on the planet</i><br>
<i>&gt; and then off the planet.  That realization should occur</i><br>
<i>&gt; in any other technological civilization as well (if you</i><br>
<i>&gt; wait long enough).</i><br>
<i>&gt; </i><br>
<i>&gt; Dr. Hekimi (the discover of the clk gene in nemetodes), once</i><br>
<i>&gt; made the comment to me -- "if man can imagine it and it</i><br>
<i>&gt; is possible, sooner or later he will do it".  That seems</i><br>
<i>&gt; very to true me, it seems to arise from the nature</i><br>
<i>&gt; of competition and the direct or indirect advantages</i><br>
<i>&gt; one derives from creating something new, different or better.</i><br>

<p>
<a name="0512qlink8">Your error is to assume that it will happen sooner rather than later.
Much the same errors the early christians made in expecting the 2nd
coming in their lifetimes. Sounds to much like a religious attitude</a> to
me.

<p>
<i>&gt; </i><br>
<a name="0512qlink9"><a href="0475.html#0495qlink7">&gt; If evolving to the limit of physics *is* feasible, and "life"</a><br>
<i>&gt; is designed to "evolve", can you make a case for the cessation</i><br>
<i>&gt; of evolution?</i><br>

<p>
Evolution is a result of pressures against the survival of the
individual. Once the individual is practially immortal and intelligent
enough to handle most eventualities in the physical world, they have no
further need of evolution.</a>

<p>
<i>&gt; </i><br>
<a href="0475.html#0495qlink8">&gt; &gt; You are also wrongly assuming that following a singularity by some</a><br>
<i>&gt; &gt; percentage of the population that the rest of the population just</i><br>
<i>&gt; &gt; dissapears.</i><br>
<i>&gt; No, not really.  It doesn't matter in my mind whether</i><br>
<i>&gt;   (1) Bill Gates turns himself into an M-Brain and turns off the</i><br>
<i>&gt;       sun on the rest of us.</i><br>
<i>&gt;   (2) We all (every single individual who wants it) turns themselves</i><br>
<i>&gt;       into a unified collective M-brain and</i><br>
<i>&gt;         (a) Takes the Hydrogen in Jupiter and leaves the solar system,</i><br>
<i>&gt;             leaving behind the luddites who didn't want to join us.</i><br>
<i>&gt;         (b) Dismantles every single aggregate of atoms in the solar</i><br>
<i>&gt;             system (other planets, asteroids, earth (and the luddites</i><br>
<i>&gt;             on it), the sun, etc.) for reformation into an optimal</i><br>
<i>&gt;             computational architecture.</i><br>

<p>
<a name="0512qlink10">Your points here illustrate the proof of my statement. You refuse to
acknowledge that once some part of the population 'transcends' (quotes
being to denote your mystical attitude toward that condition), that the
remaining population will not automatically be transcended or be
destroyed. That we have stone age cultures in coexistence with our own
right now illustrates the fallacy of your argument. Your argument
assumes a level of evil and callousness in the motives of transcended
beings that I personally would take as an argument to stamp out all
efforts to transcend.</a>

<p>
<i>&gt; </i><br>
<a href="0475.html#0495qlink9">&gt; The point would be that in in both (1) and (2) you still get an M-Brain</a><br>
<i>&gt; and M-Brains seem to have lifetimes of the order of the longevity of</i><br>
<i>&gt; the universe.  In 2a the luddites probably have a maximum lifetime</i><br>
<i>&gt; of a few billion years (until the sun becomes a red-giant), unless</i><br>
<i>&gt; they decide to move the planet or "manage" the sun (then they aren't</i><br>
<i>&gt; luddites any more).  Since the M-Brains are now at the top of the</i><br>
<i>&gt; evolutionary ladder (biggest, most intelligent, longest lived, able</i><br>
<i>&gt; to anticipate and avoid any potential hazards, etc.) they have to become</i><br>
<i>&gt; the most populous "species".  [Survival of the fittest.]</i><br>
<i>&gt; </i><br>
<i>&gt; M-brains don't *have* to harvest or dismantle any of the</i><br>
<i>&gt; luddites or their star (there is plenty of other material around</i><br>
<i>&gt; from which to construct and power themselves at the time of the</i><br>
<i>&gt; singularity).  Whether they chose to behave that way may depend a</i><br>
<i>&gt; lot on the path by which they develop -- a self-evolving</i><br>
<i>&gt; AI with no "moral" code probably would consume us to optimize</i><br>
<i>&gt; itself, on the other hand if the M-brain is constructed from</i><br>
<i>&gt; uploads of us, it might harbor some nostalgia towards the Earth</i><br>
<i>&gt; and/or the sun and leave them intact.</i><br>

<p>
<a name="0512qlink11">You fallaciously assume that an AI will not develop a moral code, as if
there is no objective morality. Sorry, null program.</a>

<p>
<i>&gt; </i><br>
<a href="0475.html#0495qlink10">&gt; For all of this not to happen, I believe you have to make the case</a><br>
<i>&gt; that substantially all of the individuals who are members of an</i><br>
<i>&gt; evolving technological species (on the slippery slope towards the</i><br>
<i>&gt; singularity), universally decide -- "This far and no further".</i><br>
<i>&gt; The "anti-technology-police" would have to enforce the decision on the</i><br>
<i>&gt; non-believers.  As Ben Bova has pointed out in his recent "Immortality"</i><br>
<i>&gt; book, that is a very difficult thing to do because of the benefits</i><br>
<i>&gt; one personally derives from breaking the rules.  [BTW, this book</i><br>
<i>&gt; is worth reading -- see my review comment on Amazon.]</i><br>

<p>
<a name="0512qlink12">Read it. You still are evading the point. Just as there are stone age
cultures alive and functioning today, 20th century cultures will survive
into and beyond the time of any supposed date of 'singularity'. You need
to stop looking at it as some sort of Day of Rapture that all will
participate in or go to hell.</a>

<p>
<i>&gt; </i><br>
<a href="0475.html#0495qlink11">&gt; [Yes, a technological civilization, might consist of a species</a><br>
<i>&gt; that has a single or collective mind (instead of a collection</i><br>
<i>&gt; of individual minds), but are they all?]</i><br>
<i>&gt; </i><br>
<i>&gt; You may believe that an M-brain is a bad idea, but I assure you,</i><br>
<i>&gt; that if I get my hands on a nanoassembler first, I'm not stopping</i><br>
<i>&gt; until I've got around 10^20 distributed replicated copies of myself</i><br>
<i>&gt; [that leaves room for anyone else that wants to hop on the boat,</i><br>
<i>&gt; since the idea of talking to that many copies of myself for the</i><br>
<i>&gt; next 100 billion years or so seems really unpleasant... :-)].</i><br>

<p>
<a name="0512qlink13">If you do so and do not accept any objective morality of survival and
coexistence, then I will be sure to nuke you before you attain your
goal. Kapisch?</a>

<p>
Mike Lorrey
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0496.html">[ Next ]</a><a href="0494.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0475.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
</ul>
</body></html>
