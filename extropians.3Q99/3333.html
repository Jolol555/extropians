<!-- received="Wed Sep  1 23:53:04 1999 MDT" -->
<!-- sent="Thu, 2 Sep 1999 15:52:32 +1000 " -->
<!-- name="O'Regan, Emlyn" -->
<!-- email="Emlyn.ORegan@actew.com.au" -->
<!-- subject="RE: Singularity?" -->
<!-- id="65FD40142926D011AAB208002BE22D3201FCE0A3@mailcivic1.actew.oz.au" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=45 -->
<html><head><title>extropians: RE: Singularity?</title>
<meta name=author content="O'Regan, Emlyn">
<link rel=author rev=made href="mailto:Emlyn.ORegan@actew.com.au" title ="O'Regan, Emlyn">
</head><body>
<h1>RE: Singularity?</h1>
O'Regan, Emlyn (<i>Emlyn.ORegan@actew.com.au</i>)<br>
<i>Thu, 2 Sep 1999 15:52:32 +1000 </i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3333">[ date ]</a><a href="index.html#3333">[ thread ]</a><a href="subject.html#3333">[ subject ]</a><a href="author.html#3333">[ author ]</a>
<!-- next="start" -->
<li><a href="3334.html">[ Next ]</a><a href="3332.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3325.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a href="3325.html#3333qlink1">&gt; The determining factor on this issue is how hard AI and intelligence</a><br>
<i>&gt; enhancement turn out to be.  If intelligence is staggeringly complex, and</i><br>
<i>&gt; requires opaque data structures that are generally inscrutable to beings</i><br>
<i>&gt; of</i><br>
<i>&gt; human intelligence, we get one kind of future (nanotech and human</i><br>
<i>&gt; enhancement come online relatively slowly, AI creeps along at a modest</i><br>
<i>&gt; rate,</i><br>
<i>&gt; and enhanced humans have a good shot at staying in charge).</i><br>
<i>&gt; </i><br>
Are you saying that in this scenario, there is no real AI? That may be the
case, which would be a sad thing in my opinion.

<p>
If you are saying that AI is possible but (*really*) hard, you still have
the situation that when just more than human-level intelligence is reached
in AIs, it has been built by humans. Thus the AI is at more competent than
the people who created it, and so is competent, one would suppose, to take
the work further, unless human intelligence is the absolute pinnacle of
intelligence (maybe this is true), or there is some huge qualitative
discontinuity between human intelligence and the next higher stable form of
intelligence.

<p>
It's hard to imagine that, given human equivalent AI, it could be
prohibitively difficult to make a more intelligent being. Intuitively,
throwing in fast enough hardware should be equivalent to a qualitative leap
in intelligence, given a big enough difference between normal speed and fast
(say 1000 times for an absolute guess?)

<p>
<i>&gt; If intelligence</i><br>
<a href="3325.html#3333qlink2">&gt; can be achieved using relatively comprehensible programming techniques,</a><br>
<i>&gt; such</i><br>
<i>&gt; that a sentient AI can understand its own operation, we get a very</i><br>
<i>&gt; different</i><br>
<i>&gt; kind of future (very fast AI progresss leading to a rapid Singularity,</i><br>
<i>&gt; with</i><br>
<i>&gt; essentially no chance for humanity to keep up).  Either way, the kind of</i><br>
<i>&gt; future we end up in has absolutely nothing to do with the decisions we</i><br>
<i>&gt; make.</i><br>
<i>&gt; </i><br>
Just some quibbles (yeah, I'll be first against the wall when the
singularity comes - Hey Mr AI, you've got something hanging out of your
nose, ha ha, hey, who turned out the lights, ACKCKK (gurgle))

<p>
Emlyn
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3334.html">[ Next ]</a><a href="3332.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3325.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
</body></html>
