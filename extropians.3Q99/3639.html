<!-- received="Mon Sep  6 14:35:27 1999 MDT" -->
<!-- sent="Mon, 06 Sep 1999 15:36:56 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Is this world a computer simulation?" -->
<!-- id="37D425DA.206C11CF@pobox.com" -->
<!-- inreplyto="Is this world a computer simulation?" -->
<!-- version=1.10, linesinbody=66 -->
<html><head><title>extropians: Re: Is this world a computer simulation?</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Is this world a computer simulation?</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Mon, 06 Sep 1999 15:36:56 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3639">[ date ]</a><a href="index.html#3639">[ thread ]</a><a href="subject.html#3639">[ subject ]</a><a href="author.html#3639">[ author ]</a>
<!-- next="start" -->
<li><a href="3640.html">[ Next ]</a><a href="3638.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3630.html">Matt Gingell</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Matt Gingell wrote:
<br>
<i>&gt; </i><br>
<a href="3630.html#3639qlink1">&gt; This is an interesting line of speculation, but not one that I think is really</a><br>
<i>&gt; worth worrying about. Given the amount of horror humanity’s gone through this</i><br>
<i>&gt; century, if we are being simulated by an intelligence interested in minimizing</i><br>
<i>&gt; suffering, then either it has fundamental reasons for not getting involved – it</i><br>
<i>&gt; doesn’t want to damage the integrity of the simulation, for instance – or it’s</i><br>
<i>&gt; motivations are sufficiently inscrutable to make discussion pointless. If we</i><br>
<i>&gt; were going to raise a general morality violation, we would have done so by now –</i><br>
<i>&gt; the sky would have turned dark blue and novas would have lined up to form a</i><br>
<i>&gt; register dump and the vendors 800 number.</i><br>

<p>
<a name="3656qlink1">Which makes sense, except that I'm speculating that the Way Things Are
has "evolved" to maximize the reproductive rate of the simulations.  In
which case, there's an obvious adaptive selection pressure for rewriting
nonconformist Powers - or, on the very dimly bright side, stopping
nanodisasters - that doesn't exist for stopping random suffering or
optimizing for pleasure.  Such worlds may exist, but they don't
reproduce.  Besides, at least one major sub-hypothesis is that the
Powers involved are insane.</a>

<p>
On the other hand, one has to wonder at forty thousand years of
nontechnological history and fifteen billion years of simulation
dangling uselessly, assuming all that was real, which my intuitions say
they are.  Oh, hell.  Who am I to pretend that I have any idea what
constitutes a "selection pressure" when it's acting on a recursive chain
of gods, most of which are probably insane?  I don't even know what the
Anthropic Principle means any more; I'm not sure how to compute the
relative probabilities.

<p>
<a href="3630.html#3639qlink2">&gt; If pleasure were being maximized, we’d be disembodied strings of code floating</a><br>
<a name="3656qlink2"><i>&gt; in virtual tanks full of virtual opiates. If pain were being minimized, we</i><br>
</a><i>&gt; wouldn’t be here. If an optimal compromise between the two had been found, there</i><br>
<i>&gt; ’d be nothing in the universe but endless mirrors of Earth. The incredibly</i><br>
<i>&gt; arbitrary nature of the universe at a macroscopic level makes me doubt there’s a</i><br>
<a name="3656qlink4"><i>&gt; God paying attention to us, synthetic or otherwise.</i><br>

<p>
Oh, you mean the way that if Powers exist they should expand at
lightspeed, and if alien races exist they should expand pretty fast
anyway, and Earth's sun is hardly old, so therefore intelligent life is
impossible and we aren't here?</a>

<p>
<a name="3656qlink5">"Unlightenment":  The stage at which you know so much about the
Universe, and you've accumulated so much to be explained and have
learned to create such rigorous explanations, that you have more
constraints than degrees of freedom - and they don't fit.  I cannot
think of *any* good explanation for the Great Filter Paradox.  My
visualization of the Universe has now reached the point where it
contains active impossibilities.  I give up.  Did I mention that I've
given up?  Well, I've given up.  All I bloody know about the bloody
facts is that the bloody answer is probably bloody obvious to any entity
with half a bloody brain, so I'm going to treat the bloody world like
it's actually real, bloody impossibilities and all, and try to
manipulate the observed bloody regularities so a transhuman pops out, at
which point my *bloody* job is bloody well OVER.</a>

<p>
<a name="3656qlink6"><a href="3630.html#3639qlink3">&gt; ps. You used the word 'culture' with a capital C the other day. Was</a> that an Iain</a><br>
<a name="3656qlink7"><i>&gt; Banks reference?</i><br>
</a>
<p>
Hm.  Where?
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3640.html">[ Next ]</a><a href="3638.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3630.html">Matt Gingell</a>
<!-- nextthread="start" -->
</ul>
</body></html>
