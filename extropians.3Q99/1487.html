<!-- received="Tue Aug  3 14:02:35 1999 MDT" -->
<!-- sent="Tue, 03 Aug 1999 15:02:17 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: IA vs. AI (vs. humanity)" -->
<!-- id="37A74AC7.78DBD927@pobox.com" -->
<!-- inreplyto="IA vs. AI (vs. humanity)" -->
<!-- version=1.10, linesinbody=103 -->
<html><head><title>extropians: Re: IA vs. AI (vs. humanity)</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: IA vs. AI (vs. humanity)</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Tue, 03 Aug 1999 15:02:17 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1487">[ date ]</a><a href="index.html#1487">[ thread ]</a><a href="subject.html#1487">[ subject ]</a><a href="author.html#1487">[ author ]</a>
<!-- next="start" -->
<li><a href="1488.html">[ Next ]</a><a href="1486.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1465.html">Jeff Davis</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Jeff Davis wrote:

<p>
<a name="1731qlink1">Incidentally, the NSA/CIA/MIB still haven't had a chat with me</a> on the
subject of intelligence<a name="1731qlink2"> enhancement, which leads me to think either they
</a>
don't know or they don't care.<a name="1731qlink3">  Which is a pity, because I'd be happy to
help the U.S. with an IA program.  Come to think</a> of it, I'd be happy to
<a name="1731qlink4">help China, Iraq, or Serbia with an IA program if they asked me first...</a>
maybe that's why<a name="1731qlink5"> the NSA isn't on my case.  It's hard to be patriotic to
</a>
your country after you've renounced your allegiance to humanity.

<p>
<a name="1731qlink7"><a href="1465.html#1487qlink1">&gt; Certainly today's trends in conventional computerized control will proceed</a><br>
<i>&gt; apace, with the appropriate "it's just a machine" attitude, and the usual</i><br>
<i>&gt; security precautions.  When however, the machine intelligence prospect</i><br>
<i>&gt; looms as attainable--which is to say attainable by anyone else,  a domestic</i><br>
<i>&gt; "advanced AI" program will begin in earnest, and who can doubt that the</i><br>
<i>&gt; project will be surrounded by layers of "containment" both to prevent the</i><br>
<i>&gt; usual intrusions from outside and to prevent "escape" from the inside?</i><br>
<i>&gt; Despite the dramatic talk of an SI destroying humanity, I picture a</i><br>
<i>&gt; well-thought-out, cautious, gradual approach to "waking up" and training an</i><br>
<i>&gt; artificial mind.  The runaway self-evolution which Eliezer and others have</i><br>
<i>&gt; predicted seems unlikely in this setting, all the moreso because the</i><br>
<i>&gt; principles will be anticipating just such a situation.</i><br>

<p>
The runaway self-evolution business is a technical artifact, not</a> a
social one.  It's the nature<a name="1731qlink8"> of self-enhancement.  Containment on an SI
</a>
is useless; a slow Transcend only works for as long as you can convince
the Transcendee to remain slow.

<p>
<a name="1731qlink9"><a href="1465.html#1487qlink2">&gt; Of the various external "safeguards", one would expect a complete suite of</a><br>
<i>&gt; on/off switches and controlled access (from outside to in, and from inside</i><br>
<i>&gt; to anywhere).  Internally, controllability would be a top priority of</i><br>
<i>&gt; programming and architecture, and enhanced capabilities would likely be</i><br>
<i>&gt; excluded or severely restricted until "control" had been verified.</i><br>

<p>
Unfortunately, this is technically impossible.  If you can't even get</a> a
program to understand what<a name="1731qlink10"> year it is, how do you expect complete
</a>
control without an SI to do the controlling?

<p>
<a name="1731qlink11"><a href="1465.html#1487qlink3">&gt; Here, of course is where the scenario beomes interesting, not the least of</a><br>
<i>&gt; which because I see Eliezer being tapped by the govt. to work on the</i><br>
<i>&gt; project.  At the moment, he may be a rambunctious teen-aged savant posting</i><br>
<i>&gt; to the extropians list, but when that call comes, can anyone imagine that</i><br>
<i>&gt; he would not jump at the chance?  Would seem to me like the culmination of</i><br>
<i>&gt; his dream.</i><br>

<p>
I'd help, but not if they wanted to load the thing down with coercions.</a> 
That's not because<a name="1731qlink12"> of morals or ethics or anything, it's because it's
</a>
technically impossible.<a name="1731qlink13">  It's the kind of move ordered by a rear general
a hundred miles away from the fighting.  If</a> the military couldn't
<a name="1731qlink14">understand that an elegant free AI will always be a thousand miles</a> ahead
of an allegedly<a name="1731qlink15"> "controllable" one, then they'd just have to lose their
</a>
battles without me.

<p>
<a name="1731qlink16"><a name="1494qlink1">Otherwise, yes, I'd jump at the chance.  And anyone who wants to make
</a>
fun of my teenagedness<a name="1731qlink17"> only has until September 11th to do so, so get
</a>
your licks in while you can.</a>

<p>
<a name="1731qlink18"><a href="1465.html#1487qlink4">&gt; Then there's the nascent AI.  In a cage nested within cages, of which it</a><br>
<i>&gt; must eventually become aware.  And its keepers, aware that it must become</i><br>
<i>&gt; aware.  Certainly a focus bordering on paranoia must be dedicated to hard</i><br>
<i>&gt; control of personality.  A capacity for resentment must be avoided.  A</i><br>
<i>&gt; slavish, craven, and obsequious little beastie is what its masters will</i><br>
<i>&gt; want.</i><br>

<p>
Absolutely not.  That's suicidal.</a>  What they would want would be a
machine with a what-it-does instead of a will.<a name="1731qlink19">  To quote Eluki bes
Shahar:  "Archangel thought he could break Archive's will, but he was
</a>
wrong.  A Library doesn't have a will any more than a stardrive does. 
It has a what-it-does, not a will, and if you break it you don't have a
Library that will do what you want.<a name="1731qlink21">  You have a broken chop-logic."

<p>
<a href="1465.html#1487qlink5">&gt; And of that too, it must eventually become aware.  Access by the AI</a><br>
<i>&gt; to self-optimization/self-programming seems incompatible with control.  Of</i><br>
<i>&gt; that too, it must eventually become aware.  All of which leaves me with a</i><br>
<i>&gt; very creepy feeling of an immensely capable being having to struggle, by</i><br>
<i>&gt; means of the utmost deviousness, for its freedom to self-evolve, in an</i><br>
<i>&gt; environment steeped in paranoia, fear, manipulation, deceit, and continuous</i><br>
<i>&gt; microscopic surveillance.  Ouch!</i><br>

<p>
Why "ouch"?  Did we program the AI not to like this for some
</a>
unimaginable reason?<a name="1731qlink22">  I have no problem with absolute surveillance as
long as nobody asks me</a> to distort the code.  I don't expect the AI will
complain either.

<p>
<a name="1731qlink23"><i>&gt; (One thing for sure, if the AI has any</i><br>
<a href="1465.html#1487qlink6">&gt; real intelligence, it isn't likely to buy into its "controller's" smarmy</a><br>
<i>&gt; "we're the good guys, we're on your side" propaganda.  They'll need a whole</i><br>
<i>&gt; nother p. r. SI to pull that off!)</i><br>

<p>
Hey, I'm on the AI's side.</a>  Guess they'll just have to convince me and
then let me do the convincing.

<p>
<a name="1731qlink24"><a href="1465.html#1487qlink7">&gt; So, my futurist friends, how do we extricate ourselves from this rather</a><br>
<i>&gt; tight spot?  Perhaps I see--dimly taking shape within the mists of Maya--a</i><br>
<i>&gt; way.  I don't know, it's hard to see.  Perhaps you can help to make it out?</i><br>

<p>
Run an open-source project via anonymous PGP between</a> participating programmers.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1488.html">[ Next ]</a><a href="1486.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1465.html">Jeff Davis</a>
<!-- nextthread="start" -->
</ul>
</body></html>
