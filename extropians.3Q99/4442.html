<!-- received="Sat Sep 18 19:28:30 1999 MDT" -->
<!-- sent="Sat, 18 Sep 1999 20:30:14 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Artilects &amp; stuff" -->
<!-- id="37E43CA5.DE32415B@pobox.com" -->
<!-- inreplyto="Artilects &amp; stuff" -->
<!-- version=1.10, linesinbody=204 -->
<html><head><title>extropians: Re: Artilects &amp; stuff</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Artilects &amp; stuff</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sat, 18 Sep 1999 20:30:14 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4442">[ date ]</a><a href="index.html#4442">[ thread ]</a><a href="subject.html#4442">[ subject ]</a><a href="author.html#4442">[ author ]</a>
<!-- next="start" -->
<li><a href="4443.html">[ Next ]</a><a href="4441.html">[ Previous ]</a>
<b>In reply to:</b> <a href="4379.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
den Otter wrote:
<br>
<i>&gt; </i><br>
<a href="3431.html#4442qlink1">&gt; &gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</a><br>
<i>&gt; &gt; Okay, suppose I accept this.  I still don't see why any SI will</i><br>
<i>&gt; &gt; automatically drop off all its emotions *except* selfishness and then</i><br>
<i>&gt; &gt; you think this is a good thing.  That's really where you lost me.</i><br>
<i>&gt; </i><br>
<i>&gt; Well, unless suicide is the big meaning of life (and the SI actually</i><br>
<i>&gt; gives a hoot about such things), it will need to retain</i><br>
<i>&gt; self-preservation in its mental structure. You need to be alive to do</i><br>
<i>&gt; things. I'm not saying that the SI will drop "all of its emotions", btw.</i><br>
<i>&gt; More likely it would modify them, and/or get rid of some, and/or add new</i><br>
<i>&gt; ones.Or perhaps it *would* get rid of them, but that's just one of many</i><br>
<i>&gt; options.</i><br>

<p>
Okay, but it wouldn't need to have an all-consuming "survival" goal,
much less a goal of "growth".  For example, the only reason why it would
have a survival goal could be to fulfil its ultimate supergoal of "Help
den Otter have ultimate fun."  Or even "Survive long enough to upload
den Otter, then commit suicide."

<p>
Are these goals impossible or unstable?  Why?  If one set of goals is
intrinsically irrational, it's not a very long step to conclude that a
single goal is maximally rational and all SIs converge to it, and it's a
pretty short step from there to Externalism.

<p>
And if they're not irrational - *I* think they are, but then I'm an
Externalist; you aren't - why not a Servant AI?

<p>
<a href="4379.html#4442qlink2">&gt; &gt; But why not?  There's one emotional whim, "must find the truth".  And</a><br>
<i>&gt; &gt; another emotional whim, "stay alive".  And another, "be happy".  In me</i><br>
<i>&gt; &gt; the first one is stronger than the other two.  In you the two are</i><br>
<i>&gt; &gt; stronger than the one.  I don't get the part where your way is more</i><br>
<i>&gt; &gt; rational than mine, especially if all this is observer-dependent.</i><br>
<i>&gt; </i><br>
<i>&gt; What it all comes down to is scoring "satisfaction points", correct?</i><br>

<p>
No.  Precisely and exactly wrong.  How many satisfaction points does the
concept of "scoring satisfaction points" gain you?

<p>
<a href="4379.html#4442qlink3">&gt; That's what drives us on. Even you. You set more or less random</a><br>
<i>&gt; goals, and reaching those goals will give you satisfaction (or remove</i><br>
<i>&gt; frustration) and thus generate points.</i><br>

<p>
Maybe that's the way the human mind is built - to treat projected future
satisfaction as present satisfaction, and present satisfaction as the
arbiter of choices - but surely you won't deny me the right to alter
that?  To unbind the semantics of the mind?  There are cognitive
elements in this game other than simple satisfaction, and they can
affect each other, build self-sustaining loops that arbitrate choices
directly.  To you, I suppose, it would seem an unnatural and illogical
knot, something like circular logic, a twisted takeover by elements of
rationality that should rightfully be serving emotion.  But I did it. 
I'm free.

<p>
<i>&gt; Ok, now that we've determined</i><br>
<a href="4379.html#4442qlink4">&gt; that we're in the same race, we can compare strategies. Some</a><br>
<i>&gt; strategies will get you more points than others. For you, finding the</i><br>
<i>&gt; truth (or creating a SI which may do that) is obviously worth a lot of</i><br>
<i>&gt; points, but, and this is a key issue, it isn't the *only* thing that</i><br>
<i>&gt; is worth points (to you).</i><br>

<p>
It's the only thing whose point-worth I choose to acknowledge - whose
future point-worth is worth points to me.  This kind of arbitrary power
over the present may be harder to enforce, as you note below - but it
isn't too hard with respect to the mere projection of emotional satisfaction.

<p>
&gt; Other, more "mundane" activities, like<br>
<a href="4379.html#4442qlink5">&gt; watching _Buffy the Vampire Slayer_, are also a source of points.</a><br>

<p>
I know that all work and no play will cause me to chip and crack like a
cheap plate.  Me especially, emphasis on "Special"...  Perhaps I've
struck and rationalized the wrong happy medium, influenced by the
built-in point-allocation systems, but that's hardly the same as
accusing me of engaging in non-Singularitarian activities.

<p>
Besides, even _Buffy_ serves my purposes directly.  I've been
experimenting with daimones, _Aristoi_ style.  Also known as LPs
(Limited Personalities), morphs, and switchable self-symbols. 
Apparently watching this show for nine hours straight (I was on
vacation) has interesting side effects.

<p>
<a href="4379.html#4442qlink6">&gt; If all goals are essentially equal (in that they are ways to get</a><br>
<i>&gt; satisfaction/reduce discomfort), then the logical thing is to</i><br>
<i>&gt; pick a strategy that will earn you maximal emotional currency.</i><br>

<p>
Big *if*.  If all goals are essentially equal, you can program an SI
with whatever goals you like and get a Servant.  If all goals are
essentially equal, than earning minimal emotional currency is precisely
as logical as earning maximal emotional currency.

<p>
You've made a big deal out of asking me to acknowledge that my
Externalist principles are driven simply by an emotional desire to find
the truth, some "higher meaning" to life.  Why can't you acknowledge
that maximizing projected future emotional satisfaction is driven by a
desire to maximize future satisfaction?  If you deny me the
representational binding between my mental model of the satisfaction of
unspecified External goals and whatever specific External goals may or
may not exist, I can deny you the binding between your future
satisfactions and your current desires to maximize projected future satisfactions.

<p>
<a href="4379.html#4442qlink7">&gt; Short but dangerous thrills like smoking Crack cocaine or</a><br>
<i>&gt; spawning SIs may get you a lot of points at once, but then</i><br>
<i>&gt; you die and that's it, no more points. If you settle (temporarily)</i><br>
<i>&gt; for lesser pleasures (less points), but survive (much) longer,</i><br>
<i>&gt; you will ultimately gain a lot more points, perhaps even</i><br>
<i>&gt; an infinite amount (the happy god scenario). You win!</i><br>

<p>
Once again, you're assuming a semantic binding between future and
present that I've quite deliberately broken.  Suppose that I take over
your brain, and through dark neurosurgeries cause you to be tremendously
satisfied, emotionally satiated, by hapless servitude to me.  Would you
value this more than the simple pleasure of, oh, eating ice cream?  Even
though the absolute emotional points are orders higher?  No, because you
view it as "invalid" satisfaction points.  The binding is broken.  I've
simply done the same thing on a much larger scale.

<p>
<a href="4379.html#4442qlink8">&gt; To recap, the best thing for you would be to drop your</a><br>
<i>&gt; relatively dangerous goal of creating an ASI, and compensate</i><br>
<i>&gt; for the loss of (anticipation-fun) points by concentrating on</i><br>
<i>&gt; other fun stuff, and/or modifying your mental structure (if</i><br>
<i>&gt; possible) so that "finding the truth" is linked to uploading</i><br>
<i>&gt; and living forever. This new memeplex could fill the void left</i><br>
<i>&gt; by the previous one ("Singularity at all costs").</i><br>

<p>
But this talk of voids and filling uninterests me.  My goal isn't
emotional satisfaction, it's doing whatever is rationally correct.

<p>
<a href="4379.html#4442qlink9">&gt; I don't have any "grand" goals myself, really. Ok, so of course</a><br>
<i>&gt; I want to become immortal, god-like, explore all of reality and</i><br>
<i>&gt; beyond etc. etc., but that's hardly the meaning of life. I don't</i><br>
<i>&gt; really care about the meaning of life. Maybe it exists, maybe</i><br>
<i>&gt; not. Maybe I'll find it, maybe not. Who cares; I've done just fine</i><br>
<i>&gt; without it so far, and I don't see why this would have to change</i><br>
<i>&gt; in the future.</i><br>

<p>
You still believe in yourself and your world.  You are not sufficiently confused.

<p>
<a href="4379.html#4442qlink10">&gt; &gt; I particularly don't get the part where an SI converges to your point of</a><br>
<i>&gt; &gt; view.  And if it doesn't converge, why not build an SI whose innate</i><br>
<i>&gt; &gt; desires are to make you happy, and everyone else who helped out on the</i><br>
<i>&gt; &gt; project happy, and everyone on the planet happy, thus hopscotching the</i><br>
<i>&gt; &gt; whole uploading technological impossibility and guaranteeing yourself a</i><br>
<i>&gt; &gt; much larger support base?</i><br>
<i>&gt; </i><br>
<i>&gt; Is this a joke? I mean, *this* coming from *you*?</i><br>

<p>
It would be a joke, coming from me.  The question I keep asking is - why
isn't it coming from you?  I mean, why *don't* you believe this?

<p>
<a href="4379.html#4442qlink11">&gt; See above. Ultimately goals are about avoiding "bad" feelings and</a><br>
<i>&gt; generating good ones. Punishment &amp; reward. Reason is just a tool</i><br>
<i>&gt; to get more reward than punishment. We should always keep this</i><br>
<i>&gt; in mind when selecting "random" goals.</i><br>

<p>
I wish I was a clever guy like you and could know stuff like that.  I'd
feel so much more confident if I thought I understood the Universe that
well.  [Sarcasm.]

<p>
<a href="4379.html#4442qlink12">&gt; I'm "pretty sure", but of course I can never be "completely sure"</a><br>
<i>&gt; either. It doesn't matter; I can't really lose with my approach.</i><br>

<p>
Of course you can.  You can dither over uploading and sabotage AI and
get eaten by grey goo as a result, when all along SIs would have
converged to friendliness and freely upgraded you to godhood.  Your
philosophy implies actions, and actions can *always* go wrong.  I
acknowledge the risks I'm taking; why can't you?

<p>
<i>&gt; I'd</i><br>
<a href="4379.html#4442qlink13">&gt; have forever to decide what to do next. You on the other hand</a><br>
<i>&gt; are betting everything on a force that, once released, is totally</i><br>
<i>&gt; beyond your control.</i><br>

<p>
Ask me if I think there are *any* controllable forces in the game... 
You put too much faith in the essential benevolence of the Universe.

<p>
<i>&gt; My way (careful, gradual uploading) is</i><br>
<a href="4379.html#4442qlink14">&gt; better because it would allow me to have some measure of</a><br>
<i>&gt; control over the situation. Having control increases the</i><br>
<i>&gt; chances of achieving your goals, whatever they may be.</i><br>

<p>
So does acknowledging reality.  I might also go for gradual uploading,
if I thought it was practically possible.  It ain't.

<p>
<a href="4379.html#4442qlink15">&gt; &gt; I'll certainly agree with that part.  Like I said, the part I don't get</a><br>
<i>&gt; &gt; is where you object to an AI.</i><br>
<i>&gt; </i><br>
<i>&gt; The AI may decide that it doesn't want me around. The AI is</i><br>
<i>&gt; only useful from *my* pov if it helps me to transcend, and I</i><br>
<i>&gt; don't think that's very likely; I'm probably just a worthless bug</i><br>
<i>&gt; from *its* pov.</i><br>

<p>
Once again - if you, den Otter, program the AI with a servile point of
you, why do *you*, all-goals-are-equal guy, think it would converge to
anything else?

<p>
And I note you still haven't answered my future-past emotional-unbinding
demonstration - you know, the "This scenario is worth 20 points" business.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="4443.html">[ Next ]</a><a href="4441.html">[ Previous ]</a>
<b>In reply to:</b> <a href="4379.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
</body></html>
