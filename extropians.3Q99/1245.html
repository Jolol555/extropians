<!-- received="Tue Jul 27 18:00:15 1999 MDT" -->
<!-- sent="Tue, 27 Jul 1999 19:30:42 +0200" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: IA vs. AI was: longevity vs singularity" -->
<!-- id="199907272359.QAA27448@geocities.com" -->
<!-- inreplyto="IA vs. AI was: longevity vs singularity" -->
<!-- version=1.10, linesinbody=218 -->
<html><head><title>extropians: Re: IA vs. AI was: longevity vs singularity</title>
<meta name=author content="den Otter">
<link rel=author rev=made href="mailto:neosapient@geocities.com" title ="den Otter">
</head><body>
<h1>Re: IA vs. AI was: longevity vs singularity</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Tue, 27 Jul 1999 19:30:42 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1245">[ date ]</a><a href="index.html#1245">[ thread ]</a><a href="subject.html#1245">[ subject ]</a><a href="author.html#1245">[ author ]</a>
<!-- next="start" -->
<li><a href="1246.html">[ Next ]</a><a href="1244.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1121.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1271.html">Max More</a>
</ul>
<!-- body="start" -->



<hr>
<br>
<a name="1298qlink1"><a href="0817.html#1245qlink1">&gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</a><br>

<p>
<a href="1121.html#1245qlink2">&gt; Has it occurred to you that if the first uploads *are* charitably</a><br>
<i>&gt; inclined, then it's *much* safer to be in the second wave?  The first</i><br>
<i>&gt; uploads are likely to be more in the nature of suicide volunteers,</i><br>

<p>
The first uploads would no doubt be animals (of increasing complexity),
followed by tests with humans (preferably people who don't grasp
the full potential of being uploaded, for obvious reasons). Of course,
<a name="1298qlink2">the strictest possible safety measures should be observed at all 
</a>
times. Only when these tests are concluded with satisfactory results,
should the (actual) synchronized upload procedure be executed.</a>

<p>
<a name="1298qlink3"><a href="1121.html#1245qlink3">&gt; especially when you consider that a rough, destructive, but adequate</a><br>
<i>&gt; scan is likely to come before a perfect, nondestructive scan.</i><br>

<p>
Well, IMHO the scan version of uploading is utterly useless from the
individual's point of view (copy paradox and all that), and I certainly
wouldn't waste any time on this method. In fact, I'm just as opposed
to it as I am to conscious AI.</a>

<p>
<a name="1298qlink4"><a name="1271qlink1"><a href="1121.html#1245qlink4">&gt; You're staking an awful lot on the selfishness of superintelligences. </a><br>

<p>
I'm simply being realistic; when you realize how incredibly slow, 
predictable and messy humans will be compared to even an early 
SI, it is hard to imagine that it will bother helping us. Do we "respect"
ants? Hardly. Add to that the fact that the SI either won't have our
emotional (evolutionary) baggage to start with, or at least can modify 
it at will, and it becomes harder still to believe that it would be 
willing to keep humans around, let alone actively uplifting them.</a>

<p>
<a name="1298qlink5">Why would it want to do that? There is no *rational* reason to 
allow or create competition, and the SI would supposedly be the
very pinnacle of rationality.</a> It's absurd to think that a true SI would
<a name="1271qlink2">still run on the programming of tribal monkey-men, which are
weak and imperfect and therefore forced to cooperate. That's why
evolution has come up with things like altruism, bonding, honor 
and all the rest. Nice for monkey-men, but utterly useless 
for a supreme, near-omnipotent and fully self-contained SI. If
it has a shred of reason in its bloated head, it will shed those
vestigal handicaps asap, if it ever had them in the first place. 
And of course, we'd be next as we'd just be annoying microbes 
which can spawn competition. Competition means loss of control 
over resources, and a potential threat. Not good. *Control* is good.</a> 
<a name="1271qlink3">Total control is even better. The SI wouldn't rest before it had 
brought "everything" under its control, or die trying. Logical, don't 
</a>you think? Forget PC and try to think like a SI (a superbly powerful 
and exquisitely rational machine).
</a>

<p>
<a name="1298qlink6">Or are you hoping for an insane Superpower? Not something you'd
want to be around, I reckon.</a>

<p>
<a name="1298qlink7">Synchronized uploading would create several SIs at once, and though
there's a chance that they'd decide to fight eachother for supremacy, 
it's more likely that they'd settle for some kind of compromize.</a> 

<p>
<a name="1298qlink8"><a href="1121.html#1245qlink5">&gt; Maybe you don't have the faintest speck of charity in your soul, but if</a><br>
<i>&gt; uploading and upgrading inevitably wipes out enough of your personality</i><br>
<i>&gt; that anyone would stop being cooperative - well, does it really make</i><br>
<i>&gt; that much of a difference who this new intelligence "started out" as? </i><br>
<i>&gt; It's not you.  I know that you might identify with a selfish SI, but my</i><br>
<i>&gt; point is that if SIs are *inevitably* selfish, if *anyone* would</i><br>
<i>&gt; converge to selfishness, that probably involves enough of a personality</i><br>
<i>&gt; change in other departments that even you wouldn't call it you.</i><br>

<p>
To transcend is to change, dramatically, about that I have no doubt.
So what, I'm not who I was when I was, say, 2 or 5 or 12. In some
aspects I'm the polar opposite of what I was then, but I still consider
myself to be me. Drugs can change the mind temporarily almost 
beyond recognition, and while you dream your dream persona can
be quite different from the "real" you, both outside and inside,</a> yet 
you still feel that it's "you". So, I'm not too worried about ascensionrelated
 personality changes, as long as I remain conscious and 
reasonably in control while it happens. Sooner or later, the monkeyman
 will have to pass on.

<p>
<a name="1298qlink9"><a href="1121.html#1245qlink6">&gt; &gt; -Stopping you from writing an AI wouldn't be all that hard, if I really</a><br>
<i>&gt; &gt; wanted to. ;-)</i><br>
<i>&gt; </i><br>
<i>&gt; Sure.  One bullet, no more Specialist.  Except that that just means it</i><br>
<i>&gt; takes a few more years.  You can't stop it forever.  </i><br>

<p>
Maybe not forever, but perhaps long enough to tip the balance in favor
of uploading.</a>

<p>
<a name="1298qlink10"><i>&gt; All you can do is</i><br>
<a href="1121.html#1245qlink7">&gt; speed up the development of nanotech...relatively speaking.  We both</a><br>
<i>&gt; know you can't steer a car by selectively shooting out the tires.</i><br>

<p>
No, but you *can* slow it down that way.</a>
 
<p>
<a name="1298qlink11"><a href="1121.html#1245qlink8">&gt; &gt; You can run and/or hide hide from nanotech, even</a><br>
<i>&gt; &gt; fight it successfully, but you can't do that with a superhuman</i><br>
<i>&gt; &gt; AI, i.e. nanotech leaves some room for error, while AI doesn't (or</i><br>
<i>&gt; &gt; much less in any case). As I've said before, intelligence is the</i><br>
<i>&gt; &gt; ultimate weapon, infinitely more dangerous than stupid nanites.</i><br>
<i>&gt; </i><br>
<i>&gt; Quite.  And an inescapable one.  See, what *you* want is unrealistic</i><br>
<i>&gt; because you want yourself to be the first one to upload, </i><br>

<p>
That's *among* the first to upload, which is something else entirely.
<a name="1271qlink4">Well, yes of course I want that; after all, the alternative is to meekly
wait and hope that whoever/whatever turns SI first will have mercy on
your soul. If I had that kind of attitude I'd be a devout Christian, not
a transhumanist. Wanting to be among the first to upload is morally 
right, if nothing else, just like signing up for suspension is morally
right, regardless whether it will work or not. It's man's duty (so</a> to
speak) to reject oppression of any kind, which means spitting death
in the face, among other things. AI could very well be death/
oppression in sheep's clothing (which reminds me of the movie 
"Screamers", btw, with the "cute" killer kid), so we should treat it 
</a>accordingly. 

<p>
<a href="1172.html#1245qlink9">&gt; which excludes</a><br>
<i>&gt; you from cooperation with more than a small group </i><br>

<p>
Theoretically a group of almost any size could do this, more or
less SETI-style (but obviously with a good security system in
place to prevent someone from ascending on the sly). I'm not 
excluding anyone, people exclude *themselves* by either not 
caring or giving in to defeatism, wishfull thinking etc.

<p>
<i>&gt; and limits your</i><br>
<a name="1298qlink12"><a href="1121.html#1245qlink10">&gt; ability to rely on things like open-source projects and charitable</a><br>
<i>&gt; foundations.  </i><br>

<p>
Why would it limit that ability? Even if you'd want to keep your project
secret you could cooperate with people and organizations which 
might somehow advance your cause, without them ever knowing.
Happens all the time. Anyway, it *isn't* secret. On the contrary,
it's all over the web.</a>

<p>
<a name="1298qlink13">What *they* want is unrealistic because they want to
<br>
&gt; freeze progress.<br>
</a>
<p>
Who is "they"?

<p>
<a name="1298qlink14"><a href="1121.html#1245qlink11">&gt; Both of you are imposing all kinds of extra constraints.  You're always</a><br>
<i>&gt; going to be at a competitive disadvantage relative to a pure</i><br>
<i>&gt; Singularitarian </i><br>

<p>
What's a "pure" Singularitarian anyway, someone who wants a 
Singularity asap at almost any cost? Someone who wants a
Singularity for its own sake?</a>

<p>
<a name="1298qlink15"><a href="1121.html#1245qlink12">&gt; or the classic "reckless researcher", who doesn't demand</a><br>
<i>&gt; that the AI be loaded down with coercions, or that nanotechnology not be</i><br>
<i>&gt; unleashed until it can be used for space travel, or that nobody uploads</i><br>
<i>&gt; until everyone can do it simultaneously, or that nobody has access to</i><br>
<i>&gt; the project except eight people, and so on ad nauseam.  The open-source</i><br>
<i>&gt; free-willed AI project is going to be twenty million miles ahead while</i><br>
<i>&gt; you're still dotting your "i"s and crossing your "t"s.</i><br>

<p>
Just because something is easier, doesn't mean that it's the right 
thing to do. Instead of trying to find an intelligent solution, you're
actively contributing to the problem; it's like punching holes in an
already sinking ship (and actually taking great pride in it too), while 
instead you should be looking for, or building, a life raft.</a> 
 
<p>
<a name="1298qlink16"><a href="1121.html#1245qlink13">&gt; A-priori chance that you, personally, can be in the first 6 people to</a><br>
<i>&gt; upload:  1e-9.</i><br>
<i>&gt; Extremely optimistic chance:  1%</i><br>

<p>
Why 6? It could be 600 or 6000 for all I care, as long as uploading 
happens simultaneously. If, say, half of all serious transhumanists
decided to go for it [uploading], we'd each stand more than a
1% chance, simply because 99.99...% of the world's population
lacks vision.</a>

<p>
<a name="1298qlink17"><a href="1121.html#1245qlink14">&gt; Extremely pessimistic chance that AIs are benevolent: 10%</a><br>
<i>&gt; </i><br>
<i>&gt; Therefore it's 10 times better to concentrate on AI.</i><br>

<p>
Well, see above. Besides, a 90% chance of the AI killing us
isn't exactly an appealing situation. Would you get into a
machine that kills you 90% of the time, and gives total,
unprecedented bliss 10% of the time? The rational thing is
to look for something with better odds...</a>

<p>
<a name="1298qlink18">Humanity (those not in the project) would benefit too from
the mass upload approach, because either [uploaded] people 
retain the key parts of their original personality, which means
that the "good guys" (fanatical altruists) would protect the 
mehums, while the "bad guys" probably wouldn't risk a lethal 
conflict over the issue and some compromize would be made, 
OR personalities would change beyond recognition in which
case humanity wouldn't be any worse off than in the case of 
an AI transcension. Conclusion: survival chances likely better 
than 10% for *everyone*, and up to 50% for those directly
involved, depending on speed of AI development, nanotech
etc. A long shot, but the only one that makes sense.</a>

<p>
<a name="1298qlink19">What needs to be done: start a project with as many people as 
possible to firgure out ways to a) enhance human intelligence
with available technology, using anything and everything that's
reasonably safe</a> and effective and b) develop an as detailed as
possible scenario for actual (gradual) uploading that can be
implemented as soon as nanotech becomes functional. c) 
Spread awareness among AI researchers, related 
individuals and institutions about the possible negative 
implications for humanity and themselves. AI can be very 
useful for all sorts of things, including human uploading, but 
for chrissake don't make it *too* smart. d) Obviously, some
people are better suited to take care of funding, while others
do research, while yet others fill the gaps in between. There's
something useful to do for everyone. 

<p>
Well, it ain't gonna happen, of course, but it would certainly be 
a good idea. Better than anything suggested so far, anyway.
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1246.html">[ Next ]</a><a href="1244.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1121.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1271.html">Max More</a>
</ul>
</body></html>
