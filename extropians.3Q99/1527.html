<!-- received="Wed Aug  4 02:23:17 1999 MDT" -->
<!-- sent="Wed, 4 Aug 1999 01:23:13 -0700 (PDT)" -->
<!-- name="Robert J. Bradbury" -->
<!-- email="bradbury@www.aeiveos.com" -->
<!-- subject="Re: IA vs. AI (vs. humanity) (fwd)" -->
<!-- id="Pine.SV4.3.91.990804011029.5360C-100000@www.aeiveos.com" -->
<!-- inreplyto="IA vs. AI (vs. humanity) (fwd)" -->
<!-- version=1.10, linesinbody=45 -->
<html><head><title>extropians: Re: IA vs. AI (vs. humanity) (fwd)</title>
<meta name=author content="Robert J. Bradbury">
<link rel=author rev=made href="mailto:bradbury@www.aeiveos.com" title ="Robert J. Bradbury">
</head><body>
<h1>Re: IA vs. AI (vs. humanity) (fwd)</h1>
Robert J. Bradbury (<i>bradbury@www.aeiveos.com</i>)<br>
<i>Wed, 4 Aug 1999 01:23:13 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1527">[ date ]</a><a href="index.html#1527">[ thread ]</a><a href="subject.html#1527">[ subject ]</a><a href="author.html#1527">[ author ]</a>
<!-- next="start" -->
<li><a href="1528.html">[ Next ]</a><a href="1526.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1465.html">Jeff Davis</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
&gt; Jeff Davis &lt;jdavis@socketscience.com&gt; wrote:<br>

<p>
&gt; removed<br>
<p>
   ...entertaining &amp; realistic story about one probable path to AI...

<p>
<a href="1465.html#1527qlink1">&gt; So the AI either stays locked up until it's really and truly socialized</a><br>
<i>&gt; (boring but safe), or we hope that in its first self-liberated round of</i><br>
<i>&gt; self-enhancement it jumps immediately to forgiveness and tolerance (klaatu</i><br>
<i>&gt; barada nikto).</i><br>

<p>
<a href="1465.html#1527qlink2">&gt; I seem to have painted myself into a corner, and I don't like stories with</a><br>
<i>&gt; unhappy endings.  The government at its best would be a poor master for a</i><br>
<i>&gt; superior intelligence, and the spook/militarist/domination-and-control</i><br>
<i>&gt; culture is hardly the government at its best.</i><br>

<p>
<a href="1465.html#1527qlink3">&gt; So, my futurist friends, how do we extricate ourselves from this rather</a><br>
<i>&gt; tight spot?</i><br>

<p>
I think it has been mentioned already.  Open-source.

<p>
If AI@home gets off the ground it is likely to bypass any efforts
by the military/government.  Question: Is the processing power being
used by SETI@home now greater than that being used by the NSA?
Perhaps not, but I suspect it isn't far off.  It becomes a simple
question of enrollment.  The government is unlikely to enroll
the population as a whole into devoting its computers &amp; electricity
to AI self-evolution (for the purposes of the government).  I
doubt that the government could construct a computer bigger
than one that a few million private individuals could collectively
piece together.  The cry of the Z generation may be "I want my
pet AI and I want it now!".  Think of all of those Nintendo/Sony
game-stations connected by fiber and co-evolving an AI.

<p>
However, there is a thread of darkness here as well.  If AI@home
does succeed it will be very difficult to control.  It only takes
one individual to take the safety-interlocks off and the AI
becomes self modifying.  As I pointed out in a much earlier message,
the moral thing for the AI to do is to eliminate its creators
since humans cannot be engineered/guaranteed to be "trustable".
Of course self-modifying AI's could fall into that category
as well.  The situation is resolved if there is only one AI.
On a good day I'm pretty sure I can trust myself.

<p>
Robert
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1528.html">[ Next ]</a><a href="1526.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1465.html">Jeff Davis</a>
<!-- nextthread="start" -->
</ul>
</body></html>
