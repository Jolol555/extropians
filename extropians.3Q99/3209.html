<!-- received="Tue Aug 31 17:08:32 1999 MDT" -->
<!-- sent="Tue, 31 Aug 1999 15:36:50 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: &gt;H RE: Present dangers to transhumanism" -->
<!-- id="37CC3CD1.DCB3337C@pobox.com" -->
<!-- inreplyto="&gt;H RE: Present dangers to transhumanism" -->
<!-- version=1.10, linesinbody=53 -->
<html><head><title>extropians: Re: &gt;H RE: Present dangers to transhumanism</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: &gt;H RE: Present dangers to transhumanism</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Tue, 31 Aug 1999 15:36:50 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3209">[ date ]</a><a href="index.html#3209">[ thread ]</a><a href="subject.html#3209">[ subject ]</a><a href="author.html#3209">[ author ]</a>
<!-- next="start" -->
<li><a href="3210.html">[ Next ]</a><a href="3208.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3192.html">hal@finney.org</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
hal@finney.org wrote:
<br>
<i> &gt;</i><br>
<a href="3192.html#3209qlink1"> &gt; Eliezer S. Yudkowsky, &lt;sentience@pobox.com&gt;, writes:</a><br>
<i> &gt;  &gt; Frankly, I think a fairly large percentage of you *are* naive</i><br>
<i> &gt;  &gt; technophiles.  You think you can take controlled sips from a tidal wave.</i><br>
<i> &gt;</i><br>
<i> &gt; However, many us don't subscribe to the runaway AI/singularity scenario.</i><br>

<p>
I was mostly referring to the nanoSanta tendency.  I've changed
philosophies quite a bit over the last three years, but one thing that's
remained constant since the day I posted "Staring Into the Singularity"
is that I despise childhood fantasies of omnipotence; it shows a lack of
both imagination and maturity.  Yes, nanotech is powerful enough to
build you a luxury Mercedes, or heal your hand if it gets run over by a
car.  But it's way, way, way more powerful than that.  Even leaving out
all IA, AI, and military implications, it would change the fabric of
society past all recognition.  Accounts of twentieth-century American
culture unaltered by nanotech are as foolish as accounts of a
computer-assisted hunter-gatherer society using guided missiles to hunt
down deer.

<p>
But no, they don't think that.  They think in Santa syllogisms.  They
want a Mercedes.  Nanotech is big and powerful.  Nanotech will give them
a Mercedes.  QED.

<p>
What *else* will it give them?  Supercomputers easily capable of running
AIs?  Uploading?  Vingean headbands?  A mass diaspora from Earth?  They
don't know.  They don't care.  All they want is a Mercedes.  What if I
try to point out the military implications?  They don't want military
implications.  Repeat the words "active shields".  This is a magic cure.

<p>
<a href="3192.html#3209qlink2"> &gt; As Robin Hanson pointed out, we don't know how quickly the difficulty of</a><br>
<i> &gt; increasing intelligence grows as you become smarter.  Is it easier for</i><br>
<i> &gt; an IQ of 400 to go to an IQ of 800 than for an IQ of 200 to go to 400?</i><br>
<i> &gt; We have no evidence one way or the other.  It is uncharted territory.</i><br>

<p>
Those two statements are entirely unequivalent.  Yes, it's uncharted
territory.  That doesn't mean we have no grounds for believing some
statements over others.<a name="3216qlink1">  I don't see any grounds for believing in a
"difficulty" that will prevent a nanocomputer with a million times the
raw computing power of a human from being at least as much smarter than
humans as humans are from chimpanzees,</a> or in a difficulty that prevents
an AI running over the Internet<a name="3216qlink2"> from being intelligent enough to use
rapid infrastructure to recompile the planet's mass and upload the
</a>
population.<a name="3216qlink3">  Whether difficulties occur after that is something of a
moot point, don't you think?</a>
<pre>
--
            sentience@pobox.com          Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3210.html">[ Next ]</a><a href="3208.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3192.html">hal@finney.org</a>
<!-- nextthread="start" -->
</ul>
</body></html>
