<!-- received="Mon Aug  2 20:15:06 1999 MDT" -->
<!-- sent="Mon, 2 Aug 1999 22:15:02 -0400 (EDT)" -->
<!-- name="Xiaoguang Li" -->
<!-- email="xli03@emory.edu" -->
<!-- subject="stability of goals" -->
<!-- id="Pine.GSO.4.05.9908022142160.25078-100000@jet.cc.emory.edu" -->
<!-- inreplyto="37A51E3A.6AE7529B@pobox.com" -->
<!-- version=1.10, linesinbody=28 -->
<html><head><title>extropians: stability of goals</title>
<meta name=author content="Xiaoguang Li">
<link rel=author rev=made href="mailto:xli03@emory.edu" title ="Xiaoguang Li">
</head><body>
<h1>stability of goals</h1>
Xiaoguang Li (<i>xli03@emory.edu</i>)<br>
<i>Mon, 2 Aug 1999 22:15:02 -0400 (EDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1434">[ date ]</a><a href="index.html#1434">[ thread ]</a><a href="subject.html#1434">[ subject ]</a><a href="author.html#1434">[ author ]</a>
<!-- next="start" -->
<li><a href="1435.html">[ Next ]</a><a href="1433.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1386.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="1442qlink1">	fascinated by the recent exchanges between Eliezer S. Yudkowsky and den
Otter regarding the feasibility of AI vs. IA and the inescapable mystery
of SI motivations, i visited Eliezer's Singularity Analysis. since i find
Eliezer's view that objective goals are possible at all extremely
refreshing in this postmodernist, existential zeitgeist, Eliezer's section
on Superintelligence Motivations catch my attention especially.</a>
<p>
	although much of Eliezer's vocabulary are foreign to an AI-layman
as myself, i believe that his explanation is clear enough that i
have caught at least a glimpse of his vision. his treatment of stability
as a key of goal valuation seems concise and elegant. however, the mention
of stability touched off a few associations in my brain and elicited some
questions.
<p>
<a name="1446qlink1"><a name="1445qlink1"><a name="1442qlink2">	if the most stable system of goals is the most rational by Occam's
Razor, then might not death</a> be a candidate? it seems intuitively sound 
</a>
<a name="1446qlink2">that if an entity were to commit a random action, that action would most
likely bring the entity closer to destruction than to empowerment; in
other words, is not entropy (cursed be that word) the default state of the
<a name="1446qlink3">universe and therefore the most stable by Occam's Razor?</a> thus if a SI
decides to pursue the goal of suicide, it may find that by and large any
action most convenient at the moment would almost certainly advance its
<a name="1478qlink1">goal and thus possess a positive valuation in its goal system. could it be
that only us petty slaves of evolution are blinded to the irrevocable
</a>
course of the universe and choose to traverse it upstream?</a>
</a>

<p>
sincerely,
<br>
xgl
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1435.html">[ Next ]</a><a href="1433.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1386.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
