<!-- received="Wed Jul 28 22:09:19 1999 MDT" -->
<!-- sent="Wed, 28 Jul 1999 22:42:50 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: IA vs. AI was: longevity vs singularity" -->
<!-- id="379FCDB5.CD7113FC@pobox.com" -->
<!-- inreplyto="IA vs. AI was: longevity vs singularity" -->
<!-- version=1.10, linesinbody=377 -->
<html><head><title>extropians: Re: IA vs. AI was: longevity vs singularity</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: IA vs. AI was: longevity vs singularity</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Wed, 28 Jul 1999 22:42:50 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1298">[ date ]</a><a href="index.html#1298">[ thread ]</a><a href="subject.html#1298">[ subject ]</a><a href="author.html#1298">[ author ]</a>
<!-- next="start" -->
<li><a href="1299.html">[ Next ]</a><a href="1297.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1245.html">den Otter</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1332.html">Eugene Leitl</a>
</ul>
<!-- body="start" -->

<p>
den Otter wrote:
<br>
<i>&gt; </i><br>
<a href="1245.html#1298qlink1">&gt; ----------</a><br>
<i>&gt; &gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; Has it occurred to you that if the first uploads *are* charitably</i><br>
<i>&gt; &gt; inclined, then it's *much* safer to be in the second wave?  The first</i><br>
<i>&gt; &gt; uploads are likely to be more in the nature of suicide volunteers,</i><br>
<i>&gt; </i><br>
<a name="1375qlink2"><i>&gt; The first uploads would no doubt be animals (of increasing complexity),</i><br>
<i>&gt; followed by tests with humans (preferably people who don't grasp</i><br>
<i>&gt; the full potential of being uploaded, for obvious reasons).</i><br>

<p>
You have to be kidding.  Short of grabbing random derelicts off the
street, there's simply no way you could do that.  Or were you planning
to grab random derelicts off the street?</a>  And did I mention that by the
<a name="1375qlink3">time you can do something like that, in secret, on one supercomputer,
much less run 6000 people on one computer and then upgrade them in
<a name="1332qlink1">synchronization, what *I* will be doing with distributed.net</a> will -
hell, even Eugene Leitl's genetic algorithm would Transcend</a> at that
<a name="1332qlink2">point.  Douglas Lenat could just rerun EURISKO.  *Spreadsheet*</a> programs
would Transcend.

<p>
<a href="1245.html#1298qlink2">&gt; Of course,</a><br>
<i>&gt; the strictest possible safety measures should be observed at all</i><br>
<i>&gt; times. Only when these tests are concluded with satisfactory results,</i><br>
<i>&gt; should the (actual) synchronized upload procedure be executed.</i><br>

<p>
If only the Straumers had stamped "Thank you for observing all safety
precautions" on that ancient archive!  This seal of Solomon would have
kept the Blight from getting out.
<br>
You're dreaming.

<p>
<a name="1375qlink4">Besides, there's more to uploading than scanning.  Like, the process of
upgrading to Powerdom.  How are you going to conduct those tests, hah?</a>

<p>
<a href="1245.html#1298qlink3">&gt; &gt; especially when you consider that a rough, destructive, but adequate</a><br>
<i>&gt; &gt; scan is likely to come before a perfect, nondestructive scan.</i><br>
<i>&gt; </i><br>
<a name="1375qlink5"><i>&gt; Well, IMHO the scan version of uploading is utterly useless from the</i><br>
<i>&gt; individual's point of view (copy paradox and all that), and I certainly</i><br>
<i>&gt; wouldn't waste any time on this method. In fact, I'm just as opposed</i><br>
<i>&gt; to it as I am to conscious AI.</i><br>

</a><p>
So.  No backups.

<p>
<a href="1245.html#1298qlink4">&gt; &gt; You're staking an awful lot on the selfishness of superintelligences.</a><br>
<i>&gt; </i><br>
<i>&gt; I'm simply being realistic; when you realize how incredibly slow,</i><br>
<i>&gt; predictable and messy humans will be compared to even an early</i><br>
<i>&gt; SI, it is hard to imagine that it will bother helping us. Do we "respect"</i><br>
<i>&gt; ants? Hardly. Add to that the fact that the SI either won't have our</i><br>
<i>&gt; emotional (evolutionary) baggage to start with, or at least can modify</i><br>
<i>&gt; it at will, and it becomes harder still to believe that it would be</i><br>
<i>&gt; willing to keep humans around, let alone actively uplifting them.</i><br>

<p>
This holds just as true for uploaded humans, except that being all
"messy" we're even less likely to wind up in a state remotely resembling
what we started out with.

<p>
<a href="1245.html#1298qlink5">&gt; Why would it want to do that? There is no *rational* reason to</a><br>
<i>&gt; allow or create competition, and the SI would supposedly be the</i><br>
<i>&gt; very pinnacle of rationality. It's absurd to think that a true SI would</i><br>
<i>&gt; still run on the programming of tribal monkey-men, which are</i><br>
<i>&gt; weak and imperfect and therefore forced to cooperate. That's why</i><br>
<i>&gt; evolution has come up with things like altruism, bonding, honor</i><br>
<i>&gt; and all the rest. Nice for monkey-men, but utterly useless</i><br>
<i>&gt; for a supreme, near-omnipotent and fully self-contained SI. If</i><br>
<i>&gt; it has a shred of reason in its bloated head, it will shed those</i><br>
<i>&gt; vestigal handicaps asap, if it ever had them in the first place.</i><br>
<i>&gt; And of course, we'd be next as we'd just be annoying microbes</i><br>
<i>&gt; which can spawn competition. Competition means loss of control</i><br>
<i>&gt; over resources, and a potential threat. Not good. *Control* is good.</i><br>
<i>&gt; Total control is even better. The SI wouldn't rest before it had</i><br>
<i>&gt; brought "everything" under its control, or die trying. Logical, don't</i><br>
<i>&gt; you think?<a name="1375qlink6"> Forget PC and try to think like a SI (a superbly powerful</i><br>
<i>&gt; and exquisitely rational machine).</i><br>

<p>
*You* try to think like an SI.  Why do you keep on insisting on the
preservation of the wholly human emotion of selfishness?</a>  I don't
<a name="1375qlink7">understand how you can be so rational about everything except that! 
Yes, we'll lose bonding, honor, love, all the romantic stuff.  But not
altruism.  Altruism is the default state of intelligence.  Selfishness
takes *work*, it's a far more complex emotion.</a>  I'm not talking about
<a name="1375qlink8">feel-good altruism or working for the benefit of other subjectivities. 
I'm talking about the idea of doing what's right, making the correct
choice, acting on truth.</a>  Acting so as to increase personal power
<a name="1375qlink9">doesn't make rational sense except in terms of a greater goal.</a>

<p>
<a name="1375qlink10">I *know* that we'll be bugs to SIs.  I *know* they won't have any of the
emotions that are the source of cooperation in humans.  I *still* see so
many conflicting bits of reasoning and evidence that you might as well
flip a coin as ask me whether they'll be benevolent.  That's my
probability:  50%.</a>

<p>
<a name="1375qlink11">If Powers are hungry, why haven't they expanded continually, at
lightspeed, starting with the very first intelligent race in this
Universe?  Why haven't they eaten Earth already?  Or if Powers upload
mortals because of a mortally understandable chain of logic, so as to
encourage Singularities, why don't they encourage Singularities by
sending helper robots?  We aren't the first ones.  There are visible
galaxies so much older that they've almost burned themselves out.  I
don't see *any* reasonable interpretation of SI motives that is
consistent with observed evidence.  Even assuming all Powers commit
suicide just gives you the same damn question with respect to mortal aliens.</a>

<p>
<a name="1375qlink12">And, speaking of IA motivations, look at *me*.  Are my motivations
human?</a>  Not by your definition.  That's all from some trivial little
twist in the quantitative levels of abilities... nothing qualitative.

<p>
<a name="1375qlink13"><a href="1245.html#1298qlink6">&gt; Or are you hoping for an insane Superpower? Not something you'd</a><br>
<i>&gt; want to be around, I reckon.</i><br>

<p>
No, *you're* the one who wants an insane SI.  Selfishness is insane.</a> 
<a name="1375qlink14">Anything is insane unless there's a rational reason for it, and there is
no rational reason I have ever heard of for an asymmetrical world-model.
 Using asymmetric reflective reasoning, what you would call
"subjectivity", violates Occam's Razor, the Principle of Mediocrity,
non-anthropocentrism, and... I don't think I *need* any "and" after</a> that.

<p>
<a name="1375qlink15"><a href="1245.html#1298qlink7">&gt; Synchronized uploading would create several SIs at once, and though</a><br>
<i>&gt; there's a chance that they'd decide to fight eachother for supremacy,</i><br>
<i>&gt; it's more likely that they'd settle for some kind of compromize.</i><br>
</a>
<p>
Or that they'd merge.

<p>
<a href="1245.html#1298qlink8">&gt; &gt; Maybe you don't have the faintest speck of charity in your soul, but if</a><br>
<i>&gt; &gt; uploading and upgrading inevitably wipes out enough of your personality</i><br>
<i>&gt; &gt; that anyone would stop being cooperative - well, does it really make</i><br>
<i>&gt; &gt; that much of a difference who this new intelligence "started out" as?</i><br>
<i>&gt; &gt; It's not you.  I know that you might identify with a selfish SI, but my</i><br>
<i>&gt; &gt; point is that if SIs are *inevitably* selfish, if *anyone* would</i><br>
<i>&gt; &gt; converge to selfishness, that probably involves enough of a personality</i><br>
<i>&gt; &gt; change in other departments that even you wouldn't call it you.</i><br>
<i>&gt; </i><br>
<i>&gt; To transcend is to change, dramatically, about that I have no doubt.</i><br>
<i>&gt; So what, I'm not who I was when I was, say, 2 or 5 or 12. In some</i><br>
<i>&gt; aspects I'm the polar opposite of what I was then, but I still consider</i><br>
<i>&gt; myself to be me. Drugs can change the mind temporarily almost</i><br>
<i>&gt; beyond recognition, and while you dream your dream persona can</i><br>
<i>&gt; be quite different from the "real" you, both outside and inside, yet</i><br>
<i>&gt; you still feel that it's "you". So, I'm not too worried about ascension-</i><br>
<i>&gt; related personality changes, as long as I remain conscious and</i><br>
<i>&gt; reasonably in control while it happens. Sooner or later, the monkey-</i><br>
<i>&gt; man will have to pass on.</i><br>

<p>
<a name="1375qlink16">I find it hard to believe you can be that reasonable about sacrificing
all the parts of yourself, and so unreasonable about insisting that the
end result start out as you.  If two computer programs converge to
exactly the same state, does it really make a difference to you whether
the one labeled "den Otter" or "Bill Gates" is chosen for the seed?</a>

<p>
<a href="1245.html#1298qlink9">&gt; &gt; &gt; -Stopping you from writing an AI wouldn't be all that hard, if I really</a><br>
<i>&gt; &gt; &gt; wanted to. ;-)</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; Sure.  One bullet, no more Specialist.  Except that that just means it</i><br>
<i>&gt; &gt; takes a few more years.  You can't stop it forever.</i><br>
<i>&gt; </i><br>
<i>&gt; Maybe not forever, but perhaps long enough to tip the balance in favor</i><br>
<i>&gt; of uploading.</i><br>

<p>
See below.

<p>
<a name="1375qlink17"><a href="1245.html#1298qlink10">&gt; &gt; All you can do is</a><br>
<i>&gt; &gt; speed up the development of nanotech...relatively speaking.  We both</i><br>
<i>&gt; &gt; know you can't steer a car by selectively shooting out the tires.</i><br>
<i>&gt; </i><br>
<i>&gt; No, but you *can* slow it down that way.</i><br>

<p>
Of course you can.  But does it really matter all that much, to either
of us, whether a given outcome happens in ten years or twenty?</a>  What
<a name="1375qlink18">matters is the relative probabilities of the outcomes, and trying to
slow things down may increase the probability of *your* outcome relative
to *my* outcome, but it also increases the probability of planetary
destruction relative to *either* outcome... increases it by a lot more.</a> 
<a name="1308qlink1">You can't selectively shoot the Yudkowsky who wants to bring about a
pure Singularity without also killing the Yudkowsky who's working on
neurosurgical intelligence enhancement and the Yudkowsky who's studying
AI motivations.</a>

<p>
<a href="1245.html#1298qlink11">&gt; &gt; &gt; You can run and/or hide hide from nanotech, even</a><br>
<i>&gt; &gt; &gt; fight it successfully, but you can't do that with a superhuman</i><br>
<i>&gt; &gt; &gt; AI, i.e. nanotech leaves some room for error, while AI doesn't (or</i><br>
<i>&gt; &gt; &gt; much less in any case). As I've said before, intelligence is the</i><br>
<i>&gt; &gt; &gt; ultimate weapon, infinitely more dangerous than stupid nanites.</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; Quite.  And an inescapable one.  See, what *you* want is unrealistic</i><br>
<i>&gt; &gt; because you want yourself to be the first one to upload,</i><br>
<i>&gt; </i><br>
<i>&gt; That's *among* the first to upload, which is something else entirely.</i><br>
<i>&gt; Well, yes of course I want that; after all, the alternative is to meekly</i><br>
<i>&gt; wait and hope that whoever/whatever turns SI first will have mercy on</i><br>
<i>&gt; your soul. If I had that kind of attitude I'd be a devout Christian, not</i><br>
<i>&gt; a transhumanist. Wanting to be among the first to upload is morally</i><br>
<i>&gt; right, if nothing else, just like signing up for suspension is morally</i><br>
<i>&gt; right, regardless whether it will work or not. It's man's duty (so to</i><br>
<i>&gt; speak) to reject oppression of any kind, which means spitting death</i><br>
<i>&gt; in the face, among other things. AI could very well be death/</i><br>
<i>&gt; oppression in sheep's clothing (which reminds me of the movie</i><br>
<i>&gt; "Screamers", btw, with the "cute" killer kid), so we should treat it</i><br>
<i>&gt; accordingly.</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; which excludes</i><br>
<i>&gt; &gt; you from cooperation with more than a small group</i><br>
<i>&gt; </i><br>
<a name="1375qlink19"><i>&gt; Theoretically a group of almost any size could do this, more or</i><br>
<i>&gt; less SETI-style (but obviously with a good security system in</i><br>
<i>&gt; place to prevent someone from ascending on the sly). I'm not</i><br>
<i>&gt; excluding anyone, people exclude *themselves* by either not</i><br>
<i>&gt; caring or giving in to defeatism, wishfull thinking etc.</i><br>

<p>
I see.  You're going to simultaneously upload six million people?</a>  And
<a name="1375qlink20">then upgrade them in such a way as to maintain synchronization of
intelligence at all times?  Probability:  Ze-ro.</a>

<p>
<i>&gt; &gt; and limits your</i><br>
<a href="1245.html#1298qlink12">&gt; &gt; ability to rely on things like open-source projects and charitable</a><br>
<i>&gt; &gt; foundations.</i><br>
<i>&gt; </i><br>
<i>&gt; Why would it limit that ability? Even if you'd want to keep your project</i><br>
<i>&gt; secret you could cooperate with people and organizations which</i><br>
<i>&gt; might somehow advance your cause, without them ever knowing.</i><br>
<i>&gt; Happens all the time. Anyway, it *isn't* secret. On the contrary,</i><br>
<i>&gt; it's all over the web.</i><br>

<p>
<a name="1375qlink21">I think you overestimate the tendency of other people to be morons. 
"Pitch in to help us develop an open-source nanotechnology package, and
we'll conquer the world, reduce you to serfdom, evolve into gods, and
crush you like bugs!"</a>  Even "Help us bring about the end of the world in
<a name="1375qlink22">such a way that it means something" has more memetic potential than
that.  There's a *lot* more evolution devoted to avoiding suckerhood
than lemminghood.</a>

<p>
<a href="1245.html#1298qlink13">&gt; What *they* want is unrealistic because they want to</a><br>
<i>&gt; &gt; freeze progress.</i><br>
<i>&gt; </i><br>
<i>&gt; Who is "they"?</i><br>

<p>
The ones who want to keep Life As We Know It around indefinitely.

<p>
<a href="1245.html#1298qlink14">&gt; &gt; Both of you are imposing all kinds of extra constraints.  You're always</a><br>
<i>&gt; &gt; going to be at a competitive disadvantage relative to a pure</i><br>
<i>&gt; &gt; Singularitarian</i><br>
<i>&gt; </i><br>
<a name="1375qlink23"><i>&gt; What's a "pure" Singularitarian anyway, someone who wants a</i><br>
<i>&gt; Singularity asap at almost any cost? Someone who wants a</i><br>
<i>&gt; Singularity for its own sake?</a></i><br>

<p>
Yep.

<p>
<a href="1245.html#1298qlink15">&gt; &gt; or the classic "reckless researcher", who doesn't demand</a><br>
<i>&gt; &gt; that the AI be loaded down with coercions, or that nanotechnology not be</i><br>
<i>&gt; &gt; unleashed until it can be used for space travel, or that nobody uploads</i><br>
<i>&gt; &gt; until everyone can do it simultaneously, or that nobody has access to</i><br>
<i>&gt; &gt; the project except eight people, and so on ad nauseam.  The open-source</i><br>
<i>&gt; &gt; free-willed AI project is going to be twenty million miles ahead while</i><br>
<i>&gt; &gt; you're still dotting your "i"s and crossing your "t"s.</i><br>
<i>&gt; </i><br>
<i>&gt; Just because something is easier, doesn't mean that it's the right</i><br>
<i>&gt; thing to do. Instead of trying to find an intelligent solution, you're</i><br>
<i>&gt; actively contributing to the problem; it's like punching holes in an</i><br>
<i>&gt; already sinking ship (and actually taking great pride in it too), while</i><br>
<i>&gt; instead you should be looking for, or building, a life raft.</i><br>

<p>
Why, thank you!  I rather like that metaphor.  The one about punching
holes in the ship, I mean, not the part about the life raft.  There is
no life raft.<a name="1375qlink25">  Humanity *will* sink.  That is simply not something
subject to alteration.  Everything must either grow, or die.  If we
embrace the change, we stand the best chance of growing.  If not, we
die.  So let's punch those holes and hope we can breathe water.</a>

<p>
<a href="1245.html#1298qlink16">&gt; &gt; A-priori chance that you, personally, can be in the first 6 people to</a><br>
<i>&gt; &gt; upload:  1e-9.</i><br>
<i>&gt; &gt; Extremely optimistic chance:  1%</i><br>
<i>&gt; </i><br>
<i>&gt; Why 6? It could be 600 or 6000 for all I care, as long as uploading</i><br>
<i>&gt; happens simultaneously. If, say, half of all serious transhumanists</i><br>
<i>&gt; decided to go for it [uploading], we'd each stand more than a</i><br>
<i>&gt; 1% chance, simply because 99.99...% of the world's population</i><br>
<i>&gt; lacks vision.</i><br>

<p>
<a name="1375qlink26">Believe me, den Otter, if I were to start dividing the world's
population into two camps by level of intelligence, we wouldn't be on
<a name="1375qlink27">the same side.</a>  You may be high-percentile but you're still human, not human-plus-affector.
</a>

<p>
<a name="1375qlink28">And, once again, you are being unrealistic about the way technologies
develop.  High-fidelity (much less identity-fidelity) uploading simply
isn't possible without a transhuman observer to help.</a>  I could be wrong
about this, but I really don't think I am - not from looking at the
technology.  Again, those 6000 are suicide volunteers.<a name="1375qlink29">  Any uploadee is
a suicide volunteer until there's an SI (whether IA or AI) to help. 
There just isn't any realistic way of becoming the One Power because a
high-fidelity transition from human to Power requires a Power to help.</a>

<p>
<a href="1245.html#1298qlink17">&gt; &gt; Extremely pessimistic chance that AIs are benevolent: 10%</a><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; Therefore it's 10 times better to concentrate on AI.</i><br>
<i>&gt; </i><br>
<i>&gt; Well, see above.<a name="1375qlink30"> Besides, a 90% chance of the AI killing us</i><br>
<i>&gt; isn't exactly an appealing situation. Would you get into a</i><br>
<i>&gt; machine that kills you 90% of the time, and gives total,</i><br>
<i>&gt; unprecedented bliss 10% of the time? The rational thing is</i><br>
<i>&gt; to look for something with better odds...</i><br>

<p>
Yes, but you haven't offered me better odds.  You've asked me to accept
a 1% probability of success instead.</a>  I think you're just favoring that
1% probability because it *looks* like something you can increase, while
the probability of a friendly SI is absolute.  Well, but IMHO the
probability of a friendly IA is absolute.  And even if you can increase
the probability, you aren't going to be able to increase it to 10%.  I
think you're taking a very sentimental and unrealistic approach to
navigating the future.

<p>
<a href="1245.html#1298qlink18">&gt; Humanity (those not in the project) would benefit too from</a><br>
<i>&gt; the mass upload approach, because either [uploaded] people</i><br>
<i>&gt; retain the key parts of their original personality, which means</i><br>
<i>&gt; that the "good guys" (fanatical altruists) would protect the</i><br>
<i>&gt; mehums, while the "bad guys" probably wouldn't risk a lethal</i><br>
<i>&gt; conflict over the issue and some compromize would be made,</i><br>
<i>&gt; OR personalities would change beyond recognition in which</i><br>
<i>&gt; case humanity wouldn't be any worse off than in the case of</i><br>
<i>&gt; an AI transcension. Conclusion: survival chances likely better</i><br>
<i>&gt; than 10% for *everyone*, and up to 50% for those directly</i><br>
<i>&gt; involved, depending on speed of AI development, nanotech</i><br>
<i>&gt; etc. A long shot, but the only one that makes sense.</i><br>

<p>
Well, this is where I fundamentally disagree with you.  It comes down to
that "OR" statement you made.  I think that the second branch is much
more likely - and humanity wouldn't be any worse off than in the case of
an AI Transcend, but said Transcend becomes much less probable relative
to nuclear war or grey goo problems.  What you're overlooking is that
what counts isn't just the relative desirability of the *results* of the
two scenarios, but the relative *probability* of the two scenarios
compared to known undesirable scenarios.  An IA Transcend involves so
many constraints, preconditions, and necessities that it would need a
*huge* desirability advantage to be preferable to an AI Transcend.

<p>
<a name="1375qlink31">As far as I can tell, your evaluation of the desirability advantage is
based solely on your absolute conviction that rationality is equivalent
to selfishness.  I've got three questions for you on that one.  First: 
Why is selfishness, an emotion implemented in the limbic system, any
less arbitrary than honor?</a>  Second:  I know how to program altruism into
<a name="1375qlink32">an AI; how would you program selfishness?</a>  Third:  What the hell makes
<a name="1375qlink33">you think you know what rationality really is, mortal?</a>

<p>
<a name="1375qlink34">And I think an IA Transcend has a definite probability of being less
desirable than an AI Transcend.  Even from a human perspective.  In
fact, I'll go all the way and say that from a completely selfish
viewpoint, not only would I rather trust an AI than an upload, I'd
rather trust an AI than *me*.</a>  And I mean that from a strictly selfish
standpoint!  Human minds are too goddamn messy and they are NOT designed
to tolerate architectural changes.  The risk of destructive insanity is
far, far higher.  No matter what it is you want to preserve in an SI, it
has a better chance of being there - zero, in my opinion, but a more
plausible zero - if you put it in a clean, elegant AI instead of a messy
human conviction.<a name="1375qlink35">  I think you're expressing a faith in the human mind
that borders on the absurd just because you happen to be human.</a>

<p>
<a name="1375qlink36"><a href="1245.html#1298qlink19">&gt; What needs to be done: start a project with as many people as</a><br>
<i>&gt; possible to firgure out ways to a) enhance human intelligence</i><br>
<i>&gt; with available technology, using anything and everything that's</i><br>
<i>&gt; reasonably safe and effective</i><br>

<p>
*Laugh*.  And he says this, of course, to the author of "Algernon's Law:
 A practical guide to intelligence enhancement using modern technology."</a>
<a name="1375qlink37"> Which is the *other* problem with steering a car by shooting out the
tires...  Taking potshots at me would do a lot more to cripple IA than
AI.  And, correspondingly, going on the available evidence, IAers will
tend to devote their lives to AI.</a>
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1299.html">[ Next ]</a><a href="1297.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1245.html">den Otter</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1332.html">Eugene Leitl</a>
</ul>
</body></html>
