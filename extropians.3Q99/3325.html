<!-- received="Wed Sep  1 22:40:34 1999 MDT" -->
<!-- sent="Wed, 1 Sep 1999 23:37:51 -0500" -->
<!-- name="Billy Brown" -->
<!-- email="ewbrownv@mindspring.com" -->
<!-- subject="RE: Singularity?" -->
<!-- id="NDBBLBGGEJLACCFCPNINGEKBCCAA.ewbrownv@mindspring.com" -->
<!-- inreplyto="14284.47793.744900.59868@lrz.de" -->
<!-- version=1.10, linesinbody=41 -->
<html><head><title>extropians: RE: Singularity?</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:ewbrownv@mindspring.com" title ="Billy Brown">
</head><body>
<h1>RE: Singularity?</h1>
Billy Brown (<i>ewbrownv@mindspring.com</i>)<br>
<i>Wed, 1 Sep 1999 23:37:51 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3325">[ date ]</a><a href="index.html#3325">[ thread ]</a><a href="subject.html#3325">[ subject ]</a><a href="author.html#3325">[ author ]</a>
<!-- next="start" -->
<li><a href="3326.html">[ Next ]</a><a href="3324.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3231.html">Eugene Leitl</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3331.html">Eugene Leitl</a>
</ul>
<!-- body="start" -->

<p>
Eugene Leitl wrote:
<br>
<a href="3231.html#3325qlink1">&gt; "Will be determined by the laws of physics" is a remarkably</a><br>
<i>&gt; contentless statement. Everything's (apart from Divine Intervention,</i><br>
<i>&gt; which most people here believe don't exist) determined by the laws of</i><br>
<i>&gt; physics. So what?</i><br>
and
<br>
<a href="3231.html#3325qlink2">&gt; To make this somewhat less noise: I think rushing AI is at least as</a><br>
<i>&gt; bad as rushing nano. Relying on best-case scenario (where the the</i><br>
<i>&gt; first transcendee is deliberately holding back (*all*) the horses to</i><br>
<i>&gt; allow everybody to go on the bus) is foolish at best. To begin, the</i><br>
<i>&gt; perpetuator might be not human to start with.</i><br>
<i>&gt;</i><br>
<i>&gt; And say hello to oblivion,</i><br>

<p>
OK, OK, I'll say it the long-winded way:

<p>
<a name="3333qlink1"><a name="3331qlink1">The determining factor on this issue is how hard AI and intelligence
enhancement turn out to be.  If intelligence is staggeringly complex,</a> and
<a name="3331qlink2">requires opaque data structures that are generally inscrutable to beings of
human intelligence, we get one kind of future (nanotech</a> and human
<a name="3331qlink3">enhancement come online relatively slowly, AI creeps along at a modest rate,
and enhanced humans have a good shot at staying in charge).  If</a> intelligence
<a name="3333qlink2"><a name="3331qlink4">can be achieved using relatively comprehensible programming techniques, such
that a sentient AI can understand its own operation, we get a very different
kind of future (very fast AI progresss leading to a rapid Singularity, with
</a>
essentially no chance for humanity to keep up).  Either way, the kind of
future we end up in has absolutely nothing to do with the decisions we make.</a>

<p>
Personally, I feel that the first scenario is somewhat more likely than the
second.  However, I can't know for sure until we get a lot closer to
actually having sentient AI.  It therefore pays to take out some insurance
by doing what I can to make sure that if we do end up in the second kind of
future we won't screw things up.  So far, the best way I can see to
influence events is to try to end up being one of the people doing the work
(although I'm working on being one of the funding sources, which could
potentially be a better angle).</a>

<p>
Billy Brown, MCSE+I
<br>
ewbrownv@mindspring.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3326.html">[ Next ]</a><a href="3324.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3231.html">Eugene Leitl</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3331.html">Eugene Leitl</a>
</ul>
</body></html>
