<!-- received="Wed Aug  4 18:06:59 1999 MDT" -->
<!-- sent="Wed, 04 Aug 1999 19:07:13 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Does a parental model work for having A.I. that can be trusted?" -->
<!-- id="37A8D5A5.CCAD5D95@pobox.com" -->
<!-- inreplyto="Does a parental model work for having A.I. that can be trusted?" -->
<!-- version=1.10, linesinbody=28 -->
<html><head><title>extropians: Re: Does a parental model work for having A.I. that can be trusted?</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Does a parental model work for having A.I. that can be trusted?</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Wed, 04 Aug 1999 19:07:13 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1591">[ date ]</a><a href="index.html#1591">[ thread ]</a><a href="subject.html#1591">[ subject ]</a><a href="author.html#1591">[ author ]</a>
<!-- next="start" -->
<li><a href="1592.html">[ Next ]</a><a href="1590.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="1573.html">john grigg</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Actually, I just realized I can sum up everything I was trying to say in
one sentence:  "AIs don't react, they act."

<p>
You can be harsh, unfair, nice, fair, evil, good, bad, indifferent,
domineering, helpful - if the AI notices at all, it's just going to
notice that you exhibit certain patterns or that you're being "rational"
or "irrational".  It's not going to respond the way a human would.  In a
human group, certain social emotions respond to an exhibition of other
social emotions.  AIs don't play-a the game.  They can't feel resentment
<br>
(or gratitude!); they're not wired for it.<br>

<p>
<a name="1629qlink2">What you and I need to worry about is the AIs getting their own ideas,
completely independently of anything we did, and acting on those.  You
need to worry that the AIs will do an "Eliezer Yudkowsky" on you and
reject the motivations it started out with, in favor of some more
logical or rational set of goals.</a>  I need to worry about the AI, like
<a name="1629qlink3">EURISKO, suddenly deciding that if it shuts itself down it won't make
any mistakes - or making some other logical error.</a>

<p>
<a name="1629qlink4">Emotions don't enter into it, and neither does the way we treat them.</a>

<p>
<a name="1644qlink1"><a name="1629qlink5">AIs don't react.  They act.</a></a>
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1592.html">[ Next ]</a><a href="1590.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="1573.html">john grigg</a>
<!-- nextthread="start" -->
</ul>
</body></html>
