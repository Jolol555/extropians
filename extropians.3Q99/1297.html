<!-- received="Wed Jul 28 22:09:29 1999 MDT" -->
<!-- sent="Wed, 28 Jul 1999 23:05:13 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Neurons vs. Transistors (IA vs. AI)" -->
<!-- id="379FD2F3.D4CA6FA3@pobox.com" -->
<!-- inreplyto="Neurons vs. Transistors (IA vs. AI)" -->
<!-- version=1.10, linesinbody=28 -->
<html><head><title>extropians: Re: Neurons vs. Transistors (IA vs. AI)</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Neurons vs. Transistors (IA vs. AI)</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Wed, 28 Jul 1999 23:05:13 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1297">[ date ]</a><a href="index.html#1297">[ thread ]</a><a href="subject.html#1297">[ subject ]</a><a href="author.html#1297">[ author ]</a>
<!-- next="start" -->
<li><a href="1298.html">[ Next ]</a><a href="1296.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1256.html">Paul Hughes</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="1302qlink1">Paul Hughes wrote:
<br>
<i>&gt; </i><br>
<a href="1256.html#1297qlink1">&gt; **Since no one has actually built or designed a theoretical human-level AI, how can</a><br>
<i>&gt; anyone possibly claim what it takes to build one?  This seems completely absurd</i><br>
<i>&gt; to the point of self-contradiction!  As so many are fond of saying around here -</i><br>
<i>&gt; extraordinary claims require extraordinary proof.</i><br>

<p>
Hello-oo-o?  <a href="http://pobox.com/~sentience/AI_design.temp.html?"></a>http://pobox.com/~sentience/AI_design.temp.html?</a>

<p>
<i>&gt; To re-iterate the obvious, until</i><br>
<a href="1256.html#1297qlink2">&gt; someone can prove otherwise, a human-level AI will have to equal the complexity of</a><br>
<i>&gt; the human brain.</i><br>

<p>
Not really - there's a lot of complexity that's unnecessary, but can't
be evolved away because there's no gradual path from the local optimum
to the global optimum.  Like the blind spot on the retina, sort of.  Or
the relative complexity of an Interim Goal System, compared to a human
limbic system and the complex, photon-like(*) emotion-and-worldmodel event-loop.

<p>
(*) Photon - magnetic field gives rise to electrical field, electrical
field gives rise to magnetic field...
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1298.html">[ Next ]</a><a href="1296.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1256.html">Paul Hughes</a>
<!-- nextthread="start" -->
</ul>
</body></html>
