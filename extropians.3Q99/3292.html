<!-- received="Wed Sep  1 17:03:48 1999 MDT" -->
<!-- sent="Wed, 01 Sep 1999 18:05:06 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Principle of Nonsuppression" -->
<!-- id="37CDB117.1532EEDA@pobox.com" -->
<!-- inreplyto="Principle of Nonsuppression" -->
<!-- version=1.10, linesinbody=81 -->
<html><head><title>extropians: Re: Principle of Nonsuppression</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Principle of Nonsuppression</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Wed, 01 Sep 1999 18:05:06 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3292">[ date ]</a><a href="index.html#3292">[ thread ]</a><a href="subject.html#3292">[ subject ]</a><a href="author.html#3292">[ author ]</a>
<!-- next="start" -->
<li><a href="3293.html">[ Next ]</a><a href="3291.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3285.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
den Otter wrote:
<br>
<i>&gt; </i><br>
<i>&gt; ----------</i><br>
<a href="3285.html#3292qlink1">&gt; &gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</a><br>
<i>&gt; &gt; I see two possible problems:</i><br>
<i>&gt; &gt;</i><br>
<a name="3374qlink2"><i>&gt; &gt; 1)  We're so busy trying to sabotage each other's efforts that we all</i><br>
<i>&gt; &gt; wind up getting eaten by goo / insane Powers.</i><br>
<i>&gt; </i><br>
<i>&gt; Let's hope we can somehow prevent that...</i><br>

<p>
Ooh!</a>  Yeah!  Great plan!

<p>
<a name="3374qlink3"><a href="3285.html#3292qlink2">&gt; &gt; 2)  Iraq gets nanotechnology instead of the US / the AI project has to</a><br>
<i>&gt; &gt; be run in secret and is not subject to public supervision and error correction.</i><br>
<i>&gt; </i><br>
<i>&gt; Note that "curbing" AI (or any other dangerous technology) by no means</i><br>
<i>&gt; has to involve government-imposed bans. There are other (better) ways to</i><br>
<i>&gt; do this.</i><br>
</a>
<p>
Such as...

<p>
<a href="3285.html#3292qlink3">&gt; Besides, do you really belief that the US military would ever drop their</a><br>
<i>&gt; nano/AI research projects because of some sissy civillian ban? They're</i><br>
<i>&gt; not *that* stupid. Not that such a ban would be likely to be imposed in</i><br>
<i>&gt; the first place; there's too much money riding on this.</i><br>

<p>
What, you mean like with strong encryption and control of the digital economy?

<p>
<a name="3374qlink4"><i>&gt; No, scaring the</i><br>
<a href="3285.html#3292qlink4">&gt; public and the government would more likely result in a tightening of</a><br>
<i>&gt; project security, which is quite good because it would buy us some time.</i><br>

<p>
Buy *who* some time?</a>

<p>
<a name="3374qlink5"><a href="3285.html#3292qlink5">&gt; &gt; "Trying to suppress a dangerous technology only inflicts more damage."</a><br>
<i>&gt; &gt; (Yudkowsky's Threats #2.)</i><br>
<i>&gt; </i><br>
<i>&gt; How defeatist. I'd say that suppressing the proliferation of nukes, for</i><br>
<i>&gt; example, was a *great* idea. Otherwise we probably wouldn't be here</i><br>
<i>&gt; right now. Stupid as they may be, big governments do offer fairly good</i><br>
<i>&gt; stability, on average.</i><br>

<p>
Yes, nuclear weapons are an interesting case.  I should say that trying
to suppress the *creation* of a technology - research and development -
only inflicts more damage.  I'm fully in favor of suppressing the
*proliferation* of dangerous technology.<a name="3367qlink1">  Once Zyvex has nanotechnology,
</a>
<a name="3374qlink6">I'd be fully in favor of their immediately conquering the world to
prevent anyone else from getting</a> it.  That's what should've been done
with nuclear weapons.<a name="3374qlink7">  If the "good guys" refuse to conquer the world
each time a powerful new weapon is developed, sooner or later a bad guy
is going to get to the crux point first.</a>  Alas, I don't think Zyvex's
</a>
<a name="3374qlink8">resources will suffice for the "matter programming" needed.</a>

<p>
<a href="3285.html#3292qlink6">&gt; &gt; Can anyone really think that ve can panic politicians or the media into</a><br>
<i>&gt; &gt; pushing for laws that suppress nanotech/AI without their noticing the</i><br>
<i>&gt; &gt; existence of AI/nanotech/uploading/neurohacking/genetic engineering?</i><br>
<i>&gt; &gt; You push for the "Nanotechnology Suppression Act" and you'll get the</i><br>
<i>&gt; &gt; "Comprehensive Ultratechnology Regulation Bill".</i><br>
<i>&gt; </i><br>
<a name="3374qlink9"><i>&gt; So what? Laws can be broken, twisted, evaded. Like we were waiting for</i><br>
<i>&gt; the government's blessing in the first place.</i><br>

<p>
Okay, now I don't get it.  Are you under the impression you'll find it
easier to evade nanotechnology laws than I'll find it to evade AI laws?</a>

<p>
<a href="3285.html#3292qlink7">&gt; &gt; Who are the "Transtopians", anyway?  My guess is</a><br>
<i>&gt; &gt; den Otter.</i><br>
<i>&gt; </i><br>
<a name="3374qlink10"><i>&gt; The writings are mine, obviously. Anyone who agrees with the principles</i><br>
<i>&gt; can call himself a "Transtopian". And yes, there are actually</i><br>
<i>&gt; like-minded people out there, strangely enough. Of course, as this is</i><br>
<i>&gt; the fringe of a fringe movement, you can't expect it to be very big.</i><br>

</a><p>
An... interesting... perspective.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3293.html">[ Next ]</a><a href="3291.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3285.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
</body></html>
