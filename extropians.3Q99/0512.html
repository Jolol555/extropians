<!-- received="Sat Jul 10 22:58:05 1999 MDT" -->
<!-- sent="Sat, 10 Jul 1999 21:55 PDT" -->
<!-- name="Robert J. Bradbury" -->
<!-- email="bradbury@aeiveos.com" -->
<!-- subject="SETSIs" -->
<!-- id="378824540.106f@aeiveos.com" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=267 -->
<html><head><title>extropians: SETSIs</title>
<meta name=author content="Robert J. Bradbury">
<link rel=author rev=made href="mailto:bradbury@aeiveos.com" title ="Robert J. Bradbury">
</head><body>
<h1>SETSIs</h1>
Robert J. Bradbury (<i>bradbury@aeiveos.com</i>)<br>
<i>Sat, 10 Jul 1999 21:55 PDT</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#512">[ date ]</a><a href="index.html#512">[ thread ]</a><a href="subject.html#512">[ subject ]</a><a href="author.html#512">[ author ]</a>
<!-- next="start" -->
<li><a href="0513.html">[ Next ]</a><a href="0511.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0495.html">Michael S. Lorrey</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->


<p>
<a href="0065.html#0512qlink1">&gt; "Michael S. Lorrey" &lt;mike@lorrey.com&gt; wrote:</a><br>
<i>&gt; </i><br>
<i>&gt; A society may grow spatially and fill the volume of its solar system</i><br>
<i>&gt; with habitats, build a Niven ring or Dyson Sphere, may settle nearby</i><br>
<i>&gt; stars, and develop human equivalent artificial intelligence, yet not</i><br>
<i>&gt; NEED to develop computers beyond one or two magnitudes of human IQ.</i><br>

<p>
Correct.  But this does not constitute an argument that *all* civilizations
will not go beyond this level.

<p>
<a href="0495.html#0512qlink2">&gt; Technological advancement is not a goal in and of itself, it merely</a><br>
<i>&gt; serves to optimize human existence, and it does so at economically cost</i><br>
<i>&gt; effective rates. Dyson's major error in his calcuations was to assume no</i><br>
<i>&gt; increase in efficiency of utilization, which is not only crucial to make</i><br>
<i>&gt; further growth cost effective, but establishes resource re-utilization</i><br>
<i>&gt; at rates of greater efficiency as a cost effective strategy, eventually</i><br>
<i>&gt; concluding in a species being able to utilize resources at more and more</i><br>
<i>&gt; efficient rates.</i><br>

<p>
Huh?  Where does it say this.  I've got the Dyson paper (and can send
you a copy) and as far as I can tell it says nothing of the sort.
Either you are trying to claim that productivity increases do not grow
at compounded rates or I really don't understand.

<p>
<i>&gt; </i><br>
<a href="0495.html#0512qlink3">&gt; Since our population growth rate is already at around 1%, if we increase</a><br>
<i>&gt; the education and intelligence of the population more, the growth rate</i><br>
<i>&gt; will fall even faster, possibly into the negative percentages much like</i><br>
<i>&gt; european countries (which is why they can sustain lower economic growth</i><br>
<i>&gt; rates and maintain standards of living with the US). As longevity</i><br>
<i>&gt; increases in a population, the desire to reproduce decreases, thus</i><br>
<i>&gt; reducing the necessary economic growth rates to maintain improving</i><br>
<i>&gt; standards of living. </i><br>

<p>
Ok, great.  Lets assume that this trend continues and effectively
*eliminates* the desire for reproduction.  It doesn't, in me at
least, eliminate the desire to know and understand more.  In
that regard, I am currently biologically constrained and would
like to "break out".  Giving me more space doesn't help much.
Giving me better technologies with which to engineer my mind
and more energy to run it does.  Now we are back to J-Brains
(planet sized supercomputers vs. solar-system sized supercomputers).

<p>
<i>&gt; </i><br>
<a href="0495.html#0512qlink4">&gt; Comparing the behavior of non-intelligent species to the long term</a><br>
<i>&gt; behavior of intelligent species if fallacious and fraudulent.</i><br>
<i>&gt;</i><br>
Why? -- though we may be intelligent we demonstrate all the behavior
patterns of a non-intelligent species -- strong mating urges, violence,
colonization of available ecological niches, etc.

<p>
The key phrase would seem to be "long term", the problem would be
in modifying those behavior patterns at the rate the singularity occurs
(which is weeks).  If an "intelligent species" cannot prevent or
adapt to the singularity, then all generally acepted paradigms
for "normal" behavior become irrelevant.

<p>
It would seem that consensus driven political systems require
a significant delay in adapting to rapid change around them.
[I'll cite the record industry and MP3 as an example.]
If the change is rapid and you aren't prepared to deal
with it, then it is over before you can respond.  Right
now, you have an opportunity to convince people that
building and uploading into an M-Brain is a very bad
idea.  But once the construction starts (as it only takes
a matter of weeks to build) anything you plan to do would
be too late.

<p>
<i>&gt; </i><br>
<a href="0495.html#0512qlink5">&gt; And it is yet to be proven that some or all 'exponential growth paths'</a><br>
<i>&gt; are not paths of the third order, maxing out at some plateau, the</i><br>
<i>&gt; highest being that of light speed.</i><br>

<p>
Exactly, a major reason to build M/J-brains is to get our communication
bandwidth to highly parallel light-speed (instead of sound or neuronal
speeds).  And you do max out at the light-speed plateau -- that why I keep
arguing that evolution (as we typically think of it) stops there.

<p>
<i>&gt; </i><br>
<a href="0495.html#0512qlink6">&gt; Since greater intelligence and education result in lower population</a><br>
<i>&gt; growth rates, its not hard to imagine a future where the population</i><br>
<i>&gt; falls down to the levels of the previous hunter gatherer society (about</i><br>
<i>&gt; 2 million per planet), yet each individual is of high intelligence, and</i><br>
<i>&gt; engages in much intellectual interaction outside of the feral/agronomist</i><br>
<i>&gt; lifestyle. </i><br>

<p>
No disagreement.  The transhumanists who are willing to embrace
evolutionary technologies survive and everyone else eventually
dies off.  Self-selection into the environments with the best
opportunities wins.

<p>
<i>&gt; </i><br>
<a href="0495.html#0512qlink7">&gt; Moore's Law has yet to be shown to have no upper limit. Better yet,</a><br>
<i>&gt; light speed itself puts a limit on maximum growth of computational</i><br>
<i>&gt; technology. Making Moore's Law a Holy Mantra is an error of faith. Not</i><br>
<i>&gt; very scientific.</i><br>

<p>
True, though the the 5 atom gate thickness limit on CMOS is going
to be a difficult nut.  We hit that circa 2012.  If you want
really hard limits you have to go to the Bremermann/Bekinstein
bound.  There are probably 3 steps --
<pre>
  (a) Go to the limit of current devices
  (b) Go to the limit of single-electron-quantum-dot devices
  (c) Go to the limit of the B/B bound.

</pre>
<p>
It is very unclear at this point how you would make the (b) - (c)
transition.

<p>
The point of my invoking Moore's law (and others) is that
economics drives evolution.  In our "environment" if you
derive a better economic solution you get supported.
Faster computers, more variety, fancier VR environments
*will* get funding unless you find a way to change
human desires or economic systems.

<p>
<i>&gt;</i><br>
<a href="0495.html#0512qlink8">&gt; Your error is to assume that it will happen sooner rather than later.</a><br>
<i>&gt; Much the same errors the early christians made in expecting the 2nd</i><br>
<i>&gt; coming in their lifetimes. Sounds to much like a religious attitude to</i><br>
<i>&gt; me.</i><br>

<p>
I assure you it is not religious other than it results from the
fact of doing the calculations as to how fast asteroids and
planets can be disassembled.  [Rapid disassembly gives you an
exponential increase in power harvesting capacity, which in
turn enables increased intelligence, which in turn enables
a more rapid design of optimal architectures ...]
If you can demonstrate that this cannot proceed quickly,
then you might be correct.

<p>
To understand this growth rate, I would suggest you review:

<UL>
  <li>  Doubling time of a nanoassembler: 5 sec. (Nanosystems, pg 407)
  <li>  Doubling time for self-replicating nanomachines (e.g. bacteria):
       20 minutes (many medical texts, contact me if you want citations)
  <li>  Doubling time for large scale capital stocks (&lt; 10,000 sec = 3 hrs)
     (Nanosystems, pg 1); delivery of 1 kg product by &lt; 1 kg manufacturing
     system (&lt; 1 hr; Nanosystems, pgs 421-425).


</UL>
<p>
These growth rates allow you to manipulate galactic masses within
a few weeks (if you have the material &amp; energy locally available.

<p>
So, if we get on this path, growth to the SI level is rapid.
I would argue that the burden of proof falls on you to show we do
not get on this path, or if we do, we rapidly get off of it.
       
<p>
The trend line we are on now leads directly to this!
<br>
<i>&gt; </i><br>
<i>&gt; &gt; </i><br>
<a href="0495.html#0512qlink9">&gt; &gt; If evolving to the limit of physics *is* feasible, and "life"</a><br>
<i>&gt; &gt; is designed to "evolve", can you make a case for the cessation</i><br>
<i>&gt; &gt; of evolution?</i><br>
<i>&gt; </i><br>
<i>&gt; Evolution is a result of pressures against the survival of the</i><br>
<i>&gt; individual. Once the individual is practially immortal and intelligent</i><br>
<i>&gt; enough to handle most eventualities in the physical world, they have no</i><br>
<i>&gt; further need of evolution.</i><br>

<p>
Exactly.  So you have to make the case that they would
 (a) Convince themselves that evolution is no longer needed
and
<br>
 (b) "edit it out" of their personna(s).

<p>
I'm sorry but even when you cease reproduction, preacefully coexist
with the other few million planetary inhabitants, etc. you still have
the problem that:
<br>
  H-Brains do not have the capacities to prevent/avoid galactic
  catastrophies.
<br>
and
<br>
  M-Brains do.

<p>
Which survives the longest?
<br>
Which do you choose?

<p>
<i>&gt; </i><br>
<a href="0495.html#0512qlink10">&gt; Your points here illustrate the proof of my statement. You refuse to</a><br>
<i>&gt; acknowledge that once some part of the population 'transcends' (quotes</i><br>
<i>&gt; being to denote your mystical attitude toward that condition), that the</i><br>
<i>&gt; remaining population will not automatically be transcended or be</i><br>
<i>&gt; destroyed. That we have stone age cultures in coexistence with our own</i><br>
<i>&gt; right now illustrates the fallacy of your argument. Your argument</i><br>
<i>&gt; assumes a level of evil and callousness in the motives of transcended</i><br>
<i>&gt; beings that I personally would take as an argument to stamp out all</i><br>
<i>&gt; efforts to transcend.</i><br>

<p>
It isn't a "mystical" disconnect so much as a logical/economic one.
The difference in recources is 10^13x+.  If I walk up to you tomorrow
and say -- hey, I'm going to offer you 10x what you are getting paid
now to empty garbage -- Will you accept the offer?

<p>
Perhaps not.  But as the saying goes (almost) everyone has their
price.

<p>
I agree entirely that there will be "takers" and decliners" of
the M-Brain "offer".  My basic premise would be that in the
long term, the M-brain "takers" survive, while the "decliners"
die out.  Simple, ruthlessly efficient, doesn't care one bit
what you "think" about it "natural selection".

<p>
For that not to occur you have to argue "conservation"
agendas for the M-Brains.  I'm sorry, but I don't notice
you saying "I cannot take a walk in the forrest, for
I knoweth that I shall stepeth on the nematodes".
To win this argument, you have to argue an extraordinarily
higher "caring" for lower life forms by M-brains than we
ourselves demonstrate.

<p>
<a href="0495.html#0512qlink11">&gt; You fallaciously assume that an AI will not develop a moral code, as if</a><br>
<i>&gt; there is no objective morality. Sorry, null program.</i><br>

<p>
I would argue that any AI would evolve a program for its self-preservation.
Whether it evolves any "morality", or one "consistent" with our own
remains a very open question.

<p>
As I discussed in another thread -- humans (currently) have unreliable
"morality".  AIs might be programed with/evolve completely
reliable (trustrable) morality.   Go ahead, make the argument
for the preservation of an untrustable morality over the
trustable morality...

<p>
<i>&gt; </i><br>
<a href="0495.html#0512qlink12">&gt; Read it. You still are evading the point. Just as there are stone age</a><br>
<i>&gt; cultures alive and functioning today, 20th century cultures will survive</i><br>
<i>&gt; into and beyond the time of any supposed date of 'singularity'. You need</i><br>
<i>&gt; to stop looking at it as some sort of Day of Rapture that all will</i><br>
<i>&gt; participate in or go to hell.</i><br>

<p>
I've argued consistantly for the preservation of diversity
(even in the face of what may be miniscule ROIs).

<p>
SIs and planetary/tribal cultures are in little conflict with each
other (any more than the humans/insects are).  The SI consciousness(es)
may well decide to preserve the history/background culture.

<p>
In the long run however, you have to argue that the SI is
*so concerned* that it will take an active role in the
preservation of the environment [we don't want our great
.... children to be roasted by the sun going into a red
giant phase, so lets stop it shall we....]

<p>
<i>&gt; </i><br>
<a href="0495.html#0512qlink13">&gt; If you do so and do not accept any objective morality of survival and</a><br>
<i>&gt; coexistence, then I will be sure to nuke you before you attain your</i><br>
<i>&gt; goal. Kapisch?</i><br>
<i>&gt; </i><br>
I, personally, do generally accept the morality/survival goals.
[Fundamentally do unto others as they would have them do unto you.]

<p>
So, by definition, I should do nothing to hinder your personal
survival.  Ok, that makes absolute sense.  Do you require that I
take a pro-active role in promoting your survival?  How much of my
own resources am I required to dedicate to these promotional activities?

<p>
Regarding "nuking", thats fine.  The problem that comes to mind is
that you have to nuke me, Bill Gates, Larry Ellison and a host of
other people who are used to "survival of the fittest" business
tactics where "the terminator wins".  Can you honestly think or
believe that you will be successful in eliminating all of us?
The problem is that it is like a chess game, eliminate the queen
and the rook or the bishop assumes the position of the most valuable
piece.  All you do by eliminating an individual is transfer the
power.

<p>
Robert
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0513.html">[ Next ]</a><a href="0511.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0495.html">Michael S. Lorrey</a>
<!-- nextthread="start" -->
</ul>
</body></html>
