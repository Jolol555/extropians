<!-- received="Tue Aug 17 04:13:40 1999 MDT" -->
<!-- sent="17 Aug 1999 12:13:34 +0200" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="SI Comparative Advantage [was Re: Free Will]" -->
<!-- id="b491zd23eb5.fsf@mdj.nada.kth.se" -->
<!-- inreplyto="Mon, 16 Aug 1999 11:57:53 -0600 (MDT)" -->
<!-- version=1.10, linesinbody=46 -->
<html><head><title>extropians: SI Comparative Advantage [was Re: Free Will]</title>
<meta name=author content="Anders Sandberg">
<link rel=author rev=made href="mailto:asa@nada.kth.se" title ="Anders Sandberg">
</head><body>
<h1>SI Comparative Advantage [was Re: Free Will]</h1>
Anders Sandberg (<i>asa@nada.kth.se</i>)<br>
<i>17 Aug 1999 12:13:34 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2266">[ date ]</a><a href="index.html#2266">[ thread ]</a><a href="subject.html#2266">[ subject ]</a><a href="author.html#2266">[ author ]</a>
<!-- next="start" -->
<li><a href="2267.html">[ Next ]</a><a href="2265.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2224.html">Brent Allsop</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Brent Allsop &lt;allsop@fc.hp.com&gt; writes:

<p>
<a href="2224.html#2266qlink1">&gt; Anders Sandberg &lt;asa@nada.kth.se&gt; replied:</a><br>
<i>&gt; </i><br>
<i>&gt; &gt; proff@suburbia.net (Julian Assange) writes:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; &gt; Those SI's which have motivation to control the substrates on which</i><br>
<i>&gt; &gt; &gt; SI's are built shall proliferate and others will become food.</i><br>
<i>&gt; &gt; &gt; The idea that SI's will do anything else with humanity but grind it up</i><br>
<i>&gt; &gt; &gt; for spare atoms and is nothing short of religious faith.</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; So what about the law of comparative advantage?</i><br>

<p>
[See the excellent explanations posted by J. R. Molloy,
Technotranscendence and Robert J. Bradbury; I knew I could count on
the list to produce much better answers that me (since I'm not that
good at economics yet). Thanks!]

<p>
Anyway, what has this to do with whether SIs like sauteed humans? As I
see it, it shows that it makes economic sense to have even "lesser"
creatures in your economic system. 

<p>
Also, I'm not a big fan of the "instant god" scenario of posthuman
development, which assumes that the first AI/upload/whatever
self-improves so quickly it becomes a near-omnipotent entity able to
determine the future of everybody and lacks any allegiances to the
ants around it. I consider it much more likely that the ascent to
transcendence will be a continous process of self-improvement where it
is highly advantageous for the would-be-SIs to outsource much of the
work to other economic entities ("I'm too busy designing my new
cognitive core, I want you to write my new personality/user
interface. I pay you with an old cognitive core"). This would favor
specialisation rather than the SI being a jack-of-all-trades, and
would in the end lead to a situation with a long gaussian with a long
tail of interconnected economic entities and the SI(s) at the end of
the tail. This also disfavours eating people, since there are
competitors/partners of close to the same power level and economic
ties even with the "ants".


<pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
asa@nada.kth.se                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2267.html">[ Next ]</a><a href="2265.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2224.html">Brent Allsop</a>
<!-- nextthread="start" -->
</ul>
</body></html>
