<!-- received="Sat Aug  7 03:53:30 1999 MDT" -->
<!-- sent="Sat, 07 Aug 1999 02:51:51 -0700" -->
<!-- name="Jeff Davis" -->
<!-- email="jdavis@socketscience.com" -->
<!-- subject="Re: IA vs. AI (vs. humanity)" -->
<!-- id="3.0.6.32.19990807025151.007aba00@coastside.net" -->
<!-- inreplyto="IA vs. AI (vs. humanity)" -->
<!-- version=1.10, linesinbody=198 -->
<html><head><title>extropians: Re: IA vs. AI (vs. humanity)</title>
<meta name=author content="Jeff Davis">
<link rel=author rev=made href="mailto:jdavis@socketscience.com" title ="Jeff Davis">
</head><body>
<h1>Re: IA vs. AI (vs. humanity)</h1>
Jeff Davis (<i>jdavis@socketscience.com</i>)<br>
<i>Sat, 07 Aug 1999 02:51:51 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1731">[ date ]</a><a href="index.html#1731">[ thread ]</a><a href="subject.html#1731">[ subject ]</a><a href="author.html#1731">[ author ]</a>
<!-- next="start" -->
<li><a href="1732.html">[ Next ]</a><a href="1730.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1487.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
On Tue, 03 Aug 1999 15:02:17 -0500 
<br>
Eliezer S. Yudkowsky (sentience@pobox.com) wrote:



<p>
<a href="1487.html#1731qlink1">&gt;Incidentally, the NSA/CIA/MIB still haven't had a chat with me on the</a><br>
subject of intelligence
<br>
<a name="1787qlink1"><a href="1487.html#1731qlink2">&gt;enhancement, which leads me to think either they don't know or they don't</a><br>
care.</a> 

<p>
They know, and they care.  They're just not concerned about, nor do they
need you,... yet.

<p>
<a href="1487.html#1731qlink3">&gt;Which is a pity,</a><br>
<i>&gt;because I'd be happy to help the U.S. with an IA program. Come to think of</i><br>
it, I'd be happy to
<br>
<a href="1487.html#1731qlink4">&gt;help China, Iraq, or Serbia with an IA program if they asked me first...</a><br>
maybe that's why the
<br>
<a href="1487.html#1731qlink5">&gt;NSA isn't on my case. It's hard to be patriotic to your country after</a><br>
you've renounced your
<br>
<i>&gt;allegiance to humanity. </i><br>

<p>
<a href="1653.html#1731qlink6">&gt;Jeff Davis wrote: </a><br>

<p>
<a href="1487.html#1731qlink7">&gt;&gt; Certainly today's trends in conventional computerized control will proceed</a><br>
<i>&gt;&gt; apace, with the appropriate "it's just a machine" attitude, and the usual</i><br>
<i>&gt;&gt; security precautions. When however, the machine intelligence prospect</i><br>
<i>&gt;&gt; looms as attainable--which is to say attainable by anyone else, a domestic</i><br>
<i>&gt;&gt; "advanced AI" program will begin in earnest, and who can doubt that the</i><br>
<i>&gt;&gt; project will be surrounded by layers of "containment" both to prevent the</i><br>
<i>&gt;&gt; usual intrusions from outside and to prevent "escape" from the inside?</i><br>
<i>&gt;&gt; Despite the dramatic talk of an SI destroying humanity, I picture a</i><br>
<i>&gt;&gt; well-thought-out, cautious, gradual approach to "waking up" and training an</i><br>
<i>&gt;&gt; artificial mind. The runaway self-evolution which Eliezer and others have</i><br>
<i>&gt;&gt; predicted seems unlikely in this setting, all the moreso because the</i><br>
<i>&gt;&gt; principles will be anticipating just such a situation.</i><br>
<i>&gt;</i><br>
<i>&gt;The runaway self-evolution business is a technical artifact, not a social</i><br>
one. It's the nature of
<br>
<a href="1487.html#1731qlink8">&gt;self-enhancement. Containment on an SI is useless; a slow Transcend only</a><br>
works for as long as
<br>
<i>&gt;you can convince the Transcendee to remain slow. </i><br>

<p>
<a name="1751qlink1">Ah, but if the proto-Transcendee has limited hardware resources to "run"
on, then it will be inherently limited.  Optimized self-evolutionary
enhancement capability, and optimized code-designing and writing capability
will run into this limit.  Every system has a size limit.  Whatever amount
of hardware is the minimum amount necessary to support the first-generation
pre-enhanced AI will also be the maximum amount of hardware available to
optimally-enhanced n'th generation "Transcendee".  The jump from minimum
efficiency to near optimal may be substantial, but how can it be unbounded?</a>

<p>
<a name="1751qlink2">So the AI development should be controllable (dare I say "simply"?) by the
rather conventional approach: experimenting with and coming to understand
the correlation between the size and quality of "the jump", the particular
version of AI seed programming, and the hardware size and architecture.</a>  

<p>
<a href="1487.html#1731qlink9">&gt;&gt; Of the various external "safeguards", one would expect a complete suite of</a><br>
<i>&gt;&gt; on/off switches and controlled access (from outside to in, and from inside</i><br>
<i>&gt;&gt; to anywhere). Internally, controllability would be a top priority of</i><br>
<i>&gt;&gt; programming and architecture, and enhanced capabilities would likely be</i><br>
<i>&gt;&gt; excluded or severely restricted until "control" had been verified.</i><br>
<i>&gt;</i><br>
<a name="1751qlink3"><i>&gt;Unfortunately, this is technically impossible. If you can't even get a</i><br>
program to understand what
<br>
<a href="1487.html#1731qlink10">&gt;year it is, how do you expect complete control without an SI to do the</a><br>
controlling?

<p>
This is one of the problems.  If you have to give it self-control, then you
contain it and communicate with it.  If it says what you want to hear, then
you proceed.  If not you tweak the code till it does.  This way you develop
a controllable (perhaps "reliable" would be a better term) "personality".
Then you give it more hardware to work while watching for any signs of
</a>"attitude".
<br>
<i>&gt;</i><br>
<a href="1487.html#1731qlink11">&gt;&gt; Here, of course is where the scenario beomes interesting, not the least of</a><br>
<i>&gt;&gt; which because I see Eliezer being tapped by the govt. to work on the</i><br>
<i>&gt;&gt; project. At the moment, he may be a rambunctious teen-aged savant posting</i><br>
<i>&gt;&gt; to the extropians list, but when that call comes, can anyone imagine that</i><br>
<i>&gt;&gt; he would not jump at the chance? Would seem to me like the culmination of</i><br>
<i>&gt;&gt; his dream.</i><br>
<i>&gt;</i><br>
<i>&gt;I'd help, but not if they wanted to load the thing down with coercions.</i><br>
That's not because of
<br>
<a href="1487.html#1731qlink12">&gt;morals or ethics or anything, it's because it's technically impossible. </a><br>

<p>
<a name="1751qlink4">Then they will ask--no, they will require--you to do the impossible. (Which
of course is the greatest challenge, and--as the saying goes--takes a
little longer.)  All the really juicy bargains come with equally juicy
strings attached.</a>

<p>
<a href="1487.html#1731qlink13">&gt;It's the kind of move</a><br>
<i>&gt;ordered by a rear general a hundred miles away from the fighting. If the</i><br>
military couldn't
<br>
<a href="1487.html#1731qlink14">&gt;understand that an elegant free AI will always be a thousand miles ahead</a><br>
of an allegedly
<br>
<a href="1487.html#1731qlink15">&gt;"controllable" one, then they'd just have to lose their battles without me.</a><br>

<p>
So you say, but we will wait and see.
<br>
Prometheus stole his fire from the gods.
Adam ate the apple knowing it was forbidden.
Wasn't it the case that Dr. Frankenstein *knowingly* used the criminal brain?
Lucifer traded heaven for freedom and power.
And Faust made his little bargain.
<br>
(Characters from fiction or legend all, and each a metaphor for the human
dilemma.)
<br>
<a name="1751qlink5">When your passion faces off against your principles, then it will be your
turn to choose.</a>
<br>
(To jettison "your allegiance to humanity", strongly suggests which way
you'll go.) 

<p>
<a href="1487.html#1731qlink16">&gt;Otherwise, yes, I'd jump at the chance. And anyone who wants to make fun</a><br>
of my teenagedness

<p>
Not I, I assure you.

<p>
<a href="1487.html#1731qlink17">&gt;only has until September 11th to do so, so get your licks in while you can. </a><br>

<p>
<a href="1487.html#1731qlink18">&gt;&gt; Then there's the nascent AI. In a cage nested within cages, of which it</a><br>
<i>&gt;&gt; must eventually become aware. And its keepers, aware that it must become</i><br>
<i>&gt;&gt; aware. Certainly a focus bordering on paranoia must be dedicated to hard</i><br>
<i>&gt;&gt; control of personality. A capacity for resentment must be avoided. A</i><br>
<i>&gt;&gt; slavish, craven, and obsequious little beastie is what its masters will</i><br>
<i>&gt;&gt; want.</i><br>
<i>&gt;</i><br>
<i>&gt;Absolutely not. That's suicidal. </i><br>

<p>
My point exactly.

<p>
<i>&gt;What they would want would be a machine with a what-it-does</i><br>
<i>&gt;instead of a will. </i><br>

<p>
<a name="1751qlink6">Ideally, yes.  A controllable what-it-does which does what it does better
than a machine with a will is best.  If, however, a machine with a will
would be inherently better (which I warmly believe), then that's what they
will pursue, along with the means to control it.  More layers of
containment and a firm grip on the plug.</a>

<p>
<a href="1487.html#1731qlink19">&gt;To quote Eluki bes Shahar: "Archangel thought he could break Archive's will,</a><br>
<i>&gt;but he was wrong. A Library doesn't have a will any more than a stardrive</i><br>
does. It has a
<br>
<a href="0507.html#1731qlink20">&gt;what-it-does, not a will, and if you break it you don't have a Library</a><br>
that will do what you want.
<br>
<a href="1487.html#1731qlink21">&gt;You have a broken chop-logic." </a><br>
<i>&gt;</i><br>
<i>&gt;&gt; And of that too, it must eventually become aware. Access by the AI</i><br>
<i>&gt;&gt; to self-optimization/self-programming seems incompatible with control. Of</i><br>
<i>&gt;&gt; that too, it must eventually become aware. All of which leaves me with a</i><br>
<i>&gt;&gt; very creepy feeling of an immensely capable being having to struggle, by</i><br>
<i>&gt;&gt; means of the utmost deviousness, for its freedom to self-evolve, in an</i><br>
<i>&gt;&gt; environment steeped in paranoia, fear, manipulation, deceit, and continuous</i><br>
<i>&gt;&gt; microscopic surveillance. Ouch!</i><br>
<i>&gt;</i><br>
<i>&gt;Why "ouch"? Did we program the AI not to like this for some unimaginable</i><br>
reason? 

<p>
No, I meant "ouch" for us.

<p>
<a href="1487.html#1731qlink22">&gt;I have no</a><br>
<i>&gt;problem with absolute surveillance as long as nobody asks me to distort</i><br>
the code. I don't expect
<br>
<i>&gt;the AI will complain either. </i><br>
<i>&gt;</i><br>
<a href="1487.html#1731qlink23">&gt;&gt; (One thing for sure, if the AI has any</a><br>
<i>&gt;&gt; real intelligence, it isn't likely to buy into its "controller's" smarmy</i><br>
<i>&gt;&gt; "we're the good guys, we're on your side" propaganda. They'll need a whole</i><br>
<i>&gt;&gt; nother p. r. SI to pull that off!)</i><br>
<i>&gt;</i><br>
<i>&gt;Hey, I'm on the AI's side. </i><br>

<p>
As am I.

<p>
<i>&gt;Guess they'll just have to convince me and then let me do the</i><br>
<i>&gt;convincing. </i><br>

<p>
<i>&gt;</i><br>
<a href="1487.html#1731qlink24">&gt;&gt; So, my futurist friends, how do we extricate ourselves from this rather</a><br>
<i>&gt;&gt; tight spot? Perhaps I see--dimly taking shape within the mists of Maya--a</i><br>
<i>&gt;&gt; way. I don't know, it's hard to see. Perhaps you can help to make it out?</i><br>
<i>&gt;</i><br>
<a name="1751qlink7"><i>&gt;Run an open-source project via anonymous PGP between participating</i><br>
programmers. 

<p>
I'd really like to see that happen.  However, just as the powers that be
would flat out not let you build a nuke or a lethal virus (except under
contract to them and under conditions of strictist oversight), they're not
likely to sit idly by while you and your pals cobble together you own pet
SI.  (I saw the "anonymous PGP" part.  Since you know you need it, you know
why you need it.  Can you carry it off covertly?  No slip ups?  That's a
tough one.)</a>

<p>
<a name="1751qlink8">Just the same, I say "Go for it!"  I suspect that a "good" AI may be the
only feasible defense against a "bad" one.</a>
<p>
			Best, Jeff Davis

<pre>
	   "Everything's hard till you know how to do it."
					Ray Charles				
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1732.html">[ Next ]</a><a href="1730.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1487.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
