<!-- received="Thu Aug  5 15:34:43 1999 MDT" -->
<!-- sent="Thu, 05 Aug 1999 16:34:46 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Navigating nano, AI, and uploading (was IA vs AI)" -->
<!-- id="37AA0372.FCABD33E@pobox.com" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=182 -->
<html><head><title>extropians: Navigating nano, AI, and uploading (was IA vs AI)</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Navigating nano, AI, and uploading (was IA vs AI)</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Thu, 05 Aug 1999 16:34:46 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1657">[ date ]</a><a href="index.html#1657">[ thread ]</a><a href="subject.html#1657">[ subject ]</a><a href="author.html#1657">[ author ]</a>
<!-- next="start" -->
<li><a href="1658.html">[ Next ]</a><a href="1656.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1639.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
(WAS: Re: IA vs. AI was: longevity vs singularity)

<p>
den Otter wrote:
<br>
<i>&gt; </i><br>
<a href="1639.html#1657qlink1">&gt; ----------</a><br>
<i>&gt; &gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; I think we have a serious technological disagreement on the costs and</i><br>
<i>&gt; &gt; sophistication of uploading.  My uploading-in-2040 estimate is based on</i><br>
<i>&gt; &gt; the document "Large Scale Analysis of Neural Structures" by Ralph Merkle</i><br>
<i>&gt; &gt; (<a href="http://www.merkle.com/merkleDir/brainAnalysis.html">http://www.merkle.com/merkleDir/brainAnalysis.html</a>) which says</i><br>
<i>&gt; &gt; "Manhattan Project, one person, 2040" - and, I believe, that's for</i><br>
<i>&gt; &gt; destructive uploading.</i><br>
<i>&gt; </i><br>
<i>&gt; Then somewhere else you wrote:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; I would</i><br>
<i>&gt; &gt; expect virtually everything Drexler ever wrote about to be developed</i><br>
<i>&gt; &gt; within a year of the first assembler, after which, if the planet is</i><br>
<i>&gt; &gt; still around, we'll start to get the really *interesting* technologies.</i><br>
<i>&gt; &gt; Nanotechnology is ten times as powerful and versatile as electricity.</i><br>
<i>&gt; &gt; What we forsee is only the tip of the iceberg, the immediate</i><br>
<i>&gt; &gt; possibilities.  Don't be fooled by their awesome raw power into</i><br>
<i>&gt; &gt; categorizing drextechs as "high" nanotechnology.  Drextechs are the</i><br>
<i>&gt; &gt; obvious stuff.  Like I said, I would expect almost all of it to follow</i><br>
<i>&gt; &gt; almost immediately.</i><br>
<i>&gt; </i><br>
<i>&gt; So you've said yourself that nanotech is to be expected around 2015,</i><br>
<i>&gt; and that even today's most advanced Drextech designs would soon be</i><br>
<i>&gt; obsolete. How does this jive with a 25(!!!) year gap between the first</i><br>
<i>&gt; assemblers and uploading? I bet that if nanotech would be as potent as</i><br>
<i>&gt; you assume, full uploading could be feasible in 2020, about the same</i><br>
<i>&gt; time as your AI. If time is no longer a factor, uploading becomes more</i><br>
<i>&gt; than ever the superior option.</i><br>

<p>
The key phrase is CRNS.  As you point out, Merkle's article does not
include nanotech.  In the event that nanotechnology was developed, then
we have a new measure of time: NRNS.  I would expect spaceship
technology at about 6m NRNS, a working lunar colony at 2y NRNS,
uploading at 4y NRNS, active shields against gray goo at 1y NRNS, and
military attack goo at 3m NRNS.  There will never be shields against
military goo - see later post.

<p>
<a href="1639.html#1657qlink2">&gt; Merkle's article is a conservative extrapolation of current technology,</a><br>
<i>&gt; *it does not include nanotech*. I didn't see that quote "Manhattan</i><br>
<i>&gt; project, one person, 2040" either. He simply concludes that:  "If we</i><br>
<i>&gt; use the [conventional] technology that will be available in 10 to</i><br>
<i>&gt; 20 years, if we increase the budget to about one billion dollars,</i><br>
<i>&gt; and if we use specially designed special purpose hardware --</i><br>
<i>&gt; then we can determine the structure of an organ that has long</i><br>
<i>&gt; been of the greatest interest to all humanity, the human brain".</i><br>

<p>
You're right.  Interesting.  Maybe I'm using the wrong source, but
that's the only one I could find, and I really thought that was the one.
 Looking into it, now, I see that what's being proposed isn't actually
uploading, it's just getting a detailed wiring diagram from a series of
brain sections over the course of three years.  I don't think that would
preserve sentience or even basic computational ability - it would just
be a useful hint for AIers, which is what the article is about.  I
really seem to recall a technological estimate for actual non-nanotech
uploading that said 2040.

<p>
<a href="1639.html#1657qlink3">&gt; &gt; You're talking about off-the-shelf uploading.</a><br>
<i>&gt; &gt; You're talking about nondestructive-uploading kiosks at the local</i><br>
<i>&gt; &gt; supermarket.</i><br>
<i>&gt; </i><br>
<i>&gt; No, though these could very well be feasible with 2020 nanotech.</i><br>

<p>
Yeah, about 6y NRNS.

<p>
<a href="1639.html#1657qlink4">&gt; &gt; That's 2060 CRNS</a><br>
<i>&gt; </i><br>
<i>&gt; Yeah, right, 45 years after the first functional assembler. Wow,</i><br>
<i>&gt; progress will actually be *slowing down* in the next century.</i><br>
<i>&gt; Would this result in an anti-singularity or something?</i><br>

<p>
[snip]

<p>
<a href="1639.html#1657qlink5">&gt; &gt; Then they'd damage uploading more.  It's a lot harder to run an</a><br>
<i>&gt; &gt; uploading project using PGP.</i><br>
<i>&gt; </i><br>
<i>&gt; Why would this be harder for IA than for AI?</i><br>

<p>
Because one could at least try to run an AI on a network of home
computers using secure IP, while IA neurosurgery requires a complete
hospital, neuroimaging equipment, and a lab capable of adapting
(admittedly off-the-shelf) technology originally developed for epilepsy
for brain stimulation.

<p>
Uploading requires either a huge lab or nanotechnology.  You can't hide
the lab, and nanotechnology is a whole 'nother issue.

<p>
<a href="1639.html#1657qlink6">&gt; &gt; Do I think nanotechnology is going to blow up the world?  Yes.</a><br>
<i>&gt; </i><br>
<i>&gt; .......but probably not before we (most likely in a substantially</i><br>
<i>&gt; augmented form) can escape to space.</i><br>

<p>
Well, that's where I disagree.  In the event that nanotechnology is
developed, say by Zyvex, either the U.S. government will confiscate all
of it, or Zyvex will have to keep it secret until it can fight the
government and win - and it'll have to develop all the applications on
its own, which makes that possibility fairly unlikely, especially since
the phrase "military applications" doesn't seem to have passed through
their mind.

<p>
So the US confiscates it, and either conquers the world, or keeps it a
secret, or the secret gets out and somebody tries to launch a preemptive
nuclear strike.  I'm not really sure how this would play out, but it
seems to me to end either in a dictatorship or in a nanowar.  I don't
see where the lunar colonies would come from.

<p>
<a href="1639.html#1657qlink7">&gt; &gt; Do I</a><br>
<i>&gt; &gt; lift the smallest finger against it?  No.</i><br>
<i>&gt; </i><br>
<i>&gt; Unless nanotech is absolutely crucial to build your AI, this statement</i><br>
<i>&gt; doesn't make any sense. This race is way too important to worry</i><br>
<i>&gt; about good sportsmanship.</i><br>

<p>
Maybe it's too important *not* to worry about good sportsmanship.  If I
were to assassinate my own role model, not only would I tick off every
single person who could conceivably have helped me, and not only would I
kill the only person with enough wariness and influence to keep things
from getting worse if something *does* go wrong, but I'd also be
inviting retaliatory strikes on AI researchers.  I really don't think
anyone's chances of survival would be helped by a deathmatch among transhumanists.

<p>
I can't keep nanotechnology from being developed - only keep it from it
being developed by the people I can influence, who are probably the best
of all evils.

<p>
<a href="1639.html#1657qlink8">&gt; &gt; I really don't see that much of a difference between vaporizing the</a><br>
<i>&gt; &gt; Earth and just toasting it to charcoal.  Considered as weapons, AIs and</i><br>
<i>&gt; &gt; nanotech have equal destructive power; the difference is that an AI can</i><br>
<i>&gt; &gt; have a conscience.</i><br>
<i>&gt; </i><br>
<i>&gt; It has a *will*, an intelligence (but not necessarily a conscience in</i><br>
<i>&gt; the sense that it feels "guilt"). An ASI is an infinitely more</i><br>
<i>&gt; formidable weapon than nanotech because it can come up with new ways to</i><br>
<i>&gt; crush your defences and kill you at a truly astronomical speed.</i><br>
<i>&gt; Like the Borg, who adjust their shielding after you've shot a couple</i><br>
<i>&gt; of them, only *a lot* more efficient. Nanotech is just stupid goo</i><br>
<i>&gt; that will try to disassemble anything it comes into contact with it</i><br>
<i>&gt; (unless it's another goo nanite -- hey, you could base your defenses</i><br>
<i>&gt; on that). So...avoid contact. Unless the goo is controlled by a</i><br>
<i>&gt; SI (not that it would bother with such hideously primitive</i><br>
<i>&gt; technology), it can be tricked, avoided and destroyed. Try that</i><br>
<i>&gt; with a SI...</i><br>

<p>
It's not goo I'm worried about, it's deliberately developed weapons. 
Yudkowsky's Fourth Threat:  "Technologies with military potential are
*always* used."  I don't think there's much more of a defense against
nanotechnological weaponry than there is against SIs.  I mean, yes, an
SI is unimaginably large overkill while a nanowar is just a little
overkill, but, like I said, what's the difference between vaporizing the
planet and toasting it to charcoal?  Is there that much of a difference
between being shot and being nuked?  Either way, you're dead.

<p>
<a href="1639.html#1657qlink9">&gt; These possibilities are non-trivial, certainly when the military</a><br>
<i>&gt; start fooling around with AI, in which case they're likely to</i><br>
<i>&gt; be the first to have one up and running. Hell, those guys might</i><br>
<i>&gt; even try to stuff the thing into a cruise missile. So no, I don't</i><br>
<i>&gt; see why I should trust an AI more than myself.</i><br>

<p>
I do worry about the possibilities of "AI abuse".  Maybe I don't worry
enough.  Maybe I'm dissing Zyvex for being naive about military
applications while being just as stupid myself.  Maybe *any* technology
capable of saving the world will be capable of blowing it up two years
earlier.  One of the things that makes navigating the future so
interesting is that the problems are not guaranteed solvable.  Sometimes
you're just doomed.  But...

<p>
I guesstimate that, in self-enhancing AI, there's a sharp snap from
prehuman to posthuman intelligence.  With any luck, none of the seed
stages will be intelligent enough to be a major threat, none of the end
stages will be dumb enough to be controllable, and none of the
intervening stages will last long enough to be a problem.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1658.html">[ Next ]</a><a href="1656.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1639.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
</body></html>
