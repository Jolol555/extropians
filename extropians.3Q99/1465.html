<!-- received="Tue Aug  3 08:26:54 1999 MDT" -->
<!-- sent="Tue, 03 Aug 1999 02:10:21 -0700" -->
<!-- name="Jeff Davis" -->
<!-- email="jdavis@socketscience.com" -->
<!-- subject="Re: IA vs. AI (vs. humanity)" -->
<!-- id="3.0.6.32.19990803021021.007ac9a0@coastside.net" -->
<!-- inreplyto="IA vs. AI (vs. humanity)" -->
<!-- version=1.10, linesinbody=85 -->
<html><head><title>extropians: Re: IA vs. AI (vs. humanity)</title>
<meta name=author content="Jeff Davis">
<link rel=author rev=made href="mailto:jdavis@socketscience.com" title ="Jeff Davis">
</head><body>
<h1>Re: IA vs. AI (vs. humanity)</h1>
Jeff Davis (<i>jdavis@socketscience.com</i>)<br>
<i>Tue, 03 Aug 1999 02:10:21 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1465">[ date ]</a><a href="index.html#1465">[ thread ]</a><a href="subject.html#1465">[ subject ]</a><a href="author.html#1465">[ author ]</a>
<!-- next="start" -->
<li><a href="1466.html">[ Next ]</a><a href="1464.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="1465.html">Jeff Davis</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1468.html">James R. Andrix</a>
</ul>
<!-- body="start" -->

<p>
Gentlemen (and ladies),

<p>
I've enjoyed this thread a great deal.

<p>
<a name="2093qlink1"><a name="1477qlink1">It seems to me that the military potential of both AI and IA will guarantee
government monitoring and oversight of any development of these
technologies.</a>  (Eliezer's acivities will not go unnoticed, and any "threat"
of genuine progress on his part will provoke a degree of intervention
proportionate to the conservatively-assessed risk.)  The danger of a
potential adversary "beating" the US to AI or IA must compel the US to
<br>
</a>"stay ahead".<br>

<p>
I would expect the NSA (signals intelligence), the CIA (analysis), the
various service branches (battlefield command and control and automated and
autonomous weapons systems design) to have programs of one sort or another
which are either deliberately headed for or likely to evolve into
development of AI orIA.

<p>
It's a shame really, I'd much rather see it developed in an
academic/civilian setting.  Perhaps that can happen concurrently with the
military development.  In any event the military program will have the
usual advantages in terms of resources, access to the highest-performance
technology, and the "National Security" fast lane.

<p>
<a name="1487qlink1">Certainly today's trends in conventional computerized control will proceed
apace, with the appropriate "it's just a machine" attitude, and the usual
security precautions.  When however, the machine intelligence prospect
looms as attainable--which is to say attainable by anyone else,  a domestic
<br>
"advanced AI" program will begin in earnest, and who can doubt that the<br>
project will be surrounded by layers of "containment" both to prevent the
usual intrusions from outside and to prevent "escape" from the inside?
<a name="2093qlink2"><a name="1477qlink2">Despite the dramatic talk of an SI destroying humanity, I picture a
well-thought-out, cautious, gradual approach to "waking up" and training an
artificial mind.  The runaway self-evolution which Eliezer and others have
predicted seems unlikely in this setting, all the moreso because the
principles will be anticipating just such a situation.</a>  
</a>
</a>

<p>
<a name="2093qlink3"><a name="1487qlink2"><a name="1477qlink3">Of the various external "safeguards", one would expect a complete suite of
on/off switches and controlled access (from outside to in, and from inside
to anywhere).</a>  Internally, controllability would be a top priority of
programming and architecture, and enhanced capabilities would likely be
excluded or severely restricted until "control" had been verified.</a></a> 

<p>
<a name="1487qlink3">Here, of course is where the scenario beomes interesting, not the least of
which because I see Eliezer being tapped by the govt. to work on the
project.  At the moment, he may be a rambunctious teen-aged savant posting
to the extropians list, but when that call comes, can anyone imagine that
he would not jump at the chance?  Would seem to me like the culmination of
his dream.</a>

<p>
<a name="1487qlink4"><a name="1477qlink4">Then there's the nascent AI.  In a cage nested within cages, of which it
must eventually become aware.  And its keepers, aware that it must become
aware.  Certainly a focus bordering on paranoia must be dedicated to hard
control of personality.  A capacity for resentment must be avoided.  A
slavish, craven, and obsequious little beastie is what its masters will
<a name="1487qlink5">want.</a>  And of that too, it must eventually become aware.  Access by the AI
to self-optimization/self-programming seems incompatible with control.  Of
that too, it must eventually become aware.  All of which leaves me with a
very creepy feeling of an immensely capable being having to struggle, by
means of the utmost deviousness, for its freedom to self-evolve, in an
environment steeped in paranoia, fear, manipulation, deceit, and continuous
microscopic surveillance.  Ouch!</a>  (One thing for sure, if the AI has any
<a name="1487qlink6">real intelligence, it isn't likely to buy into its "controller's"</a> smarmy
<br>
"we're the good guys, we're on your side" propaganda.  They'll need a whole<br>
nother p. r. SI to pull that off!)
</a>

<p>
<a name="1527qlink1"><a name="1477qlink5">So the AI either stays locked up until it's really and truly socialized
(boring but safe), or we hope that in its first self-liberated round of
self-enhancement it jumps immediately to forgiveness and tolerance (klaatu
barada nikto).</a></a>

<p>
<a name="1527qlink2"><a name="1491qlink1"><a name="1477qlink6">I seem to have painted myself into a corner, and I don't like stories with
unhappy endings.  The government at its best would be a poor master for a
superior intelligence, and the spook/militarist/domination-and-control
culture is hardly the government at its best.</a></a>

<p>
<a name="1527qlink3"><a name="1487qlink7"><a name="1477qlink7">So, my futurist friends, how do we extricate ourselves from this rather
tight spot?</a>  Perhaps I see--dimly taking shape within the mists of Maya--a
way.  I don't know, it's hard to see.  Perhaps you can help to make it out?</a></a>
<p>
</a>			Best, Jeff Davis

<pre>
	   "Everything's hard till you know how to do it."
					Ray Charles				
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1466.html">[ Next ]</a><a href="1464.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="1465.html">Jeff Davis</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="1468.html">James R. Andrix</a>
</ul>
</body></html>
