<!-- received="Thu Jul 29 10:11:59 1999 MDT" -->
<!-- sent="Thu, 29 Jul 1999 11:11:49 -0500" -->
<!-- name="Billy Brown" -->
<!-- email="ewbrownv@mindspring.com" -->
<!-- subject="RE: The Major League Extinction Challenge" -->
<!-- id="01BED9B3.2C43D950.ewbrownv@mindspring.com" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=70 -->
<html><head><title>extropians: RE: The Major League Extinction Challenge</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:ewbrownv@mindspring.com" title ="Billy Brown">
</head><body>
<h1>RE: The Major League Extinction Challenge</h1>
Billy Brown (<i>ewbrownv@mindspring.com</i>)<br>
<i>Thu, 29 Jul 1999 11:11:49 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1315">[ date ]</a><a href="index.html#1315">[ thread ]</a><a href="subject.html#1315">[ subject ]</a><a href="author.html#1315">[ author ]</a>
<!-- next="start" -->
<li><a href="1316.html">[ Next ]</a><a href="1314.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1299.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky [SMTP:sentience@pobox.com] wrote:
<br>
<a href="1299.html#1315qlink1">&gt; Problem is, choosing to commit suicide is still a choice - and that's</a><br>
<i>&gt; not what I'm hypothesizing.  At that level, I don't have the vaguest</i><br>
<i>&gt; notion of what would really happen if an SI's goal system collapsed.</i><br>
<i>&gt; The whole lapse-to-quiesence thing in Elisson is a design feature that</i><br>
<i>&gt; involves a deliberate tradeoff of optimization to achieve a graceful </i><br>
shutdown.

<p>
I know.  But if they all have the same hoghly optimized cognitive 
architecture by the time they reach this point, and it happens to give a 
similar result, then we could actually get the whole civilization to shut 
down.  Of course, that still leaves eveyone's automation running, and a lot 
of it is likely to be sentient...

<p>
No, I don't think it could really happen either.  The whole scenario is 
just too contrived, and it definitely requires that the target civilization 
make mistakes (which isn't exactly a guide to useful thought when you're 
talking about SIs).  However, it beats anything else I've seen.  Killing 
off a civilization like this without sterilizing the universe is pretty 
hard to do.

<p>
<a href="1299.html#1315qlink2">&gt; Well, if you're interested in a not-so-known-laws-of-physics</a><br>
<i>&gt; speculation:  The various colonies achieve SI more or less</i><br>
<i>&gt; simultaneously, or unavoidably.  The first thing an SI does is leave our</i><br>
<i>&gt; Universe.  But, this requires a large-scale energetic event - like, say,</i><br>
<i>&gt; a supernova.</i><br>
<i>&gt;</i><br>
<i>&gt; Still doesn't solve the Great Filter Paradox, though.  Some hivemind</i><br>
<i>&gt; races will have the willpower to avoid Singularity, period.  This</i><br>
<i>&gt; scenario takes mortals and Powers out of the picture during a</i><br>
<i>&gt; Singularity, but it doesn't account for the deliberate hunting-down that</i><br>
<i>&gt; would be needed.</i><br>

<p>
Agreed.

<p>
<a href="1299.html#1315qlink3">&gt; I think the most plausible argument is this:  Every advance in</a><br>
<i>&gt; technology has advanced the technology of offense over the technology of</i><br>
<i>&gt; defense, while decreasing the cost required for global destruction.</i><br>
<i>&gt; There are no shields against nuclear weapons - not right now, anyway -</i><br>
<i>&gt; and we've certainly managed to concentrate that power more than it's</i><br>
<i>&gt; ever been concentrated before.  In fact, the more technology advances,</i><br>
<i>&gt; the easier it becomes to cause mass destruction by *accident*.  It holds</i><br>
<i>&gt; true from nuclear weapons, to biological warefare, to the Y2K crash, to</i><br>
<i>&gt; nanotechnology.  All you really need to assume is that the trend</i><br>
<i>&gt; continues.  Eventually one guy with a basement lab can blow up the</i><br>
<i>&gt; planet and there's nothing anyone can do about it.</i><br>

<p>
Maybe.  But I think your example is an artifact of the nature of our recent 
historical situation.  Nuclear weapons are a superweapon primarily because 
we can't disperse ourselves properly (Earth being too small for that 
purpose), and biological weapons have mass-destruction potential because we 
can't improve our immune systems.

<p>
If we were going to have a future without AI/IA, I think it is clear that 
these trends would reverse within a century.  Spreading into interplanetary 
space would give us plenty of room to build economical defenses against 
nuclear weapons, and the combination of sealed environments and competent 
genetic engineering would make bioweapons a very limited threat. 
 Evaluating the potential of nanotechnology is always tricky, but I think 
it actually ends up giving the defender a substantial advantage once it 
reaches a reasonable level of maturity.

<p>
IMO, the hard part is surviving the transition period we are currently in, 
where we have increasingly advanced destructive technologies but there is 
no room in which to deploy countermeasures.

<p>
Billy Brown, MCSE+I
<br>
ewbrownv@mindspring.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1316.html">[ Next ]</a><a href="1314.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1299.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
