<!-- received="Tue Aug  3 13:56:35 1999 MDT" -->
<!-- sent="3 Aug 1999 13:00:20 -0700" -->
<!-- name="paul@i2.to" -->
<!-- email="paul@i2.to" -->
<!-- subject="Re: AI vs. uploading" -->
<!-- id="19990803200020.19895.cpmta@c004.sfo.cp.net" -->
<!-- inreplyto="AI vs. uploading" -->
<!-- version=1.10, linesinbody=82 -->
<html><head><title>extropians: Re: AI vs. uploading</title>
<meta name=author content="paul@i2.to">
<link rel=author rev=made href="mailto:paul@i2.to" title ="paul@i2.to">
</head><body>
<h1>Re: AI vs. uploading</h1>
<i>paul@i2.to</i><br>
<i>3 Aug 1999 13:00:20 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1488">[ date ]</a><a href="index.html#1488">[ thread ]</a><a href="subject.html#1488">[ subject ]</a><a href="author.html#1488">[ author ]</a>
<!-- next="start" -->
<li><a href="1489.html">[ Next ]</a><a href="1487.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1443.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
On Tue, 03 August 1999, "Eliezer S. Yudkowsky" wrote:

<p>
<a href="1443.html#1488qlink1">&gt; Let me get this straight.  You're using the lousy behavior of humans as</a><br>
<i>&gt; an argument *against* AI?</i><br>

<p>
No, I'm using the lousy behavior and lack of ethics among many strong AI
researchers as an argument against AI. Now if Minsky wants to build
a goal system around somebody like Mahatma Ghandi, then I'll start
listening again.

<p>
<a href="1443.html#1488qlink2">&gt; &gt; The</a><br>
<i>&gt; &gt; assholes can barely behave themselves in the most controlled</i><br>
<i>&gt; &gt; social settings, and you want me to hedge my bets and my</i><br>
<i>&gt; &gt; life in the hands of this lot?  Gimme a break!</i><br>
<i>&gt; </i><br>
<i>&gt; I think you are perhaps overgeneralizing just a tad.  In fact, you've</i><br>
<i>&gt; just insulted... let's see, Douglas Hofstadter, Douglas Lenat, and...</i><br>
<i>&gt; well, actually that exhausts my store of people I'd challenge you to a</i><br>
<i>&gt; duel over, but I'm still a touch offended</i><br>

<p>
You shouldn't be offended, because I'm only speaking of the people
I have personally met.  And I have never met Hofstadter or Lenat.
 
<p>
<a href="1443.html#1488qlink3">&gt; &gt; Eli, you want to create an SI before nanotech destroys</a><br>
<i>&gt; &gt; everything.  People like Den Otter, Max More and myself want</i><br>
<i>&gt; &gt; to IA ourselves to singularity before the SI's destroy us!</i><br>
<i>&gt; </i><br>
<i>&gt; Why are you including Max More on the list?  I have him down as an</i><br>
<i>&gt; benovelent-Powers neutral in the IA-vs-AI debate, unless he's changed</i><br>
<i>&gt; his mind lately.</i><br>

<p>
Uhm, he quite clearly stated in an earlier post that regardless of the
benevolence or hostility of SI's, *we* better deal with them
from a position of strength either way.  I asked him to clarify what
this position of strength was. but he has not responded.  Now unless
he knows of another trick up his sleeve, this position of strength must
come from our intelligence enhancement.  Until he responds otherwise,
he is clearly arguing for at minimum IA == AI.

<p>
<a href="1443.html#1488qlink4">&gt; As for you and den Otter... won't you feel like *such* idiots if the</a><br>
<i>&gt; Powers would have been benevolent but you manage to kill yourself with</i><br>
<i>&gt; grey goo instead?</i><br>

<p>
No I *won't* feel like an idiot!  See, that is where you and I differ in
the extreme.  I actually believe in my own ability to make the right
decisions.  The very reason I am extropian in the first place is a strong
belief in evolving my *own* mind to become a power.  I have esteem,
self-confidence and faith in my own abilities.  You appear to have little
belief in yourself or others, which is why you seem obsessed with
having an SI carry on the torch of evolution instead. Their is no evidence
that destroying ourselves with goo is any more likely than SI's
destroying us with goo.  Therefore, until proven otherwise,
I favor my own ethics and goals over the goals and ethics of anyone
else, human or SI.

<p>
<a href="1443.html#1488qlink5">&gt; Anyway, a touch of IA and you guys will be following me into Externalist-land.</a><br>

<p>
Huh?

<p>
<a href="1443.html#1488qlink6">&gt; Actually, it's 2015 vs. 2020, although Drexler's been talking about</a><br>
<i>&gt; 2012, so I'd figured on 2010 for an attempted target date and 2005 for</i><br>
<i>&gt; the earliest possible target date.</i><br>
<i>&gt; </i><br>
<i>&gt; Am I less of a fool?  Yes, I'm ten years less of a fool.  I'm the</i><br>
<i>&gt; absolute minimum of fool that I can possibly manage.</i><br>

<p>
You have yet to prove any of this. These dates are
still conjecture. The only place we agree, is that unless there
are breakthroughs in unforeseen directions, the sequence of technological
possibilities are the same. Unlike many extropians I don't believe
fervently in the dictum that if we can do something we should so
something - and that includes conscious AI's.  If it is even remotely
possible to evolve our own intelligence at the same speed as an AI,
then we should so so. Bottom line, I know and trust my goals. They
exist today and are tangible.  If you want a larger audience, I suggest you
start studying ethics!  Until I see a strong ethical basis among AI researchers
I will remain steadfastly opposed to them.

<p>
Paul Hughes
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1489.html">[ Next ]</a><a href="1487.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1443.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
