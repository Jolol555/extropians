<!-- received="Fri Sep 10 23:27:37 1999 MDT" -->
<!-- sent="Fri, 10 Sep 1999 23:28:23 -0600" -->
<!-- name="David Blenkinsop" -->
<!-- email="blenl@sk.sympatico.ca" -->
<!-- subject="Re: Dreams of Autarky" -->
<!-- id="37D9E877.59A67A26@sk.sympatico.ca" -->
<!-- inreplyto="Dreams of Autarky" -->
<!-- version=1.10, linesinbody=123 -->
<html><head><title>extropians: Re: Dreams of Autarky</title>
<meta name=author content="David Blenkinsop">
<link rel=author rev=made href="mailto:blenl@sk.sympatico.ca" title ="David Blenkinsop">
</head><body>
<h1>Re: Dreams of Autarky</h1>
David Blenkinsop (<i>blenl@sk.sympatico.ca</i>)<br>
<i>Fri, 10 Sep 1999 23:28:23 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3919">[ date ]</a><a href="index.html#3919">[ thread ]</a><a href="subject.html#3919">[ subject ]</a><a href="author.html#3919">[ author ]</a>
<!-- next="start" -->
<li><a href="3920.html">[ Next ]</a><a href="3918.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3830.html">Robin Hanson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="4335.html">Robin Hanson</a>
</ul>
<!-- body="start" -->


<p>
Robin Hanson wrote:
<br>
<i>&gt; </i><br>
<a href="3830.html#3919qlink1">&gt; It occurs to me that a common theme in many dreams of the future is an</a><br>
<i>&gt; unusual degree of autarky, or independence.</i><br>

<p>
That seems right, although sf stories usually dramatize the "what could
go wrong" aspect of this. I think of "interstellar ark" stories in
particular. IIRC, Heinlein's story "Universe" had a two-headed mutant
barbarian from a spaceship's drive core trying to invade the farming
communities in the shielded areas (or something like that).
  
<p>
<i>&gt; </i><br>
<a href="3830.html#3919qlink2">&gt; I find it hard to escape wondering if these dreams of autarky are not</a><br>
<i>&gt; mostly the</i><br>
<i>&gt; result of humans not yet coming fully to terms with our new interdependence.</i><br>
<i>&gt; Biology created largely autonomous creatures, and only recently discovered a</i><br>
<i>&gt; world-wide division of labor in us.  So our cells are largely</i><br>
<i>&gt; autonomous manufacturing plants, our minds are general and broadly</i><br>
<i>&gt; capable, and we picture our ideal political unit and future home to be the</i><br>
<i>&gt; self-sufficient small tribe of our evolutionary heritage.  (Such tribes had</i><br>
<i>&gt; much to fear from strong dependence on neighboring tribes.)</i><br>

<p>
<a name="4335qlink1">Don't forget that it's just plain simpler to look at a complicated
global situation by way of ignoring interdependencies most of the time,
so as to focus on whatever the local problems are. You are doing this
yourself when you think of organisms as "autonomous" while ignoring the
ever present demands of ecology and of trying to make a living in those
biological niches! Admittedly, though, the long distance trading
relationships of humans are a really new form of interdependence.</a>
Ordinary animals don't deliberately give up one thing so that they can
get something else from other animals halfway around the globe.

<p>
Anyway, as examples of autarky dreams, you mention strong AI, nanotech,
space colonies, and:

<p>
<a href="3830.html#3919qlink3">&gt; LOCAL SINGULARITY:  This imagines one small group suddenly grows big enough</a><br>
<i>&gt; to take over everything.  Our familiar world economy grows as a unit, with</i><br>
<i>&gt; innovations and advances in each part of the world depending on advances</i><br>
<i>&gt; made in all other parts of the world.  The problem of designing smaller</i><br>
<i>&gt; chips, for example, keeps getting harder, but a richer world can afford to</i><br>
<i>&gt; spend more and more solving this problem.  A local singularity, in contrast,</i><br>
<i>&gt; would have a small group continue to make dramatic advances by substantially</i><br>
<i>&gt; improving their own ability to make more advances.  The abilities of such a</i><br>
<i>&gt; group might quickly grow so large that they can essentially take over</i><br>
<i>&gt; everything.</i><br>

<p>
<a name="4335qlink2">Ah, more about pros/cons of a Singularity, or as you indicate more
specifically, a "Local Singularity", with the tech power of one group
steamrollering the world! One tricky thing about this seemingly
unprecedented scenario is that it's not really unprecedented? What's a
political "coup" but someone seizing of all the military connections
needed to control the soldiers, tanks, etc., in some particular country?</a>
<pre>
At that point, the coup-master either does things to destroy the free
trade economy, or more likely does his best to control more and more of
it. Basically, an isolated super-tech advance is just one more way
</pre>
for someone to try this sort of thing.

<p>
<a name="4335qlink3">Is this something to worry about, that the world's most powerful
countries might get tech-couped, and just roll over the rest of the
globe in some strange new empire? This seems to be the center of your
concerns, but I'm not sure that your comments do anything to prove that
it's really impossible!</a> Scary stuff, this, and if the existing "Powers
That Be" find some reason to get seriously concerned about it, maybe
*they'll* evolve into something as bad as a coup-master would have been!
I'd see your own emphasis on the likelihood of economic interdependence
as a way to forestall any scary tech-coup scenarios?
  
<p>
<i>&gt;</i><br>
<i>&gt; </i><br>
<a href="3830.html#3919qlink4">&gt; So manufacturing plants may slowly get smaller and better, without a</a><br>
<i>&gt; sudden "assembler" revolution.  Local space may stay un-colonized until we can</i><br>
<i>&gt; cheaply send lots of mass up there.  Software may slowly get smarter and be</i><br>
<i>&gt; much</i><br>
<i>&gt; smarter than people long before anyone bothers to make a single module</i><br>
<i>&gt; that can pass a Turing test.  And distant stars may not get colonized for a</i><br>
<i>&gt; long long time.</i><br>

<p>
<a name="4335qlink4">Fortunately, the step by step nature of technology advance *does* tend
to argue against the likelihood of an easy or thorough global coup.
Actually, to be a bit more precise about it, surely the halting, trial
and error process of technology argues against a global coup done solely
or even primarily through tech advance!</a> Here I go with a recollected
science fictional reference again; does anyone recall an Arthur Clarke
short story where the high tech military cruisers got all beat up by the
relatively low tech guys because of not having the bugs worked out yet?
Inasmuch as AI would have to be a *practical* development, I'd tend to
agree that it isn't some sort of "world takeover magic".

<p>
As to comments about manufacturing plants and space colonies, aren't
these really quite separate issues? Here's an analogy. Suppose someone a
hundred years ago had gotten quite upset about the prospect that
wireless telegraphy, or radio, just might allow some empire to expand
it's armies farther than ever before, maybe even taking over the world!
If you think about it, there must have been an actual day in history
when the first army issue radio transmitters appeared in the hands of
field commanders, *that*, I suppose, would be the "Radio Singularity" --
which we've already passed, sometime early on in our current, twentieth
century! Suppose that our previously concerned person had made a
prediction that radio was too difficult, that its adoption couldn't
happen for a long time because that would ruin the interdependence of
the world! That would surely have been a wrong way of looking at things,
wouldn't it?

<p>
In sum, concerns about technology based coups don't seem to me a good
reason for judging one way or the other as to whether any special
technology will be readily put into use. For nano-factories, what are
the likely mechanics of them, and will some kinds be much easier to
debug than others? For space colonies, do the nano-factories provide for
economical lunar mines and resource use, so that people can actually
build the things? Maybe political philosophy really does get in the way
of thinking about the future, but why not at least take the prospects
for breakthroughs into account as best we can?


<p>
David Blenkinsop &lt;blenl@sk.sympatico.ca&gt;


<p>
"We're going to do the same thing we do every day, Pinky, TRY TO TAKE
OVER THE WORLD . . . "

<p>
I always wanted an excuse to use that line for a sig :^)
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3920.html">[ Next ]</a><a href="3918.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3830.html">Robin Hanson</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="4335.html">Robin Hanson</a>
</ul>
</body></html>
