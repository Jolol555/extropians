<!-- received="Fri Jul  2 14:24:39 1999 MDT" -->
<!-- sent="Fri, 02 Jul 1999 16:21:30 -0400" -->
<!-- name="Christopher Maloney" -->
<!-- email="dude@chrismaloney.com" -->
<!-- subject="Re: Seed AI" -->
<!-- id="377D1F4A.7078@chrismaloney.com" -->
<!-- inreplyto="Seed AI" -->
<!-- version=1.10, linesinbody=102 -->
<html><head><title>extropians: Re: Seed AI</title>
<meta name=author content="Christopher Maloney">
<link rel=author rev=made href="mailto:dude@chrismaloney.com" title ="Christopher Maloney">
</head><body>
<h1>Re: Seed AI</h1>
Christopher Maloney (<i>dude@chrismaloney.com</i>)<br>
<i>Fri, 02 Jul 1999 16:21:30 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#67">[ date ]</a><a href="index.html#67">[ thread ]</a><a href="subject.html#67">[ subject ]</a><a href="author.html#67">[ author ]</a>
<!-- next="start" -->
<li><a href="0068.html">[ Next ]</a><a href="0066.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0001.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Sending this again ... sorry if it gets duplicated, I don't know
what the deal is.  I've waited 16 hrs and it didn't show up.


<p>
Eliezer S. Yudkowsky wrote:
<br>
<i>&gt; </i><br>
<a href="0001.html#0067qlink1">&gt; "O'Regan, Emlyn" wrote:</a><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; Eliezer,</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; The seed AI concept has been rattling around in my head for a while. What's</i><br>
<i>&gt; &gt; buggin me is the concept of environment. I asked you what environment you</i><br>
<i>&gt; &gt; expected would be necessary for the AI a while back, and you said that the</i><br>
<i>&gt; &gt; environment would be the AI's code itself</i><br>
<i>&gt; </i><br>
<i>&gt; Yup.</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; (I think - I could have missed</i><br>
<i>&gt; &gt; your point, please have patience with a mere mortal).</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; Absolutely, this must be the core of the environment. But there must be more</i><br>
<i>&gt; &gt; than this, mustn't there? For an AI to optimise itself, there must be some</i><br>
<i>&gt; &gt; definition of optimal, which implies a frame of reference. I think that</i><br>
<i>&gt; &gt; frame must be external to the code, because the idea of optimising you code</i><br>
<i>&gt; &gt; to make you better at optimising your code has an unfortunately circular and</i><br>
<i>&gt; &gt; empty feel to it.</i><br>
<i>&gt; </i><br>
<i>&gt; Only to the same mathematicians who gave us propositional logic.  I</i><br>
<i>&gt; mean, it sounds circular in theory, but in practice, it doesn't work</i><br>
<i>&gt; that way, for a very simple reason which, unfortunately, doesn't</i><br>
<i>&gt; translate into English without a blackboard.  What I'd like to say is</i><br>
<i>&gt; that the AI is reductive and the elements present obvious</i><br>
<i>&gt; suboptimization metrics, and that the elements sum to noncodic abilities</i><br>
<i>&gt; as well as codic abilities, so codic optimization is non-sterile.</i><br>
<i>&gt; </i><br>
<i>&gt; Okay, try this.  The AI isn't composed of a single, "code-optimizing"</i><br>
<i>&gt; domdule, right?  It's composed of a causal analysis module and a</i><br>
<i>&gt; combinatorial design module and a heuristic soup module and so on.</i><br>
<i>&gt; These architectural modules, plus analogies to other application</i><br>
<i>&gt; domdules, plus the codic domdule, all sum to the "code-optimizing"</i><br>
<i>&gt; ability.  In a given optimization problem, you have subproblems that are</i><br>
<i>&gt; spread across the domdules.  The performance on the subproblems, and the</i><br>
<i>&gt; contribution of individual domdules to the success on subproblems, allow</i><br>
<i>&gt; for local optimization.</i><br>
<i>&gt; </i><br>
<i>&gt; No, I'm still being too complex.  Remember EURISKO?  It had heuristics</i><br>
<i>&gt; optimizing heuristics?  There were even heuristics being optimized to be</i><br>
<i>&gt; better heuristic-optimizers?  It wasn't sterile.  Why?  Because the</i><br>
<i>&gt; heuristics were also being optimized for all sorts of other problems.</i><br>
<i>&gt; And, more importantly, because "Examine nearby cases" being applied to</i><br>
<i>&gt; "Investigate extreme cases" to yield "Investigate cases close to</i><br>
<i>&gt; extremes" is a lot more specific than "optimize heuristics for</i><br>
<i>&gt; heuristic-optimizing".  The general case sounds sterile and circular</i><br>
<i>&gt; because it's monolithic and general.  But in actuality, you have</i><br>
<i>&gt; sub-abilities dealing with subproblems.</i><br>

<p>
It sounds like one could argue that that is all humanity is
engaged in now, and ever has been.  In a way, everything is
a "sub-problem" of the problem of advancement to the next
level.  Or is that stretching it too much?

<p>
<a name="0074qlink2">I mean, when I think about your suggestion, I can imagine what
(for example) my wife would say.  Something like "what a dry
and sterile existence"!  But I would argue, and maybe you would
too, that the "sub-problems" are really what life is all about.
And I always think it's rather Anthro-centric to think that 
appreciating art and beauty and so forth are peculiar human
<a name="0074qlink3">abilities.</a>  I personally think that they are just ways of 
thinking about patterns and analogies, an ability which an AI 
will certainly have.  I.e., "sub-problems"!</a>



<p>
<a href="0001.html#0067qlink2">&gt; You don't even need any other kinds of problems at all.  The subproblems</a><br>
<i>&gt; of the general problem of self-optimization provide enough diversity to</i><br>
<i>&gt; prevent the circularity you're worried about.  The major reason for</i><br>
<i>&gt; programming other environments would be to provide sources of analogies</i><br>
<i>&gt; and incremental paths to ideas that would bottleneck otherwise - the</i><br>
<i>&gt; same reason a hacker learns languages ve'll never program in.  But it's</i><br>
<i>&gt; not *necessary*.</i><br>
<i>&gt; </i><br>
<i>&gt; -</i><br>
<i>&gt; </i><br>
<i>&gt; BTW - the surrealism involved in saying "Stop holding on to the past" to</i><br>
<i>&gt; a Singularitarian is considerably larger than that involved in saying it</i><br>
<i>&gt; to a 19-year-old.</i><br>
<i>&gt; --</i><br>
<a name="2775qlink3"><i>&gt;            sentience@pobox.com          Eliezer S. Yudkowsky</i><br>
<i>&gt;         <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a></i><br>
<i>&gt; Running on BeOS           Typing in Dvorak          Programming with Patterns</i><br>
<i>&gt; Voting for Libertarians   Heading for Singularity   There Is A Better Way</i><br>
</a>

<pre>
-- 
Chris Maloney
<a href="http://www.chrismaloney.com">http://www.chrismaloney.com</a>

"Knowledge is good"
-- Emil Faber
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0068.html">[ Next ]</a><a href="0066.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0001.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
