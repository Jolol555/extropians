<!-- received="Sun Aug  1 20:34:59 1999 MDT" -->
<!-- sent="Sun, 01 Aug 1999 21:34:51 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: IA vs. AI was: longevity vs singularity" -->
<!-- id="37A503C9.C334D02B@pobox.com" -->
<!-- inreplyto="IA vs. AI was: longevity vs singularity" -->
<!-- version=1.10, linesinbody=133 -->
<html><head><title>extropians: Re: IA vs. AI was: longevity vs singularity</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: IA vs. AI was: longevity vs singularity</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sun, 01 Aug 1999 21:34:51 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1382">[ date ]</a><a href="index.html#1382">[ thread ]</a><a href="subject.html#1382">[ subject ]</a><a href="author.html#1382">[ author ]</a>
<!-- next="start" -->
<li><a href="1383.html">[ Next ]</a><a href="1381.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1374.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
den Otter wrote:
<br>
<i>&gt; </i><br>
<i>&gt; ----------</i><br>
<i>&gt; &gt; From: Max More &lt;max@maxmore.com&gt;</i><br>
<a href="1374.html#1382qlink1">&gt; &gt; This must be where we differ. No, I don't think total control is desirable</a><br>
<i>&gt; &gt; or beneficial, even if it were me who had that total control. If true</i><br>
<i>&gt; &gt; omnipotence were possible, maybe what you are saying would follow, but</i><br>
<i>&gt; &gt; omnipotence is a fantasy to be reserved for religions. Even superpowerful</i><br>
<i>&gt; &gt; and ultraintelligent beings should benefit from cooperation and exchange.</i><br>
<i>&gt; </i><br>
<i>&gt; I find it extremely hard to imagine how something which can expand</i><br>
<i>&gt; and modify its mind and body at will could ever need peers to</i><br>
<i>&gt; cooperate with. If a SI can't entertain itself it isn't a real SI, and</i><br>
<i>&gt; when it runs into some obstacle it can simply manufacture more</i><br>
<i>&gt; computing modules, and/or experiment with new thought structures.</i><br>

<p>
No argument here.

<p>
<a href="1374.html#1382qlink2">&gt; I think it's fair to assume that a SI would be essentially immortal,</a><br>
<i>&gt; so there's no need to hurry.</i><br>

<p>
I don't know.  My original Singularity scenario was a whole
branch-1e60-Universes-per-second deal, so losing one second would drop
the "ultimate good" by a factor of 1e60.  Now that I've managed to
eliminate virtually every single Great Filter solution short of reality
being a computer simulation, which I still don't believe, I've simply
given up and admitted I don't know.  Whether or not Powers experience
any kind of "urgency" is another coinflip.

<p>
<i>&gt; Even if there's such a thing as the end</i><br>
<a href="1374.html#1382qlink3">&gt; of the universe, it would still have billions of years to find a solution,</a><br>
<i>&gt; which is ample time for even a human-level intelligence. Needless</i><br>
<i>&gt; (or perhaps not) to say, a SI would never be "lonely" because a)</i><br>
<i>&gt; it could and no doubt would drop our evolution-imposed urge for</i><br>
<i>&gt; company, it having outlived its usefulness, and b) it could</i><br>
<i>&gt; simply spawn another mind child, or otherwise fool around with</i><br>
<i>&gt; its consciousness, taking as much (or little) risk as it wanted</i><br>
<i>&gt; should it ever feel like it.</i><br>

<p>
This all sounds right to me.

<p>
<a href="1374.html#1382qlink4">&gt; &gt; Despite my disagreement with your zero-sum assumptions (if I'm getting your</a><br>
<i>&gt; &gt; views right--I only just starting reading this thread and you may simply be</i><br>
<i>&gt; &gt; running with someone else's assumptions for the sake of the argument), I</i><br>
<i>&gt; &gt; agree with this. While uploads and SI's may not have any inevitable desire</i><br>
<i>&gt; &gt; to wipe us out, some might well want to, and I agree that it makes sense to</i><br>
<i>&gt; &gt; deal with that from a position of strength.</i><br>
<i>&gt; </i><br>
<i>&gt; Exactly, just to be on the safe side we should only start experimenting</i><br>
<i>&gt; with strong AI after having reached a trans/posthuman status our-</i><br>
<i>&gt; selves. If you're going to play God, better have His power. Even</i><br>
<i>&gt; if I'm completely wrong about rational motivations, there could be</i><br>
<i>&gt; a billion other reasons why a SI would want to harm humans.</i><br>

<p>
You keep on talking about what you're going to do because of your goals.
 That's legitimate.  But, don't you think you should try to first
project the situation *without* your intervention?  It's all well and
good to try to influence reality but you should really have some idea of
what you're influencing.

<p>
When I try a Yudkowskyless projection, I get nanowar before AI before
uploading.  I'm trying to accelerate AI because that's the first
desirable item in the sequence.  Uploading is just too far down.  If it
was the other way around I'd be a big uploading fan and I wouldn't
bother with AI except as a toy.

<p>
That's the way navigating the future is supposed to be; find the most
probable desirable future, find the leverage points, and apply all
possible force to get there.  Clean, simple, elegant.  The problem with
selfness - not "selfishness", but holding on to your initial priorities
- is that it introduces all kinds of constraints and unnecessary risks.

<p>
<a href="1374.html#1382qlink5">&gt; &gt; I'm not sure how much we can influence the relative pace of research into</a><br>
<i>&gt; &gt; unfettered independent SIs vs. augmentation of human intelligence, but I</i><br>
<i>&gt; </i><br>
<i>&gt; We won't know until we try. Nothing to lose, so why not? It's</i><br>
<i>&gt; *definitely not* a waste of time, like Eliezer (who has a</i><br>
<i>&gt; different agenda anyway) would like us to belief.</i><br>

<p>
I beg your pardon.  I have never, ever said that IA is a waste of time. 
*Uploading* is pretty much a waste of time.  Neurohacking is a good
thing.  Of course, the legalities and the time-to-adolescence means that
concentrating on finding natural neurohacks like yours truly will be
more cost-effective.

<p>
<a href="1374.html#1382qlink6">&gt; &gt; too favor the latter. Unlike Hans Moravec and (if I've read him right,</a><br>
<i>&gt; &gt; Eliezer), I have no interest in being superceded by something better. I</i><br>
<i>&gt; &gt; want to *become* something better.</i><br>
<i>&gt; </i><br>
<i>&gt; I saw an interview with Moravec the other day in some Discovery</i><br>
<i>&gt; Channel program about (surprise, surprise) robots. He seemed</i><br>
<i>&gt; to be, yet again, sincere in his belief that it's somehow right</i><br>
<i>&gt; that AIs will replace us, that the future belongs to them and</i><br>
<i>&gt; not to us. He apparently finds comfort in the idea that they'll</i><br>
<i>&gt; remember us as their "parents", an idea shared by many</i><br>
<i>&gt; AI researchers, afaik. Well, personally I couldn't care less</i><br>
<i>&gt; about offspring, artificial or biological; I want to experience</i><br>
<i>&gt; the future myself.</i><br>

<p>
If what you say about Moravec is true, then he's still only half a
fanatic.  I don't *care* whether or not the AIs replace us or upload us,
because it's *not my job* to care about that sort of thing.  It's up to
the first SIs.  If den Otter does manage to upgrade himself to Power
level, then I'll accept den Otter's word on the matter.  I know exactly
how much I know about what really matters - zip.  And I know how much
everyone else knows - zip, plus a lot of erroneous preconceptions.

<p>
Once you discard all the ideas you started out with and consider things
rationally, it just doesn't make sense to try and second-guess SIs. 
They know more than I do.  Period.  There aren't any grounds on which I
could argue with them.  See, Moravec is making the same mistake as
everyone else, just on the opposite side.  If Moravec actually
*advocated* the extinction of the human race, or tried to program it
into the first AIs, I'd move against him just as I'd move against anyone
who advocated our survival at all costs, or against anyone who tried to
program Asimov Laws into AIs.  It's not a question of my having an
allegiance to AIs, even.  If I thought that AIs would cause harm, I'd
move against them.

<p>
I serve an unspecified that-which-is-right.  I don't know what is
*specifically* right, but I'm going to do my best to make sure someone
finds out - not necessarily me, because that's not necessarily right. 
Occam's Razor.  As a subgoal of that-which-is-right, I'm going to try
and create an SI and protect it from interference.  That's all that's
necessary.  Everything else can be trimmed away.  That's the way
navigating should be; clean, simple, elegant and with the absolute
minimum of specifications.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1383.html">[ Next ]</a><a href="1381.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1374.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
</body></html>
