<!-- received="Wed Jul 28 22:09:24 1999 MDT" -->
<!-- sent="Wed, 28 Jul 1999 22:58:40 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The Major League Extinction Challenge" -->
<!-- id="379FD16A.4B9C7907@pobox.com" -->
<!-- inreplyto="The Major League Extinction Challenge" -->
<!-- version=1.10, linesinbody=63 -->
<html><head><title>extropians: Re: The Major League Extinction Challenge</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: The Major League Extinction Challenge</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Wed, 28 Jul 1999 22:58:40 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1299">[ date ]</a><a href="index.html#1299">[ thread ]</a><a href="subject.html#1299">[ subject ]</a><a href="author.html#1299">[ author ]</a>
<!-- next="start" -->
<li><a href="1300.html">[ Next ]</a><a href="1298.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1254.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Billy Brown wrote:
<br>
<i>&gt; </i><br>
<a href="1254.html#1299qlink1">&gt; For the record, Eliezer Yudkowsky gets credit for the only plausible</a><br>
<i>&gt; extinction event I've seen so far (see</i><br>
<i>&gt; <a href="http://singularity.posthuman.com/sing_analysis.html#zones">http://singularity.posthuman.com/sing_analysis.html#zones</a>, option #2).  It</i><br>
<i>&gt; requires making some assumptions that many people would disagree with, but</i><br>
<i>&gt; at least it is arguably possible.</i><br>

<p>
Even that one doesn't work.  Are all of your SIs upgrading themselves at
exactly the same time?  I mean, let's think about what this would
require.  It would require the *entire* human civilization to be
uploaded, and then upgraded, in such a way that *nobody*, including den
Otter, saw what was coming while still capable of desiring to avoid it. 
Maybe if you merged the entire civilization into one entity, *before*
trying it...  Yeah, that would work.  I can believe that the threshold
for collapse is sufficiently far above the threshold for not wanting to
avoid a collapse that it would take out all SIs.  So either the
civilization merges, or all the individuals upgrade at roughly the same
rate - anyway, everyone makes it to the IGS-using stage.  Some time
later - subjectively, of course - they advance to the point where they
give and die.  Okay, it sounds plausible.

<p>
<a name="1315qlink1">Problem is, choosing to commit suicide is still a choice - and that's
not what I'm hypothesizing.  At that level, I don't have the vaguest
notion of what would really happen if an SI's goal system collapsed. 
The whole lapse-to-quiesence thing in Elisson is a design feature that
involves a deliberate tradeoff of optimization to achieve a graceful shutdown.
</a>

<p>
<a href="1254.html#1299qlink2">&gt; Are there any other candidates.</a><br>

<p>
<a name="1315qlink2">Well, if you're interested in a not-so-known-laws-of-physics
speculation:  The various colonies achieve SI more or less
simultaneously, or unavoidably.  The first thing an SI does is leave our
Universe.  But, this requires a large-scale energetic event - like, say,
a supernova.

<p>
Still doesn't solve the Great Filter Paradox, though.  Some hivemind
races will have the willpower to avoid Singularity, period.  This
scenario takes mortals and Powers out of the picture during a
Singularity, but it doesn't account for the deliberate hunting-down that
would be needed.
</a>

<p>
-

<p>
<a name="1315qlink3">I think the most plausible argument is this:  Every advance in
technology has advanced the technology of offense over the technology of
defense, while decreasing the cost required for global destruction. 
There are no shields against nuclear weapons - not right now, anyway -
and we've certainly managed to concentrate that power more than it's
ever been concentrated before.  In fact, the more technology advances,
the easier it becomes to cause mass destruction by *accident*.  It holds
true from nuclear weapons, to biological warefare, to the Y2K crash, to
nanotechnology.  All you really need to assume is that the trend
continues.  Eventually one guy with a basement lab can blow up the
planet and there's nothing anyone can do about it.
</a>
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1300.html">[ Next ]</a><a href="1298.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1254.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
</body></html>
