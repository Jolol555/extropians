<!-- received="Mon Sep  6 16:40:15 1999 MDT" -->
<!-- sent="Mon, 6 Sep 1999 18:49:03 -0400" -->
<!-- name="Matt Gingell" -->
<!-- email="mjg223@nyu.edu" -->
<!-- subject="Re: Is this world a computer simulation?" -->
<!-- id="036101bef8ba$054a1820$84ee7a80@matts" -->
<!-- inreplyto="Is this world a computer simulation?" -->
<!-- version=1.10, linesinbody=83 -->
<html><head><title>extropians: Re: Is this world a computer simulation?</title>
<meta name=author content="Matt Gingell">
<link rel=author rev=made href="mailto:mjg223@nyu.edu" title ="Matt Gingell">
</head><body>
<h1>Re: Is this world a computer simulation?</h1>
Matt Gingell (<i>mjg223@nyu.edu</i>)<br>
<i>Mon, 6 Sep 1999 18:49:03 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3656">[ date ]</a><a href="index.html#3656">[ thread ]</a><a href="subject.html#3656">[ subject ]</a><a href="author.html#3656">[ author ]</a>
<!-- next="start" -->
<li><a href="3657.html">[ Next ]</a><a href="3655.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3639.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<hr>
<br>
From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;

<p>
<a name="3668qlink1"><a href="3639.html#3656qlink1">&gt;Which makes sense, except that I'm speculating that the Way Things Are</a><br>
<i>&gt;has "evolved" to maximize the reproductive rate of the simulations. In</i><br>
<i>&gt;which case, there's an obvious adaptive selection pressure for rewriting</i><br>
<i>&gt;nonconformist Powers - or, on the very dimly bright side, stopping</i><br>
<i>&gt;nanodisasters - that doesn't exist for stopping random suffering or</i><br>
<i>&gt;optimizing for pleasure. Such worlds may exist, but they don't</i><br>
<i>&gt;reproduce. Besides, at least one major sub-hypothesis is that the</i><br>
<i>&gt;Powers involved are insane.</i><br>

<p>
If a universe were adapted enough to prevent nano-disaster among its children,
then surely it would be pushing us towards Singularity hard enough we'd notice.
In any case, you'd expect something that highly evolved to short-circuit the
process by launching a simulation of itself at the instant it started a
simulation of itself.
</a>

<p>
<a name="3668qlink2">But there's no point in trying to generalize from just our node of the search
tree. There are too many unknowns to make this an interesting discussion, even
if God isn't nuts.
</a>

<p>
<i>&gt;&gt; If pleasure were being maximized, we'd be disembodied strings of code</i><br>
floating
<br>
<a href="3639.html#3656qlink2">&gt;&gt; in virtual tanks full of virtual opiates. If pain were being minimized, we</a><br>
<i>&gt;&gt; wouldn't be here. If an optimal compromise between the two had been found,</i><br>
there
<br>
<a href="3630.html#3656qlink3">&gt;&gt; 'd be nothing in the universe but endless mirrors of Earth. The incredibly</a><br>
<i>&gt;&gt; arbitrary nature of the universe at a macroscopic level makes me doubt</i><br>
there's a
<br>
<a href="3639.html#3656qlink4">&gt;&gt; God paying attention to us, synthetic or otherwise.</a><br>
<i>&gt;</i><br>
<i>&gt;Oh, you mean the way that if Powers exist they should expand at</i><br>
<i>&gt;lightspeed, and if alien races exist they should expand pretty fast</i><br>
<i>&gt;anyway, and Earth's sun is hardly old, so therefore intelligent life is</i><br>
<i>&gt;impossible and we aren't here?</i><br>

<p>
I'm not going to list all the hypothetical solutions to Fermi's paradox here.

<p>
<a href="3639.html#3656qlink5">&gt;"Unlightenment": The stage at which you know so much about the</a><br>
<i>&gt;Universe, and you've accumulated so much to be explained and have</i><br>
<i>&gt;learned to create such rigorous explanations, that you have more</i><br>
<i>&gt;constraints than degrees of freedom - and they don't fit. I cannot</i><br>
<i>&gt;think of *any* good explanation for the Great Filter Paradox. My</i><br>
<i>&gt;visualization of the Universe has now reached the point where it</i><br>
<i>&gt;contains active impossibilities. I give up. Did I mention that I've</i><br>
<i>&gt;given up? Well, I've given up. All I bloody know about the bloody</i><br>
<i>&gt;facts is that the bloody answer is probably bloody obvious to any entity</i><br>
<i>&gt;with half a bloody brain, so I'm going to treat the bloody world like</i><br>
<i>&gt;it's actually real, bloody impossibilities and all, and try to</i><br>
<i>&gt;manipulate the observed bloody regularities so a transhuman pops out, at</i><br>
<i>&gt;which point my *bloody* job is bloody well OVER.</i><br>

<p>
<a name="3668qlink3">Here's my plan - focus on the one tiny corner of the world I think I can make
some sense of, think I'm on to something, work like a dog for years and years,
let my ego get away from me and publish ludicrously overblown projections, fail
spectacularly and get laughed out of the scientific community in a flap that
makes the cold-fusion thing look friendly, drink myself half way to oblivion,
and end up choking on my tongue in a Budapest hotel the day before Science
publishes the article explaining everything.
</a>

<p>
<a href="3639.html#3656qlink6">&gt;&gt; ps. You used the word 'culture' with a capital C the other day. Was that an</a><br>
Iain
<br>
<a href="3639.html#3656qlink7">&gt;&gt; Banks reference?</a><br>
<i>&gt;</i><br>
<i>&gt;Hm. Where?</i><br>
<i>&gt;--</i><br>

<p>
From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;
Sent: Thursday, September 02, 1999 10:34 AM
Subject: Re: &gt;H RE: Present dangers to transhumanism

<p>
<a name="3668qlink4">So then why aren't the aliens here already? Equipped with
nanotechnology, they sweep from star to star at slower-than-light
speeds, engaging in Dysoneering and playing Culture to primitive
cultures, and run out of stars in any given galaxy in, say, less than a
million years. No spacetime engineering; that's a Power's game.
</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3657.html">[ Next ]</a><a href="3655.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3639.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
