<!-- received="Sun Sep 12 00:04:01 1999 MDT" -->
<!-- sent="Sun, 12 Sep 1999 00:04:35 -0600" -->
<!-- name="Aaron Davidson" -->
<!-- email="ajd@ualberta.ca" -->
<!-- subject="Re: Nanotech control systems (was Re: Transhuman Beach Party)" -->
<!-- id="l03110700b400f058dd56@[24.108.5.39]" -->
<!-- inreplyto="Pine.SV4.3.91.990910232559.12539A-100000@www.aeiveos.com" -->
<!-- version=1.10, linesinbody=41 -->
<html><head><title>extropians: Re: Nanotech control systems (was Re: Transhuman Beach Party)</title>
<meta name=author content="Aaron Davidson">
<link rel=author rev=made href="mailto:ajd@ualberta.ca" title ="Aaron Davidson">
</head><body>
<h1>Re: Nanotech control systems (was Re: Transhuman Beach Party)</h1>
Aaron Davidson (<i>ajd@ualberta.ca</i>)<br>
<i>Sun, 12 Sep 1999 00:04:35 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4005">[ date ]</a><a href="index.html#4005">[ thread ]</a><a href="subject.html#4005">[ subject ]</a><a href="author.html#4005">[ author ]</a>
<!-- next="start" -->
<li><a href="4006.html">[ Next ]</a><a href="4004.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3921.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<i>&gt;On Fri, 10 Sep 1999, J. R. Molloy wrote:</i><br>
<i>&gt;</i><br>
<a href="3921.html#4005qlink1">&gt;The problem isn't "wiring them together" but having them "do</a><br>
<i>&gt;something" once wired.  Nobody has ever had a 70 million cell</i><br>
<i>&gt;neural net before.  How deep do you make it?  How broad do you</i><br>
<i>&gt;make it?  How many inputs &amp; outputs should each neuron have?</i><br>
<i>&gt;</i><br>
<i>&gt;I'm not sure how flexible the de Garis architecture really is</i><br>
<i>&gt;but if the depth can be up to 10 deep, and the net has</i><br>
<i>&gt;1000 inputs and 100 outputs, and each neuron can interact with 500</i><br>
<i>&gt;others, the possible number of configurations is huge.  [I'm not an</i><br>
<i>&gt;expert on neural nets, so if someone can explain this better</i><br>
<i>&gt;please do so.]</i><br>

<p>
It gets even worse than this -- and what you mention above still is indeed
an active researche problem. There isn't even a good formula for choosing
depth as a function of say, input size. Once you have the physical layout
of the NN, there are dozens of different ways to connect the neurons. There
are timing issues, and there are a kazillion ways to train the net. The
learning algorithm is very important because in solving different problems,
some algorithms which work great for one type of domain get easily stuck at
local maxima or oscillate wildly, in another domain.

<p>
Some attempts at solving these problems involve dynamically reconfiguring
nets, that change their structure as they learn. Neurons are killed off if
they are deemed to be hindering or useless. New layers can be removed if
the net is too large to generalize a problem (and instead turns into a
giant look-up table), or  added if the net is underfitting the data.
Genetic Algorithms have been used to try and find optimal organizations.
The list goes on and on. No one has yet found a way to build a general
purpose neural net system. We still need to custom design NN's to work well
for different domains.

<p>
--Aaron

<pre>
+-------------------------------------------------------------------------+
</pre>
| Aaron Davidson &lt;ajd@ualberta.ca&gt; <a href="http://ugweb.cs.ualberta.ca/~davidson/">http://ugweb.cs.ualberta.ca/~davidson/</a> |
<pre>
+-------------------------------------------------------------------------+
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="4006.html">[ Next ]</a><a href="4004.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3921.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
</ul>
</body></html>
