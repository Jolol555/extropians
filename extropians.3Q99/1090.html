<!-- received="Sat Jul 24 17:24:42 1999 MDT" -->
<!-- sent="Sun, 25 Jul 1999 01:08:03 +0200" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: IA vs. AI was: longevity vs singularity" -->
<!-- id="199907242324.QAA12059@geocities.com" -->
<!-- inreplyto="IA vs. AI was: longevity vs singularity" -->
<!-- version=1.10, linesinbody=72 -->
<html><head><title>extropians: Re: IA vs. AI was: longevity vs singularity</title>
<meta name=author content="den Otter">
<link rel=author rev=made href="mailto:neosapient@geocities.com" title ="den Otter">
</head><body>
<h1>Re: IA vs. AI was: longevity vs singularity</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Sun, 25 Jul 1999 01:08:03 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1090">[ date ]</a><a href="index.html#1090">[ thread ]</a><a href="subject.html#1090">[ subject ]</a><a href="author.html#1090">[ author ]</a>
<!-- next="start" -->
<li><a href="1091.html">[ Next ]</a><a href="1089.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1049.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->



<hr>
<br>
<a href="0817.html#1090qlink1">&gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</a><br>
<i>&gt; &gt; On Thu, 22 July 1999, "den Otter" wrote:</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; &gt; Eliezer seems to</i><br>
<i>&gt; &gt; &gt; favor the AI approach (create a seed superintelligence and</i><br>
<i>&gt; &gt; &gt; hope that it will be benevolent towards humans instead of</i><br>
<i>&gt; &gt; &gt; using our atoms for something more useful), which is</i><br>
<a name="1092qlink1"><i>&gt; &gt; &gt; IMHO reckless to the point of being suicidal.</i><br>
<i>&gt; </i><br>
<i>&gt; Funny, those are almost exactly the words I used to describe trying to</i><br>
<i>&gt; stop or slow down the Singularity.</a></i><br>

<p>
That's hardly surprising; both options are extremely risky. Still, when
it comes to destructive potential, the Singularity wins hands down
so we really shouldn't be so eager to cause one. It's like using a 
nuke against a riot (sort of).
 
<p>
<a href="1049.html#1090qlink2">&gt; &gt; &gt; A much better, though still far from ideal, way would be</a><br>
<i>&gt; &gt; &gt; to focus on human uploading, and when the technology</i><br>
<i>&gt; &gt; &gt; is operational upload everyone involved in the project</i><br>
<i>&gt; &gt; &gt; simultaneously.  </i><br>
<i>&gt; </i><br>
<a name="1121qlink1"><i>&gt; A suggestion bordering on the absurd.  Uploading becomes possible at</i><br>
<i>&gt; 2040 CRNS.  It becomes available to the average person at 2060 CRNS. </i><br>
<i>&gt; Transhuman AI becomes possible at 2020 CRNS.  Nanotechnology becomes</i><br>
<i>&gt; possible at 2015 CRNS.</a></i><br>
<i>&gt; </i><br>
<i>&gt; If you can stop all war in the world and succeed in completely</i><br>
<i>&gt; eliminating drug use, then maybe I'll believe you when you assert that</i><br>
<i>&gt; you can stop nanowar for 45 years, prevent me from writing an AI for 40,</i><br>
<i>&gt; and stop dictators (or, for that matter, everyone on this list) from</i><br>
<i>&gt; uploading themselves for 20.  Synchronized Singularity simply isn't feasible.</i><br>

<p>
-there's still a gap of 5 years between nanotech and AI, ample time
to wipe out civilization if nanotech is as dangerous as you seem to
assume.

<p>
<a name="1121qlink2">-I'd have to stop nanowar, assuming I'd want to do that, for 25 and 
not 45 years (because the Singularity comes immediately after
the first wave of uploads. Anyone who's serious about survival,
let alone ascension, must be in the first wave. Needless to say,
be sure to get rich). Consequently, I'd have to hold back AI for
20 years, not 40. Still very hard, but feasible with the proper
support.</a>

<p>
<a name="1121qlink3">-Stopping you from writing an AI wouldn't be all that hard, if I really
wanted</a> to. ;-)

<p>
<a name="1121qlink4">-If nanotech is advanced enough to destroy the world, it can 
surely also be used to move to space and live there long enough 
</a>to transcend.<a name="1121qlink5"> You can run and/or hide hide from nanotech, even
fight it successfully, but you can't do that with a superhuman
AI, i.e. nanotech leaves some room for error, while AI doesn't (or 
much less in any case). As I've said before, intelligence is the 
ultimate weapon, infinitely more dangerous than stupid nanites.</a>

<p>
<a name="1121qlink6">-Synchronized Singularity *is* feasible for a limited number of people 
(some of which may choose to uplift the rest later). Yes, it's extremely 
hard, but not impossible. More importantly, it's the only real option 
someone who values his existence has. The AI should only be used 
after all else has failed and the world is going to hell in a handbasket.</a>
 
<p>
<a name="1121qlink7"><a href="1049.html#1090qlink3">&gt; after all, I've openly declared that</a><br>
<i>&gt; my first allegiance is not to humanity.</i><br>

<p>
No, it should be to yourself, of course. Anyway, so you're willing to 
kill everyone on earth, including yourself, to achieve...what, exactly?
</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="1091.html">[ Next ]</a><a href="1089.html">[ Previous ]</a>
<b>In reply to:</b> <a href="1049.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
