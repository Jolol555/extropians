<!-- received="Sat Nov 30 21:47:43 1996 MST" -->
<!-- sent="Sat, 30 Nov 1996 20:20:41 -0800" -->
<!-- name="Hal Finney" -->
<!-- email="hal@rain.org" -->
<!-- subject="Re: Extropian Form Letter (was: an exhortation to action)" -->
<!-- id="199612010420.UAA12175@crypt.hfinney.com" -->
<!-- inreplyto="Extropian Form Letter (was: an exhortation to action)" -->
<title>extropians: Re: Extropian Form Letter (was: an exhortation to action)</title>
<h1>Re: Extropian Form Letter (was: an exhortation to action)</h1>
Hal Finney (<i>hal@rain.org</i>)<br>
<i>Sat, 30 Nov 1996 20:20:41 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3481">[ date ]</a><a href="index.html#3481">[ thread ]</a><a href="subject.html#3481">[ subject ]</a><a href="author.html#3481">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3482.html">Kathryn Aegis: "Re: VHEMT"</a>
<li> <b>Previous message:</b> <a href="3480.html">Michael Lorrey: "Re: My new site"</a>
<li> <b>Maybe in reply to:</b> <a href="3514.html">Hal Dunn: "Extropian Form Letter (was: an exhortation to action)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="3486.html">Eliezer Yudkowsky: "Re: Extropian Form Letter (was: an exhortation to action)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
From: Eliezer Yudkowsky &lt;sentience@pobox.com&gt;<br>
<i>&gt; If I may make a suggestion?  Start a new thread called "Extropian Form</i><br>
<i>&gt; Letter", devoted to building an outline for an article/letter/pamphlet</i><br>
<i>&gt; that would introduce the untutored to the Singularity.  I believe that</i><br>
<i>&gt; the Singularity meme would spread faster than the Extropian culture</i><br>
<i>&gt; because it's smaller, slimmer, less vulnerable, easier to remember and</i><br>
<i>&gt; easier to grasp, plus it's all you need to know to start looking into</i><br>
<i>&gt; the future.</i><br>
<p>
The term "Singularity" was invented by Vernor Vinge in his novel,<br>
"Marooned in Realtime".  It did have basically the same meaning you are<br>
applying, a case of runaway intelligence amplification.  Interestingly,<br>
in the novel it was largely a matter of the creation of a group mind, with<br>
people able to interact directly at the mental level.  This is somewhat<br>
different from typical Extropian ideas, which are more individualistic in<br>
nature and would picture augmented individuals rather than groups.<br>
<p>
The notion of a singularity as the culmination of technological promise<br>
is interesting, but the future will undoubtedly be a very different place<br>
even if it never happens quite like this.<br>
<p>
Also, keep in mind that if a singularity happens as a result of subjective<br>
speedup of human-like minds, then the qualitative effects will be different<br>
from what a non-participant will see.  If everyone is thinking and living<br>
life ten times as fast as before, then compared to clock time progress may<br>
occur ten times faster.  But by subjective time progress will occur no<br>
faster than before.  We have just re-scaled the passage of time so that<br>
more things happen between each clock tick.<br>
<p>
<i>&gt; Title:  "History Ends In 2025, If We're Smart Enough."</i><br>
<i>&gt; Reasons:  Introduce "End of History" to grab attention, "2025" to</i><br>
<i>&gt; emphasize immediate importance, "If We're Smart Enough" to dissociate</i><br>
<i>&gt; from tired doomsday memes (Greenhouse Effect) and religious/apocalyptic</i><br>
<i>&gt; memes that would only appeal to groups of a particular faith.</i><br>
<p>
While the 2025 date may be effective for dramatic impact, I think there<br>
is a lot of uncertainty over how quickly the various changes you predict<br>
will actually occur.<br>
<p>
<i>&gt; 1.  Computers double in power every two subjective years.</i><br>
<i>&gt; 2.  Recursive intelligence amplification.</i><br>
<p>
As I believe we have discussed before, the doubling we are seeing already<br>
does depend on the existence of ever-faster computers.  So we are already<br>
in the middle of a feedback loop, and in fact that is really why we are<br>
able to see exponential progress (feedback is a characteristic of such a<br>
growth curve).<br>
<p>
As for actually getting intelligence amplification through the literal<br>
creation of either artificial intelligence or uploading, both of these<br>
are very uncertain in the time frame of three decades.  People have been<br>
predicting breakthroughs in AI since the 1950's, but actually progress<br>
has been almost non-existant.  And uploading will require breakthroughs<br>
in a very large number of areas.  There are also significant ethical<br>
questions about exposing conscious observers to the possibly painful or<br>
horrifying experiences which this research would entail, which might<br>
further retard experimentation in these areas.  Our current AI is so<br>
far from reality that even consideration of such hazards seems almost<br>
laughable, but if real progress is ever made this could become relevant.<br>
<p>
<i>&gt; 3.  Defines "Singularity."</i><br>
<p>
We have discussed different meanings for "The Singularity" on this list.<br>
Some people take it to mean a literally infinite rate of progress, as the<br>
name suggests.  Others see it as more of an event horizon, a point in<br>
history where things become so different that people who come before<br>
can't comprehend how things will be afterwards.<br>
<p>
Really in a lot of ways we are in the middle of a singularity by this<br>
definition right now.  Our species goes back hundreds of thousands of<br>
years at least.  Yet people from only a few thousand years ago would<br>
find most of our present activities utterly incomprehensible.  In this<br>
view the "comprehension horizon" will continue to shrink as progress<br>
increases in speed.<br>
<p>
<i>&gt; 4.  Is this a good thing?  Names factions.</i><br>
<i>&gt; 5.  Doomsday faction.</i><br>
<i>&gt; 6.  Uploading faction.</i><br>
<i>&gt; 7.  Intro to nanotechnology.</i><br>
<i>&gt; 8.  Nearness of nanotechnology.</i><br>
<p>
I am a skeptic about the short term success of nanotech.  This technology<br>
has a fundamental chicken and egg problem: it is very hard to build an<br>
assembler without having an assembler.  Once we get there things will be<br>
fine, but that path is very uncertain.<br>
<p>
There are going to be millions of atoms in an assembler.  It is going to<br>
take a long time before we are able to put that many together accurately<br>
using macroscopic techniques.  How much progress has there been in the<br>
last few years since they spelled out "IBM" with atoms?  These things<br>
move very slowly.<br>
<p>
Personally I think the biotech or self-assembly paths are more<br>
promising although they get less attention these days from nanotech fans.<br>
But any way you go there are a lot of research problems to solve, and it<br>
really isn't possible to lay out a timeline with any degree of accuracy<br>
right now.<br>
<p>
<i>&gt; 9.  Nanotechnology replaces economy.</i><br>
<i>&gt; 10. Gray goo problem.</i><br>
<i>&gt; 11. Intelligence amplification.</i><br>
<i>&gt; 12. Runaway positive feedback of IA.</i><br>
<i>&gt; 13. Replacement of human society: End of History.</i><br>
<i>&gt; 14. Singularity provides Interim Meaning of Life.</i><br>
<i>&gt; 15. Summary: History is about to end.</i><br>
<p>
I'm not sure about this Meaning of Life stuff though.  By (some people's)<br>
definition, we can't really understand what life after the Singularity<br>
will be like.  Maybe the people will have more meaningful lives, but<br>
maybe not!  We really can't know.<br>
<p>
<p>
Even where I disagree with the specifics about the timeline, I do think<br>
this is a plausible "big picture" approach to human history and it is<br>
a good outline of issues that are worth thinking about.<br>
<p>
Hal<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3482.html">Kathryn Aegis: "Re: VHEMT"</a>
<li> <b>Previous message:</b> <a href="3480.html">Michael Lorrey: "Re: My new site"</a>
<li> <b>Maybe in reply to:</b> <a href="3514.html">Hal Dunn: "Extropian Form Letter (was: an exhortation to action)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="3486.html">Eliezer Yudkowsky: "Re: Extropian Form Letter (was: an exhortation to action)"</a>
<!-- reply="end" -->
</ul>
