<!-- received="Sat Dec 14 22:15:06 1996 MST" -->
<!-- sent="Sat, 14 Dec 1996 23:06:03 -0600" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Vegetarianism and Ethics" -->
<!-- id="v0153051aaed8a6a3d838@[24.1.1.26]" -->
<!-- inreplyto="Vegetarianism and Ethics" -->
<title>extropians: Re: Vegetarianism and Ethics</title>
<h1>Re: Vegetarianism and Ethics</h1>
Eliezer Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sat, 14 Dec 1996 23:06:03 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4063">[ date ]</a><a href="index.html#4063">[ thread ]</a><a href="subject.html#4063">[ subject ]</a><a href="author.html#4063">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4064.html">John K Clark: "Brin on privacy"</a>
<li> <b>Previous message:</b> <a href="4062.html">Kennita Watson: "Re: Ants"</a>
<li> <b>Maybe in reply to:</b> <a href="4042.html">Lyle Burkhead: "Vegetarianism and Ethics"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; I'm not sure what you see as the distinction between 'justification' and</i><br>
<i>&gt; 'value'.</i><br>
In Lenat's AM, the justification slot was "because primes are<br>
interesting right now" while the value slot was 500.<br>
<p>
<i>&gt; What does negative value mean for goals?</i><br>
A negative goal is something to be avoided.<br>
<p>
<i>&gt; How is "Power-perfect reasoning" different from human reasoning?</i><br>
<p>
A Power can rebuild its own architecture if it gets in the way or<br>
becomes too limited.  Humans might never find a Meaning because we<br>
aren't built to think about self-justifying things - our continuing<br>
failure to find the inherently obvious First Cause comes to mind - and<br>
our emotional architectures mess up our goal-evaluation systems.<br>
<p>
<i>&gt; Can you really have a goal-less architecture?</i><br>
<p>
Sure.  A spreadsheet program comes to mind.  It would be pretty<br>
difficult to build a thinking being with no goals, but I suppose one<br>
could try. <br>
<p>
<i>&gt; Of course, there is no guarantee that the system will ever find an</i><br>
<i>&gt; Ultimate Meaning.  Halting problem...</i><br>
<p>
Frankly, the main problem on my mind was whether there *was* one, not<br>
whether the system would find it.  If there is one, I think we can rely<br>
on a Power seeing it as obvious once the right cognitive architecture is<br>
in place, just like the First Cause.<br>
<p>
(As always, I'd like to remind my audience that the First Cause is<br>
obvious to Nothingness and should therefore be equally obvious to any<br>
mind as complex as the basic substrate of reality, whatever it is.)<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4064.html">John K Clark: "Brin on privacy"</a>
<li> <b>Previous message:</b> <a href="4062.html">Kennita Watson: "Re: Ants"</a>
<li> <b>Maybe in reply to:</b> <a href="4042.html">Lyle Burkhead: "Vegetarianism and Ethics"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
