<!-- received="Wed Sep  4 10:09:44 1996 MST" -->
<!-- sent="Vv&!& 	Fx	VzivzvxvvvvPpfB7D	F|	V~Puiv 04 Sep 95 " -->
<!-- name=""N_\\Bostrom_at_{posbn}"@smtplink.lse.ac.uk" -->
<!-- email=""N_\\Bostrom_at_{posbn}"@smtplink.lse.ac.uk" -->
<!-- subject="Re: Thinking about the future..." -->
<!-- id="9608048418.AA841882180@smtplink.lse.ac.uk" -->
<!-- inreplyto="Thinking about the future..." -->
<title>extropians: Re: Thinking about the future...</title>
<h1>Re: Thinking about the future...</h1>
<i>"N_\\Bostrom_at_{posbn}"@smtplink.lse.ac.uk</i><br>
<i>Vv&!& 	Fx	VzivzvxvvvvPpfB7D	F|	V~Puiv 04 Sep 95 </i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#764">[ date ]</a><a href="index.html#764">[ thread ]</a><a href="subject.html#764">[ subject ]</a><a href="author.html#764">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0765.html">Howard Julien: "Re: Alt.Extropians Getting MORE SIGNAL-LESS NOISE"</a>
<li> <b>Previous message:</b> <a href="0763.html">QueeneMUSE@aol.com: "Re: Thinking about the future..."</a>
<li> <b>Maybe in reply to:</b> <a href="0738.html">Eric Watt Forste: "Thinking about the future..."</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0775.html">QueeneMUSE@aol.com: "Re: Thinking about the future..."</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
          Robin Hanson wrote:<br>
<i>          &gt;But realistically, any one AI probably won't be too far </i><br>
<i>          &gt;ahead of any other AI, so they can police each other.</i><br>
          <br>
          The issues are complex. To begin with, the claim that it is <br>
          improbable that one AI would be far ahead of any other AI <br>
          can be challenged. Suppose there are possible breakthroughs <br>
          to be made in computer technology. Once an AI became <br>
          sufficiently intelligent, it could think out a radical <br>
          improvement to its design; which would make it more <br>
          intelligent, allowing it to accelerate further; and so on. <br>
          (I think this is Dan Clemmensen's view.) For example, the AI <br>
          might be the first to make efficient use of nanotechnology. <br>
          If nanotech has such potentials as Drexler thinks, access to <br>
          a nanotech laboratory would be all the AI would need in <br>
          order to take off. The contest would be over before anyone <br>
          except the AI had realised it had begun.<br>
          <br>
          In the slower scenario I depicted in the last letter, <br>
          however, it is unlikely that the AI would be alone. Its <br>
          discoveries would be diffused and employed to build other <br>
          AIs.<br>
          <br>
          But even in this case it is dubious that these other AIs <br>
          would be an effective protection against possible malicious <br>
          intentions of the first AI. They would, in effect, be its <br>
          offspring, and could therefore be expected to inherit some <br>
          of its basic properties, even values. If the first &gt;AI is <br>
          bad, it might influence the development of computers so that <br>
          among subsequent machines a specific value set would be <br>
          prevalent which would allow them to form a tacit consensus <br>
          concerning the desirability of finally getting rid of the <br>
          human pest. <br>
          <br>
          It is also worth considering what would make the grandpa &gt;AI <br>
          bad in the first place:<br>
          <br>
          1) Accident, misprogramming.<br>
          <br>
          2) Constructed by a bad group of humans, for military or <br>
          commercial purposes. This group is presumably very powerful <br>
          if they are the first to build an &gt;AI. The success of this <br>
          enterprise will make them even more powerful. Thus the <br>
          values present in the group (community, company, state, <br>
          culture) that makes the first &gt;AI will not unlikely be the <br>
          value set which is programmed into subsequent &gt;AIs as well.<br>
          <br>
          3) Moral convergence. Sufficiently intelligent beings might <br>
          tend to converge in what they value, possibly because values <br>
          are in some relevant sense "objective". They just see the <br>
          truth that to annihilate humans is for the best. (In this <br>
          case we should perhaps let it happen?)<br>
          <br>
          Notice that in case (2) and (3) policing would not work even <br>
          in the slow scenario and even if the first &gt;AI itself had <br>
          little influence on the construction of other &gt;AIs.<br>
          <br>
          Nicholas Bostrom              n.bostrom@lse.ac.uk<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0765.html">Howard Julien: "Re: Alt.Extropians Getting MORE SIGNAL-LESS NOISE"</a>
<li> <b>Previous message:</b> <a href="0763.html">QueeneMUSE@aol.com: "Re: Thinking about the future..."</a>
<li> <b>Maybe in reply to:</b> <a href="0738.html">Eric Watt Forste: "Thinking about the future..."</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0775.html">QueeneMUSE@aol.com: "Re: Thinking about the future..."</a>
<!-- reply="end" -->
</ul>
