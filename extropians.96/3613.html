<!-- received="Tue Dec  3 22:18:41 1996 MST" -->
<!-- sent="Tue, 03 Dec 1996 21:05:13 -0600" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Singularity-worship" -->
<!-- id="199612040452.UAA04787@hss.caltech.edu" -->
<!-- inreplyto="Singularity-worship" -->
<title>extropians: Re: Singularity-worship</title>
<h1>Re: Singularity-worship</h1>
Eliezer Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Tue, 03 Dec 1996 21:05:13 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3613">[ date ]</a><a href="index.html#3613">[ thread ]</a><a href="subject.html#3613">[ subject ]</a><a href="author.html#3613">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3614.html">Kennita Watson: "Time and Life (Extension)"</a>
<li> <b>Previous message:</b> <a href="3612.html">Robin Hanson: "Go "Old Farts" Go!"</a>
<li> <b>Maybe in reply to:</b> <a href="4069.html">John K Clark: "Singularity-worship"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="3626.html">Eric Watt Forste: "Re: Singularity-worship"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Twirlip said:<br>
<i>&gt; I'm not sure that I'm right.  I can't be; even if someone could be, I</i><br>
<i>&gt; don't know the mathematics yet to appreciate the proof if there was one.</i><br>
<i>&gt; But I don't see how you can be so sure that you're right, or that we're</i><br>
<i>&gt; so wrong.  You can't extrapolated up a damage gradient and assume the</i><br>
<i>&gt; curve will keep on going; it could plateau at some universal point.</i><br>
<p>
I have a technical definition of "smartness" in terms of our cognitive<br>
architectures, available in "Staring."  To quote the first paragraph:<br>
<p>
<i>&gt; Smartness is the measure of what you see as obvious, what you can</i><br>
<i>&gt; see as obvious in retrospect, what you can invent, and what you can</i><br>
<i>&gt; comprehend. To be a bit more precise about it, smartness is the</i><br>
<i>&gt; measure of your semantic primitives (what is simple in retrospect),</i><br>
<i>&gt; the way in which you manipulate the semantic primitives (what is</i><br>
<i>&gt; obvious), the way your semantic primitives can fit together (what</i><br>
<i>&gt; you can comprehend), and the way you can manipulate those</i><br>
<i>&gt; structures (what you can invent). If you speak complexity theory,</i><br>
<i>&gt; the difference between obvious and obvious in retrospect, or</i><br>
<i>&gt; inventable and comprehensible, is somewhat like the difference</i><br>
<i>&gt; between P and NP.</i><br>
<p>
Similarly, I have a model of *a* (not the) Singularity, based on the<br>
concept of a Perceptual Transcend:<br>
<p>
<i>&gt; A Perceptual Transcend occurs when all things that were comprehensible </i><br>
<i>&gt; become obvious in retrospect, and all things that were inventable become</i><br>
<i>&gt; obvious. A Perceptual Transcend occurs when the semantic</i><br>
<i>&gt; structures of one generation become the semantic primitives of the</i><br>
<i>&gt; next. To put it another way, one PT from now, the whole of human</i><br>
<i>&gt; knowledge becomes perceiveable in a single flash of experience, in</i><br>
<i>&gt; the same way that we now perceive an entire picture at once.</i><br>
<p>
A Perceptual Transcend seems fairly easy to engineer.  Given a hell of a<br>
lot of computing power, it would seem possible to automate any semantic<br>
structures as semantic primitives.  Whether a qualitatively new layer of<br>
semantic structures can be invented is an open question; I would argue<br>
that even if qualitatively new types of thought are not emergent, one<br>
can hack up a Power simply by forming semantic structures just like the<br>
old ones out of the new primitives.  This Power should then be able to<br>
evolve new and more appropriate types of semantic structures on top of<br>
the primitives.  Even if this is impossible, it does seem fairly clear<br>
that a sufficient amount of computing power would allow a brute-force<br>
perception of all human knowledge.  This does open a worst-case scenario<br>
with all progress automated and no conscious creativity being involved;<br>
but if worst comes to worst we can always go back to being mortal.<br>
<p>
One Perceptual Transcend from now is a Singularity for all intents and<br>
purposes.  I doubt there's going to be an additional Perceptual<br>
Transcend after that, since there are probably better ways of doing<br>
things.  I invented Perceptual Transcends over the course of maybe<br>
thirty seconds, so I have an unjustified but probable faith that the<br>
Powers can do better.<br>
<p>
<i>&gt;From where I stand, I have a cognitive model that leads, by</i><br>
straightforward mechanisms, to Powers incomprehensible from any human<br>
standpoint, with semantic primitives that our brains are inadequate to<br>
comprehend and could only simulate through billions of years of labor. <br>
Turing-equivalence is practically irrelevant to this scenario. <br>
Singularity seems like a fine name.<br>
<p>
So what's the basis of your unmodeled, unbacked claim that there isn't<br>
going to be a Singularity?  It seems to me like pure faith, but then I<br>
now understand that my own statements sounded like that too.  There's no<br>
way you could have known that behind an apparently worshipful statement<br>
like "The Powers will be ethical" was 20K of thinking about goal-based<br>
cognitive architectures.  Which brings us too:<br>
<p>
<i>&gt; And why should the Meaning of Life be observer-independent?  Meaning of</i><br>
<i>&gt; whose life?  My meaning of a chicken's life is to feed me.</i><br>
<p>
In a goal-based cognitive architecture, actions are motivated by goals. <br>
Goals have the attributes of value and justification.  A goal can have<br>
varying fulfillment value depending on its justification.  It is<br>
advantageous to have cognitive mechanisms for questioning the<br>
justification of goals.  This applies not only to vast life-guiding<br>
goals, but simple goals like crossing the room to get a tissue; if a<br>
tissue is at hand, crossing the room is unnecessary.<br>
<p>
Most goals formulated are subgoals; their acheivement helps us acheive a<br>
greater goal.  In the example above, the goal was getting a tissue and<br>
blowing one's nose to ultimately stop itching, a sensation defined as<br>
unpleasant by evolution.  Most of our goals ultimately ground in either<br>
evolution-defined goals, or goals defined by our upbringing which we<br>
haven't questioned yet.  The Meaning of Life can be defined as a<br>
self-justifying goal with no other goals as prerequisites - in this, it<br>
is similar to the First Cause.  Although the evolution-defined goals<br>
might appear to meet this standard, they are not self-justifying, they<br>
are simply manufactured without any justification whatsoever.<br>
<p>
There is hence valid logical reason to believe the Meaning of Life to be<br>
observer-independent, much as one would expect the First Cause to be<br>
independent of anything else whatsoever.  If the Meaning of Life turns<br>
out to be a particular subjective experience such as pleasure, there is<br>
still no reason - despite the privacy of subjective experience - why the<br>
ethical justification wouldn't apply to everyone else as well, much as<br>
an explanation of consciousness must apply to all minds in general<br>
despite the nature of conscious experience.<br>
<p>
Any entity which questions all goals - an activity which would be<br>
automatic one PT beyond us, for example - will formulate the concept of<br>
the Meaning of Life.  I couldn't solve the Meaning so I formulated the<br>
Interim Meaning - "Find the Meaning via Singularity" - which seems like<br>
a valid logical chain for a Power as well.  Depending on the ability of<br>
such a Meaning, Interim or otherwise, to override built-in goals, the<br>
Power will be ethical.  This is a very good reason not to screw around<br>
with Laws of Robotics - it could wind up backfiring in a major, major<br>
way.  If our lives have any objective meaning, the Powers will know<br>
that.  If they don't, I - ethically speaking - don't care if we get<br>
ground up for spare atoms.  I do believe, however, that our lives have<br>
meaning.  That's not all faith, but the rest is unpublished.<br>
<p>
So yeah, the Powers will be ethical and they won't knock us off.  Is<br>
this statement now sufficiently atheist for you?<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3614.html">Kennita Watson: "Time and Life (Extension)"</a>
<li> <b>Previous message:</b> <a href="3612.html">Robin Hanson: "Go "Old Farts" Go!"</a>
<li> <b>Maybe in reply to:</b> <a href="4069.html">John K Clark: "Singularity-worship"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="3626.html">Eric Watt Forste: "Re: Singularity-worship"</a>
<!-- reply="end" -->
</ul>
