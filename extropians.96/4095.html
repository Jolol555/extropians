<!-- received="Sun Dec 15 23:15:16 1996 MST" -->
<!-- sent="Sun, 15 Dec 1996 22:11:05 -0800" -->
<!-- name="Romana Machado" -->
<!-- email="romana@glamazon.com" -->
<!-- subject="Can we develop a better definition of extropy?" -->
<!-- id="v02110100aeda862756f3@[206.184.133.208]" -->
<!-- inreplyto="" -->
<title>extropians: Can we develop a better definition of extropy?</title>
<h1>Can we develop a better definition of extropy?</h1>
Romana Machado (<i>romana@glamazon.com</i>)<br>
<i>Sun, 15 Dec 1996 22:11:05 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4095">[ date ]</a><a href="index.html#4095">[ thread ]</a><a href="subject.html#4095">[ subject ]</a><a href="author.html#4095">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4096.html">Kathryn Aegis: "guidelines/ethics"</a>
<li> <b>Previous message:</b> <a href="4094.html">my inner geek: "(Fwd) URL"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
In the interest of clear understanding, and developing a clear way of<br>
saying exactly what extropianism is - I've been looking at the present<br>
definitions of extropy.<br>
<p>
Of the two definitions of extropy available on Anders' site<br>
(<a href="http://www.aleph.se/Trans/Words/e.html#EXTROPY">http://www.aleph.se/Trans/Words/e.html#EXTROPY</a>) I prefer "the collection<br>
of forces which oppose entropy." It's still kind of hard to use, though,<br>
because it assumes that the person you are talking to knows what entropy<br>
is. That's a big assumption, because entropy stumps most people.<br>
<p>
Michelle Coltart has published an essay on this topic at:<br>
(<a href="http://canyon.ucsd.edu/infoville/schoolhouse/class_html/michelle.html">http://canyon.ucsd.edu/infoville/schoolhouse/class_html/michelle.html</a>)<br>
<p>
<i>&gt;The term "entropy", with it's multiple uses, is a confusing and often</i><br>
<i>&gt;&gt;misleading</i><br>
<i>&gt;term. In 1949, Claude Shannon defined entropy as a measure of the 'amount o=</i><br>
f<br>
<i>&gt;information' contained in a message sent along a transmission line (Denbigh=</i><br>
 &amp;<br>
<i>&gt;Denbigh, p. 101) This is often referred to as informational entropy: [1] H=</i><br>
 =3D -=E5<br>
<i>&gt;pi log2 pi i =3D 1...n where n stands for the number of different symbols. =</i><br>
Before<br>
<i>&gt;Shannon introduced the idea of informational entropy, however, the term</i><br>
<i>&gt;entropy was used in physics. In statistical mechanics, the term entropy</i><br>
<i>&gt;refers to</i><br>
<i>&gt;a very similar equation: [2] SG =3D -k=E5 pi ln pi i =3D 1...n where n=</i><br>
 stands for the<br>
<i>&gt;number of different microstates. Are these two equations the same? They are</i><br>
<i>&gt;certainly similar in appearance, but seem to be different in meaning. The f=</i><br>
act<br>
<i>&gt;that the two equation have the same name, coupled with the fact that they</i><br>
<i>&gt;are so</i><br>
<i>&gt;similar has caused much of the confusion surrounding the term entropy.</i><br>
...<br>
<i>&gt; It is said that when Shannon came up with his equation he was inclined to</i><br>
<i>&gt;name it either 'uncertainty' or 'information', using these terms in their</i><br>
<i>&gt;technical</i><br>
<i>&gt;sense. However he was: subsequently persuaded by von Neumann to call it</i><br>
<i>&gt;entropy! 'It is already in use under that name,' von Neumann [reportedly</i><br>
<i>&gt;said],'and besides it will give you a great edge in debates because nobody</i><br>
<i>&gt;really</i><br>
<i>&gt;knows what entropy is anyway.'(Denbigh &amp; Denbigh, p. 105) Thus much of our</i><br>
<i>&gt;confusion over entropy stems from this incident. Another source of confusio=</i><br>
n<br>
<i>&gt;about entropy stems from the lack of understanding of physical entropy. Sin=</i><br>
ce<br>
<i>&gt;people tend to erroneously think of physical entropy as meaning disorder an=</i><br>
d<br>
<i>&gt;chaos, thus informational entropy gets confused with disorder and chaos as</i><br>
<i>&gt;&gt;well.</i><br>
<p>
<p>
Wasn't the second definition of extropy - "a measure of intelligence,<br>
information, energy, life, experience, diversity, opportunity, and growth"<br>
- recently discussed here? I missed it, so I'm bringing it up again.<br>
<p>
If extropy is a measure - a metric - what does it measure? To what system<br>
is it intended to be applied? What sort of thing's extent or dimensions<br>
does it determine?<br>
<p>
Some of the qualities it purports to measure can be measured, and some  -<br>
well, I'm in the dark.<br>
<p>
=46or instance, there are rigorous and well-accepted ways of measuring<br>
"energy"  - at least within physics - and "information" (well, as we saw<br>
above this is the amount of entropy in a signal)."Growth", a change of<br>
dimension over time, is clear enough, as is "diversity", how many different<br>
forms of some thing there are.<br>
<p>
The measure of intelligence is often a hotly debated topic, though there<br>
are metrics of questionable validity currently in use. It would be hard, I<br>
think, to come up with a measure of life, a measure of opportunity, or a<br>
measure of experience, though if any you of have a suggestion I'd like to<br>
see it.<br>
<p>
<p>
<p>
<p>
Romana Machado romana@glamazon.com<br>
erotic site: <a href="http://www.glamazon.com/">http://www.glamazon.com/</a> "Romana Machado's Peek of the Week"<br>
personal site: <a href="http://www.fqa.com/romana/">http://www.fqa.com/romana/</a> "Romana Machado World Headquarters=<br>
"<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4096.html">Kathryn Aegis: "guidelines/ethics"</a>
<li> <b>Previous message:</b> <a href="4094.html">my inner geek: "(Fwd) URL"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
