<!-- received="Tue Sep  3 13:39:59 1996 MST" -->
<!-- sent="Tue, 03 Sep 96 18:47:14 GMT" -->
<!-- name="N.Bostrom@lse.ac.uk" -->
<!-- email="N.Bostrom@lse.ac.uk" -->
<!-- subject="Re: Thinking about the future..." -->
<!-- id="9608038418.AA841802732@smtplink.lse.ac.uk" -->
<!-- inreplyto="Thinking about the future..." -->
<title>extropians: Re: Thinking about the future...</title>
<h1>Re: Thinking about the future...</h1>
<i>N.Bostrom@lse.ac.uk</i><br>
<i>Tue, 03 Sep 96 18:47:14 GMT</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#721">[ date ]</a><a href="index.html#721">[ thread ]</a><a href="subject.html#721">[ subject ]</a><a href="author.html#721">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0722.html">E. Shaun Russell: "Re: anon.penet.fi"</a>
<li> <b>Previous message:</b> <a href="0720.html">E. Shaun Russell: "Re: evolution and "I""</a>
<li> <b>Maybe in reply to:</b> <a href="0738.html">Eric Watt Forste: "Thinking about the future..."</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0731.html">Robin Hanson: "Re: Thinking about the future..."</a>
<li> <b>Reply:</b> <a href="0731.html">Robin Hanson: "Re: Thinking about the future..."</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
          About assuring that an &gt;AI won't harm humans, Robin Hanson <br>
          said that the police would hunt it down if it misbehaved.<br>
          <br>
          OK, let's assume that an &gt;AI is positively maliscious. It is <br>
          easy to see what would happen to a moderate &gt;AI in a <br>
          laboratory if it misbehaved. It would be switched off. It <br>
          could be more problematic if it were out there in the real <br>
          world, perhaps with the responsibility for a banking system. <br>
          Economic losses could be great, but we would hardly risk <br>
          total destruction unless we gave it unrestricted power over <br>
          the US nuclear arsenal or such.<br>
          <br>
          But when transhumanists talk about &gt;AI they hardly mean a <br>
          moderate &gt;AI -like a very brilliant human and then some. We <br>
          speculate about a machine that would be a million times <br>
          faster than any human brain, and with correspondingly great <br>
          memory capacity. Could such a machine, given some time, not <br>
          manipulate a human society by subtle suggestions that seem <br>
          very reasonable but unnoticeable affects a general change in <br>
          attitude and policy? And all the time it would look as if it <br>
          were a perfectly decent machine, always concerned about our <br>
          wellfare...<br>
          <br>
          How likely is it that a malicious &gt;AI could bring disaster <br>
          to a human society that were initially determined to take <br>
          the necessary precautions?<br>
          <br>
          Society is a quasi-chaotical system: a small perturbation of <br>
          initial conditions will often lead to large unexpected <br>
          consequences later on. But there are also regularities and <br>
          causal relations that can be to some extent predicted. The <br>
          manipulative power of an &gt;AI would depend on the existence <br>
          of such sociological regularities that would be obvious to <br>
          the &gt;AI but would look like irrelevant coincidence to human <br>
          observers. It is a question of how much sociology and <br>
          psychology there is to be discovered between what we know <br>
          now and what an &gt;AI could learn in the course of few years. <br>
          <br>
          Exactly how much manipulation it would take depends on how <br>
          close we are to the nearest road to disaster. Perhaps we are <br>
          close. A deadly virus that would happen to be produced as a <br>
          by-product of some medical experiment; a little incident <br>
          that would lead to escalating hatred between China and USA; <br>
          the list goes on...<br>
          <br>
          My contention is that with only one full-blown &gt;AI in the <br>
          world, if it were malicious, the odds would be on the side <br>
          that it could annihilate humanity within decades.<br>
          <br>
          Nicholas Bostrom      n.bostrom@lse.ac.uk<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0722.html">E. Shaun Russell: "Re: anon.penet.fi"</a>
<li> <b>Previous message:</b> <a href="0720.html">E. Shaun Russell: "Re: evolution and "I""</a>
<li> <b>Maybe in reply to:</b> <a href="0738.html">Eric Watt Forste: "Thinking about the future..."</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0731.html">Robin Hanson: "Re: Thinking about the future..."</a>
<li> <b>Reply:</b> <a href="0731.html">Robin Hanson: "Re: Thinking about the future..."</a>
<!-- reply="end" -->
</ul>
