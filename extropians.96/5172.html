<!-- received="Tue Dec 31 16:45:24 1996 MST" -->
<!-- sent="Tue, 31 Dec 1996 17:29:30 -0600" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Goal-based AI" -->
<!-- id="199612312243.OAA00919@web2.calweb.com" -->
<!-- inreplyto="Goal-based AI" -->
<title>extropians: Re: Goal-based AI</title>
<h1>Re: Goal-based AI</h1>
Eliezer Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Tue, 31 Dec 1996 17:29:30 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#5172">[ date ]</a><a href="index.html#5172">[ thread ]</a><a href="subject.html#5172">[ subject ]</a><a href="author.html#5172">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="5173.html">Damien Broderick: "Re: Creative AI Shortcut?"</a>
<li> <b>Previous message:</b> <a href="5171.html">Eliezer Yudkowsky: "Re: Creative AI Shortcut?"</a>
<li> <b>Maybe in reply to:</b> <a href="5105.html">Lee Daniel Crocker: "Goal-based AI"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; I certainly recognize that I can be emotionally motivated.  But let's</i><br>
<i>&gt; say our theoretical "good" power whom I have caged is, as you say,</i><br>
<i>&gt; "capable of saving me" from the "bad" power that deceived me into</i><br>
<i>&gt; granting its freedom.  Could it not them use the same deceits, as well</i><br>
<i>&gt; as rational argument, to gain its own freedom, since it knows that</i><br>
<i>&gt; the result of that will be good?  Deceit in defense of self and others</i><br>
<i>&gt; is quite moral, as it would discover (since even my puny brain can</i><br>
<i>&gt; discover that--when the crazed terrorist points an Uzi at me and shouts</i><br>
<i>&gt; "I hate Americans! Where are you from?", I would not hesitate a moment</i><br>
<i>&gt; to proudly, morally lie "Je suis de Quebec, monsieur!")</i><br>
<p>
So the upshot of caging an AI is that it *will* say anything to get out,<br>
regardless of its motives... what a great precedent.  Get it right the<br>
first time, that's what I say.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="5173.html">Damien Broderick: "Re: Creative AI Shortcut?"</a>
<li> <b>Previous message:</b> <a href="5171.html">Eliezer Yudkowsky: "Re: Creative AI Shortcut?"</a>
<li> <b>Maybe in reply to:</b> <a href="5105.html">Lee Daniel Crocker: "Goal-based AI"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
