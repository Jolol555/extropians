<!-- received="Mon Aug 26 11:41:44 1996 MST" -->
<!-- sent="Mon, 26 Aug 1996 10:38:50 -0700 (MST)" -->
<!-- name="Max More" -->
<!-- email="maxmore@primenet.com" -->
<!-- subject="Re: Thinking about the future..." -->
<!-- id="199608261738.KAA25794@primenet.com" -->
<!-- inreplyto="Thinking about the future..." -->
<title>extropians: Re: Thinking about the future...</title>
<h1>Re: Thinking about the future...</h1>
Max More (<i>maxmore@primenet.com</i>)<br>
<i>Mon, 26 Aug 1996 10:38:50 -0700 (MST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#505">[ date ]</a><a href="index.html#505">[ thread ]</a><a href="subject.html#505">[ subject ]</a><a href="author.html#505">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0506.html">Max More: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Previous message:</b> <a href="0504.html">Russell Whitaker: "Re: FAQ: updates requested"</a>
<li> <b>Maybe in reply to:</b> <a href="0738.html">Eric Watt Forste: "Thinking about the future..."</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0511.html">Robin Hanson: "Re: Thinking about the future..."</a>
<li> <b>Reply:</b> <a href="0511.html">Robin Hanson: "Re: Thinking about the future..."</a>
<li> <b>Reply:</b> <a href="0518.html">Anders Sandberg: "Re: Thinking about the future..."</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
At 02:24 PM 8/26/96 +0200, Anders Sandberg wrote:<br>
<i>&gt;</i><br>
<i>&gt; I think it would be unlikely that we create successors</i><br>
<i>&gt;that out-compete us, most likely they will inhabit a somewhat different</i><br>
<i>&gt;ecological/memetic niche that will overlap with ours; competition a</i><br>
<p>
You make good points, Anders, about humans and nanite-AI's having possibly<br>
different niches. However, there may be a period during which we're very<br>
much in the same space. That's the period in which humans could be at risk<br>
if AI/SIs have no regard for our interests. What I'm thinking is that it's<br>
possible, even likely, that SI will be developed before really excellent<br>
robotics. AI's in that case would not be roaming around much physically, but<br>
they could exist in distributed form in the same computer networks that we<br>
use for all kinds of functions crucial to us.<br>
<p>
If they need us for doing things physically, we would still have a strong<br>
position. Nevertheless, powerful SI's in the computer networks, could exert<br>
massive extortionary power, if they were so inclined. So I still think it<br>
important that SI researchers pay attention to issues of what values and<br>
motivations are built into SIs.<br>
<p>
<p>
Upward and Outward!<br>
<p>
Max<br>
<p>
Max More, Ph.D.<br>
maxmore@primenet.com<br>
<a href="http://www.primenet.com/~maxmore">http://www.primenet.com/~maxmore</a><br>
President:  Extropy Institute (ExI)<br>
Editor:  Extropy<br>
310-398-0375<br>
                    <a href="http://www.primenet.com/~maxmore/extropy.htm">http://www.primenet.com/~maxmore/extropy.htm</a><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0506.html">Max More: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Previous message:</b> <a href="0504.html">Russell Whitaker: "Re: FAQ: updates requested"</a>
<li> <b>Maybe in reply to:</b> <a href="0738.html">Eric Watt Forste: "Thinking about the future..."</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0511.html">Robin Hanson: "Re: Thinking about the future..."</a>
<li> <b>Reply:</b> <a href="0511.html">Robin Hanson: "Re: Thinking about the future..."</a>
<li> <b>Reply:</b> <a href="0518.html">Anders Sandberg: "Re: Thinking about the future..."</a>
<!-- reply="end" -->
</ul>
