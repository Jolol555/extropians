<!-- received="Sun Sep 22 17:40:04 1996 MST" -->
<!-- sent="Sun, 22 Sep 96 16:36:02 PDT" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@hss.caltech.edu" -->
<!-- subject="Re: Darwinian Extropy" -->
<!-- id="9609222336.AA25896@hss.caltech.edu" -->
<!-- inreplyto="324580C9.470E@shirenet.com" -->
<title>extropians: Re: Darwinian Extropy</title>
<h1>Re: Darwinian Extropy</h1>
Robin Hanson (<i>hanson@hss.caltech.edu</i>)<br>
<i>Sun, 22 Sep 96 16:36:02 PDT</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1086">[ date ]</a><a href="index.html#1086">[ thread ]</a><a href="subject.html#1086">[ subject ]</a><a href="author.html#1086">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1087.html">Dan Clemmensen: "Re: Darwinian Extropy"</a>
<li> <b>Previous message:</b> <a href="1085.html">Robin Hanson: "Re: The Singularity and Nanotechnology"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Dan Clemmensen writes:<br>
<i>&gt; &gt;&gt; ... to make your scenario plausible, you need a plausible process which</i><br>
<i>&gt; &gt;&gt; creates this massive convergence to a preference with almost no weight</i><br>
<i>&gt; &gt;&gt; on long-time-scale returns.</i><br>
<i>&gt;</i><br>
<i>&gt;The SI can think so fast that on it's time-scale any possible</i><br>
<i>&gt;extra-system return is too far into the future to be useful in</i><br>
<i>&gt;comparison to the forgone computational capability represented by the</i><br>
<i>&gt;extra-system probe's mass. I proposed that the SI would increase its</i><br>
<i>&gt;speed by several orders of magnitude by converting its mass into a</i><br>
<i>&gt;neutron star.</i><br>
<p>
You seem to think that there is some natural discount rate, determined<br>
by the computer hardware.  I don't see why.<br>
<p>
<i>&gt;Unfortunately, as you say there seems to be little in the way a human</i><br>
<i>&gt;or corporation can do in the way of useful self-augmentation. I</i><br>
<i>&gt;contend that an SI that includes a substantial computer component is</i><br>
<i>&gt;very amenable to useful self-augmentation, while people and</i><br>
<i>&gt;organizations are not. The reason: the SI can understand itself and it</i><br>
<i>&gt;can reprogram itself. I contend that this is fundamentally different</i><br>
<i>&gt;than the process used by a human or a corporation atempting</i><br>
<i>&gt;self-augmentation.</i><br>
<p>
Why do you think an SI will understand itself any more than we<br>
understand ourselves?  And even if it could, that doesn't mean such<br>
understanding will lead to much improvement.  <br>
<p>
Robin D. Hanson  hanson@hss.caltech.edu  <a href="http://hss.caltech.edu/~hanson/">http://hss.caltech.edu/~hanson/</a><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1087.html">Dan Clemmensen: "Re: Darwinian Extropy"</a>
<li> <b>Previous message:</b> <a href="1085.html">Robin Hanson: "Re: The Singularity and Nanotechnology"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
