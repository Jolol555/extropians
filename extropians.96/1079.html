<!-- received="Sun Sep 22 12:10:12 1996 MST" -->
<!-- sent="Sun, 22 Sep 1996 14:09:13 -0400" -->
<!-- name="Dan Clemmensen" -->
<!-- email="dgc@shirenet.com" -->
<!-- subject="Re: Darwinian Extropy" -->
<!-- id="2.2.16.19960922084643.3f171c1e@popd.ix.netcom.com" -->
<!-- inreplyto="Darwinian Extropy" -->
<title>extropians: Re: Darwinian Extropy</title>
<h1>Re: Darwinian Extropy</h1>
Dan Clemmensen (<i>dgc@shirenet.com</i>)<br>
<i>Sun, 22 Sep 1996 14:09:13 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1079">[ date ]</a><a href="index.html#1079">[ thread ]</a><a href="subject.html#1079">[ subject ]</a><a href="author.html#1079">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1080.html">Chris Hind: "Re: sloppy posts"</a>
<li> <b>Previous message:</b> <a href="1078.html">Dan Clemmensen: "Re: The Singularity and Nanotechnology"</a>
<li> <b>Maybe in reply to:</b> <a href="0848.html">Tim_Robbins@aacte.nche.edu: "Darwinian Extropy"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1101.html">Eugene Leitl: "Re: objectivists hate libertariens"</a>
<li> <b>Reply:</b> <a href="1101.html">Eugene Leitl: "Re: objectivists hate libertariens"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Robin Hanson wrote:<br>
<i>&gt; </i><br>
<i>&gt; Dan Clemmensen writes:</i><br>
<i>&gt; &gt;&gt; &gt;... There may not be a lot of diverse SIs</i><br>
<i>&gt; &gt;&gt; &gt;in the universe. There may be only one per system, and they may all</i><br>
<i>&gt; &gt;&gt; &gt;have reached the same super-logical conclusion that star travel is</i><br>
<i>&gt; &gt;&gt; &gt;uneconomical in terms of the resources that SIs use.</i><br>
<i>&gt; &gt;&gt;</i><br>
<i>&gt; &gt;&gt; ... to make your scenario plausible, you need a plausible process which</i><br>
<i>&gt; &gt;&gt; creates this massive convergence to a preference with almost no weight</i><br>
<i>&gt; &gt;&gt; on long-time-scale returns.</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt;... An SI is likely to have conscious control of its internal</i><br>
<i>&gt; &gt;archecture, so the postulated subconscious human group-think may not</i><br>
<i>&gt; &gt;be relevant.</i><br>
<i>&gt; &gt;Please note: I'm still not arguing that my model of an SI is the</i><br>
<i>&gt; &gt;correct one, only that it's plausible.</i><br>
<i>&gt; </i><br>
<i>&gt; It seems to me that in the absence of a process pushing conformity,</i><br>
<i>&gt; one should expect diversity, at least when we're talking about</i><br>
<i>&gt; motivations across the entire visible universe.  Yes, it's possible</i><br>
<i>&gt; there is such a process we don't know anything about, but this simple</i><br>
<i>&gt; statement does not make the conclusion "plausible", only "posibble".</i><br>
<i>&gt; Otherwise any not-logically-impossible conclusion would be</i><br>
<i>&gt; "plausible".</i><br>
<p>
I thought that I had presented the process in another portion of<br>
my post: The SI can think so fast that on it's time-scale any possible<br>
extra-system return is too far into the future to be useful in<br>
comparison<br>
to the forgone computational capability represented by the extra-system<br>
probe's<br>
mass. I proposed that the SI would increase its speed by several orders<br>
of magnitude by converting its mass into a neutron star.<br>
<p>
<i>&gt; </i><br>
<i>&gt; &gt;Your [Anders'] scenario may be plausible, but I feel that my scenario</i><br>
<i>&gt; &gt;is more likely: the Initial SI (for example an experimenter together</i><br>
<i>&gt; &gt;with a workstation and a bunch of software) is capable of rapid</i><br>
<i>&gt; &gt;self-augmentation.  Since the experimenter and the experiment are</i><br>
<i>&gt; &gt;likely to be oriented toward developing an SI, the self-augmentation</i><br>
<i>&gt; &gt;is likely to result in rapid intelligence gain.</i><br>
<i>&gt; </i><br>
<i>&gt; Most complex systems we know of are capable of rapid</i><br>
<i>&gt; self-augmentation.  People can change, companies can change, and</i><br>
<i>&gt; nations can change.  *Useful* rapid change is a lot harder, however,</i><br>
<i>&gt; and you have offered no plausible argument why such useful rapid</i><br>
<i>&gt; change is any more likely here than for other complex systems.  Again,</i><br>
<i>&gt; yes, it is logically possible.  But that is hardly a plausibility</i><br>
<i>&gt; argument.</i><br>
<i>&gt; </i><br>
<p>
Unfortunately, as you say there seems to be little in the way a human or<br>
corporation can do in the way of useful self-augmentation. I contend<br>
that<br>
an SI that includes a substantial computer component is very amenable to<br>
useful self-augmentation, while people and organizations are not. The<br>
reason:<br>
the SI can understand itself and it can reprogram itself. I contend that<br>
this<br>
is fundamentally different than the process used by a human or a<br>
corporation<br>
atempting self-augmentation.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1080.html">Chris Hind: "Re: sloppy posts"</a>
<li> <b>Previous message:</b> <a href="1078.html">Dan Clemmensen: "Re: The Singularity and Nanotechnology"</a>
<li> <b>Maybe in reply to:</b> <a href="0848.html">Tim_Robbins@aacte.nche.edu: "Darwinian Extropy"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1101.html">Eugene Leitl: "Re: objectivists hate libertariens"</a>
<li> <b>Reply:</b> <a href="1101.html">Eugene Leitl: "Re: objectivists hate libertariens"</a>
<!-- reply="end" -->
</ul>
