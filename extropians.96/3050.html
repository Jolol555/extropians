<!-- received="Sun Nov 17 06:45:35 1996 MST" -->
<!-- sent="Sun, 17 Nov 1996 14:41:11 +0100" -->
<!-- name="Max M" -->
<!-- email="maxmcorp@inet.uni-c.dk" -->
<!-- subject="Re: the robot scenario" -->
<!-- id="199611171340.OAA16844@inet.uni-c.dk" -->
<!-- inreplyto="the robot scenario" -->
<title>extropians: Re: the robot scenario</title>
<h1>Re: the robot scenario</h1>
Max M (<i>maxmcorp@inet.uni-c.dk</i>)<br>
<i>Sun, 17 Nov 1996 14:41:11 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3050">[ date ]</a><a href="index.html#3050">[ thread ]</a><a href="subject.html#3050">[ subject ]</a><a href="author.html#3050">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3051.html">Chris Hind: "Re: Global Climate Manipulation &amp; Control"</a>
<li> <b>Previous message:</b> <a href="3049.html">Lyle Burkhead: "Transforming Ourselves"</a>
<li> <b>Maybe in reply to:</b> <a href="3176.html">Max M: "the robot scenario"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="3172.html">Max M: "Re: the robot scenario"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; Have you read _The Time Ships_ by Baxter by any chance? He discusses this</i><br>
<p>
<i>&gt; scenario in the book, in the final part of the book. </i><br>
<p>
No can't say that i have. But i have recieved a lot of reading suggestions<br>
that i will check out. Hopefully they have them at amazon.com as american<br>
books are horribly expensive here in Denmark. 3 times the US price at<br>
least.<br>
<p>
<i>&gt;I often argue for what could be seen as the robot scenario </i><br>
<i>&gt;with humans as hangers-on: by developing AI based on human-computer </i><br>
<i>&gt;symbiosis, we ensure that we will remain important parts of any spreading </i><br>
<i>&gt;AI civilization.</i><br>
<p>
This means that very early on we must take the question of robo-control<br>
into consideration.<br>
<p>
<p>
<p>
<i>&gt; Why should I care whether my descendants are meat or metal?</i><br>
<i>&gt; The best choice would be to go there myself;</i><br>
<p>
agreed<br>
<p>
<p>
<i>&gt; I have often considered the possibility that our creations will outsmart </i><br>
<i>&gt; humanity... they will simply become too smart for humans at our current</i><br>
level &gt; of intelligence.<br>
<p>
<i>&gt; The only real solution I see is self-augmentation/transformation and</i><br>
intense <br>
<i>&gt; cyborganization.</i><br>
<p>
Maybe something as simple as making robot societies self controlling.<br>
Robots  looking after robots. Sort of a police function.<br>
<p>
<i>&gt; We also need to train our minds and develop a very strong mental</i><br>
discipline &gt; within ourselves.<br>
<p>
That will probably be close to impossible for the majority of people living<br>
today.<br>
<p>
<i>&gt; It's survival of the fittest.  It always has been, and it always will be.</i><br>
<p>
By saying that you imply that humans are merely a slightly higher evolved<br>
animal and i don't agree with that. Humans have intelligence. Maybe some<br>
future race will have a higher intelligence but that doesn't mean that they<br>
will per default win some kind of survival battle. We are different than<br>
animals in a lot of ways. For one thing we can make concious decisions and<br>
go for an explicit goal.<br>
<p>
<i>&gt; We cannot remain the type of creatures we are now.  We must transform </i><br>
<i>&gt; ourselves to survive.  We also cannot wait until machine takeover is</i><br>
imminent. <br>
<i>&gt; We must start *now*, using whatever we have available to transform</i><br>
ourselves, <br>
<i>&gt; disciplining our minds, training ourselves to be more intelligent,</i><br>
creative, <br>
<i>&gt; resourceful and adaptable.</i><br>
<p>
<i>&gt; Strange paradox: we must change profoundly to survive.  That's life.</i><br>
<p>
yes<br>
<p>
<p>
<i>&gt; Wait a minute -- destroyed?  Just because an entity is more powerful </i><br>
<i>&gt; and intelligent than humans doesn't mean it's going to set out to destroy</i><br>
<p>
<i>&gt; us.</i><br>
<p>
It could also happen out of shere negligence on their part. Like a lot of<br>
species are destroyed by os now<br>
<p>
<i>&gt; Apparently you expect some kind of Paget scenario, in which our </i><br>
<i>&gt; robot-creations declare war on us.</i><br>
<p>
It's certainly a possibility, but it can be made a lot less likely if we<br>
think it into our designs from the beginning.<br>
<p>
<p>
MAX M<br>
New Media Director<br>
<p>
Private: maxmcorp@inet.uni-c.dk<br>
         <a href="http://inet.uni-c.dk/~maxmcorp">http://inet.uni-c.dk/~maxmcorp</a><br>
<p>
Work:    maxm@novavision.dk<br>
         <a href="http://www.novavision.dk/">http://www.novavision.dk/</a><br>
<p>
This is my way cool signature message!!<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3051.html">Chris Hind: "Re: Global Climate Manipulation &amp; Control"</a>
<li> <b>Previous message:</b> <a href="3049.html">Lyle Burkhead: "Transforming Ourselves"</a>
<li> <b>Maybe in reply to:</b> <a href="3176.html">Max M: "the robot scenario"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="3172.html">Max M: "Re: the robot scenario"</a>
<!-- reply="end" -->
</ul>
