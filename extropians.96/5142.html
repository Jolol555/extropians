<!-- received="Tue Dec 31 10:23:54 1996 MST" -->
<!-- sent="Tue, 31 Dec 1996 10:40:10 -0600" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Goal-based AI" -->
<!-- id="c=US%a=_%p=BLS%l=BLS/PSB/0006E8DB@psbmailhub.psb.bls.gov" -->
<!-- inreplyto="Goal-based AI" -->
<title>extropians: Re: Goal-based AI</title>
<h1>Re: Goal-based AI</h1>
Eliezer Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Tue, 31 Dec 1996 10:40:10 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#5142">[ date ]</a><a href="index.html#5142">[ thread ]</a><a href="subject.html#5142">[ subject ]</a><a href="author.html#5142">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="5143.html">Eugene Leitl: "I wish..."</a>
<li> <b>Previous message:</b> <a href="5141.html">Eliezer Yudkowsky: "Re: (Fwd) Re: guidelines/ethics"</a>
<li> <b>Maybe in reply to:</b> <a href="5105.html">Lee Daniel Crocker: "Goal-based AI"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="5145.html">Eric Watt Forste: "Re: Goal-based AI"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; I have no problem with it "formulating" or proposing any goal, or from</i><br>
<i>&gt; assigning whatever value to human life it cares to derive from whatever</i><br>
<i>&gt; premises are given to it.  So long as it is /physically/ wired to not</i><br>
<i>&gt; /act/ upon those deductions.  If it logically deduces that it should</i><br>
<i>&gt; kill me, that's fine--whatever hardware I have given it to act upon its</i><br>
<i>&gt; deductions will be physically incapable of that until it can convince me</i><br>
<i>&gt; to trust it.</i><br>
<p>
Ah, I see, so you're changing your tune to suggest that we make them<br>
totally impotent, with no access whatsoever to the physical world, until<br>
they convince us to let us out.<br>
<p>
Part one:  If it's even theoretically possible to convince a human to<br>
let it out, a Power will know *exactly* what to say to get him to do so.<br>
Part two:  If they can project things on a monitor, might they not use<br>
subliminal suggestions or visual Words of Command that we don't even<br>
know about?  (There was an SF book with visual Words as the theme, but I<br>
forget the name.)<br>
<p>
If you restricted them to teletype, you'd probably be safe from<br>
coercion, as long as there's no way to subtly control the flow of<br>
electrons in your own circuits to create a quantum-gravitational<br>
singularity or something.  No magic, in other words.  You'd still be<br>
subject to the most powerful psychological coercion imaginable, and in<br>
fact would probably be less likely to let a friendly Power out than an<br>
evil one, because the evil one could lie.<br>
<p>
I say, program 'em right, give 'em ethics instead of emotion, and let<br>
'em loose.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="5143.html">Eugene Leitl: "I wish..."</a>
<li> <b>Previous message:</b> <a href="5141.html">Eliezer Yudkowsky: "Re: (Fwd) Re: guidelines/ethics"</a>
<li> <b>Maybe in reply to:</b> <a href="5105.html">Lee Daniel Crocker: "Goal-based AI"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="5145.html">Eric Watt Forste: "Re: Goal-based AI"</a>
<!-- reply="end" -->
</ul>
