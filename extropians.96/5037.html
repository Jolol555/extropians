<!-- received="Sun Dec 29 18:58:57 1996 MST" -->
<!-- sent="Sun, 29 Dec 1996 19:24:45 -0600" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Profiting on tragedy? (was Humour)" -->
<!-- id="v01530511aeec52a7ae10@[24.1.1.26]" -->
<!-- inreplyto="Profiting on tragedy? (was Humour)" -->
<title>extropians: Re: Profiting on tragedy? (was Humour)</title>
<h1>Re: Profiting on tragedy? (was Humour)</h1>
Eliezer Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sun, 29 Dec 1996 19:24:45 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#5037">[ date ]</a><a href="index.html#5037">[ thread ]</a><a href="subject.html#5037">[ subject ]</a><a href="author.html#5037">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="5038.html">Michael Lorrey: "Re: SPACE: Lunar Billboard?"</a>
<li> <b>Previous message:</b> <a href="5036.html">Eliezer Yudkowsky: "Re: "good luck, how fortunate!""</a>
<li> <b>Maybe in reply to:</b> <a href="4815.html">John P. Satta: "Profiting on tragedy? (was Humour)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="5150.html">Eric Watt Forste: "Re: Profiting on tragedy? (was Humour)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; And I'm supposed to accept your wild speculations over mine?  If that's</i><br>
<i>&gt; what will happen when I hard-wire a robot not to kill me, then so be it.</i><br>
<i>&gt; I leave those wires where they are.  If, and only if, I can rationally</i><br>
<i>&gt; convince myself--with solid reason, not analogy and extrapolation--that</i><br>
<i>&gt; clipping those wires will be in my interest, will I consider it.</i><br>
<p>
1)  Do you have a specific procedure for hard-wiring the robot, other<br>
than the default procedure of setting a particular goal to a permanently<br>
high value and perhaps prohibiting opposing goals from being formulated?<br>
2)  Do you know anything about goal-based cognitive architectures?<br>
3)  Can you make any specific objection to my speculation, as a student<br>
of AI, that doing this will (a) destabilize the goal system (b) not work<br>
and (c) if it does work, turn out disastrously.<br>
<p>
The system you're describing is that of the human emotional system<br>
"hardwired" - emotions act (on one axis) by affecting the perceived<br>
value of goals.  Humans, for solid architectural (not evolutionary)<br>
reasons, can override these goals, often to the great detriment of our<br>
evolutionary value - celibacy, for instance.  If evolution hasn't<br>
succeeding in hardwiring the goals so that they can't be altered -<br>
without turning the person in question into a moron - how do you suppose<br>
you will?<br>
<p>
There's a whole group of thought-trains that would, under normal<br>
circumstances, lead to the value of the "protect humans" goal being<br>
questioned.  Are you going to cut them all off at the roots?  How,<br>
without crippling the entire system?  If not at the roots, I think the<br>
result might be to create "islands" in the goal-system of artificially<br>
high protect-humans goals, surrounded by a sea of resentment, with the<br>
sea winding up directing most of the actions.<br>
<p>
Are you going to completely prohibit the system from redesigning its own<br>
emotions, even looking at them?  Are you going to have a little checker<br>
that prevents knowledge like "the humans are preventing me from thinking<br>
this" from being memorized?  Halting problem...<br>
<p>
Asimov's Three Laws are wild speculations.  I, speaking from my limited<br>
but still present knowledge of cognitive science, say that we might run<br>
into some little problems in the actual implementation.  I don't see how<br>
this is any different than an amateur physicist taking exception to Star<br>
Trek's "Warp Drive".<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="5038.html">Michael Lorrey: "Re: SPACE: Lunar Billboard?"</a>
<li> <b>Previous message:</b> <a href="5036.html">Eliezer Yudkowsky: "Re: "good luck, how fortunate!""</a>
<li> <b>Maybe in reply to:</b> <a href="4815.html">John P. Satta: "Profiting on tragedy? (was Humour)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="5150.html">Eric Watt Forste: "Re: Profiting on tragedy? (was Humour)"</a>
<!-- reply="end" -->
</ul>
