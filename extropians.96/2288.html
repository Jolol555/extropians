<!-- received="Tue Oct 29 09:18:09 1996 MST" -->
<!-- sent="Tue, 29 Oct 1996 08:15:39 -0800" -->
<!-- name="Eric Watt Forste" -->
<!-- email="arkuat@pobox.com" -->
<!-- subject="Re: Sacred Cows" -->
<!-- id="199610291615.IAA04567@idiom.com" -->
<!-- inreplyto="Sacred Cows" -->
<title>extropians: Re: Sacred Cows</title>
<h1>Re: Sacred Cows</h1>
Eric Watt Forste (<i>arkuat@pobox.com</i>)<br>
<i>Tue, 29 Oct 1996 08:15:39 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2288">[ date ]</a><a href="index.html#2288">[ thread ]</a><a href="subject.html#2288">[ subject ]</a><a href="author.html#2288">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2289.html">Robin Hanson: "Re: Sacred Cows"</a>
<li> <b>Previous message:</b> <a href="2287.html">Anders Sandberg: "Re: Sacred Cows"</a>
<li> <b>Maybe in reply to:</b> <a href="2268.html">David Musick: "Sacred Cows"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2289.html">Robin Hanson: "Re: Sacred Cows"</a>
<li> <b>Reply:</b> <a href="2289.html">Robin Hanson: "Re: Sacred Cows"</a>
<li> <b>Reply:</b> <a href="2292.html">Anders Sandberg: "Re: Sacred Cows"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Anders Sandberg wrote:<br>
<i>&gt;  As for extropian/transhuman sacred cows, there are of course the</i><br>
<i>&gt;  idea that things will get better in some sense in our future, and</i><br>
<i>&gt;  some see the Singularity or the utility of nanotech, uploading or</i><br>
<i>&gt;  AI as fundamental and unassailably true. Another dangerous sacred</i><br>
<i>&gt;  cow is the belief that we know what we are doing and are not a</i><br>
<i>&gt;  group of technophiles playing world saviours in our spare time.</i><br>
<p>
The idea that things *will* get better in our future is of course<br>
a variety of blind faith. Isn't that why we like to talk about<br>
dynamic optimism instead? A mild take on dynamic optimism is that<br>
it simply claims that we are *certain* to fail in finding solutions<br>
to our problems if we give up seeking those solutions. Dynamic<br>
optimism is no guarantee that solutions will be found; it is simply<br>
a motivating principle that keeps us seeking solutions (and saves<br>
us from disasturbation).<br>
<p>
The faith of the Singularity and the faith in some particular<br>
orthodoxy of nanotech, uploading, or AI are all regularly barbecued<br>
on this list. Generally, what I see on this list is that when one<br>
person makes an assertion of the form "Because of the inevitability<br>
of my-favorite-techno, such and such is also inevitable" I often<br>
also see followup requests asking for elucidation and defense of<br>
both the premise and the deduction. For me personally, the Singularity<br>
signifies the fact that the future gets cloudier faster as we try<br>
to look into it from our own time than it did for thinkers and<br>
futurists of the past. The Singularity signifies nothing more to<br>
me than that. I've always been more interested in the prospect of<br>
the Diaspora than the prospect of the Singularity.<br>
<p>
Even if AI is successful, unless we develop an ability to understand<br>
and recreate human motivations, AI will only create a new race of<br>
slaves that *could* be poisonous to our culture.  They're writing<br>
books on android epistemology now, but android ethics is still<br>
stuck in the stage that Asimov left it in, which was a simple<br>
apologia for the enslavement of nonhumans. (An argument, I hope I<br>
need remind no one, which could easily be extended so as to argue<br>
for the enslavement of posthumans. Don't laugh quite so fast.)<br>
<p>
Nanotech seems plausible to me, but it's no sacred cow, because it<br>
doesn't seem to be an unalloyed good. We all are aware of the<br>
possibility of engineered viruses designed not to attack our<br>
computers, but *us*, and this possibility becomes more serious each<br>
passing year. That's one motivator for us to work on these problems,<br>
so that we'll be in a position to engineer a defense against such<br>
things when one is called for. I liked Neal Stephenson's idea (in<br>
THE DIAMOND AGE) that the notion of "defense in depth" will take<br>
on new meaning in such an environment. It helped make me more aware<br>
of the staggering complexity of the social changes that nanotech<br>
will probably induce. But unlike Rich Artym, I don't conclude from<br>
this that all my current theoretical apparatus is rendered impotent<br>
and worthless in the face of such changes.<br>
<p>
And as for uploading, while I'm familiar with the arguments that<br>
uploading shouldn't (in principle) have any more disruptive effect<br>
on me than drinking a cup of coffee, these fragile and evanescent<br>
thought-experiments don't convince me, because it seems to me that<br>
a distinction that lies at the heart of such gedankenexperiments<br>
is the very poorly understood distinction between signal and noise.<br>
The distinction of signal from noise is a value-judgement passed on<br>
"pieces" of information, and I have yet to find anyone who claims<br>
to be able to explain to me how value-judgments are performed or<br>
how this system might be optimized. (Meanwhile, I work at this<br>
problem myself from time to time.)<br>
<p>
Will nanotech do more good than harm? An open question. We might<br>
want to exert ourselves to see that it does.<br>
<p>
Will uploading be possible without inducing such profound psychological<br>
transformation that we could not claim that the original human<br>
uploader "lives on"? Another open question.<br>
<p>
If "true AI" (whatever that means) proves possible, will human<br>
beings and human power-institutions accept such entities as anything<br>
more than slaves? What kind of conflicts could this give rise to?<br>
(A study of history shows that conflict between self-motivating<br>
systems such as humans--or AIs--is a greatly destructive force.)<br>
Actually, I'm pretty optimistic about this one, but in this post<br>
I'm trying to cook up some barbecue.<br>
<p>
I hope you find it tasty.<br>
<p>
The most important sacred cow Anders mentioned is the idea that we<br>
know what we are doing. One of my favorite philosophers, William<br>
Bartley, made the observation (and he emphasised it) that we never<br>
know what we are saying and that we never know what we are doing.<br>
All our words and all our actions will have far more unintended<br>
consequences than intended ones. I don't know about the rest of<br>
the list, but I'm quite confident that we have no idea what we're<br>
doing. We ought to give it a good college try anyway.<br>
<p>
Eric Watt Forste ++ <a href="mailto:arkuat@pobox.com">mailto:arkuat@pobox.com</a> ++ <a href="http://www.c2.org/~arkuat/">http://www.c2.org/~arkuat/</a><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2289.html">Robin Hanson: "Re: Sacred Cows"</a>
<li> <b>Previous message:</b> <a href="2287.html">Anders Sandberg: "Re: Sacred Cows"</a>
<li> <b>Maybe in reply to:</b> <a href="2268.html">David Musick: "Sacred Cows"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2289.html">Robin Hanson: "Re: Sacred Cows"</a>
<li> <b>Reply:</b> <a href="2289.html">Robin Hanson: "Re: Sacred Cows"</a>
<li> <b>Reply:</b> <a href="2292.html">Anders Sandberg: "Re: Sacred Cows"</a>
<!-- reply="end" -->
</ul>
