<!-- received="Mon Aug 26 11:41:46 1996 MST" -->
<!-- sent="Mon, 26 Aug 1996 10:38:46 -0700 (MST)" -->
<!-- name="Max More" -->
<!-- email="maxmore@primenet.com" -->
<!-- subject="Re: Controlling AI (was: Thinking about the future...)" -->
<!-- id="199608261738.KAA25765@primenet.com" -->
<!-- inreplyto="Controlling AI (was: Thinking about the future...)" -->
<title>extropians: Re: Controlling AI (was: Thinking about the future...)</title>
<h1>Re: Controlling AI (was: Thinking about the future...)</h1>
Max More (<i>maxmore@primenet.com</i>)<br>
<i>Mon, 26 Aug 1996 10:38:46 -0700 (MST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#506">[ date ]</a><a href="index.html#506">[ thread ]</a><a href="subject.html#506">[ subject ]</a><a href="author.html#506">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0507.html">Russell Whitaker: "Re: Lets Move HotWired Thread to alt.extropians"</a>
<li> <b>Previous message:</b> <a href="0505.html">Max More: "Re: Thinking about the future..."</a>
<li> <b>Maybe in reply to:</b> <a href="0491.html">Peter Voss: "Controlling AI (was: Thinking about the future...)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0516.html">Sean Hastings: "Re: Controlling AI (was: Thinking about the future...)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
At 01:06 PM 8/26/96 GMT2, Stephen de Vries wrote:<br>
<i>&gt;</i><br>
<i>&gt;What if you invented an algorithm which is crucial to the workings of </i><br>
<i>&gt;a very succesfull a-life organism which is set to replace humanity.  </i><br>
<i>&gt;A meme you have created will be immortal, and the father-mother of a </i><br>
<i>&gt;new era in evolution.  Do you want to live on through your genes, or </i><br>
<i>&gt;your memes ?</i><br>
<p>
Peter will have his own answer, but mine is: NEITHER! Of course I'm not<br>
particularly interested in living on through my genes (although I'd take<br>
that option if it were the *only* one). While I want my memes, or some of<br>
them, or those that survive critical testing, to live on, I'm vastly more<br>
interested in *me* living on. <br>
<p>
I am not my memes. I have ideas, as well as memories, dispositions, values,<br>
and so on, but it's *me* -- the active, choosing being that I want to<br>
survive and flourish, not primarily some ideas that I have.<br>
<p>
So I thoroughly agree with Peter Voss's suggestions. I will encourage AI<br>
researchers developing full blown AI/SI's to build in a value system that<br>
makes them less likely to destroy humans or disregard our interests. I would<br>
much rather support the development of synthetic intelligence systems that<br>
can directly augment our own intelligence, so we don't get left behind.<br>
<p>
Fortunately, I suspect, SI (Synthetic Intelligence) research will mostly be<br>
devoted to developing systems specialized for particular purposes, like air<br>
traffic control, finding patterns in cosmological data, discovering new<br>
mathematical theorums, inventing new drugs, etc. That's where the money<br>
lies. I don't see so much commercial interest in pushing for a humanlike<br>
intelligence. Obviously, though, once the components are developed, someone<br>
will want to put them together just to show that it can be done.<br>
<p>
I delight in advanced technology, but have no interest in self-sacrifice. I<br>
want to live forever (or as long as I choose). This is vastly more important<br>
to me than seeing that some other lifeform is created. I'm glad that Peter<br>
raised this question. It deserves serious consideration.<br>
<p>
BTW, I finished reading Host (by Peter James). I found it extremely hard to<br>
put down and can highly recommend it, despite some criticisms I have of it.<br>
Extremely accurate in its description of cryonics and AI, and genuinely<br>
scary. Reading it may lead the AI-at-any-cost folks to reconsider!<br>
<p>
<p>
Upward and Outward!<br>
<p>
Max<br>
<p>
Max More, Ph.D.<br>
maxmore@primenet.com<br>
<a href="http://www.primenet.com/~maxmore">http://www.primenet.com/~maxmore</a><br>
President:  Extropy Institute (ExI)<br>
Editor:  Extropy<br>
310-398-0375<br>
                    <a href="http://www.primenet.com/~maxmore/extropy.htm">http://www.primenet.com/~maxmore/extropy.htm</a><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0507.html">Russell Whitaker: "Re: Lets Move HotWired Thread to alt.extropians"</a>
<li> <b>Previous message:</b> <a href="0505.html">Max More: "Re: Thinking about the future..."</a>
<li> <b>Maybe in reply to:</b> <a href="0491.html">Peter Voss: "Controlling AI (was: Thinking about the future...)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0516.html">Sean Hastings: "Re: Controlling AI (was: Thinking about the future...)"</a>
<!-- reply="end" -->
</ul>
