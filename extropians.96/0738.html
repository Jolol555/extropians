<!-- received="Tue Sep  3 19:54:57 1996 MST" -->
<!-- sent="Tue, 3 Sep 1996 18:56:51 -0700" -->
<!-- name="Eric Watt Forste" -->
<!-- email="arkuat@factory.net" -->
<!-- subject="Thinking about the future..." -->
<!-- id="v02140b60ae528e59d152@[204.162.114.176]" -->
<!-- inreplyto="" -->
<title>extropians: Thinking about the future...</title>
<h1>Thinking about the future...</h1>
Eric Watt Forste (<i>arkuat@factory.net</i>)<br>
<i>Tue, 3 Sep 1996 18:56:51 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#738">[ date ]</a><a href="index.html#738">[ thread ]</a><a href="subject.html#738">[ subject ]</a><a href="author.html#738">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0739.html">Eric Watt Forste: "Re: The Great Filter"</a>
<li> <b>Previous message:</b> <a href="0737.html">Sean Morgan: "Extropy mentioned in Analog"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0498.html">Stephen de Vries: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0498.html">Stephen de Vries: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0505.html">Max More: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0517.html">N.Bostrom@lse.ac.uk: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0567.html">QueeneMUSE@aol.com: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0584.html">Stephen de Vries: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0590.html">N.Bostrom@lse.ac.uk: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0613.html">Dan Clemmensen: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0721.html">N.Bostrom@lse.ac.uk: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0735.html">Dan Clemmensen: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0736.html">Eric Watt Forste: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0762.html">Peter C. McCluskey: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0763.html">QueeneMUSE@aol.com: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0764.html">"N_\\Bostrom_at_{posbn}"@smtplink.lse.ac.uk: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0775.html">QueeneMUSE@aol.com: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0790.html">Dan Clemmensen: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0791.html">Dan Clemmensen: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0792.html">Dan Clemmensen: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0810.html">QueeneMUSE@aol.com: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0818.html">Dan Clemmensen: "Re: Thinking about the future..."</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
At 11:47 AM 9/3/96, N.Bostrom@lse.ac.uk wrote:<br>
<i>&gt;          But when transhumanists talk about &gt;AI they hardly mean a</i><br>
<i>&gt;          moderate &gt;AI -like a very brilliant human and then some. We</i><br>
<i>&gt;          speculate about a machine that would be a million times</i><br>
<i>&gt;          faster than any human brain, and with correspondingly great</i><br>
<i>&gt;          memory capacity. Could such a machine, given some time, not</i><br>
<i>&gt;          manipulate a human society by subtle suggestions that seem</i><br>
<i>&gt;          very reasonable but unnoticeable affects a general change in</i><br>
<i>&gt;          attitude and policy? And all the time it would look as if it</i><br>
<i>&gt;          were a perfectly decent machine, always concerned about our</i><br>
<i>&gt;          wellfare...</i><br>
<p>
The ability of such a superintelligence to do this I don't doubt at all. I<br>
don't know if it's possible, but likewise, I can't assert that it's<br>
impossible. I've seen too much turmoil in the world caused by mere books<br>
written by mehums.  ;)<br>
<p>
<p>
<i>&gt;          How likely is it that a malicious &gt;AI could bring disaster</i><br>
<i>&gt;          to a human society that were initially determined to take</i><br>
<i>&gt;          the necessary precautions?</i><br>
<i>&gt;</i><br>
[snip]<br>
<i>&gt;</i><br>
<i>&gt;          My contention is that with only one full-blown &gt;AI in the</i><br>
<i>&gt;          world, if it were malicious, the odds would be on the side</i><br>
<i>&gt;          that it could annihilate humanity within decades.</i><br>
<p>
What I do doubt, however, is that any superintelligence could develop a set<br>
of values and understandings that would lead it to conclude that such a<br>
course of action would be conducive to whatever *other* projects it was<br>
trying to undertake. If the superintelligence actually had, as its primary<br>
value and its primary project, the destruction of human civilization, then<br>
yes, it could succeed. But this project would be directly incompatible with<br>
the swift and efficient implementation of so many other possible projects<br>
that the superintelligence might plausibly value, that I think a "basic"<br>
value for the destruction of human civilization would rapidly get damped<br>
down by competition from other projects the SI might value that would<br>
benefit from using cooperation with human civilization as a means. Just as<br>
a "basic" value of bloodlust in human beings *usually* gets damped down by<br>
competition from the other basic values for sex, money, power-by-consent,<br>
etc. which in the long run and in the context of modern civilization are<br>
ill-served by a taste for nonconsensual violence. The strength of this<br>
damping process for humans will depend on the degree of violence that is<br>
routine in the society in which the human finds emself, but I suspect that<br>
"superintelligence" (whatever that turns out to mean) will strengthen this<br>
process of damping down value-projects that are too incompatible with too<br>
many other competing value-projects.<br>
<p>
If the superintelligence is ambitious enough and powerful enough to be a<br>
true value pluralist, to find it boring and trivial to undertake any course<br>
of action other than the simultaneous execution of many different<br>
orthogonal projects in realtime (and I think this does describe the<br>
behavior we observe in many of the brightest human beings so far), then I<br>
don't think we'll have too much to fear from it. Perhaps I'm being an<br>
ostrich about this, but I'd love to hear some solid counterarguments to the<br>
position I'm taking.<br>
<p>
Eric Watt Forste      &lt;arkuat@pobox.com&gt;     <a href="http://www.c2.org/~arkuat/">http://www.c2.org/~arkuat/</a><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0739.html">Eric Watt Forste: "Re: The Great Filter"</a>
<li> <b>Previous message:</b> <a href="0737.html">Sean Morgan: "Extropy mentioned in Analog"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0498.html">Stephen de Vries: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0498.html">Stephen de Vries: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0505.html">Max More: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0517.html">N.Bostrom@lse.ac.uk: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0567.html">QueeneMUSE@aol.com: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0584.html">Stephen de Vries: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0590.html">N.Bostrom@lse.ac.uk: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0613.html">Dan Clemmensen: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0721.html">N.Bostrom@lse.ac.uk: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0735.html">Dan Clemmensen: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0736.html">Eric Watt Forste: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0762.html">Peter C. McCluskey: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0763.html">QueeneMUSE@aol.com: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0764.html">"N_\\Bostrom_at_{posbn}"@smtplink.lse.ac.uk: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0775.html">QueeneMUSE@aol.com: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0790.html">Dan Clemmensen: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0791.html">Dan Clemmensen: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0792.html">Dan Clemmensen: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0810.html">QueeneMUSE@aol.com: "Re: Thinking about the future..."</a>
<li> <b>Maybe reply:</b> <a href="0818.html">Dan Clemmensen: "Re: Thinking about the future..."</a>
<!-- reply="end" -->
</ul>
