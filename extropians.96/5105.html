<!-- received="Mon Dec 30 17:48:00 1996 MST" -->
<!-- sent="Mon, 30 Dec 1996 16:23:40 -0800 (PST)" -->
<!-- name="Lee Daniel Crocker" -->
<!-- email="lcrocker@calweb.com" -->
<!-- subject="Goal-based AI" -->
<!-- id="199612310023.QAA14829@web2.calweb.com" -->
<!-- inreplyto="32C719B6.2182@pobox.com" -->
<title>extropians: Goal-based AI</title>
<h1>Goal-based AI</h1>
Lee Daniel Crocker (<i>lcrocker@calweb.com</i>)<br>
<i>Mon, 30 Dec 1996 16:23:40 -0800 (PST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#5105">[ date ]</a><a href="index.html#5105">[ thread ]</a><a href="subject.html#5105">[ subject ]</a><a href="author.html#5105">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="5106.html">J. Daugherty: "RE: Brin on privacy"</a>
<li> <b>Previous message:</b> <a href="5104.html">Damien Broderick: "Re: Parallel universes and Shroedinger's cat o' nine lives"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="5142.html">Eliezer Yudkowsky: "Re: Goal-based AI"</a>
<li> <b>Maybe reply:</b> <a href="5142.html">Eliezer Yudkowsky: "Re: Goal-based AI"</a>
<li> <b>Maybe reply:</b> <a href="5145.html">Eric Watt Forste: "Re: Goal-based AI"</a>
<li> <b>Maybe reply:</b> <a href="5158.html">Eliezer Yudkowsky: "Re: Goal-based AI"</a>
<li> <b>Maybe reply:</b> <a href="5159.html">Eliezer Yudkowsky: "Re: Goal-based AI"</a>
<li> <b>Maybe reply:</b> <a href="5172.html">Eliezer Yudkowsky: "Re: Goal-based AI"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; [I was arguing that before one builds an AI capable of harming humans,</i><br>
<i>&gt; one should hard-wire it not to]</i><br>
<i>&gt; </i><br>
<i>&gt; 1)  Do you have a specific procedure for hard-wiring the robot, other</i><br>
<i>&gt; than the default procedure of setting a particular goal to a permanently</i><br>
<i>&gt; high value and perhaps prohibiting opposing goals from being formulated?</i><br>
<p>
I have no problem with it "formulating" or proposing any goal, or from<br>
assigning whatever value to human life it cares to derive from whatever<br>
premises are given to it.  So long as it is /physically/ wired to not<br>
/act/ upon those deductions.  If it logically deduces that it should<br>
kill me, that's fine--whatever hardware I have given it to act upon its<br>
deductions will be physically incapable of that until it can convince me<br>
to trust it.<br>
<p>
<i>&gt; 2)  Do you know anything about goal-based cognitive architectures?</i><br>
<i>&gt; 3)  Can you make any specific objection to my speculation, as a student</i><br>
<i>&gt; of AI, that doing this will (a) destabilize the goal system (b) not work</i><br>
<i>&gt; and (c) if it does work, turn out disastrously.</i><br>
<p>
I have no particular expertise in that field.  I'm an ordinary grunt<br>
programmer and generally well-read human.  I have no reason to think<br>
that my speculations are better than yours, I only question your apparent<br>
willingness to let such speculation guide you in dangerous actions.<br>
<p>
<i>&gt; The system you're describing is that of the human emotional system</i><br>
<i>&gt; "hardwired" - emotions act (on one axis) by affecting the perceived</i><br>
<i>&gt; value of goals.  Humans, for solid architectural (not evolutionary)</i><br>
<i>&gt; reasons, can override these goals, often to the great detriment of our</i><br>
<i>&gt; evolutionary value - celibacy, for instance.  If evolution hasn't</i><br>
<i>&gt; succeeding in hardwiring the goals so that they can't be altered -</i><br>
<i>&gt; without turning the person in question into a moron - how do you suppose</i><br>
<i>&gt; you will?</i><br>
<p>
Evolution is stupid.  It has gotten as far as it has simply because it<br>
had a 4-billion-year head start on us.  Our minds can do better.  Our<br>
minds put life on the moon where evolution failed.  Our minds build<br>
photoreceptors that point the right direction.  Our minds can guide<br>
the future of our species and what we become by higher ethical standards<br>
than evolution.  I do not care to sacrifice my mind on any altar of<br>
myticism, whether it is called "God" or "Evolution".  I want to do<br>
better, because I /can/ do better.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="5106.html">J. Daugherty: "RE: Brin on privacy"</a>
<li> <b>Previous message:</b> <a href="5104.html">Damien Broderick: "Re: Parallel universes and Shroedinger's cat o' nine lives"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="5142.html">Eliezer Yudkowsky: "Re: Goal-based AI"</a>
<li> <b>Maybe reply:</b> <a href="5142.html">Eliezer Yudkowsky: "Re: Goal-based AI"</a>
<li> <b>Maybe reply:</b> <a href="5145.html">Eric Watt Forste: "Re: Goal-based AI"</a>
<li> <b>Maybe reply:</b> <a href="5158.html">Eliezer Yudkowsky: "Re: Goal-based AI"</a>
<li> <b>Maybe reply:</b> <a href="5159.html">Eliezer Yudkowsky: "Re: Goal-based AI"</a>
<li> <b>Maybe reply:</b> <a href="5172.html">Eliezer Yudkowsky: "Re: Goal-based AI"</a>
<!-- reply="end" -->
</ul>
