<!-- received="Tue Dec  3 00:32:48 1996 MST" -->
<!-- sent="Mon, 2 Dec 1996 23:21:20 -0800 (PST)" -->
<!-- name="Twirlip of Greymist" -->
<!-- email="phoenix@ugcs.caltech.edu" -->
<!-- subject="Re: Singularity-worship" -->
<!-- id="199612030721.XAA10954@off.ugcs.caltech.edu" -->
<!-- inreplyto="sentience@pobox.com" -->
<title>extropians: Re: Singularity-worship</title>
<h1>Re: Singularity-worship</h1>
Twirlip of Greymist (<i>phoenix@ugcs.caltech.edu</i>)<br>
<i>Mon, 2 Dec 1996 23:21:20 -0800 (PST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3587">[ date ]</a><a href="index.html#3587">[ thread ]</a><a href="subject.html#3587">[ subject ]</a><a href="author.html#3587">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3588.html">Alexander 'Sasha' Chislenko: ""AFUTD""</a>
<li> <b>Previous message:</b> <a href="3586.html">E. Shaun Russell: "Re: Singularity-worship"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
[Night of the Jaded Old-Timers]<br>
<p>
On Dec 2, 11:07pm, Eliezer Yudkowsky wrote:<br>
<p>
} My point was that, rather than your conception of the Powers being *in<br>
} conflict* with mine, I had found specific logical flaws in your<br>
<p>
(I'm just grabbing the handy post.)  How can you be sure that your model<br>
of the Beyond is extrapolatable.  You say that you consider yourself to<br>
be an incomplete human, and you feel a vast sense of awe when reading<br>
GEB.  But consider BLoop and FLoop and GLoop.  In my (human-optimistic)<br>
model you're in the position of BLoop (with some peculiar advantage)<br>
admiring and appreciating the difference between itself and FLoop, which<br>
the latter might not fully do, and extrapolating to some vastly more<br>
powerful GLoop, which I (FLoop) don't believe in.  It's possible that<br>
GLoop is out there, beyond our imagination, but it also might be that<br>
FLoop is the end point of algorithmic possibility.  Similarly, if humans<br>
(at least some of them) are the sentient equivalent of FLoop or the<br>
Universal Turing machine, then Powers will be smarter -- vastly smarter<br>
-- than us in inventability, or what they consider obvious, but<br>
equivalent in comprehensability.<br>
<p>
I do thank you for giving me these terms though; I'm very happy with being <br>
able to say that future minds could be smarter in inventing things we<br>
couldn't in practical time, but with our still being able to understand<br>
the results, at least in theory at worst.<br>
<p>
I'm not sure that I'm right.  I can't be; even if someone could be, I<br>
don't know the mathematics yet to appreciate the proof if there was one.<br>
But I don't see how you can be so sure that you're right, or that we're<br>
so wrong.  You can't extrapolated up a damage gradient and assume the<br>
curve will keep on going; it could plateau at some universal point.<br>
<p>
I do think the Singularity can be a legitimate term.  It can be<br>
poetical, like 'sunrise'.  It can be technical, for SF authors, like<br>
Vinge or Niven ("Safe at Any Speed"): even if I'm right about it being<br>
comprehensible ex post facto, that doesn't mean it's very inventable.<br>
By definition, lots of rapid change should/could cause much evolutionary<br>
change in society as markets and cultures adjust to new conditions.  As<br>
I think the brain works evolutionarily, obvious there will be a limit to<br>
what it can model decently.<br>
<p>
And if someone turns the Earth into gray goo, organizes the goo into his<br>
new brain, and uses his new power to solve game #11982 (or whichever) of<br>
FreeCell: this is comprehensible.  And inventable, obviously.  But _I'd_<br>
call it a Singularity.<br>
<p>
(Very singular.)<br>
(Sorry.)<br>
<p>
} no reason why our paranoia should be binding on the Powers.  Similarly,<br>
} there is a major logical flaw in the ethical idea that the value of our<br>
} lives is diminished by the presence of the Powers - to wit, there is no<br>
} reason to presume so, and plenty of reasons why the Meaning of Life<br>
} should be observer-independent.  So our paranoia should not be retraced<br>
<p>
Er.  Value of our lives as valued by whom?  My value of my life is<br>
infinite.  (Probably not, but lets approximate.)  My value of a million<br>
strangers is somewhat low.  Presumably person J's value of her life is<br>
infinite and her value of mine is low.  Lower now that I've typed this.<br>
:)  My value of my life doesn't change in the presence of Powers -- but<br>
what's their value of my life?<br>
<p>
And why should the Meaning of Life be observer-independent?  Meaning of<br>
whose life?  My meaning of a chicken's life is to feed me.<br>
<p>
(How many vegetarians on the list?  How many for reasons other than<br>
health?)  (Rich Artym still here?  Do I have to be the subjectivist?)<br>
<p>
Merry part,<br>
 -xx- Damien R. Sullivan X-) &lt;*&gt; <a href="http://www.ugcs.caltech.edu/~phoenix">http://www.ugcs.caltech.edu/~phoenix</a><br>
<p>
For no cage was ever made to hold a creature such as he,<br>
No chain was ever forged to bind the wind.<br>
And when I think upon all the scars he left on me,<br>
The cruelest are the ones I bear within.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3588.html">Alexander 'Sasha' Chislenko: ""AFUTD""</a>
<li> <b>Previous message:</b> <a href="3586.html">E. Shaun Russell: "Re: Singularity-worship"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
