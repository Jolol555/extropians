<!-- received="Thu Sep 12 17:03:44 1996 MST" -->
<!-- sent="Tue, 10 Sep 1996 19:39:43 -0400" -->
<!-- name="Dan Clemmensen" -->
<!-- email="dgc@shirenet.com" -->
<!-- subject="Re: Darwinian Extropy" -->
<!-- id="199609110619.XAA11601@primenet.com" -->
<!-- inreplyto="Darwinian Extropy" -->
<title>extropians: Re: Darwinian Extropy</title>
<h1>Re: Darwinian Extropy</h1>
Dan Clemmensen (<i>dgc@shirenet.com</i>)<br>
<i>Tue, 10 Sep 1996 19:39:43 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#921">[ date ]</a><a href="index.html#921">[ thread ]</a><a href="subject.html#921">[ subject ]</a><a href="author.html#921">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0922.html">Dan Clemmensen: "Re: Darwinian Extropy"</a>
<li> <b>Previous message:</b> <a href="0920.html">Max More: "Newsweek on superhormone therapy"</a>
<li> <b>Maybe in reply to:</b> <a href="0848.html">Tim_Robbins@aacte.nche.edu: "Darwinian Extropy"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0922.html">Dan Clemmensen: "Re: Darwinian Extropy"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Anders Sandberg wrote:<br>
<i>&gt; </i><br>
<i>&gt; I have noted something interesting in this thread, that we seem to assume</i><br>
<i>&gt; that SI appears out of nowwhere in a vacuum. The archetypal scenario is the</i><br>
<i>&gt; nanoworkstation at MIT that during the night transcends and takes of the</i><br>
<i>&gt; world.</i><br>
<i>&gt; </i><br>
<i>&gt; In reality, it is very unlikey that we will get SI before merely HI and</i><br>
<i>&gt; HI before subhuman intelligence. When the techniques for creating better</i><br>
<i>&gt; and better minds appear, they will lead to a succession of better and</i><br>
<i>&gt; better minds. This will also lead to a better and better understanding of</i><br>
<i>&gt; the problems and risks of intelligence engineering; unless the growth is</i><br>
<i>&gt; very fast and uncontrolled we will know about some of the dangers and</i><br>
<i>&gt; possibilities.</i><br>
<i>&gt; </i><br>
<i>&gt; If we create SI, we will most likely have plenty of HI and &gt;HI, we will</i><br>
<i>&gt; know what to expect. It should be noted that even SIs have limitations,</i><br>
<i>&gt; and they won't be a single giant among lilliputs - there will be many</i><br>
<i>&gt; human-sized beings to deal with, and armies of dog-sized beings, and</i><br>
<i>&gt; trillions of lilliputs...</i><br>
<i>&gt; </i><br>
<p>
Your scenario may be plausible, but I feel that my scenario is more<br>
likely: the<br>
Initial SI (for example an experimenter together with a workstation and<br>
a bunch<br>
of software) is capable of rapid self-augmentation.  Since the<br>
experimenter and<br>
the experiment are likely to be oriented toward developing an SI, the<br>
self-augmentation<br>
is likely to result in rapid intelligence gain. Your sub-human SIs are<br>
presumably computer<br>
only AIs, lacking a human component. I don't see an AI as the likely SI.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0922.html">Dan Clemmensen: "Re: Darwinian Extropy"</a>
<li> <b>Previous message:</b> <a href="0920.html">Max More: "Newsweek on superhormone therapy"</a>
<li> <b>Maybe in reply to:</b> <a href="0848.html">Tim_Robbins@aacte.nche.edu: "Darwinian Extropy"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0922.html">Dan Clemmensen: "Re: Darwinian Extropy"</a>
<!-- reply="end" -->
</ul>
