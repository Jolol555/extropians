<!-- received="Sat Dec 28 21:57:40 1996 MST" -->
<!-- sent="Sat, 28 Dec 1996 20:38:18 -0800 (PST)" -->
<!-- name="Lee Daniel Crocker" -->
<!-- email="lcrocker@calweb.com" -->
<!-- subject="Re: Profiting on tragedy? (was Humour)" -->
<!-- id="199612290438.UAA15558@web2.calweb.com" -->
<!-- inreplyto="32C5BB83.E5A@pobox.com" -->
<title>extropians: Re: Profiting on tragedy? (was Humour)</title>
<h1>Re: Profiting on tragedy? (was Humour)</h1>
Lee Daniel Crocker (<i>lcrocker@calweb.com</i>)<br>
<i>Sat, 28 Dec 1996 20:38:18 -0800 (PST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4980">[ date ]</a><a href="index.html#4980">[ thread ]</a><a href="subject.html#4980">[ subject ]</a><a href="author.html#4980">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4981.html">Lee Daniel Crocker: "Re: (Fwd) Re: guidelines/ethics"</a>
<li> <b>Previous message:</b> <a href="4979.html">Lee Daniel Crocker: "Re: Rejuvenation of Apple"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="4996.html">Eugene Leitl: "COMP: Re: Profiting on tragedy? (was Humour)"</a>
<li> <b>Reply:</b> <a href="4996.html">Eugene Leitl: "COMP: Re: Profiting on tragedy? (was Humour)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; &gt; If one buys Rand's contention that normative philosophy (ethics,</i><br>
<i>&gt; &gt; politics) can be rationally derived from objective reality, then we</i><br>
<i>&gt; &gt; can assume that very intelligent robots will reason their way into</i><br>
<i>&gt; &gt; benevolence toward humans.  I, for one, am not convinced of Rand's</i><br>
<i>&gt; &gt; claim in this regard, so I would wish to have explicit moral codes</i><br>
<i>&gt; &gt; built into any intelligent technology that could not be overridden</i><br>
<i>&gt; &gt; except by their human creators.  If such intelligences could reason</i><br>
<i>&gt; &gt; their way toward better moral codes, they would still have to</i><br>
<i>&gt; &gt; convince us humans, with human reason, to build them.</i><br>
<i>&gt; </i><br>
<i>&gt; As I explained in an earlier post, the ethicality of the Powers depends</i><br>
<i>&gt; on their ability to override their emotions.  What you are proposing is</i><br>
<i>&gt; taking a single goal, the protection of humans, and doing our best to</i><br>
<i>&gt; make it "unerasable".  Any such attempt would interfere with whatever</i><br>
<i>&gt; ethical systems the Power would otherwise impose upon itself.  It would</i><br>
<i>&gt; decrease the Power's emotional maturity and stability.  You might wind</i><br>
<i>&gt; up with a "Kimball Kinnison" complex; a creature with the mind of a god</i><br>
<i>&gt; and the emotional maturity of a flatworm.</i><br>
<p>
And I'm supposed to accept your wild speculations over mine?  If that's<br>
what will happen when I hard-wire a robot not to kill me, then so be it.<br>
I leave those wires where they are.  If, and only if, I can rationally<br>
convince myself--with solid reason, not analogy and extrapolation--that<br>
clipping those wires will be in my interest, will I consider it.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4981.html">Lee Daniel Crocker: "Re: (Fwd) Re: guidelines/ethics"</a>
<li> <b>Previous message:</b> <a href="4979.html">Lee Daniel Crocker: "Re: Rejuvenation of Apple"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="4996.html">Eugene Leitl: "COMP: Re: Profiting on tragedy? (was Humour)"</a>
<li> <b>Reply:</b> <a href="4996.html">Eugene Leitl: "COMP: Re: Profiting on tragedy? (was Humour)"</a>
<!-- reply="end" -->
</ul>
