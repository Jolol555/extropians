<!-- received="Fri Aug 30 21:16:03 1996 MST" -->
<!-- sent="Fri, 30 Aug 1996 23:04:09 -0400" -->
<!-- name="Dan Clemmensen" -->
<!-- email="dgc@shirenet.com" -->
<!-- subject="Re: Thinking about the future..." -->
<!-- id="v02140b0fae4d573af7fc@[205.149.181.213]" -->
<!-- inreplyto="Thinking about the future..." -->
<title>extropians: Re: Thinking about the future...</title>
<h1>Re: Thinking about the future...</h1>
Dan Clemmensen (<i>dgc@shirenet.com</i>)<br>
<i>Fri, 30 Aug 1996 23:04:09 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#613">[ date ]</a><a href="index.html#613">[ thread ]</a><a href="subject.html#613">[ subject ]</a><a href="author.html#613">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0614.html">Damaged Justice: "Re: The Great Squirrel"</a>
<li> <b>Previous message:</b> <a href="0612.html">Sunah Cherwin: "Re: The Great Squirrel"</a>
<li> <b>Maybe in reply to:</b> <a href="0738.html">Eric Watt Forste: "Thinking about the future..."</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0614.html">Damaged Justice: "Re: The Great Squirrel"</a>
<li> <b>Reply:</b> <a href="0614.html">Damaged Justice: "Re: The Great Squirrel"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
QueeneMUSE@aol.com wrote:<br>
<i>&gt; </i><br>
<i>&gt; Anders Sandberg wrote:</i><br>
<i>&gt; </i><br>
<i>&gt;  &gt; &gt; I think it would be unlikely that we create successors</i><br>
<i>&gt;  &gt;that out-compete us, most likely they will inhabit a somewhat different</i><br>
<i>&gt;  &gt;ecological/memetic niche that will overlap with ours; competition a</i><br>
<i>&gt; </i><br>
<i>&gt; [ to which  Max More wrote:]</i><br>
<i>&gt;  &gt;&gt;You make good points, Anders, about humans and nanite-AI's having possibly</i><br>
<i>&gt; &gt; different niches. However, there may be a period during which we're very</i><br>
<i>&gt; &gt; much in the same space. That's the period in which humans could be at risk</i><br>
<i>&gt; &gt; if AI/SIs have no regard for our interests. What I'm thinking is that it's</i><br>
<i>&gt; &gt; possible, even likely, that SI will be developed before really excellent</i><br>
<i>&gt; &gt; robotics. AI's in that case would not be roaming around much physically,</i><br>
<i>&gt; but</i><br>
<i>&gt; &gt; they could exist in distributed form in the same computer networks that we</i><br>
<i>&gt; &gt; use for all kinds of functions crucial to us.</i><br>
<i>&gt;  &gt;&gt;  If they need us for doing things physically, we would still have a</i><br>
<i>&gt; strong</i><br>
<i>&gt; &gt; position. Nevertheless, powerful SI's in the computer networks, could exert</i><br>
<i>&gt; &gt; massive extortionary power, if they were so inclined. So I still think it</i><br>
<i>&gt; &gt; important that SI researchers pay attention to issues of what values and</i><br>
<i>&gt; &gt; motivations are built into SIs &gt;&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; Ah yes,  the programmers ( as well as the programmed AI's)  motivations could</i><br>
<i>&gt; be really useful or highly destructive! A theme for a many well loved  horror</i><br>
<i>&gt; tale, indeed! ..or a solution to much strife on our world.</i><br>
<i>&gt; Re: Values :    I am curious - we talk about the AI's replacing, destroying</i><br>
<i>&gt; or overcoming humans or &gt;H's: Realistically - what would  AI's "needs" be?</i><br>
<i>&gt; Would it have needs? - or more precisely would they precieve the concept of</i><br>
<i>&gt; needs as we do, not being subject to the fight or flight domain we have to</i><br>
<i>&gt; negotiate?We need food,nurturing,clothing,shelter,etc.. What AI conditions</i><br>
<i>&gt; correspond to that? If we (thru mimicry of intelligence as we know it)</i><br>
<i>&gt;  create them as similar to primate intelligence, then (?) reproduction</i><br>
<i>&gt; /expansion-  but if NN intelligences program themselves, how could we predict</i><br>
<i>&gt; what the agenda will be?As Max says here - they could exert massive power. Do</i><br>
<i>&gt; we assume they would inherently take our values and expand or pervert them -</i><br>
<i>&gt; an allegiance to their "creators"? Some how I don't see that, as inviting as</i><br>
<i>&gt; it sounds.</i><br>
<p>
It's likely that, if we can produce an SI, we can produce many SIs.<br>
However, my belief<br>
is that there is really only one relevant SI, and that is an SI whose<br>
motivation is to<br>
become more intelligent. This SI is the important one, because this is<br>
the one that<br>
has a built-in positive feedback mechanism. I also belief that this<br>
motivation is very<br>
likely to be a basic part of the first SI, almost by definition. The<br>
creator(s) of the<br>
first SI are likely to have this motivation themselves Otherwise, why<br>
create an SI? Further<br>
the SI may be a computer-augmented human or some other type of<br>
human-computer collaboration,<br>
in which case the SI is likely to include its creator, who surely has<br>
this motivation.<br>
<i>&gt; </i><br>
<i>&gt; Even if they could "use" us for manual labor- and what would we produce for</i><br>
<i>&gt; them?</i><br>
<p>
More intelligence. We will be useful until the SI has direct control of<br>
manufacturing<br>
and connection of additonal computational capability. The SI will be<br>
able to<br>
"use" us just as we "use" each other: by contracting for services,<br>
either by letter or over the telephone. An SI embedded in the internet<br>
will have no difficulty arranging for<br>
valid credit card numbers, bank accounts, etc. I believe that an SI will<br>
be able to design and build whatever automated tools it needs in just<br>
this way, in a matter<br>
of a few days or weeks. Once the tools are available, it will no longer<br>
need humans to provide<br>
these services. At this point utility is no longer a reason for the SI<br>
to preserve humanity.<br>
However I hope the SI will derive some other reason, using its superior<br>
intelligence. <br>
<i>&gt; </i><br>
<i>&gt; In essence, what would they want to destroy us *for*?  Comparatively</i><br>
<i>&gt; aesthetic messiness?</i><br>
<p>
The SI may not want to destroy humanity. Humanity may simply be unworthy<br>
of<br>
consideration, and get destroyed as a trivial side effect of some<br>
activity of the<br>
SI. A simple example: the SI decides to maximixed its computational<br>
speed by indreasing<br>
its mass and converting the mass into computational units. It does this<br>
by gathering<br>
all the mass in the solar system (mostly the sun's mass) This is likely<br>
to be unhealthy<br>
for the humans. the SI may then decide to increase its computatinal<br>
speed by increasing<br>
its density, and convert all that mass into a neutron star. This is<br>
likely to be more<br>
unhealthy.<br>
<p>
<i>&gt; </i><br>
<i>&gt; [PS Anders your post made me want to draw transhuman wet/dry multi- sided</i><br>
<i>&gt; organelles, but then that gave me Borg images again...]</i><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0614.html">Damaged Justice: "Re: The Great Squirrel"</a>
<li> <b>Previous message:</b> <a href="0612.html">Sunah Cherwin: "Re: The Great Squirrel"</a>
<li> <b>Maybe in reply to:</b> <a href="0738.html">Eric Watt Forste: "Thinking about the future..."</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0614.html">Damaged Justice: "Re: The Great Squirrel"</a>
<li> <b>Reply:</b> <a href="0614.html">Damaged Justice: "Re: The Great Squirrel"</a>
<!-- reply="end" -->
</ul>
