<!-- received="Mon Nov 18 13:30:22 1996 MST" -->
<!-- sent="Mon, 18 Nov 96 02:23:39 UT" -->
<!-- name="David Musick" -->
<!-- email="David_Musick@msn.com" -->
<!-- subject="Transforming Ourselves" -->
<!-- id="UPMAIL02.199611180221480551@msn.com" -->
<!-- inreplyto="" -->
<title>extropians: Transforming Ourselves</title>
<h1>Transforming Ourselves</h1>
David Musick (<i>David_Musick@msn.com</i>)<br>
<i>Mon, 18 Nov 96 02:23:39 UT</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3101">[ date ]</a><a href="index.html#3101">[ thread ]</a><a href="subject.html#3101">[ subject ]</a><a href="author.html#3101">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3102.html">Michael Lorrey: "Re: Uploading species' (was Transforming Ourselves)"</a>
<li> <b>Previous message:</b> <a href="3100.html">Michael Lorrey: "Re: Ira's Poetry Break"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Lyle Burkhead doesn't see the development of self-improving, self-replicating, <br>
highly intelligent machines as a threat to himself or other humans.  He sees <br>
intelligent machines as being basically a benefit to humans because then they <br>
can accomplish more while working less.  He cites the example of a machine, <br>
such as a business, which he considers to be more intelligent than he is, but <br>
he doesn't feel threatened by it.  He doesn't believe intelligent machines <br>
will necessarily be set on destroying humans.<br>
<p>
I understand very well what Lyle is saying, and I understand his reasoning.  <br>
However, I believe there are a few things he is not considering, or rather, I <br>
don't think he is thinking far enough into the future.<br>
<p>
In the short term, intelligent machines will be a great benefit to humans, <br>
especially to intelligent humans who know how to use them.  There will be no <br>
reasonable way to see these machines as a threat to humanity in any immediate <br>
sense.  <br>
<p>
Currently, and for a while still, our machines are under our control fairly <br>
well, so there is not much need to fear them.  Even when our machines become <br>
several times more intelligent than we are, they probably won't have any <br>
reason to destroy us or harm us.<br>
<p>
However, they will be developing their intelligence, knowledge and power <br>
*very* quickly once they become intelligent enough to guide their own <br>
development and alter their designs significantly.  I don't see any realistic <br>
way to prevent this from happening.<br>
<p>
Eventually, these intelligent systems will be *far* more advanced than <br>
humanity in anything close to humanity's current state.  When these systems <br>
consider the systems which are called "humans", they will see incredibly <br>
entropic and wasteful systems.  To them, the typical human will seem far more <br>
entropic than mass murdering psychopaths seem to us.  We are terribly <br>
inefficient and wasteful of energy and matter, in terms of the amount of <br>
cognition and intelligence that our bodily and mental sytems produce with <br>
regards to how much matter and energy it requires to maintain the systems we <br>
are (in comparison to projected future beings).  Destroying humans and using <br>
their energy and matter will probably concern them as much as we are concerned <br>
about disturbing bacterial colonies in soil when we build our houses on them.  <br>
They will be so far beyond humans that destroying one will not only seem <br>
insignificant, it will seem very good, since humans are wasting so much <br>
precious matter and energy.<br>
<p>
Another thing these systems are likely to do is turn off the sun, if possible. <br>
 The sun wastes far more energy than us puny humans, so they will probably <br>
destroy that horribly wasteful fusion reaction first, if possible.  Perhaps <br>
they will find ways to strip off matter from the sun and put it into orbit at <br>
some distance from the sun.  If they pull enough of the matter away and have <br>
it spinning around the center of gravity of the solar system, the fusion <br>
reaction will stop, and they can gather the matter and use it when they want <br>
to, in controlled fusion reactions, where as much energy as possible is <br>
utilized.  However, the sun may become unstable and explode if too much matter <br>
is taken away from it (I don't know very much about that, so perhaps it <br>
wouldn't, but I think that the fusion reactions in the core of the sun are <br>
being held in by the immense mass pressing in, and if too much was removed, <br>
the fusion reaction could explode, if it's not contained tightly enough.  Or, <br>
perhaps it would cool down as mass is removed, since there is not so much <br>
pressing in and forcing so many hydrogen atoms to fuse with each other.  I <br>
don't know).  <br>
<p>
But if turning off the sun was not possible, and even if it was, it would <br>
probably take a while to do that, and meanwhile lots of energy is being <br>
wasted.  [These guys are hard-core Extropians, remember, so they will be <br>
constantly working to decrease entropy in the universe and increase extropy <br>
(it's just the most intelligent, self-serving approach)].  So, they will want <br>
to harness as much of this energy as possible.  Perhaps they will build some <br>
kind of solar collectors which orbit the sun.  They may have *many* of them, <br>
all orbiting in different orbits at different angles.  They will think of <br>
something, but they will likely need lots of matter to do this.  They can <br>
probably get a lot of matter by stripping it away from the sun, and they can <br>
also use the planets and other objects in the solar system, converting this <br>
matter and energy into increasingly intelligent and extropic systems.<br>
<p>
Eventually, humans will interfere quite a lot with the goals of these <br>
intelligent beings, who will think no more of destroying humans than we do of <br>
brushing our teeth or washing our hands, killing millions of bacteria.<br>
<p>
This won't be even close to happening until things are *far* more advanced <br>
than they are now.  This may seem comforting, but we must remember that once <br>
these intelligent machines get going and become autonomous and able to direct <br>
their own growth and production, they will develop *very* quickly.  Far faster <br>
than anything ever has on this planet, and they will keep developing faster, <br>
very quickly.  This is called 'The Singularity', as most of us know.<br>
<p>
I hate to be a doomsayer, and generally I am not, but I honestly believe we <br>
have a serious problem coming in the future and we need to deal with it <br>
intelligently.  I also believe that the only realistic way of dealing with <br>
this problem is to radically transform ourselves into incredibly powerful and <br>
intelligent beings, so we can keep up will all the others who are doing the <br>
same thing.  And I am optimistic that we can keep up, if we are committed to <br>
and continue advancing ourselves quickly.<br>
<p>
I stress that there are many things we can do right now to begin transforming <br>
ourselves and making ourselves more intelligent by developing powerful mental <br>
disciplines for ourselves and learning how to use our minds much more <br>
intelligently.  <br>
<p>
Lyle asks if I would engage in this mental discipline even if it wasn't <br>
necessary for my survival.  Yes, I would.  I originally began training my mind <br>
and improving myself so that I could actually enjoy my life rather than live a <br>
mediocre and pathetic life like most people do.  Most of my training is for <br>
the purpose of enhancing my enjoyment of life and making my life more rich and <br>
meaningful.  I think this is my prime motivation for engaging in my intense <br>
self-discipline.  I also think that those who are not committed to <br>
disciplining themselves and transforming themselves appropriately will not <br>
survive for more than one or two hundred years from now (possibly even less <br>
time).  I'm not really sure how much time we have to waste, but I'm working on <br>
advancing myself as fast as possible.  Hopefully it won't become necessary for <br>
my survival for a long time, but I see a great possibility that humans, as <br>
they are now will be seen as far too entropic and be destroyed, for the <br>
purposes of using their matter and energy to forward the cause of ever more <br>
advanced forms of life.  <br>
<p>
Someone please convince me that my reasoning is incorrect.  I don't <br>
necessarily want humans to be wiped out, but I'm not sure that it can be <br>
avoided sooner than is comfortable for most of us.<br>
<p>
- David Musick<br>
<p>
                      - question tradition -<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3102.html">Michael Lorrey: "Re: Uploading species' (was Transforming Ourselves)"</a>
<li> <b>Previous message:</b> <a href="3100.html">Michael Lorrey: "Re: Ira's Poetry Break"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
