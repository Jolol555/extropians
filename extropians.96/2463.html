<!-- received="Sat Nov  2 18:51:10 1996 MST" -->
<!-- sent="Sun, 03 Nov 1996 12:39:20 +1000" -->
<!-- name="Damien Broderick" -->
<!-- email="damien@ariel.ucs.unimelb.edu.au" -->
<!-- subject="Re: Uploading, info theory, and threads of consciousness" -->
<!-- id="UPMAIL02.199611030120220883@msn.com" -->
<!-- inreplyto="Uploading, info theory, and threads of consciousness" -->
<title>extropians: Re: Uploading, info theory, and threads of consciousness</title>
<h1>Re: Uploading, info theory, and threads of consciousness</h1>
Damien Broderick (<i>damien@ariel.ucs.unimelb.edu.au</i>)<br>
<i>Sun, 03 Nov 1996 12:39:20 +1000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2463">[ date ]</a><a href="index.html#2463">[ thread ]</a><a href="subject.html#2463">[ subject ]</a><a href="author.html#2463">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2464.html">David D.: "Re: Extropian Holidays"</a>
<li> <b>Previous message:</b> <a href="2462.html">David Musick: "We Are All Self-Employed"</a>
<li> <b>Maybe in reply to:</b> <a href="2357.html">jamesr@best.com: "Uploading, info theory, and threads of consciousness"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2465.html">Michael Lorrey: "Re: Uploading, info theory, and threads of consciousness"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; jamesr@best.com (jamesr@best.com) writes:</i><br>
<p>
<i>&gt;&gt;Yes, but I think that most people here want to *experience* the </i><br>
<i>&gt;&gt;upload, not generate a bunch of clones who *think* they were </i><br>
<i>&gt;&gt;uploaded.  In a destructive upload, our stream of consciousness would </i><br>
<i>&gt;&gt;cease to exist.  [snips]</i><br>
<p>
<i>&gt;&gt;   I don't think </i><br>
<i>&gt;&gt;too many people would terminate their own stream of consciousness to </i><br>
<i>&gt;&gt;create new independant ones.  What would be the advantage?</i><br>
<p>
Peter McCluskey replies&gt;<br>
<p>
<i>&gt; Greater wealth through the ability to work at faster clock speeds.</i><br>
<i>&gt;More security through the ability to make distributed backups.</i><br>
<p>
I still find this kind of retort utterly baffling, even though it's<br>
undeniable that one goes happily into sleep (or perhaps less happily into<br>
medical unconsciousness) expecting that the `reconstituted' self that later<br>
wakes is continuous with the present person.  And of course the only reason<br>
for having one's head frozen at death is the conviction that a revived or<br>
uploaded brain will be just as much `me' as `I' am after a snooze.  But<br>
destructive emulation--<br>
<p>
Why should I care about *his* `greater wealth'?  *Their* `security'?<br>
<p>
Stipulate that evolutionary sieves have winnowed our genomes in favor of<br>
phenotypes whose economic behavior is chanelled by the need to sustain (and<br>
ideally increase) the components of that genotype.  On the standard<br>
Fisher/Hamilton argument, then, we will tend to be `altruistic' toward other<br>
bearers of  large chunk of the same genotype, because in some sense They R<br>
Us.  Because evolution is mindless, this allows high-level adapted<br>
structures such as brain and cultures to make `mistakes' of identification<br>
and sacrifice individuals in support of the `wrong' genotypes.  Still,<br>
evolution would be tickled pink if I sacrificed my current phenotype in<br>
order to produce a dozen copies of my exact genome, with or without memetic<br>
and individual memories.  *I*, however, might not be so ardent.<br>
<p>
Suppose I could arrange for a dozen exact copies of myself by cloning - a<br>
time-lapsed set of identical -tuples.  My genes would rejoice, but if I<br>
could only achieve this by giving up my individual life I don't think I'd be<br>
overjoyed.  Suppose, however, that these copies could also contain my exact<br>
memories to this moment, so that I had spawned a dozen copies of myself<br>
(which would, it's agreed, immediately split off from each other in terms of<br>
experience, random wiring events, `identity' in short).  Would this make me<br>
more inclined to die in order to achieve this goal?  Not me, bud.  Why<br>
should I care about *their* greater wealth, etc?  <br>
<p>
But suppose I were offered a `safe' neurological suspension attainable only<br>
if I were killed in the process (it having be found that waiting for death<br>
by senility caused the loss of too many brain cells, or something dire).<br>
Would I go gently and immediately into that good night?  Gosh... a<br>
*deathist* proposition?  These are difficult issues, the answers not at all<br>
self-evident.<br>
<p>
Damien Broderick<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2464.html">David D.: "Re: Extropian Holidays"</a>
<li> <b>Previous message:</b> <a href="2462.html">David Musick: "We Are All Self-Employed"</a>
<li> <b>Maybe in reply to:</b> <a href="2357.html">jamesr@best.com: "Uploading, info theory, and threads of consciousness"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2465.html">Michael Lorrey: "Re: Uploading, info theory, and threads of consciousness"</a>
<!-- reply="end" -->
</ul>
