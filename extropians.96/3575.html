<!-- received="Mon Dec  2 18:31:44 1996 MST" -->
<!-- sent="Mon, 02 Dec 1996 19:32:44 -0600" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Singularity-worship" -->
<!-- id="199612030123.UAA19425@smtp1.erols.com" -->
<!-- inreplyto="Singularity-worship" -->
<title>extropians: Re: Singularity-worship</title>
<h1>Re: Singularity-worship</h1>
Eliezer Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Mon, 02 Dec 1996 19:32:44 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3575">[ date ]</a><a href="index.html#3575">[ thread ]</a><a href="subject.html#3575">[ subject ]</a><a href="author.html#3575">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3576.html">Chris Hind: "Re:  ALL LISTMEMBERS READ THIS NOW!"</a>
<li> <b>Previous message:</b> <a href="3574.html">Eliezer Yudkowsky: "Re: "zero-point energy""</a>
<li> <b>Maybe in reply to:</b> <a href="4069.html">John K Clark: "Singularity-worship"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="3582.html">E. Shaun Russell: "Re: Singularity-worship"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; There is no heaven, there will be no Singularity. We can create a wonderful</i><br>
<i>&gt; posthuman future where we will be unshackled from many human limits, but it</i><br>
<i>&gt; will take critical thinking, creativity, and hard work.</i><br>
<p>
Don't accuse *me* of lying idle.  I put up the "Algernon's Law" page, "A<br>
practical guide to intelligence enhancement using current technology." <br>
You don't think that counts as "critical thinking, creativity, and hard<br>
work"?  I should think that a practical, proven method of IA counts as a<br>
major landmark.<br>
<p>
Maybe you guys think that the Singularity will solve all problems so we<br>
don't have to do anything, but that says more about you than it says<br>
about me.  I believe that certain classes of problems are<br>
"pre-Singularity" problems, including the nature of consciousness,<br>
intelligence amplification, nanotechnology, artificial intelligence, and<br>
so on.  Some of these problems I have chosen to claim as my personal<br>
responsibility, and as you can see, I have already cracked IA<br>
(Algernon's Law) and am working on a second.  Other problems are<br>
"post-Singularity":  Whether the Singularity is a good thing, whether<br>
the Powers will be ethical, whether corpsicles can be revived, and so<br>
on.  These problems need solutions only insofar as it impacts what we do<br>
now.  All we need to know for the problems above are "Almost certainly",<br>
"of course", and "consider the alternative."<br>
<p>
<i>&gt; &gt;&gt; Zero.  With the tech it takes to revive a terminal frostbite victim,</i><br>
<i>&gt; &gt;&gt; it's as easy to grow the body back as to repair it.  Besides, this is</i><br>
<i>&gt; &gt;&gt; all going to be post-Singularity and all costs at that point are</i><br>
<i>&gt; &gt;&gt; effectively zero.</i><br>
That wasn't Singularity-worship, is was an attempt to quash an<br>
unnecessary thread.  Like it or not, some bridges aren't going to be<br>
crossed with the mental powers we have now.  Reviving cryonics victims<br>
is a job for the Powers (or even merely smarter humans), not for us.<br>
<p>
Our job - or more specifically, my job, I, Eliezer "Algernon" Yudkowsky<br>
- is to concentrate on short-term, practical things like prospects for<br>
current IA. To be blunt, my good Lyle and More, what have *you* done<br>
lately?<br>
<p>
The only reason I'm on this list and putting up Web pages and generally<br>
popularizing my work instead of doing it is that I'm recovering from a<br>
dose of Prozac, which - as I discovered - switches off my creative<br>
abilities like a bloody light.  So I decided to make the best of it, and<br>
so I have:  I've put up a Singularity page and an Algernon page and am<br>
now present on the net.  When my abilities come back, as they seem to be<br>
doing, I'll probably lurk instead of posting, if I elect to spend the<br>
time wading through the mailing list at all.<br>
<p>
What's your excuse?<br>
<p>
<i>&gt; The Singularity concept has all the earmarks of an idea that can lead to</i><br>
<i>&gt; cultishness, and passivity.</i><br>
<p>
I experienced the incomprehensibility of the Beyond in a very personal<br>
way, from both sides, both in writing some unpublished work and in<br>
reading Godel, Escher, Bach.  I know damn well I ain't ever gonna match<br>
Hofstadter and that it's futile for me even to try.  I'm still<br>
discovering little (and not-so-little) gems I overlooked, years later. <br>
I can appreciate his work, but I have trouble even noticing it because<br>
key cognitive modules are crippled.  There are also some things that<br>
shouldn't be done by an ordinary human unless absolutely necessary. <br>
Designing software architectures comes to mind - if the Java VM had had<br>
Code and Method/Field primitive types, and stacks for each type, it<br>
wouldn't need a bytecode verifier, would run faster, and we could write<br>
self-modifying Java code and pass procedure references.  Seems obvious<br>
to me...<br>
<p>
So the point is:<br>
1)  I know - from Hofstadter - that the work of a mere mortal who<br>
happens to be smarter than you are is almost impossible to comprehend.<br>
2)  I know - from writing computer architectures - that being smarter in<br>
certain areas causes progress there to proceed at a vastly higher pace.<br>
3)  I know - from writing "Algernon's Law" - that this applies as well<br>
to the field of intelligence enhancement.<br>
<p>
It follows that there will be a Singularity.  I believe that the<br>
Godling's Glossary defines transhuman as "A human staring into a<br>
magnifying glass."  This doesn't give a very good flavor of the future,<br>
particularly if you're not used to dealing with people smarter than you<br>
are (such as full humans) and dumber than you are (such as ordinary<br>
mortals).  As someone with actual experience of the phenomena involved<br>
with intelligence enhancement - on both sides, as an Algernon - I am<br>
telling you that your conceptions of a merely transhuman future, derived<br>
from staring into a magnifying glass, are far short of the fact.  I know<br>
in excruciating detail the sheer blank impossibility of imagining<br>
someone smarter than you are and it is this - not any greater<br>
intelligence - that lends to my magnifying glass a greater<br>
verisimilitude, for what I magnify is not my own abilities but the<br>
experienced comparision between H and &gt;H, or between &lt;H and H.<br>
<p>
It is from that, and not from the tradition of the Apocalypse, that I<br>
derive my views on the Singularity.  I wouldn't be pulling rank on you<br>
except that 1) I expect to be retiring from this list shortly, and won't<br>
have to face the music and 2) I am sick and tired of supposed Extropians<br>
and sf fans with no appreciation for the Beyond and the alien and the<br>
incomprehensible.<br>
<p>
<i>&gt; &gt;OK, it is time for my bi-monthly reaction: I HATE THIS SILLY MEME!</i><br>
I hate any meme that applies to events later than 2010 and am attempting<br>
to point out the sheer futility - silliness, if you like - of discussing<br>
it.<br>
<p>
Just learn to live with it, guys.  There's no use in discussing<br>
anything, anything at all, that takes place in a world where there are<br>
transhumans.  They'll solve it in seconds.  I KNOW.  I was on Prozac<br>
when I wrote a program that would simulate Life on my computer.  It took<br>
me days and kept crashing.  Around a week ago, I pulled it out again and<br>
it took me maybe fifteen minutes to take out all the bugs, simplify the<br>
program so the rules could be modified in seconds, and double the<br>
speed.  Now imagine that magnified a few hundred times and you'll<br>
understand how futile it is to be concerned about the problems of<br>
transhumanity.<br>
<p>
The things to concentrate on are practical intelligence enhancements,<br>
nanotechnology, faster computers, and artificial intelligence.  It's in<br>
that order because I frankly don't think an unenhanced human is going to<br>
be able to design anything as complex as an AI architecture, faster<br>
computers don't do any good if we can't program them, nanotechnology<br>
will turn the Earth into gray goo and IA via Algernons will have a<br>
two-year turnaround time if we do it right and give us the ability to<br>
handle all the other stuff.<br>
<p>
Okay, I've faced the Singularity backlash.  I can understand why it<br>
looks like religious ecstasy to the rest of you.  You've never been<br>
faced with anything smarter than you were so you cannot possibly<br>
appreciate what it is like.  Certainly the ravings against<br>
"impracticality" demonstrates that many managed to miss the most<br>
important aspect of the Singularity, that we don't need to think about<br>
what happens afterward, just how to get there.  The business about<br>
sitting back and waiting also seems to demonstrate a communications<br>
breakdown.  I'm saying that we should make transhumans ASAP and they'll<br>
handle everything else - how do we get from there to inactivity?<br>
<p>
Enough talk.  As far as I can tell my abilities have more or less<br>
returned, so I need to work on one of the other prerequisites to the<br>
Singularity.  This message seems to be a fairly appropriate signoff.  I<br>
have work to do and I've spent enough time composing this message.<br>
<p>
I suppose I've gained something from discussing things with this<br>
list - the directions to pages on neural plasticity and the list of<br>
advertising rules seem to be the two big ones - but it would have been a<br>
lot more fun if you guys were discussing the practical cognitive<br>
pitfalls of IA, or making technical suggestions for work in AI, instead<br>
of cryo-revival costs.<br>
<p>
I shall continue to lurk, at least for a while, and may even post a<br>
couple of replies - particularly to discussion of this message - since<br>
it isn't fair to grab the last word without giving your opponents a<br>
chance to fire back.<br>
<p>
In conclusion, remember that I might not be right, but if I say you're<br>
wrong, you're almost certainly wrong.  I happen to know for a FACT that<br>
every conception of transhumans I've run across has shared the causal<br>
blind spot that all full humans are born with.  Every time I read<br>
Vinge's conception of smarter-than-human I feel like I'm staring right<br>
through the page directly into Vinge's mind; the same holds true for<br>
Flowers's Charlie or B5's Shadows and Vorlons.  If our technology holds<br>
out, there's going to be a Singularity.  Every argument you can muster<br>
against it is derived from your merely human mind; every quibble is<br>
extrapolation from the simply mortal.<br>
<p>
I've looked into the Beyond; I look at it in a magnifying glass; I see<br>
Singularity.  I may be as wrong as you are.  But the Singularity will be<br>
*at least* what I see, just as it will be *at least* as powerful as your<br>
conception of it.  I may be as wrong as you are - but if so, I was too<br>
conservative.<br>
<p>
Eliezer S. Yudkowsky.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3576.html">Chris Hind: "Re:  ALL LISTMEMBERS READ THIS NOW!"</a>
<li> <b>Previous message:</b> <a href="3574.html">Eliezer Yudkowsky: "Re: "zero-point energy""</a>
<li> <b>Maybe in reply to:</b> <a href="4069.html">John K Clark: "Singularity-worship"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="3582.html">E. Shaun Russell: "Re: Singularity-worship"</a>
<!-- reply="end" -->
</ul>
