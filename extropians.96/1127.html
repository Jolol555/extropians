<!-- received="Mon Sep 23 19:55:05 1996 MST" -->
<!-- sent="Mon, 23 Sep 1996 21:46:32 -0400" -->
<!-- name="Dan Clemmensen" -->
<!-- email="dgc@shirenet.com" -->
<!-- subject="Re: Darwinian Extropy" -->
<!-- id="199609240130.VAA27467@smtp2.erols.com" -->
<!-- inreplyto="Darwinian Extropy" -->
<title>extropians: Re: Darwinian Extropy</title>
<h1>Re: Darwinian Extropy</h1>
Dan Clemmensen (<i>dgc@shirenet.com</i>)<br>
<i>Mon, 23 Sep 1996 21:46:32 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1127">[ date ]</a><a href="index.html#1127">[ thread ]</a><a href="subject.html#1127">[ subject ]</a><a href="author.html#1127">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1128.html">Robin Hanson: "The Great Filter"</a>
<li> <b>Previous message:</b> <a href="1126.html">Ian Goddard: "RE: Are Conspiracies Stronger Than Truth?"</a>
<li> <b>Maybe in reply to:</b> <a href="0848.html">Tim_Robbins@aacte.nche.edu: "Darwinian Extropy"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Robin Hanson wrote:<br>
<i>&gt; </i><br>
<i>&gt; Dan Clemmensen writes:</i><br>
<p>
<i>&gt; </i><br>
<i>&gt; &gt;Basically, I don't believe that we understand the basics of human</i><br>
<i>&gt; &gt;cognition.Therefore our attempts at self-augmentation have no firm</i><br>
<i>&gt; &gt;basis.  We do, however, understand the basics of machine computation:</i><br>
<i>&gt; &gt;we can design and build more powerful computer hardware and software.</i><br>
<i>&gt; &gt;Since we understand this basis already, I believe that an SI can also</i><br>
<i>&gt; &gt;understand it.  I believe that an SI with a computer component will be</i><br>
<i>&gt; &gt;able to design and build ever more powerful hardware and software,</i><br>
<i>&gt; &gt;thus increasing its own capabilities. I think that this is likely to</i><br>
<i>&gt; &gt;lead not just to an improvement, but to a rapid feedback process.</i><br>
<i>&gt; </i><br>
<i>&gt; Consider an analogy with the world economy.  We understand the basics</i><br>
<i>&gt; of this, and we can change it for the better, but this doesn't imply</i><br>
<i>&gt; an explosive improvement.  Good changes are hard to find, and each one</i><br>
<i>&gt; usually makes only a minor improvement.  It seems that, in contrast,</i><br>
<i>&gt; you imagine that there are a long series of relatively easy to find</i><br>
<i>&gt; "big wins".    If it turns out that our minds are rather badly</i><br>
<i>&gt; designed, you may be right.  But our minds may be better designed than</i><br>
<i>&gt; you think.</i><br>
<i>&gt; </i><br>
<p>
Now we're getting somewhere. I really feel that your analogy is<br>
inappropriate. Our understanding of computer hardware and software<br>
is considerably more complete than is our understanding of the world<br>
economy, and we have demonstrated the ability to continue to increase<br>
the capabilities of both hardware and software enormously since the<br>
development of the computer. Furthermore, computers are already being<br>
used<br>
to assist in the further development of computing. Good changes are not<br>
hard to find, and 18 month's worth of development results in a doubling<br>
of capability. Yes, I do "imagine" that there are a long series of "big<br>
wins". I base this on the recent 30-year trend. Yes I'm very aware that<br>
Moore's law is purely empirical and that there are arguments that the<br>
rate<br>
must slow down for various physical reasons. I'm also aware that similar<br>
arguments have been advanced every year since the promulgation of<br>
Moore's<br>
law, but the rate hasn't slowed. All of this has occured before the<br>
advent<br>
of an intelligence with a computer component.<br>
<p>
I'm not arguing that our human minds are poorly designed. First, I<br>
believe<br>
that the human mind is evolved, not designed. For complex systems that<br>
I'm<br>
aware of (computer software sytems, mostly), an evolved system can<br>
generally<br>
be replaced by a designed system that uses the behaviour of the evolved<br>
system as a functional specification. This will generally result in<br>
dramatic<br>
performance improvement. By analogy, it would be possible to design an<br>
improved human brain, IF we had a complete understanding of how the<br>
brain<br>
works. Since we don't, we can't design a new one on the same principles.<br>
I argue that we don't have to. Instead, we will develop an intelligent<br>
entity that has a computer as one component. This entity will be smart<br>
enough<br>
to develop new hardware and software to augment itself. This the<br>
fundamentally<br>
new factor.<br>
<p>
We've actually already developed the primitive precursors of this<br>
entity.<br>
a computor development organization that uses its own computers to<br>
develop<br>
the next generation computer, or a software tools shop that uses its own<br>
tools to develop its next-generation tools, is such an entity. However,<br>
these primitive examples are not yet focused on self-augmentation, and<br>
are not tightly-integrated enough to precipitate a runaway fast-feedback<br>
of self-augmentation.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1128.html">Robin Hanson: "The Great Filter"</a>
<li> <b>Previous message:</b> <a href="1126.html">Ian Goddard: "RE: Are Conspiracies Stronger Than Truth?"</a>
<li> <b>Maybe in reply to:</b> <a href="0848.html">Tim_Robbins@aacte.nche.edu: "Darwinian Extropy"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
