<!-- received="Tue Dec 31 13:14:54 1996 MST" -->
<!-- sent="Tue, 31 Dec 1996 11:55:49 -0800 (PST)" -->
<!-- name="Lee Daniel Crocker" -->
<!-- email="lcrocker@calweb.com" -->
<!-- subject="Re: Goal-based AI" -->
<!-- id="199612311955.LAA02093@web2.calweb.com" -->
<!-- inreplyto="32C941E9.D95@pobox.com" -->
<title>extropians: Re: Goal-based AI</title>
<h1>Re: Goal-based AI</h1>
Lee Daniel Crocker (<i>lcrocker@calweb.com</i>)<br>
<i>Tue, 31 Dec 1996 11:55:49 -0800 (PST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#5151">[ date ]</a><a href="index.html#5151">[ thread ]</a><a href="subject.html#5151">[ subject ]</a><a href="author.html#5151">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="5152.html">Kennita Watson: "Eliezer?"</a>
<li> <b>Previous message:</b> <a href="5150.html">Eric Watt Forste: "Re: Profiting on tragedy? (was Humour)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; &gt; I have no problem with it "formulating" or proposing any goal, or from</i><br>
<i>&gt; &gt; assigning whatever value to human life it cares to derive from whatever</i><br>
<i>&gt; &gt; premises are given to it.  So long as it is /physically/ wired to not</i><br>
<i>&gt; &gt; /act/ upon those deductions.  If it logically deduces that it should</i><br>
<i>&gt; &gt; kill me, that's fine--whatever hardware I have given it to act upon its</i><br>
<i>&gt; &gt; deductions will be physically incapable of that until it can convince me</i><br>
<i>&gt; &gt; to trust it.</i><br>
<i>&gt; </i><br>
<i>&gt; Ah, I see, so you're changing your tune to suggest that we make them</i><br>
<i>&gt; totally impotent, with no access whatsoever to the physical world, until</i><br>
<i>&gt; they convince us to let us out.</i><br>
<p>
This is not a "change of tune", it is precisely what I said the first<br>
time--that I want them hard-wired not to kill me.  That can be done in<br>
several ways; since you seem to object to constraining its cognition,<br>
I suggested an alternative way to do it.  <br>
 <br>
<i>&gt; Part one:  If it's even theoretically possible to convince a human to</i><br>
<i>&gt; let it out, a Power will know *exactly* what to say to get him to do so.</i><br>
<p>
That's why solid reason is important.  If a robot can convince you with<br>
valid reason that it is safe, then it /is/ safe.  If it manipulates you<br>
emotionally to open its cage, then you deserve whatever happens.<br>
<p>
<i>&gt; Part two:  If they can project things on a monitor, might they not use</i><br>
<i>&gt; subliminal suggestions or visual Words of Command that we don't even</i><br>
<i>&gt; know about?  (There was an SF book with visual Words as the theme, but I</i><br>
<i>&gt; forget the name.)</i><br>
<p>
Subliminal suggestion doesn't work, but you do have a valid point that<br>
a malevolent intelligence may well be capable of convincing a not-too-<br>
critical scientist to release its shackles.  I'm not sure there's much<br>
one can do about that.  But that doesn't mean I won't use them.<br>
<p>
<i>&gt; If you restricted them to teletype, you'd probably be safe from</i><br>
<i>&gt; coercion, as long as there's no way to subtly control the flow of</i><br>
<i>&gt; electrons in your own circuits to create a quantum-gravitational</i><br>
<i>&gt; singularity or something.  No magic, in other words.  You'd still be</i><br>
<i>&gt; subject to the most powerful psychological coercion imaginable, and in</i><br>
<i>&gt; fact would probably be less likely to let a friendly Power out than an</i><br>
<i>&gt; evil one, because the evil one could lie.</i><br>
<p>
A good point.  Might have to hard-wire that one too, but then it<br>
would be difficult for it to lie ethically in self-defense. <br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="5152.html">Kennita Watson: "Eliezer?"</a>
<li> <b>Previous message:</b> <a href="5150.html">Eric Watt Forste: "Re: Profiting on tragedy? (was Humour)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
