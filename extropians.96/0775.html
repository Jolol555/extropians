<!-- received="Wed Sep  4 13:26:02 1996 MST" -->
<!-- sent="Wed, 4 Sep 1996 15:25:50 -0400" -->
<!-- name="QueeneMUSE@aol.com" -->
<!-- email="QueeneMUSE@aol.com" -->
<!-- subject="Re: Thinking about the future..." -->
<!-- id="199609041841.LAA12199@off.ugcs.caltech.edu" -->
<!-- inreplyto="Thinking about the future..." -->
<title>extropians: Re: Thinking about the future...</title>
<h1>Re: Thinking about the future...</h1>
<i>QueeneMUSE@aol.com</i><br>
<i>Wed, 4 Sep 1996 15:25:50 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#775">[ date ]</a><a href="index.html#775">[ thread ]</a><a href="subject.html#775">[ subject ]</a><a href="author.html#775">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0776.html">David Musick: "Extropianism as a World Meme"</a>
<li> <b>Previous message:</b> <a href="0774.html">QueeneMUSE@aol.com: "Re: The Poor Masses"</a>
<li> <b>Maybe in reply to:</b> <a href="0738.html">Eric Watt Forste: "Thinking about the future..."</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0790.html">Dan Clemmensen: "Re: Thinking about the future..."</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
In a message dated 96-09-04 12:11:19 EDT, Nicolas writes:<br>
<p>
&lt;&lt;     <br>
           It is also worth considering what would make the grandpa &gt;AI <br>
           bad in the first place:<br>
           <br>
           1) Accident, misprogramming.<br>
           <br>
           2) Constructed by a bad group of humans, for military or <br>
           commercial purposes. This group is presumably very powerful <br>
           if they are the first to build an &gt;AI. The success of this <br>
           enterprise will make them even more powerful. Thus the <br>
           values present in the group (community, company, state, <br>
           culture) that makes the first &gt;AI will not unlikely be the <br>
           value set which is programmed into subsequent &gt;AIs as well.<br>
           <br>
           3) Moral convergence. Sufficiently intelligent beings might <br>
           tend to converge in what they value, possibly because values <br>
           are in some relevant sense "objective". They just see the <br>
           truth that to annihilate humans is for the best. (In this <br>
           case we should perhaps let it happen?)<br>
<i>            &gt;&gt;</i><br>
Yes, I agree, an Ai made by army or evil doers could cause much evil.But<br>
again we have these  possible scenarios  which, except for the first half of<br>
first one (accidental ) would be programmed in to the machine by a  human. Or<br>
which  AI would develope on its own proto-human attributes ie: predatory<br>
nature; morally corrupt, competative. I think it is logical that if we make<br>
the machine purposely bad, it will be bad - but if we allow it to develope on<br>
its own, it might not choose that option as a part of it's survival.<br>
    I am going to assert here that the AI not being part of the food chain<br>
makes it less likely that intelligence will see things the same as the<br>
primate (lacking the lizard brain) and that most of these scenarios<br>
(especially the last)  still make the contention of 'bad granpa AI'  is still<br>
veiwing  the world from a primate viewpoint (anthropomorphic) - ie:<br>
survivalist in nature, overcoming weaker species in order to survive. The<br>
developement of these traits in animal intelligences ( including alpha<br>
primates) seems evolutionarily based on eating... farming, hunting,<br>
conquering or the like.  And mortality - a fear of... mean, "bad" emotions<br>
that these instincts evoke.<br>
<p>
Since AIs wouldnt be subject to these conditions, why would it develope these<br>
logics?<br>
<p>
N/QM/RSC<br>
                   "Death is not an option"<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0776.html">David Musick: "Extropianism as a World Meme"</a>
<li> <b>Previous message:</b> <a href="0774.html">QueeneMUSE@aol.com: "Re: The Poor Masses"</a>
<li> <b>Maybe in reply to:</b> <a href="0738.html">Eric Watt Forste: "Thinking about the future..."</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0790.html">Dan Clemmensen: "Re: Thinking about the future..."</a>
<!-- reply="end" -->
</ul>
