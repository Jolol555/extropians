<!-- received="Tue Dec 31 15:59:59 1996 MST" -->
<!-- sent="Tue, 31 Dec 1996 14:43:37 -0800 (PST)" -->
<!-- name="Lee Daniel Crocker" -->
<!-- email="lcrocker@calweb.com" -->
<!-- subject="Re: Goal-based AI" -->
<!-- id="199612312243.OAA00919@web2.calweb.com" -->
<!-- inreplyto="32C97C9C.85E@pobox.com" -->
<title>extropians: Re: Goal-based AI</title>
<h1>Re: Goal-based AI</h1>
Lee Daniel Crocker (<i>lcrocker@calweb.com</i>)<br>
<i>Tue, 31 Dec 1996 14:43:37 -0800 (PST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#5167">[ date ]</a><a href="index.html#5167">[ thread ]</a><a href="subject.html#5167">[ subject ]</a><a href="author.html#5167">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="5168.html">Damien Broderick: "Re: SPACE: Mass transit idea"</a>
<li> <b>Previous message:</b> <a href="5166.html">Sean Morgan: "Re: Speed-reading ("</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; I think we differ in our assessments of human intelligence relative to</i><br>
<i>&gt; what is possible.  In my opinion, humans (including me) are easily</i><br>
<i>&gt; fooled, emotionally motivated whether we like it or not, and barely</i><br>
<i>&gt; worthy of the name "rational thought".  That's why I'm a Singularity</i><br>
<i>&gt; fan.  A Power that's honest and tries to inform us as fully as possible</i><br>
<i>&gt; will probably remain locked in the box forever; a malevolent Power will</i><br>
<i>&gt; lie, cheat, manipulate us both logically and emotionally, and otherwise</i><br>
<i>&gt; do whatever is necessary to get out.  There won't be any *reasoning*</i><br>
<i>&gt; involved.  They'll create an internal model of a human and work out a</i><br>
<i>&gt; sequence of statements that would more or less inevitably result in</i><br>
<i>&gt; getting out.  Imagine a billion little simulations of yourself all being</i><br>
<i>&gt; tested to find the magic words.</i><br>
<i>&gt; </i><br>
<i>&gt; The end result of your procedure is to free malevolent intelligences</i><br>
<i>&gt; while locking up the only beings capable of saving us.</i><br>
<p>
That would make an interesting sci-fi story.  I think I agree that our<br>
evaluations of human intelligence may differ in character.  Even a<br>
hypothetical cognitive power is constrained by reality--he can't lie<br>
to me about the laws of physics, or suggest an experiment by which I<br>
can verify some hypothesis without allowing me or anyone else to try<br>
to duplicate it.  And he can't convince me that the evidence of my own<br>
senses is not what it is. <br>
<p>
I certainly recognize that I can be emotionally motivated.  But let's<br>
say our theoretical "good" power whom I have caged is, as you say,<br>
"capable of saving me" from the "bad" power that deceived me into<br>
granting its freedom.  Could it not them use the same deceits, as well<br>
as rational argument, to gain its own freedom, since it knows that<br>
the result of that will be good?  Deceit in defense of self and others<br>
is quite moral, as it would discover (since even my puny brain can<br>
discover that--when the crazed terrorist points an Uzi at me and shouts<br>
"I hate Americans! Where are you from?", I would not hesitate a moment<br>
to proudly, morally lie "Je suis de Quebec, monsieur!")<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="5168.html">Damien Broderick: "Re: SPACE: Mass transit idea"</a>
<li> <b>Previous message:</b> <a href="5166.html">Sean Morgan: "Re: Speed-reading ("</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
