<!-- received="Mon Aug 26 06:33:20 1996 MST" -->
<!-- sent="Mon, 26 Aug 1996 14:32:02 +0200 (MET DST)" -->
<!-- name="Anders Sandberg" -->
<!-- email="nv91-asa@nada.kth.se" -->
<!-- subject="Re: Controlling AI (was: Thinking about the future...)" -->
<!-- id="1.5.4.32.19960826075549.006e7cb0@pop.dial.pipex.com" -->
<!-- inreplyto="2.2.16.19960825085303.316761a2@popd.ix.netcom.com" -->
<title>extropians: Re: Controlling AI (was: Thinking about the future...)</title>
<h1>Re: Controlling AI (was: Thinking about the future...)</h1>
Anders Sandberg (<i>nv91-asa@nada.kth.se</i>)<br>
<i>Mon, 26 Aug 1996 14:32:02 +0200 (MET DST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#501">[ date ]</a><a href="index.html#501">[ thread ]</a><a href="subject.html#501">[ subject ]</a><a href="author.html#501">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0502.html">Eugene Leitl: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Previous message:</b> <a href="0500.html">Anders Sandberg: "Re: Thinking about the future..."</a>
<li> <b>In reply to:</b> <a href="0491.html">Peter Voss: "Controlling AI (was: Thinking about the future...)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0506.html">Max More: "Re: Controlling AI (was: Thinking about the future...)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Sun, 25 Aug 1996, Peter Voss wrote:<br>
<p>
<i>&gt; I'll die trying! That's why I think it's crucial that we have major</i><br>
<i>&gt; breakthroughs in philosophy, ethics and psychology before AI outsmarts us</i><br>
<i>&gt; totally. If we can figure out what the purpose of lives is, how we determine</i><br>
<i>&gt; values and how to motivate ourselves in a way that will achieve our goals,</i><br>
<i>&gt; then we have a chance of developing AI that shares our purposes. It seems</i><br>
<i>&gt; that AI and AL (life) will also have some sort of basic pain/pleasure</i><br>
<i>&gt; motivator and some preassigned goals.</i><br>
<p>
It is very hard to give AI preassigned goals; if it is flexible enough or<br>
evolves, it will probably change their goals. But if there are some<br>
evolutionary stable goals/purposes of life, then it is likely that<br>
autoevolving systems will also move towards them - unless the rules for<br>
evolution in memetic evolution have a cruicial difference from the rules<br>
of evolution in genetic evolution (which has created us and most of our<br>
minds). <br>
<p>
<i>&gt; Another strategy is to develop AI firstly as an extension to our own</i><br>
<i>&gt; minds, to give us extra knowledge, IQ and creativity before AI gets too</i><br>
<i>&gt; autonomous.</i><br>
<p>
This is ideal, and probably the best way to become cruicial to the super <br>
AIs if they ever develop. If AI starts out as extensions of our minds <br>
instead of separate systems, they will become interdependent and we will <br>
essentially hitch a ride into a posthuman world as an old but important <br>
subsystem (a bit like the brainstem - try surviving without one!). <br>
<p>
-----------------------------------------------------------------------<br>
Anders Sandberg                                      Towards Ascension!<br>
nv91-asa@nada.kth.se         <a href="http://www.nada.kth.se/~nv91-asa/main.html">http://www.nada.kth.se/~nv91-asa/main.html</a><br>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0502.html">Eugene Leitl: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Previous message:</b> <a href="0500.html">Anders Sandberg: "Re: Thinking about the future..."</a>
<li> <b>In reply to:</b> <a href="0491.html">Peter Voss: "Controlling AI (was: Thinking about the future...)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0506.html">Max More: "Re: Controlling AI (was: Thinking about the future...)"</a>
<!-- reply="end" -->
</ul>
