<!-- received="Fri Aug 30 22:34:47 1996 MST" -->
<!-- sent="Fri, 30 Aug 1996 21:36:49 -0700" -->
<!-- name="Eric Watt Forste" -->
<!-- email="arkuat@factory.net" -->
<!-- subject="The SI That Ate Chicago (was: Thinking about the future...)" -->
<!-- id="v02140b48ae4d69e383a7@[204.162.114.176]" -->
<!-- inreplyto="" -->
<title>extropians: The SI That Ate Chicago (was: Thinking about the future...)</title>
<h1>The SI That Ate Chicago (was: Thinking about the future...)</h1>
Eric Watt Forste (<i>arkuat@factory.net</i>)<br>
<i>Fri, 30 Aug 1996 21:36:49 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#616">[ date ]</a><a href="index.html#616">[ thread ]</a><a href="subject.html#616">[ subject ]</a><a href="author.html#616">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0617.html">Robin Hanson: "The Great Filter"</a>
<li> <b>Previous message:</b> <a href="0615.html">Eric Watt Forste: "The Great Filter"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0624.html">Dan Clemmensen: "Re: The SI That Ate Chicago (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="0624.html">Dan Clemmensen: "Re: The SI That Ate Chicago (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="1222.html">Hara Ra: "Re: The SI That Ate Chicago (was: Thinking about the future...)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
At 8:04 PM 8/30/96, Dan Clemmensen wrote:<br>
<i>&gt;The SI may not want to destroy humanity. Humanity may simply be</i><br>
<i>&gt;unworthy of consideration, and get destroyed as a trivial side effect</i><br>
<i>&gt;of some activity of the SI.</i><br>
<p>
I liked your point that the SI to worry about is the SI whose primary goal<br>
was further increase in intelligence. But consider that the human species,<br>
as a whole, is a repository of a significant amount of computational power.<br>
Just as most of us know to look up something up on the Web before we invest<br>
a significant amount of effort into trying to figure it out for ourselves,<br>
from scratch, just using our brains and nothing else, the SI will probably<br>
be a lot more intelligent (in terms of being able to figure out solutions<br>
to problems) if it cooperates with the existing body of human researchers<br>
than if it eats them.<br>
<p>
(As usual in discussions of Robots That Will Want to Eat Us, I encourage<br>
all participants in the discussion to study the economic principle of<br>
Comparative Advantage and make sure they understand not only how it applies<br>
to trade between nations but also how it applies to trade between<br>
individuals.)<br>
<p>
Intelligence makes a silly goal when considered in splendid isolation.<br>
Intelligence is a means to diverse ends. The SI-boogie-man that you conjure<br>
up is the ultimate idolater... it not only has a primary goal of<br>
intelligence increase, it also seems to want increased intelligence at the<br>
expense of every other possible goal. Which would defeat the whole<br>
usefulness of having superintelligence in the first place, even to its<br>
possessor. It will probably want to do other things as well... perhaps<br>
reproduce and have its decendants or copies expand outward at something<br>
close to the speed of light. And in order to do this as *quickly* as<br>
possible, it might benefit from whatever computational assistance it could<br>
get. Human beings do value pocket calculators, even though human beings are<br>
far, far smarter than pocket calculators.<br>
<p>
The greatest stupidity is having just one goal at the expense of all<br>
others. This is why heroin addicts are not people most of us envy, even<br>
though they know how to keep themselves "happy" very simply.<br>
<p>
My own suspicion is that if such an SI were brought into existence, it<br>
would rapidly develop a set of diverse goals (some of which *might* even be<br>
altruistic, based on a more penetrating understanding of Comparative<br>
Advantage and similar considerations) and use its vast powers to attempt to<br>
address all of these goals simultaneously in a balanced fashion in<br>
realtime... always the hardest sort of problem for any intelligence to deal<br>
with. It might even be smart enough to figure out that its mere existence<br>
would scare the bejesus out most of the human beings it would be hoping to<br>
suck partial computational results from (to supplement its own), and that<br>
it might want to arrange something to redress this fear.<br>
<p>
But any being whose primary goal was vastly increasing its own<br>
problem-solving ability would not be so stupid as to destroy all the other<br>
computers in existence when it could hope to spawn off components of<br>
computational tasks to them. And remember that every human being's head is<br>
full of unknown and possibly useful partial computational results.<br>
(Remember Hayek's emphasis on local knowledge?) My own suspicion is that<br>
one of its many diverse goals would be figuring out how to uplift all<br>
willing human beings etc., in order to make maximally effective use of<br>
those partial computational results in as short a time as possible.<br>
<p>
I'm pretty certain that, for reasons of my own, this would be among my<br>
many, many goals if I were to become this hypothetical first SI. Of course,<br>
I might change my mind, and none of you have any reason to believe me<br>
anyway (unless you've studied enough economics and social epistemology to<br>
follow all of my hunch-ridden argument). I'm sure the paranoids will think<br>
that this is just a trick to distract you all from my secret work in the<br>
basement lab.  ;)<br>
<p>
Eric Watt Forste      &lt;arkuat@pobox.com&gt;     <a href="http://www.c2.org/~arkuat/">http://www.c2.org/~arkuat/</a><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0617.html">Robin Hanson: "The Great Filter"</a>
<li> <b>Previous message:</b> <a href="0615.html">Eric Watt Forste: "The Great Filter"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0624.html">Dan Clemmensen: "Re: The SI That Ate Chicago (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="0624.html">Dan Clemmensen: "Re: The SI That Ate Chicago (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="1222.html">Hara Ra: "Re: The SI That Ate Chicago (was: Thinking about the future...)"</a>
<!-- reply="end" -->
</ul>
