<!-- received="Sun Aug 25 11:17:35 1996 MST" -->
<!-- sent="Sun, 25 Aug 1996 10:17:22 -0700" -->
<!-- name="Peter Voss" -->
<!-- email="p.voss@ix.netcom.com" -->
<!-- subject="Controlling AI (was: Thinking about the future...)" -->
<!-- id="2.2.16.19960825085303.316761a2@popd.ix.netcom.com" -->
<!-- inreplyto="" -->
<title>extropians: Controlling AI (was: Thinking about the future...)</title>
<h1>Controlling AI (was: Thinking about the future...)</h1>
Peter Voss (<i>p.voss@ix.netcom.com</i>)<br>
<i>Sun, 25 Aug 1996 10:17:22 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#491">[ date ]</a><a href="index.html#491">[ thread ]</a><a href="subject.html#491">[ subject ]</a><a href="author.html#491">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0492.html">Robin Hanson: "&gt;H The Great Filter"</a>
<li> <b>Previous message:</b> <a href="0490.html">Sunah Cherwin: "Re: Offending People's Minds"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0499.html">Stephen de Vries: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="0499.html">Stephen de Vries: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Reply:</b> <a href="0501.html">Anders Sandberg: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="0506.html">Max More: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="0516.html">Sean Hastings: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="0524.html">Peter Voss: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="0538.html">Max More: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="0568.html">QueeneMUSE@aol.com: "Re: Controlling AI (was: Thinking about the future...)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
At 06:48 AM 25/8/96 UT, David Musick raises an important issue:<br>
<p>
<i>&gt;... It's also only a matter of time until very advanced artificial</i><br>
intelligence &gt;develops, and develops capabilities far exceeding human<br>
capabilities.  <br>
<p>
Agree. I think this is will be a 'singularity' event, more so than<br>
nano-tech. Nano-tech without AI will be quite limited, whereas AI will be<br>
able to develop nano.<br>
<p>
<i>&gt;I also see no reason to suppose that humans will have control over these </i><br>
<i>&gt;developments, past a certain point...</i><br>
....<br>
<i>&gt;I think some very advanced life forms will eventually emerge through </i><br>
<i>&gt;technology.  .... I don't think that current forms of life will really </i><br>
<i>&gt;have much of a chance against these advanced forms.</i><br>
<p>
I'll die trying! That's why I think it's crucial that we have major<br>
breakthroughs in philosophy, ethics and psychology before AI outsmarts us<br>
totally. If we can figure out what the purpose of lives is, how we determine<br>
values and how to motivate ourselves in a way that will achieve our goals,<br>
then we have a chance of developing AI that shares our purposes. It seems<br>
that AI and AL (life) will also have some sort of basic pain/pleasure<br>
motivator and some preassigned goals. Shades of Azimov's three robotic laws<br>
? Another strategy is to develop AI firstly as an extension to our own<br>
minds, to give us extra knowledge, IQ and creativity before AI gets too<br>
autonomous. Key to both of these strategies is that we know what we're doing<br>
and why we're doing it - that we don't leave AI design up to<br>
random/evolutionary design (don't get me wrong - I am definitely not a<br>
statist. I'm talking about the preferred scientific approach).<br>
<p>
<i>&gt;This actually isn't very disturbing to me -- I sort of think it's a good </i><br>
<i>&gt;thing.  Survival of the fittest.  We're all for it when we're the fittest.  </i><br>
<p>
I don't think it's a good think at all! My life is much more important to me<br>
than the abstract concept of the theory of evolution. Seeing AI &amp; AL happen<br>
is an important value to me, but never more important than my survival and<br>
well-being. In fact, the only reason AI &amp; Al are of value to me, is to the<br>
extent that they expand my life (and the lives of people dear to me). An<br>
important philosophical issue I'd say.<br>
<p>
Peter<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0492.html">Robin Hanson: "&gt;H The Great Filter"</a>
<li> <b>Previous message:</b> <a href="0490.html">Sunah Cherwin: "Re: Offending People's Minds"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0499.html">Stephen de Vries: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="0499.html">Stephen de Vries: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Reply:</b> <a href="0501.html">Anders Sandberg: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="0506.html">Max More: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="0516.html">Sean Hastings: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="0524.html">Peter Voss: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="0538.html">Max More: "Re: Controlling AI (was: Thinking about the future...)"</a>
<li> <b>Maybe reply:</b> <a href="0568.html">QueeneMUSE@aol.com: "Re: Controlling AI (was: Thinking about the future...)"</a>
<!-- reply="end" -->
</ul>
