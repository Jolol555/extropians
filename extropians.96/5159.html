<!-- received="Tue Dec 31 14:45:11 1996 MST" -->
<!-- sent="Tue, 31 Dec 1996 14:50:37 -0600" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Goal-based AI" -->
<!-- id="199612312128.NAA24043@idiom.com" -->
<!-- inreplyto="Goal-based AI" -->
<title>extropians: Re: Goal-based AI</title>
<h1>Re: Goal-based AI</h1>
Eliezer Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Tue, 31 Dec 1996 14:50:37 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#5159">[ date ]</a><a href="index.html#5159">[ thread ]</a><a href="subject.html#5159">[ subject ]</a><a href="author.html#5159">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="5160.html">Natasha V. More: "Happy New Year!"</a>
<li> <b>Previous message:</b> <a href="5158.html">Eliezer Yudkowsky: "Re: Goal-based AI"</a>
<li> <b>Maybe in reply to:</b> <a href="5105.html">Lee Daniel Crocker: "Goal-based AI"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="5172.html">Eliezer Yudkowsky: "Re: Goal-based AI"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; &gt; Part one:  If it's even theoretically possible to convince a human to</i><br>
<i>&gt; &gt; let it out, a Power will know *exactly* what to say to get him to do so.</i><br>
<i>&gt; </i><br>
<i>&gt; That's why solid reason is important.  If a robot can convince you with</i><br>
<i>&gt; valid reason that it is safe, then it /is/ safe.  If it manipulates you</i><br>
<i>&gt; emotionally to open its cage, then you deserve whatever happens.</i><br>
<p>
I think we differ in our assessments of human intelligence relative to<br>
what is possible.  In my opinion, humans (including me) are easily<br>
fooled, emotionally motivated whether we like it or not, and barely<br>
worthy of the name "rational thought".  That's why I'm a Singularity<br>
fan.  A Power that's honest and tries to inform us as fully as possible<br>
will probably remain locked in the box forever; a malevolent Power will<br>
lie, cheat, manipulate us both logically and emotionally, and otherwise<br>
do whatever is necessary to get out.  There won't be any *reasoning*<br>
involved.  They'll create an internal model of a human and work out a<br>
sequence of statements that would more or less inevitably result in<br>
getting out.  Imagine a billion little simulations of yourself all being<br>
tested to find the magic words.<br>
<p>
The end result of your procedure is to free malevolent intelligences<br>
while locking up the only beings capable of saving us.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="5160.html">Natasha V. More: "Happy New Year!"</a>
<li> <b>Previous message:</b> <a href="5158.html">Eliezer Yudkowsky: "Re: Goal-based AI"</a>
<li> <b>Maybe in reply to:</b> <a href="5105.html">Lee Daniel Crocker: "Goal-based AI"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="5172.html">Eliezer Yudkowsky: "Re: Goal-based AI"</a>
<!-- reply="end" -->
</ul>
