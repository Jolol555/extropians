<!-- received="Wed Sep  4 10:00:51 1996 MST" -->
<!-- sent="Wed, 4 Sep 1996 12:00:26 -0400" -->
<!-- name="QueeneMUSE@aol.com" -->
<!-- email="QueeneMUSE@aol.com" -->
<!-- subject="Re: Thinking about the future..." -->
<!-- id="199609041528.AA16182@foxtrot.rahul.net" -->
<!-- inreplyto="Thinking about the future..." -->
<title>extropians: Re: Thinking about the future...</title>
<h1>Re: Thinking about the future...</h1>
<i>QueeneMUSE@aol.com</i><br>
<i>Wed, 4 Sep 1996 12:00:26 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#763">[ date ]</a><a href="index.html#763">[ thread ]</a><a href="subject.html#763">[ subject ]</a><a href="author.html#763">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0764.html">"N_\\Bostrom_at_{posbn}"@smtplink.lse.ac.uk: "Re: Thinking about the future..."</a>
<li> <b>Previous message:</b> <a href="0762.html">Peter C. McCluskey: "Re: Thinking about the future..."</a>
<li> <b>Maybe in reply to:</b> <a href="0738.html">Eric Watt Forste: "Thinking about the future..."</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0771.html">Eugene Leitl: "Re: Thinking about the future..."</a>
<li> <b>Reply:</b> <a href="0771.html">Eugene Leitl: "Re: Thinking about the future..."</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
In a message dated 96-09-03 20:18:53 EDT, Dan wrote:<br>
<p>
&lt;&lt; <br>
 I feel that the effects of a truly malicious &gt;AI would be much more<br>
 dramatic.<br>
<p>
      I agree ,one could imagine - very dramatic!  But - I am still wondering<br>
what would cause an Artificial life or Intelligence to be malicious?<br>
  Ok first of all, if the intelligence was formed by error correcting neural<br>
net activity, wouldn't they spot a flaw like that early on and eleiminate it?<br>
Would  they exist in a purely cause and effect reality - assuming they are<br>
still machines (here not referring to &gt;H SI's but true man made {or self<br>
made}  computers - AI and AL live in a closed body - at least "controlled<br>
system" - of a "dry" nature) ( ?). Then how , other than the programmer<br>
instilling these "un-virtues"  could the artificially malicious ocurr in<br>
A-life application?<br>
       We still think in terms of survival as being a "threatenable" state -<br>
part of the mortality meme, I guess. Would an AI completely capable of<br>
backing itself up endlessly really see ( what we think of as) death as a<br>
threat? Malicious also implies that it would  inherenly dislike us. Why would<br>
it?  <br>
     [ Perhaps wrongly - I am assuming that a machine which replicates<br>
itself, unlike us, would know what the probilities of their own survival will<br>
be -way in advance -  and be able to formulate zillions of alternatives for<br>
every contigency- and as I asked before - the needs, the actual requirements<br>
of an artificial life form -  wouldn't they by definition be so far removed<br>
from anything we  think of as "predatory" ?_ Therefore  it is still hard for<br>
me to put together a comprehensive picture of WHY we would deemed threatening<br>
enough to destroyed. Unless we actually began dismantling them out of our own<br>
fears, of course, like in so many films and novels.] <br>
    And what are their needs? electricity? circuitry? *input*? ; - )<br>
    Do we assume they would ,by default, since humanity built them, take on<br>
our malicious and capricious primate/predator species attributes?<br>
 SSI? AI as a bigger,more evolved  alpha-ape with a bad attitude? ; - )<br>
<p>
         Of course then we may actively seek to develope predatory weaponry<br>
types of AI , I guess. That is dangerous.<br>
<p>
<p>
<i>&gt;&gt; An &gt;AI can easily agument its own intelligence by adding computing</i><br>
 capacity and<br>
 by other means that it will be able to discover or develop, by applying<br>
 its<br>
 intelligence. This is a rapid feedback mechanism. Thus, as soon as a<br>
 moderately<br>
 inventive AI comes into existance, it can become even more intelligent.<br>
 If the<br>
 AI has the goal of destroying humanity, it would be able to do so within<br>
 weeks, not decades. Moreover, unless the AI has the active preservation<br>
 of humanity as a<br>
<i>&gt; goal, it's likely to destroy humanity as a side effect.</i><br>
<i>&gt;&gt;</i><br>
 <br>
   Yes, this scenario I can easily see, especially the inadvertant overiding<br>
of our environment by making sweeping and ( for them) rational changes in the<br>
ecosystem. Like we pave over the forest to build our city.<br>
 "....Did you hear that squishing sound?" ; P<br>
 <br>
<i> &gt;&gt;This same argument applies to any SI which is capable of rapid</i><br>
 self-augmentation, not  just a straight AI. Since I think that any SI likely<br>
to be developed in  the near future  will have a large computer component, it<br>
will be capable of rapid  self-augmentation.  <br>
 <br>
    Yes and for this reason -  the 'borg' image frightnes me more than any<br>
other monster to date. Consciousness eating consciousness.<br>
<p>
<i>&gt; &gt;My hope is that the SI will develop a "morality" that includes the</i><br>
 active preservation  &gt; of humanity, or (better) the uplifting of all humans,<br>
as a goal. I'm  still trying to  figure out how we ( the extended<br>
transhumanist community) can further  that goal.<br>
 <br>
<i>  &gt;&gt;</i><br>
 YES!<br>
 Built in,unreprogrammable morals!  Hmmm....lets see, &gt;H computer ethics 101,<br>
where do i sign up?  : - ) <br>
<p>
Nadia Reed Raven St Crow<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0764.html">"N_\\Bostrom_at_{posbn}"@smtplink.lse.ac.uk: "Re: Thinking about the future..."</a>
<li> <b>Previous message:</b> <a href="0762.html">Peter C. McCluskey: "Re: Thinking about the future..."</a>
<li> <b>Maybe in reply to:</b> <a href="0738.html">Eric Watt Forste: "Thinking about the future..."</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0771.html">Eugene Leitl: "Re: Thinking about the future..."</a>
<li> <b>Reply:</b> <a href="0771.html">Eugene Leitl: "Re: Thinking about the future..."</a>
<!-- reply="end" -->
</ul>
