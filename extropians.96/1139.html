<!-- received="Tue Sep 24 06:26:09 1996 MST" -->
<!-- sent="Tue, 24 Sep 1996 14:13:31 +0200 (MET DST)" -->
<!-- name="Eugene Leitl" -->
<!-- email="Eugene.Leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: Darwinian Extropy" -->
<!-- id="Pine.SOL.3.91.960924130514.1105G-100000@sun2" -->
<!-- inreplyto="9609231653.AA02462@hss.caltech.edu" -->
<title>extropians: Re: Darwinian Extropy</title>
<h1>Re: Darwinian Extropy</h1>
Eugene Leitl (<i>Eugene.Leitl@lrz.uni-muenchen.de</i>)<br>
<i>Tue, 24 Sep 1996 14:13:31 +0200 (MET DST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1139">[ date ]</a><a href="index.html#1139">[ thread ]</a><a href="subject.html#1139">[ subject ]</a><a href="author.html#1139">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1140.html">Eugene Leitl: "Re: Experiments With Human Subjects"</a>
<li> <b>Previous message:</b> <a href="1138.html">Anders Sandberg: "Re: Darwinian Extropy"</a>
<li> <b>In reply to:</b> <a href="1103.html">Robin Hanson: "Re: Darwinian Extropy"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1152.html">Robin Hanson: "Re: Darwinian Extropy"</a>
<li> <b>Reply:</b> <a href="1152.html">Robin Hanson: "Re: Darwinian Extropy"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Mon, 23 Sep 1996, Robin Hanson wrote:<br>
<p>
<i>&gt; Dan Clemmensen writes:</i><br>
<i>&gt; &gt; [snip]</i><br>
<i>&gt; </i><br>
<i>&gt; Not all information can be computed, if one doesn't have the right</i><br>
<p>
Information? Computed? Obviously, you are not referring to the <br>
information theory definition of information...<br>
<p>
<i>&gt; inputs.  Furthermore, even for stuff that can be computed, it's not</i><br>
<p>
A generic computation can be defined in terms of mapping an input vector <br>
to an output vector. (This is so abstract, it encompasses about <br>
everything). If this is a simulation, "rightness" metric would mean its <br>
congruency with that part of reality we set out to model (of course, not <br>
the real reality, but our measurement of the reality by means of <br>
senses/gadgets, a fingerprint delta).<br>
<p>
Concerning "right" inputs: there must be a mapping function defined over <br>
the entire space of input vectors. For such a computation there are no <br>
"wrong" inputs.<br>
<p>
A computer implementation of this must be bug-free, orelse some vectors <br>
are not mapped: the computer crashes, the mapping is not defined.<br>
<p>
<i>&gt; clear there is some maximum computational "depth" (compute cycles over</i><br>
<i>&gt; output + input length) It would be very interesting if you could prove</i><br>
<i>&gt; that a computer has some universal minimum discount rate.  That would</i><br>
<p>
I don't know, whether you refer to minimum discount in terms of <br>
complexity theory.<br>
<p>
<i>&gt; be well worth publishing.  However, it seems you are a long way from</i><br>
<i>&gt; showing this.</i><br>
<p>
If terms of computing steps, the minimum O() to sort N random numbers <br>
for a purely sequential process would be at least O(N). (Obviously, we <br>
need to look at each number at least once, and only if to make sure <br>
whether they're already sorted).<br>
<p>
That a minimum complexity threshold exist, is not open do doubt. Each <br>
clearly defined task has such threshold. Some of this is easy to prove, <br>
some of it terribly hard. Just think about NP-complete and NP-hard <br>
problems.<br>
<p>
I think this can be intuitively called "no free lunch" conjecture. (I <br>
know there is a lock on that string already, but this is a local <br>
environment definition of limited visibility scope ;) <br>
<p>
<i>&gt; &gt; Why do you think an SI will understand itself any more than we</i><br>
<i>&gt; &gt; understand ourselves?  And even if it could, that doesn't mean such</i><br>
<i>&gt; &gt; understanding will lead to much improvement.</i><br>
<p>
To improve itself, the system needs not to understand itself entirely, it <br>
isn't even possible. Improvement, probably drastical, is possible by <br>
comparatively piffling investments. Using digital evolution for <br>
optimization purposes is one such instance. I don't care for the method, <br>
just for the result I wish to obtain. The method might be just too boring <br>
(automagical) or plain too complicated.<br>
<p>
<i>&gt; &gt; </i><br>
<i>&gt; </i><br>
<i>&gt; &gt;Basically, I don't believe that we understand the basics of human</i><br>
<i>&gt; &gt;cognition.Therefore our attempts at self-augmentation have no firm</i><br>
<p>
We've made excellent progress relatively recently, which still shows no <br>
signs of slacking.<br>
<p>
<i>&gt; &gt;basis.  We do, however, understand the basics of machine computation:</i><br>
<i>&gt; &gt;we can design and build more powerful computer hardware and software.</i><br>
<p>
I think, we might learn a lot from biological cognition.<br>
<p>
<i>&gt; &gt;Since we understand this basis already, I believe that an SI can also</i><br>
<i>&gt; &gt;understand it.  I believe that an SI with a computer component will be</i><br>
<i>&gt; &gt;able to design and build ever more powerful hardware and software,</i><br>
<i>&gt; &gt;thus increasing its own capabilities. I think that this is likely to</i><br>
<i>&gt; &gt;lead not just to an improvement, but to a rapid feedback process.</i><br>
<p>
Agree.<br>
 <br>
<i>&gt; Consider an analogy with the world economy.  We understand the basics</i><br>
<i>&gt; of this, and we can change it for the better, but this doesn't imply</i><br>
<p>
This must be the overestimation of the century. We can understanding the <br>
basics of the economy, we don't understand the gestalt (top-level <br>
phenomena) at all. Some of world economy is ergodic (at least at times <br>
it is). Meaning, it is fundamentally unpredictable as is... weather <br>
(unless spacetime is deterministic, and God kindly gives you an account <br>
with root rights).<br>
<p>
If we'd have the understanding, we would be able to engineer the future <br>
trajectory of the development. I am not sure I will live to see such <br>
accomplishment.<br>
<p>
<i>&gt; an explosive improvement.  Good changes are hard to find, and each one</i><br>
<p>
Robin, positive intelligence autofeedback loops are unprecendented. It <br>
does no good looking for comparisons. Because there are none. The <br>
Cambrian explosion is often used as an (imperfect) comparison. I think it <br>
will be far more drastic.<br>
<p>
<i>&gt; usually makes only a minor improvement.  It seems that, in contrast,</i><br>
<i>&gt; you imagine that there are a long series of relatively easy to find</i><br>
<i>&gt; "big wins".    If it turns out that our minds are rather badly</i><br>
<p>
I think he is right. A computer/human hybride performs significantly <br>
better than the isolated human. And that's just the beginning.<br>
<p>
<i>&gt; designed, you may be right.  But our minds may be better designed than</i><br>
<i>&gt; you think.  </i><br>
<p>
Human IQ is (assymetrically) bell-shaped distributed. It spans the <br>
entire spectrum, from moron to Einstein. Assuming, everybody is an <br>
Einstein equivalent? Surely, this must have some impact upon the world?<br>
<p>
We don't know, whether there is a limit to cognition capacity. At a <br>
guess, there is none. The only limits are those of computational physics. <br>
<p>
Assuming, we've already hit them, or are even approaching them is not <br>
realistic, imo.<br>
<p>
'gene<br>
<p>
<i>&gt; Robin D. Hanson  hanson@hss.caltech.edu  <a href="http://hss.caltech.edu/~hanson/">http://hss.caltech.edu/~hanson/</a></i><br>
<i>&gt; </i><br>
<i>&gt; </i><br>
<p>
_________________________________________________________________________________<br>
<i>| <a href="mailto:">mailto:</a> ui22204@sunmail.lrz-muenchen.de | transhumanism &gt;H, cryonics,         |</i><br>
<i>| <a href="mailto:">mailto:</a> Eugene.Leitl@uni-muenchen.de    | nanotechnology, etc. etc.           |</i><br>
<i>| <a href="mailto:">mailto:</a> c438@org.chemie.uni-muenchen.de | "deus ex machina, v.0.0.alpha"      |</i><br>
<i>| icbmto: N 48 10'07'' E 011 33'53''      | <a href="http://www.lrz-muenchen.de/~ui22204">http://www.lrz-muenchen.de/~ui22204</a> |</i><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1140.html">Eugene Leitl: "Re: Experiments With Human Subjects"</a>
<li> <b>Previous message:</b> <a href="1138.html">Anders Sandberg: "Re: Darwinian Extropy"</a>
<li> <b>In reply to:</b> <a href="1103.html">Robin Hanson: "Re: Darwinian Extropy"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1152.html">Robin Hanson: "Re: Darwinian Extropy"</a>
<li> <b>Reply:</b> <a href="1152.html">Robin Hanson: "Re: Darwinian Extropy"</a>
<!-- reply="end" -->
</ul>
