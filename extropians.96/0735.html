<!-- received="Tue Sep  3 18:17:49 1996 MST" -->
<!-- sent="Tue, 03 Sep 1996 19:47:30 -0400" -->
<!-- name="Dan Clemmensen" -->
<!-- email="dgc@shirenet.com" -->
<!-- subject="Re: Thinking about the future..." -->
<!-- id="1.5.4.16.19960904090048.480fdd2a@pop.uniserve.com" -->
<!-- inreplyto="Thinking about the future..." -->
<title>extropians: Re: Thinking about the future...</title>
<h1>Re: Thinking about the future...</h1>
Dan Clemmensen (<i>dgc@shirenet.com</i>)<br>
<i>Tue, 03 Sep 1996 19:47:30 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#735">[ date ]</a><a href="index.html#735">[ thread ]</a><a href="subject.html#735">[ subject ]</a><a href="author.html#735">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0736.html">Eric Watt Forste: "Re: Thinking about the future..."</a>
<li> <b>Previous message:</b> <a href="0734.html">E. Shaun Russell: "Re: anon.penet.fi"</a>
<li> <b>Maybe in reply to:</b> <a href="0738.html">Eric Watt Forste: "Thinking about the future..."</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0736.html">Eric Watt Forste: "Re: Thinking about the future..."</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
N.Bostrom@lse.ac.uk wrote:<br>
<i>&gt; </i><br>
<i>&gt; </i><br>
<i>&gt;           My contention is that with only one full-blown &gt;AI in the</i><br>
<i>&gt;           world, if it were malicious, the odds would be on the side</i><br>
<i>&gt;           that it could annihilate humanity within decades.</i><br>
<i>&gt; </i><br>
<p>
I feel that the effects of a truly malicious &gt;AI would be much more<br>
dramatic.<br>
An &gt;AI can easily agument its own intelligence by adding computing<br>
capacity and<br>
by other means that it will be able to discover or develop, by applying<br>
its<br>
intelligence. This is a rapid feedback mechanism. Thus, as soon as a<br>
moderately<br>
inventive AI comes into existance, it can become even more intelligent.<br>
If the<br>
AI has the goal of destroying humanity, it would be able to do so within<br>
weeks, not decades. Moreover, unless the AI has the active preservation<br>
of humanity as a<br>
goal, it's likely to destroy humanity as a side effect.<br>
<p>
This same argument applies to any SI which is capable of rapid<br>
self-augmentation, not<br>
just a straight AI. Since I think that any SI likely to be developed in<br>
the near future<br>
will have a large computer component, it will be capable of rapid<br>
self-augmentation.<br>
<p>
My hope is that the SI will develop a "morality" that includes the<br>
active preservation<br>
of humanity, or (better) the uplifting of all humans, as a goal. I'm<br>
still trying to<br>
figure out how we ( the extended transhumanist community) can further<br>
that goal.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0736.html">Eric Watt Forste: "Re: Thinking about the future..."</a>
<li> <b>Previous message:</b> <a href="0734.html">E. Shaun Russell: "Re: anon.penet.fi"</a>
<li> <b>Maybe in reply to:</b> <a href="0738.html">Eric Watt Forste: "Thinking about the future..."</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0736.html">Eric Watt Forste: "Re: Thinking about the future..."</a>
<!-- reply="end" -->
</ul>
