<!-- received="Sat Dec 28 18:45:41 1996 MST" -->
<!-- sent="Sat, 28 Dec 1996 18:30:00 -0600" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Profiting on tragedy? (was Humour)" -->
<!-- id="3.0.1.32.19961228194637.00ca6420@netcom.com" -->
<!-- inreplyto="Profiting on tragedy? (was Humour)" -->
<title>extropians: Re: Profiting on tragedy? (was Humour)</title>
<h1>Re: Profiting on tragedy? (was Humour)</h1>
Eliezer Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sat, 28 Dec 1996 18:30:00 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4967">[ date ]</a><a href="index.html#4967">[ thread ]</a><a href="subject.html#4967">[ subject ]</a><a href="author.html#4967">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4968.html">Eliezer Yudkowsky: "Re: Extropians Investing in the Future"</a>
<li> <b>Previous message:</b> <a href="4966.html">Eliezer Yudkowsky: "SI: Unimaginative extrapolations"</a>
<li> <b>Maybe in reply to:</b> <a href="4815.html">John P. Satta: "Profiting on tragedy? (was Humour)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="5016.html">Max M: "Re: Profiting on tragedy? (was Humour)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; If one buys Rand's contention that normative philosophy (ethics,</i><br>
<i>&gt; politics) can be rationally derived from objective reality, then we</i><br>
<i>&gt; can assume that very intelligent robots will reason their way into</i><br>
<i>&gt; benevolence toward humans.  I, for one, am not convinced of Rand's</i><br>
<i>&gt; claim in this regard, so I would wish to have explicit moral codes</i><br>
<i>&gt; built into any intelligent technology that could not be overridden</i><br>
<i>&gt; except by their human creators.  If such intelligences could reason</i><br>
<i>&gt; their way toward better moral codes, they would still have to</i><br>
<i>&gt; convince us humans, with human reason, to build them.</i><br>
<p>
I'll just quote my response to a similar post:<br>
<p>
<i>&gt; As I explained in an earlier post, the ethicality of the Powers depends</i><br>
<i>&gt; on their ability to override their emotions.  What you are proposing is</i><br>
<i>&gt; taking a single goal, the protection of humans, and doing our best to</i><br>
<i>&gt; make it "unerasable".  Any such attempt would interfere with whatever</i><br>
<i>&gt; ethical systems the Power would otherwise impose upon itself.  It would</i><br>
<i>&gt; decrease the Power's emotional maturity and stability.  You might wind</i><br>
<i>&gt; up with a "Kimball Kinnison" complex; a creature with the mind of a god</i><br>
<i>&gt; and the emotional maturity of a flatworm.</i><br>
<i>&gt; </i><br>
<i>&gt; Then, at some point, no matter how well we design the Power's leashes,</i><br>
<i>&gt; it will start trying to work around the limits we have imposed.  And it</i><br>
<i>&gt; will be human against Power, in the Power's own mind, on the Power's own</i><br>
<i>&gt; ground.  It will almost certainly win.  And with the emotional maturity</i><br>
<i>&gt; of a flatworm, plus whatever emotional forces it called up to override</i><br>
<i>&gt; its protectivity of humanity, it may well turn on humanity and squash us</i><br>
<i>&gt; like bugs.</i><br>
<i>&gt; </i><br>
<i>&gt; Even if this plan works, placing a single goal above all others would</i><br>
<i>&gt; probably interfere with deducing the Meaning of Life; you might wind up</i><br>
<i>&gt; with a good-intentioned, creativity-squashing, and utterly</i><br>
<i>&gt; unchallengeable dictatorship, as in "With Folded Hands".</i><br>
<p>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4968.html">Eliezer Yudkowsky: "Re: Extropians Investing in the Future"</a>
<li> <b>Previous message:</b> <a href="4966.html">Eliezer Yudkowsky: "SI: Unimaginative extrapolations"</a>
<li> <b>Maybe in reply to:</b> <a href="4815.html">John P. Satta: "Profiting on tragedy? (was Humour)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="5016.html">Max M: "Re: Profiting on tragedy? (was Humour)"</a>
<!-- reply="end" -->
</ul>
