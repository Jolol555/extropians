<!-- received="Mon Mar  8 19:52:25 1999 MDT" -->
<!-- sent="Mon, 08 Mar 1999 20:57:16 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Hiveminds and the Great Filter" -->
<!-- id="36E48E0A.E75D150A@pobox.com" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=39 -->
<html><head><title>extropians: Hiveminds and the Great Filter</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Hiveminds and the Great Filter</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Mon, 08 Mar 1999 20:57:16 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3045">[ date ]</a><a href="index.html#3045">[ thread ]</a><a href="subject.html#3045">[ subject ]</a><a href="author.html#3045">[ author ]</a>
<!-- next="start" -->
<li><a href="3046.html">[ Next ]</a><a href="3044.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3034.html">David Blenkinsop</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3063.html">Anders Sandberg</a>
</ul>
<!-- body="start" -->

<p>
David Blenkinsop wrote:
<br>
<i>&gt; </i><br>
<a href="3034.html#3045qlink1">&gt; What I really notice here is that the above comment from Billy Brown seems to</a><br>
<i>&gt; suggest that an obstacle of something being "a few orders of magnitude more</i><br>
<i>&gt; expensive" would necessarily be overcome eventually, maybe taking only a "few</i><br>
<i>&gt; hundred years" at most before the cost barrier is reduced to something</i><br>
<i>&gt; manageable. The trouble with cost barriers is that, if the cost of breaking</i><br>
<i>&gt; through the barrier is too high, maybe no one will do it! As the old saying</i><br>
<i>&gt; says, "you have to walk before you can run"; what if most would-be space</i><br>
<i>&gt; travellers never learn to "walk"?</i><br>

<p>
<a name="3063qlink1">The problem with this theory of the Great Filter, like the simple
Singularity theory, is that it doesn't take into account psychological
differences between races.  I suspect that most races are much like
humans in the sense of being individualistic rather than altruistic, and
not very cooperative; it's a stronger evolutionary attractor.</a>

<p>
However, I also suspect that at least one in a thousand races will be
strongly cooperative, perhaps hivemind-like, or with authority-related
social emotions so strongly developed that the entire civilization can
be controlled by a single individual.  Unlike our race, such a
civilization has a real chance of deliberately avoiding a Singularity
(and surviving) if such is the group consensus; some such races will
also be relentlessly expansionist, and also interventionist.  All the
factors, total, sum to no more than a hundred thousand to one, I think -
more than enough for such races to have developed in our own galaxy or
one nearby more than a billion years ago.

<p>
Such a race would not be stopped by any cost of spaceflight, nor by a
Singularity, nor by nanowar.

<p>
Where are they?
<pre>
-- 
        sentience@pobox.com          Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/singul_arity.html">http://pobox.com/~sentience/singul_arity.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3046.html">[ Next ]</a><a href="3044.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3034.html">David Blenkinsop</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3063.html">Anders Sandberg</a>
</ul>
</body></html>
