<!-- received="Thu Feb 25 14:51:47 1999 MDT" -->
<!-- sent="Thu, 25 Feb 1999 21:48:56 +0000" -->
<!-- name="Nick Bostrom" -->
<!-- email="bostrom@ndirect.co.uk" -->
<!-- subject="Re: FAQ Additions (Posthuman mind control)" -->
<!-- id="199902252151.VAA09687@sioux.hosts.netdirect.net.uk" -->
<!-- inreplyto="36D4ACFA.CEF9E458@pobox.com" -->
<!-- version=1.10, linesinbody=49 -->
<html><head><title>extropians: Re: FAQ Additions (Posthuman mind control)</title>
<meta name=author content="Nick Bostrom">
<link rel=author rev=made href="mailto:bostrom@ndirect.co.uk" title ="Nick Bostrom">
</head><body>
<h1>Re: FAQ Additions (Posthuman mind control)</h1>
Nick Bostrom (<i>bostrom@ndirect.co.uk</i>)<br>
<i>Thu, 25 Feb 1999 21:48:56 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2338">[ date ]</a><a href="index.html#2338">[ thread ]</a><a href="subject.html#2338">[ subject ]</a><a href="author.html#2338">[ author ]</a>
<!-- next="start" -->
<li><a href="2339.html">[ Next ]</a><a href="2337.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2293.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a href="2293.html#2338qlink1">&gt; Nick Bostrom wrote:</a><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Eliezer S. Yudkowsky wrote:</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; &gt; Your posthumans will find their own goals.  In any formal goal system</i><br>
<i>&gt; &gt; &gt; that uses first-order probabilistic logic, there are lines of logic that</i><br>
<i>&gt; &gt; &gt; will crank them out, totally independent of what goals they start with.</i><br>
<i>&gt; &gt; &gt; I'm not talking theory; I'm talking a specific formal result I've</i><br>
<i>&gt; &gt; &gt; produced by manipulating a formal system.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; What?!?</i><br>
<i>&gt; </i><br>
<i>&gt; Sigh... here we go again.  Take a look at "Coding a Transhuman AI";</i><br>
<i>&gt; there's a section in there about goal systems.  That's got the full</i><br>
<i>&gt; cognitive architecture.</i><br>

<p>
There was a similar discussion some time back. Your reasoning is 
that: either there are objective values, or there aren't any. In the 
latter case it doesn't matter what we do -- the expected utility is 
zero for all possible actions. In the former case it does matter what 
we do, but we don't yet know exactly how. To maximize expected 
utility you thus set out on a search for what the objective values 
are. Nothing to lose, but you might have something to win.

<p>
Well, note first of all that you can't derive any recommended 
action unless you already assume that we have some (probabilistic) 
knowledge of objective values. If we know absolutely nothing about 
them, then, for all we know, it might have a horribly negative 
utility that somebody sets out on a search for objective values. We 
would thus have no reason to adopt that as an interim goal: it would 
lead to the worst possible outcome.

<p>
Second, an agent is rational (I maintain) iff she chooses actions 
that maximize her expected utility, where her utility function is 
determined by her present preferences. If one does not have a 
preference for spending one's life ransacking one's soul (or building 
a seed SI) in order to find out what the "objective values" are, then 
it would be irrational to do that. If you prefer to spend your life
wanking or playing Duke Nuke'm, then that's what is rational for you 
to do (given that you have taken into account the long term 
consequences of your actions). It may not be very nobel, but it 
would not be irrational either, if that's what you really want to do 
most of all.

<p>
Nick Bostrom
<br>
<a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a>      n.bostrom@lse.ac.uk
Department of Philosophy, Logic and Scientific Method
London School of Economics
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2339.html">[ Next ]</a><a href="2337.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2293.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
