<!-- received="Wed Mar 10 09:35:54 1999 MDT" -->
<!-- sent="Wed, 10 Mar 1999 10:41:33 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Hiveminds and the Great Filter" -->
<!-- id="36E6A0A1.FB5ADEBE@pobox.com" -->
<!-- inreplyto="Hiveminds and the Great Filter" -->
<!-- version=1.10, linesinbody=34 -->
<html><head><title>extropians: Re: Hiveminds and the Great Filter</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Hiveminds and the Great Filter</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Wed, 10 Mar 1999 10:41:33 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3115">[ date ]</a><a href="index.html#3115">[ thread ]</a><a href="subject.html#3115">[ subject ]</a><a href="author.html#3115">[ author ]</a>
<!-- next="start" -->
<li><a href="3116.html">[ Next ]</a><a href="3114.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="3045.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3117.html">Anders Sandberg</a>
</ul>
<!-- body="start" -->

<p>
<a name="3117qlink1">I can't speak to whether or not this is a Steady State Universe, but I
would like to note that this *would* solve the Great Filter Paradox. 
Even if the Powers expand at lightspeed, or faster, or MUCH faster,
there would still, always and eternally, be islands with no
Singularities for an arbitrary distance.</a>

<p>
The question would be figuring the formula, using (say) lightspeed
geometric vs. exponential growth-rate assumptions, the probability of a
given island being absorbed during a given time-period.  If this number
is too high, then we have the result that, even though we've managed to
survive until today, the probability is still 1000:1 (for!) that we'll
be absorbed by tomorrow, or maybe even during the next second.

<p>
In this case, Yudkowsky's Modified Anthropic Occam's Razor kicks in and
basically says "This theory has failed so many times that we might as
well assume it's wrong."

<p>
YMAOR:  That simplest explanation is true, which most uniquely predicts
the present given the past, with the least reference to the Anthropic Principle.

<p>
MAOR:  That simplest explanation is true, which predicts the present
given the past, with the least reference to the Anthropic Principle.

<p>
AOR:  That simplest explanation is true which least invokes the
Anthropic Principle.

<p>
OR:  The simplest explanation is the most probable.
<br>
<i>-- </i><br>
<pre>
        sentience@pobox.com          Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/singul_arity.html">http://pobox.com/~sentience/singul_arity.html</a>
</pre>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3116.html">[ Next ]</a><a href="3114.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="3045.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3117.html">Anders Sandberg</a>
</ul>
</body></html>
