<!-- received="Tue Jan  5 19:13:23 1999 MDT" -->
<!-- sent="Tue, 05 Jan 1999 20:14:47 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Major Technologies" -->
<!-- id="3692C703.F9D0B823@pobox.com" -->
<!-- inreplyto="Major Technologies" -->
<!-- version=1.10, linesinbody=72 -->
<html><head><title>extropians: Re: Major Technologies</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Major Technologies</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Tue, 05 Jan 1999 20:14:47 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#128">[ date ]</a><a href="index.html#128">[ thread ]</a><a href="subject.html#128">[ subject ]</a><a href="author.html#128">[ author ]</a>
<!-- next="start" -->
<li><a href="0129.html">[ Next ]</a><a href="0127.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0126.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0147.html">Anders Sandberg</a>
</ul>
<!-- body="start" -->

<p>
Billy Brown wrote:
<br>
<i>&gt; </i><br>
<a href="0126.html#0128qlink1">&gt; Eliezer S. Yudkowsky wrote:</a><br>
<i>&gt; &gt; Nanotechnology is a wild card that could stay unplayed or  enter the game</i><br>
<i>&gt; &gt; at any time.  Nanotech's first applications will be entirely</i><br>
<i>&gt; &gt; destructive.  The researchers at Zyvex or Foresight will  naively release</i><br>
<i>&gt; &gt; the information and someone will reduce the Earth to grey  goo.</i><br>
<i>&gt; &lt;snip&gt;</i><br>
<i>&gt; &gt; Most probable kill:  Grey goo; nuclear war provoked by a</i><br>
<i>&gt; &gt; nanotech threat.</i><br>
<i>&gt; </i><br>
<i>&gt; Nobody does pessimism like a countersphexist, hmm?  We could argue all day</i><br>
<i>&gt; about the potential for gray goo, but I can at least assure you that the</i><br>
<i>&gt; Foresight people don't take it lightly.  They've put a good bit of thought</i><br>
<i>&gt; into how to avoid it, and I expect they will continue to do so.</i><br>

<p>
<a name="0148qlink1"><a name="0147qlink1">With all due respect to the people at Foresight, active shields are a
pipe dream.  I don't dispute that they might be able to defend against
malfunctioning assemblers, but military red goo will win every single
time.</a>  The only halfway sane analysis I've seen is "Nanotechnology and
International Security", which lists many of the major destabilizing
factors.  Nanotech is far more destructive than nuclear weapons, can be
developed with less of a visible lead time, has better first-strike
capability, and larger benefits for use.  MAD won't work.  Threatened
nuclear powers may decide to strike first.  So the world will blow up. 
This is the logical consequence.

<p>
<a href="0126.html#0128qlink2">&gt; As far as the puny stuff goes, nukes won't end civilization.  This is a myth</a><br>
</a>
<i>&gt; perpetuated by people who haven't studies the numbers.  It would be feasible</i><br>
<i>&gt; to build enough high-yield weapons to do the job, but even at the height of</i><br>
<i>&gt; the cold war we never came close to doing it.  Today, the best we could do</i><br>
<i>&gt; would be to knock ourselves back to a pre-WWII industrial base for a couple</i><br>
<i>&gt; of decades.  The death toll would be huge, but we would still end up with a</i><br>
<i>&gt; Singularity.</i><br>

<p>
That's reassuring, but the question is whether we would stand a better
chance of survival the second time around.  My guess is that the
resource base for nanotech would pop back up faster than the Internet,
and that major technophobia would strike at nondestructive IE more than
destructive nanotech.  Stupid, but so what?

<p>
I should like to ask everyone, regardless of their personal evaluation
of the effect of a nuclear war, and regardless of their personal
preference with respect to the Singularity, not to deliberately start a
nuclear war.

<p>
<a href="0126.html#0128qlink3">&gt; &gt; Humanity's primary hope of survival lies in a quick kill via AI, and the</a><br>
<i>&gt; &gt; best way I see to do that is an Open Source effort on the scale of</i><br>
<i>&gt; &gt; Linux, which I intend to oversee at some point.  Some IE via</i><br>
<i>&gt; &gt; neurohacking may be developed fast enough to be decisive, and the</i><br>
<i>&gt; &gt; existing Specialists (such as myself) may be sufficient.</i><br>
<i>&gt; </i><br>
<i>&gt; Where do I sign up?  You've seen my own projection by now - I want to make</i><br>
<i>&gt; sure that if you get hit by a truck halfway through the project, the damn</i><br>
<i>&gt; thing still has a decent chance of being sane.</i><br>

<p>
You sign up a few years from now, actually.  I'm still trying to build
the resource base.  I'm not going to ask for time, even though I'm only
19, because Zyvex won't wait; but I still think that a brief detour may
be my fastest route to the target.

<p>
Sanity is mostly a matter of not doing stupid things than specific
precautions, and most of the major precautions I believe I put down in
_Coding_.  If I get hit by a truck, you can probably build a _sane_ seed
just by referring to the Page.
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0129.html">[ Next ]</a><a href="0127.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0126.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0147.html">Anders Sandberg</a>
</ul>
</body></html>
