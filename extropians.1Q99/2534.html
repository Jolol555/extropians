<!-- received="Sun Feb 28 15:38:36 1999 MDT" -->
<!-- sent="Sun, 28 Feb 1999 22:35:52 +0000" -->
<!-- name="Nick Bostrom" -->
<!-- email="bostrom@ndirect.co.uk" -->
<!-- subject="RE: Posthuman mind control (was RE: FAQ Additions)" -->
<!-- id="199902282238.WAA22247@sioux.hosts.netdirect.net.uk" -->
<!-- inreplyto="005101be6191$8f05d970$352501c0@mfg130" -->
<!-- version=1.10, linesinbody=82 -->
<html><head><title>extropians: RE: Posthuman mind control (was RE: FAQ Additions)</title>
<meta name=author content="Nick Bostrom">
<link rel=author rev=made href="mailto:bostrom@ndirect.co.uk" title ="Nick Bostrom">
</head><body>
<h1>RE: Posthuman mind control (was RE: FAQ Additions)</h1>
Nick Bostrom (<i>bostrom@ndirect.co.uk</i>)<br>
<i>Sun, 28 Feb 1999 22:35:52 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2534">[ date ]</a><a href="index.html#2534">[ thread ]</a><a href="subject.html#2534">[ subject ]</a><a href="author.html#2534">[ author ]</a>
<!-- next="start" -->
<li><a href="2535.html">[ Next ]</a><a href="2533.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2389.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Billy Brown wrote:

<p>
<a href="2389.html#2534qlink1">&gt; Nick Bostrom wrote:</a><br>
<i>&gt; &gt; Paradoxically, I think that when we move up to the level of an SI</i><br>
<i>&gt; &gt; this problem gets easier again, since we can formulate its values in</i><br>
<i>&gt; &gt; a human language..</i><br>
<i>&gt; </i><br>
<i>&gt; and</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; That kind of unintended consequences can be easily avoided, it seems,</i><br>
<i>&gt; &gt; if we explicitly give the SI the desire to interpret all its values</i><br>
<i>&gt; &gt; the way its human creators intended them..</i><br>
<i>&gt; </i><br>
<i>&gt; I was actually sticking to human-level and moderately superhuman entities (I</i><br>
<i>&gt; try to avoid making predictions about what an SI would do).  In that realm,</i><br>
<i>&gt; the problem we face is that specifying a set of moral principles does not</i><br>
<i>&gt; uniquely determine what an entity will actually decide to do.</i><br>

<p>
I agree that this can be a greater problem when we are talking about 
~human-level AIs. For such entities, however, there should be more 
standard safety-measures that would be adequate (confinement, or 
having a group of people closely monitor their actions). The 
potential danger would only arise with seriously superhuman 
malicious intellects.

<p>
<a href="2389.html#2534qlink2">&gt; &gt; Question: What are the criteria whereby the SI determines whether its</a><br>
<i>&gt; &gt; fundamental values "need" to be changed?</i><br>
<i>&gt; </i><br>
<i>&gt; The same criteria you or I would use.</i><br>

<p>
But I don't think we deliberately change our fundamental values. 
Non-fundamental values we may change, and the criteria are then our 
more fundamental values. Fundamental values can change too, but they 
are not deliberately (rationally) changed (except in the mind-scan 
situation I mentioned in an earlier messege).

<p>
<i>&gt;  Anyone who thinks deeply about these</i><br>
<a href="2389.html#2534qlink3">&gt; issues will discover that their fundamental values contain all sorts of</a><br>
<i>&gt; conflicts, ambiguities and limitations.  Anyone who lives in a changing</i><br>
<i>&gt; world will find the need to apply their values to situations no one has ever</i><br>
<i>&gt; thought of before.</i><br>

<p>
You could certainly decide to change the way you "apply" the values 
to a specific situation, i.e. you may change your mind as to what is 
the most effective way of serving your fundamental values.

<p>
<a href="2389.html#2534qlink4">&gt; &gt; &gt; Worse, what happens when it decides to improve its own goal system?</a><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; Improve according to what standard?</i><br>
<i>&gt; </i><br>
<i>&gt; No piece of software is perfect.  A self-enhancing entity is going to find</i><br>
<i>&gt; ways of re-writing the system to make it faster, more flexible, and better</i><br>
<i>&gt; able to deal with difficult problems.  Eventually it will find</i><br>
<i>&gt; decision-making methods that work better than ours, but use completely alien</i><br>
<i>&gt; mechanisms.  consider, for example, a mind built using an optimized</i><br>
<i>&gt; combination of conventional AI, neural networks, populations of genetic</i><br>
<i>&gt; algorithms and quantum computers.  Would we really expect such an entity to</i><br>
<i>&gt; be anything like us?</i><br>

<p>
I don't count any of that as a change in fundamental values.

<p>
<a href="2389.html#2534qlink5">&gt; there is one additional point I'd like to make about all of this, because it</a><br>
<i>&gt; is easy to overlook.  All of the mechanisms I've brought up result in the AI</i><br>
<i>&gt; adopting viewpoints that we ourselves might agree with, if we had the same</i><br>
<i>&gt; intelligence and experience.  The posthumans won't just wake up and decide</i><br>
<i>&gt; to be psychotic one morning.  If they eventually decide to adopt a morality</i><br>
<i>&gt; we don't like, it will be because our own values naturally lead to that</i><br>
<i>&gt; position.</i><br>
<i>&gt; </i><br>
<i>&gt; Now, isn't that as much as we have any right to expect?</i><br>

<p>
That depends. If selection pressures lead to the evolution of AIs 
with selfish values that are indifferent to human welfare, and the 
AIs as a result go about annihilating the human species and stealing 
our resources, then I would say emphatically NO, we have a right to 
expect more.

<p>
Nick Bostrom
<br>
<a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a>      n.bostrom@lse.ac.uk
Department of Philosophy, Logic and Scientific Method
London School of Economics
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2535.html">[ Next ]</a><a href="2533.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2389.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
</body></html>
