<!-- received="Sat Feb 27 14:56:26 1999 MDT" -->
<!-- sent="Sat, 27 Feb 1999 21:53:41 +0000" -->
<!-- name="Nick Bostrom" -->
<!-- email="bostrom@ndirect.co.uk" -->
<!-- subject="Re: Posthuman mind control" -->
<!-- id="199902272156.VAA21543@sioux.hosts.netdirect.net.uk" -->
<!-- inreplyto="36D6240B.B9DFE779@pobox.com" -->
<!-- version=1.10, linesinbody=44 -->
<html><head><title>extropians: Re: Posthuman mind control</title>
<meta name=author content="Nick Bostrom">
<link rel=author rev=made href="mailto:bostrom@ndirect.co.uk" title ="Nick Bostrom">
</head><body>
<h1>Re: Posthuman mind control</h1>
Nick Bostrom (<i>bostrom@ndirect.co.uk</i>)<br>
<i>Sat, 27 Feb 1999 21:53:41 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2468">[ date ]</a><a href="index.html#2468">[ thread ]</a><a href="subject.html#2468">[ subject ]</a><a href="author.html#2468">[ author ]</a>
<!-- next="start" -->
<li><a href="2469.html">[ Next ]</a><a href="2467.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2372.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky wrote:

<p>
&gt; First of all, I'd like to commend Nick Bostrom on the fact that his AIs<br>
<a href="2372.html#2468qlink1">&gt; are reasonably alien, non-anthropomorphized.</a><br>

<p>
Does this mean I have a chance of maybe one day making it to your 
list of semi-sane persons?  ;-)

<p>
<a href="2372.html#2468qlink2">&gt; 4.  SIs probably don't have goal systems, period.  Goal systems are</a><br>
<i>&gt; non-essential artifacts of the human cognitive architectures; the</i><br>
<i>&gt; cognitive objects labeled "goals" can be discarded as an element of AIs.</i><br>
<i>&gt;  Perhaps the concept of "choice" (in the cognitive-element sense) will</i><br>
<i>&gt; remain.  Perhaps not; there are other ways to formulate cognitive</i><br>
<i>&gt; systems.</i><br>

<p>
The way I see it, intelligent behaviour means smart, purposeful 
action. Purposeful action is to be analysed in terms of the purpose 
(goals) and the plans that the agent makes in order to achieve the 
purpose. If that is right, then the relation would be analytic; goals 
(values, purposes) are a necessary feature of any system that behaves 
intelligently. 


<p>
<a href="2372.html#2468qlink3">&gt; My SIs derive from a view of human emotions as an entire legacy system</a><br>
<i>&gt; that influences us through an entire legacy worldview that integrates</i><br>
<i>&gt; with our own.  The alienness derives from the elimination of goals as</i><br>
<i>&gt; separate entities from other cognitive elements.</i><br>

<p>
I make a distinction between emotions and values (goals). Emotions 
are not logically necessary; we can imagine a cold but shrewd 
military commander who does his job efficiently without being 
involved emotionally. His goal is to defeat the enemy, he has no 
emotions. But in order to be an intelligent agent, he has to know 
what he is trying to achieve, what his desired outcome is. This 
desired outcome is what I call his goal, and his ultimate goals I 
call fundamental values.



<p>
Nick Bostrom
<br>
<a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a>      n.bostrom@lse.ac.uk
Department of Philosophy, Logic and Scientific Method
London School of Economics
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2469.html">[ Next ]</a><a href="2467.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2372.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
