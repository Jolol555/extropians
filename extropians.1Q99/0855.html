<!-- received="Mon Jan 18 08:06:19 1999 MDT" -->
<!-- sent="Mon, 18 Jan 1999 09:06:01 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Singularity: Just Say No!" -->
<!-- id="001d01be42f4$117ef920$352501c0@mfg130" -->
<!-- inreplyto="MEPCFGBBDOMMAAAA@hotbot.com" -->
<!-- version=1.10, linesinbody=28 -->
<html><head><title>extropians: RE: Singularity: Just Say No!</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Singularity: Just Say No!</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Mon, 18 Jan 1999 09:06:01 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#855">[ date ]</a><a href="index.html#855">[ thread ]</a><a href="subject.html#855">[ subject ]</a><a href="author.html#855">[ author ]</a>
<!-- next="start" -->
<li><a href="0856.html">[ Next ]</a><a href="0854.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0834.html">Chris Wolcomb</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Chris Wolcomb wrote:
<br>
<a href="0834.html#0855qlink1">&gt; However, that is a far cry from the rhetoric on this list in</a><br>
<i>&gt; regards to the Singularity as it is often proposed and</i><br>
<i>&gt; proselytized.  The more rabid Singulatarians seem to take</i><br>
<i>&gt; pride in their Singularities delightful ability to render</i><br>
<i>&gt; everything that we are irrelevant.  Rather than the a future</i><br>
<i>&gt; where we are enhanced into more comprehensive minds, using</i><br>
<i>&gt; your reptillian/mammalian metaphor, we are just as likely to</i><br>
<i>&gt; be fully *erased* or *deleted* in the Singularities path to</i><br>
<i>&gt; greateness..</i><br>

<p>
<a name="1258qlink1">IMO, an 'instant singularity' precipitated by the sudden appearance</a> of an SI
<a name="1258qlink2">is not very likely.  However, the factors that determine whether it will
happen are not under human control.  It depends on the answers to a number
of questions about natural law (like: How hard is it to increase human
intelligence?).  If the answers turn out to be the wrong ones, the first AI
to pass a certain minimal intelligence threshold rapidly becomes an SI.  If
they don't, we have nothing to fear.  The only thing we can do that makes
much difference is to make sure our seed AIs are sane, in case one of them
actually works.</a>

<p>
I'm currently writing a more detailed analysis of the whole issue, in hopes
of outlining what all of the critical questions are.

<p>
Billy Brown, MCSE+I
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0856.html">[ Next ]</a><a href="0854.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0834.html">Chris Wolcomb</a>
<!-- nextthread="start" -->
</ul>
</body></html>
