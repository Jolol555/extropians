<!-- received="Fri Mar 26 08:36:25 1999 MDT" -->
<!-- sent="Fri, 26 Mar 1999 08:36:04 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Yudkowsky's AI (again)" -->
<!-- id="000401be7795$fb298fb0$352501c0@mfg130" -->
<!-- inreplyto="36FACF66.B2122BEB@pobox.com" -->
<!-- version=1.10, linesinbody=41 -->
<html><head><title>extropians: RE: Yudkowsky's AI (again)</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Yudkowsky's AI (again)</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Fri, 26 Mar 1999 08:36:04 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3622">[ date ]</a><a href="index.html#3622">[ thread ]</a><a href="subject.html#3622">[ subject ]</a><a href="author.html#3622">[ author ]</a>
<!-- next="start" -->
<li><a href="3623.html">[ Next ]</a><a href="3621.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3598.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3685.html">Nick Bostrom</a>
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky wrote:
<br>
<a href="3598.html#3622qlink1">&gt; I really do think that when all is said and done, predicting our</a><br>
<i>&gt; treatment at the hands of the Powers is a fifty-fifty coinflip.  I just</i><br>
<i>&gt; don't know.  What I do know is that the near-future parts of the</i><br>
<i>&gt; probability branches indicate that, after the preconditions are taken</i><br>
<i>&gt; into account, this coinflip chance is larger by an order of magnitude</i><br>
<i>&gt; than all the other happy outcomes put together..</i><br>

<p>
I think its interesting how one can arrive at the same conclusion from a
completely different direction.

<p>
Personally, I would give us a very good chance of surviving the emergence of
ultratechnology even without and early seed AI success.  The kind of
technology progression we would get in that situation looks like something
humans could cope with, and the emergence of Powers would be gradual enough
that there would never be a single invincible individual.

<p>
<a name="3685qlink1">However, I also don't think there is much chance of the Singularity being a
bad thing, from the human POV.  I've heard lots of scary-sounding "What if"
stories on this topic, but nothing that even comes close to making sense.
If IE is really so easy that a seed AI can become a Power all by itself in a
short period of time, its going to go from nanotech to something more exotic
before we even notice the change (femtotechnology? reality engineering?  who
knows?).  I won't pretend to know what it will actually do at that point,
but I can't see it being concerned about something as prosaic as its supply
of atoms.</a>

<p>
If, OTOH, IE is not that easy, then there is never going to be a single
Power.  Instead, we'll get a society of different kinds of Transhuman minds
working to improve themselves as a group.  That effectively puts us back in
my first scenario, but with a faster rate of change and even less chance of
disaster.

<p>
So, whichever way it works out, anything we can do to speed up progress
(especially progress on IE) is a good thing.  The longer we take to reach
practical immortality, the more people will die before we get there.

<p>
Billy Brown, MCSE+I
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3623.html">[ Next ]</a><a href="3621.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3598.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3685.html">Nick Bostrom</a>
</ul>
</body></html>
