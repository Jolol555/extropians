<!-- received="Tue Mar  2 14:14:59 1999 MDT" -->
<!-- sent="Tue, 02 Mar 1999 01:01:06 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Posthuman mind control (was RE: FAQ Additions)" -->
<!-- id="36DB8CB1.94EB0CD6@pobox.com" -->
<!-- inreplyto="Posthuman mind control (was RE: FAQ Additions)" -->
<!-- version=1.10, linesinbody=67 -->
<html><head><title>extropians: Re: Posthuman mind control (was RE: FAQ Additions)</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Posthuman mind control (was RE: FAQ Additions)</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Tue, 02 Mar 1999 01:01:06 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2688">[ date ]</a><a href="index.html#2688">[ thread ]</a><a href="subject.html#2688">[ subject ]</a><a href="author.html#2688">[ author ]</a>
<!-- next="start" -->
<li><a href="2689.html">[ Next ]</a><a href="2687.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2647.html">Nick Bostrom</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2754.html">Nick Bostrom</a>
</ul>
<!-- body="start" -->

<p>
<a name="2754qlink1">Nick Bostrom wrote:
<br>
<i>&gt; </i><br>
<a href="2647.html#2688qlink1">&gt; My answer to this is a superposition of three points: (1) I</a><br>
<i>&gt; explicitly allowed that fundamental values could change, only (except</i><br>
<i>&gt; for the mind-scan) the change wouldn't be rationally brought about.</i><br>
<i>&gt; For example, at puberty peoples' values may change, but it's not</i><br>
<i>&gt; because of a rational choice they made.</i><br>

<p>
Are you arguing that, say, someone who was brought up as a New Age
believer and switches to being an agnostic is not making a rational choice?</a>

<p>
<a name="2754qlink2"><i>&gt; (2) Just because somebody</i><br>
<a href="2647.html#2688qlink2">&gt; calls a certain value fundamental doesn't mean it actually is</a><br>
<i>&gt; fundamental.</i><br>

<p>
So fundamental values are... whatever values don't change?</a>  Please
<a name="2754qlink3">clarify by defining the cognitive elements constituting "fundamental
values".  I make the following assertion:  "There are no cognitive
elements which are both invariant and given the highest priority in
choosing goals."</a>

<p>
<a name="2754qlink4">Another question:  What are *your* "fundamental values" and at what age
did you discover them?</a>

<p>
<a href="2647.html#2688qlink3">&gt; (3) With</a><br>
<a name="2754qlink5"><i>&gt; imperfectly rational beings (such as humans) their might be conflicts</i><br>
<i>&gt; between what they think are their fundamental values. When they</i><br>
<i>&gt; discover that that is the case, they have to redefine their</i><br>
<i>&gt; fundamental values as the preferred weighted sum of the conflicting</i><br>
<i>&gt; values (which thereby turned out not to be truely fundamental after</i><br>
<i>&gt; all).</i><br>

<p>
Why wouldn't this happen to one of your AIs?</a>

<p>
<a href="2647.html#2688qlink4">&gt; "Do what is right" sounds almost like "Do what is the best thing to</a><br>
<i>&gt; do", which is entirely vacuous.</i><br>

<p>
I wouldn't try to bring about a Singularity if I thought it was wrong. 
Thus, "do what is right" is a goal of higher priority than "bring about
a Singularity".  If something else were more probably right, I would do
that instead.  It is thus seen that "bring about a Singularity" is not
my fundamental goal.

<p>
<a href="2647.html#2688qlink5">&gt; I suspect there would be many humans who would do exactly that. Even</a><br>
<i>&gt; if none did, such a mindset could still evolve if there were</i><br>
<i>&gt; heritable variation.</i><br>

<p>
Guess I'll have to create my AI first, then, and early enough that
nobody can afford to have it reproduce.

<p>
<a href="2647.html#2688qlink6">&gt; I'm glad to hear that. But do you hold the same if we flip the</a><br>
<i>&gt; inequality sign? I don't want to be wiped out by &gt;Hs either.</i><br>

<p>
I do not hold the same if we flip the inequality sign.  I am content to
let transhumans make their own judgements.  They would not, however,
have any claim to my help in doing so; if they need my help, they're not
transhumans.  I would, in fact, actively oppose ANY attempt to wipe out
humanity; any entity with enough intelligence to do so safely (i.e.
without the chance of it being a horrible mistake) should be able to do
so in spite of my opposition.
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2689.html">[ Next ]</a><a href="2687.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2647.html">Nick Bostrom</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2754.html">Nick Bostrom</a>
</ul>
</body></html>
