<!-- received="Tue Jan 12 11:58:34 1999 MDT" -->
<!-- sent="Tue, 12 Jan 1999 12:58:01 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Subjective Morality" -->
<!-- id="000f01be3e5d$7c1bcd30$352501c0@mfg130" -->
<!-- inreplyto="916161963.20138.193.133.230.33@unicorn.com" -->
<!-- version=1.10, linesinbody=45 -->
<html><head><title>extropians: RE: Subjective Morality</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Subjective Morality</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Tue, 12 Jan 1999 12:58:01 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#509">[ date ]</a><a href="index.html#509">[ thread ]</a><a href="subject.html#509">[ subject ]</a><a href="author.html#509">[ author ]</a>
<!-- next="start" -->
<li><a href="0510.html">[ Next ]</a><a href="0508.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0491.html">mark@unicorn.com</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
mark@unicorn.com wrote:
<br>
<a href="0491.html#0509qlink1">&gt; Hal [hal@rain.org] wrote:</a><br>
<i>&gt; &gt;I don't see how to ground this regress.  It doesn't even seem to me that</i><br>
<i>&gt; &gt;it makes sense to say that a particular ranking is objectively selected..</i><br>
<i>&gt;</i><br>
<i>&gt; I agree with Hal; I think it's easy to find an optimal moral code once you</i><br>
<i>&gt; make some basic decisions as to what you regard as a good outcome, but you</i><br>
<i>&gt; can't say that one moral system is objectively better than another because</i><br>
<i>&gt; you have to make those initial subjective decisions.</i><br>

<p>
<a name="0517qlink1">For us, now, I agree completely.  The question is this: is the fact that we
can find no firm grounding for morality a fundamental feature of reality, or
the result of our own cognitive limitations?  Is there a way of grounding
that regress, or of reformulating the problem so that you don't need to?  I
don't know, and I doubt that anyone else does - I don't know that I could
even tell the difference between a real proof and a flawed one.</a>

<p>
<a href="0491.html#0509qlink2">&gt; Now, you can probably come up with a set of rational, subjective,</a><br>
<i>&gt; transhuman axioms which most of us would agree with and work out an</i><br>
<i>&gt; optimal moral system from that; but it still wouldn't be an</i><br>
<i>&gt; *objectively* optimal system because others have different axioms. They</i><br>
might
<br>
<a href="0491.html#0509qlink3">&gt; be irrational, but irrationality is their choice.</a><br>

<p>
Actually, I doubt we could get even a majority of the list to agree to any
except the most general statements.  At best, we might get a half dozen
systems that draw very different conclusions from similar basic tenets.
That's our second morality problem - given a set of basic axioms, what
choices should we make?

<p>
The only way I can see of answering the first question is to create beings
who do not share our limitations, and see what they come up with.  Answering
the second question for any given set of axioms (proven or not) seems to be
a more tractable problem, but it would require a large-scale application of
the scientific method to the field that is unlikely to happen any time soon.

<p>
In the meantime, I regard the objective of finding the answers to these
questions  to be a high-ranking goal (though obviously not the only one - I
still have a best-guess moral system to use until/unless a real one is
found).

<p>
Billy Brown, MCSE+I
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0510.html">[ Next ]</a><a href="0508.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0491.html">mark@unicorn.com</a>
<!-- nextthread="start" -->
</ul>
</body></html>
