<!-- received="Thu Jan  7 11:13:43 1999 MDT" -->
<!-- sent="07 Jan 1999 19:13:36 +0100" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: Paths to Uploading" -->
<!-- id="b49aezv6je7.fsf@void.nada.kth.se" -->
<!-- inreplyto="Wed, 6 Jan 1999 13:06:52 -0600" -->
<!-- version=1.10, linesinbody=180 -->
<html><head><title>extropians: Re: Paths to Uploading</title>
<meta name=author content="Anders Sandberg">
<link rel=author rev=made href="mailto:asa@nada.kth.se" title ="Anders Sandberg">
</head><body>
<h1>Re: Paths to Uploading</h1>
Anders Sandberg (<i>asa@nada.kth.se</i>)<br>
<i>07 Jan 1999 19:13:36 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#207">[ date ]</a><a href="index.html#207">[ thread ]</a><a href="subject.html#207">[ subject ]</a><a href="author.html#207">[ author ]</a>
<!-- next="start" -->
<li><a href="0208.html">[ Next ]</a><a href="0206.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0161.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0216.html">Billy Brown</a>
</ul>
<!-- body="start" -->

<p>
"Billy Brown" &lt;bbrown@conemsco.com&gt; writes:

<p>
<a href="0161.html#0207qlink1">&gt; Anders Sandberg wrote:</a><br>
<i>&gt; &gt; As you can tell, I don't quite buy this scenario. To me, it sounds</i><br>
<i>&gt; &gt; more like a Hollywood meme.</i><br>
<i>&gt; </i><br>
<i>&gt; Yes, it does.  I didn't buy it myself at first - but the assumptions that</i><br>
<i>&gt; lead to that conclusion are the same ones that make a Singularity possible.</i><br>

<p>
Yes and no. The assumptions below certainly make a Singularity
possible, but the reverse is not true. Vinge's original (vanilla?)
Singularity was simply that the effective intelligence-enhancing
ability of mankind as a whole increased, and thic could happen through
far less dramatic means. 

<p>
I must admit that I'm disturbed by the *faith* many seem to put into
the Singularity. It is not a necessary part of transhumanist thinking,
it is just one wild prediction among many.

<p>
<a href="0161.html#0207qlink2">&gt; Taking the objections one at a time:</a><br>
<i>&gt; </i><br>
<i>&gt; &gt; OK, this is the standard SI apotheosis scenario. But note that it is</i><br>
<i>&gt; &gt; based on a lot of unsaid assumptions: that it is just hardware</i><br>
<i>&gt; &gt; resources that distinguish a human level AI from an SI (i.e., the</i><br>
<i>&gt; &gt; software development is fairly trivial for the AI and can be done very</i><br>
<i>&gt; &gt; fast, and adding more processor power will make the AI *smarter*),</i><br>
<i>&gt; </i><br>
<i>&gt; Actually, I assume there is a significant software problem that must be</i><br>
<i>&gt; solved as well.  That's what makes it all so fast - the first group that</i><br>
<i>&gt; figures out how to make an AI sentient enough to do computer programming</i><br>
<i>&gt; will be running their experiment on very fast hardware.</i><br>

<p>
Well, one thing a fellow researcher told me was that the world-class
lab at a famous American university where he did his postdoctoral work
had worse computers than we have in our student labs (and our
institute has little money compared to the university). So excellence
in programming doesn't give you the best hardware. But you are talking
about hardware in general, so let's continue.

<p>
<a name="0214qlink1">I think I understand your point but I think you miss mine: making a IQ
X AI requires effort E. Making an IQ 2X AI, does it require effort
0.5E, E, 2E or E^2? And will twice the intelligence double the
programming ability? You seem to assume something like this, the
ability times the available subjective time (large due to fast
computers) will make the next step happen very quickly. But if
development becomes quadratically harder, the AI, even when it devotes
all its resources, will make slower and slower progress.</a> 

<p>
<a href="0161.html#0207qlink3">&gt; &gt; that this process has a time constant shorter than days (why just this</a><br>
<i>&gt; &gt; figure? why not milliseconds or centuries?),</i><br>
<i>&gt; </i><br>
<i>&gt; The time constant for self-enhancement is a function of intelligence.</i><br>
<i>&gt; Smarter AIs will improve faster than dumb ones, and the time scale of human</i><br>
<i>&gt; activity is much harder to change than that of a software entity.  In</i><br>
<i>&gt; addition, an AI can have a very fast subjective time rate if it is running</i><br>
<i>&gt; on fast hardware.  Thus, the first smart AI will be able to implement major</i><br>
<i>&gt; changes in days, rather than months.  I would expect the time scale to</i><br>
<i>&gt; shrink rapidly after that.</i><br>

<p>
I agree with some of the above assumptions (time constant depending on
intelligemnce, smarter AI can likely improve faster, human have a hard
time changing timescales). But just because AI *could* be very fast
doesn't mean it *will* be very fast - that is the assumption you are
trying to prove. Can you give any support for why an AI program would
have a specific speed?

<p>
In addition, the human-level AI might be significantly slower than
humans, so that even if it can build a better AI it will be faster to
use humans. AI will only speed its own development if the number of
collaborating AIs times their ability divided by their speed is
smaller than the number of collaborating humans times their ability
divided by human speed. Collaboration of course introduces logistic
problems.


<p>
<a href="0161.html#0207qlink4">&gt; &gt; that there will be no</a><br>
<i>&gt; &gt; systems able to interfere with it - note that one of your original</i><br>
<i>&gt; &gt; assumptions was the existence of human-level AI;</i><br>
<i>&gt; </i><br>
<i>&gt; No, my assumption is that the first human-level AI will become an SI before</i><br>
<i>&gt; the second one comes online.</i><br>

<p>
Why is the human level so critical? Self-enhancement might be doable
by less complex systems or perhaps require significantly more advanced
systems, depending on form of intelligence and designability. And
system security could definitely be efficiently provided by
less-than-human level AI to an extent that hampers even a very bright
mind (ever tried to silently sneak past a watchdog?).

<p>
<a href="0161.html#0207qlink5">&gt; &gt; that this SI is able to invent</a><br>
<i>&gt; &gt; anything it needs to (where does it get the skills?)</i><br>
<i>&gt; </i><br>
<i>&gt; I presume it will already have a large database on programming, AI, and</i><br>
<i>&gt; common-sense information.  It will probably also have a net connection - I</i><br>
<i>&gt; would expect a program that has access to the WWW to learn faster than one</i><br>
<i>&gt; that doesn't, after all.  By the 2010 - 2030 time frame that will be enough</i><br>
<i>&gt; to get you just about any information you might want.</i><br>

<p>
Even *skills*? Wow. Somehow I'm not entirely convinced. And if you
really can learn skills through the net in this scenario, then the
*humans* will be well on their way to SI.

<p>
There is a difference between data, information, knowledge and ability. 

<p>
<a href="0161.html#0207qlink6">&gt; &gt; and will have</a><br>
<i>&gt; &gt; easy access to somebody's automated lab equipment (how many labs have</i><br>
<i>&gt; &gt; their equipment online, accessible through the Net? why are you</i><br>
<i>&gt; &gt; assuming the AI is able to hack any system, especially given the</i><br>
<i>&gt; &gt; presence of other AI?).</i><br>
<i>&gt; </i><br>
<i>&gt; Again, by this time frame I would expect most labs to be automated, and</i><br>
<i>&gt; their net connections will frequently be on the same network as their</i><br>
<i>&gt; robotics control software.  You don't need to be able to hack everyone, you</i><br>
<i>&gt; just need for someone to be stupid.</i><br>
<i>&gt; </i><br>
<i>&gt; Besides, the AI could expand several thousand fold just by cracking</i><br>
<i>&gt; unsecured systems and stealing their unused CPU time.  That speeds up its</i><br>
<i>&gt; self-enhancement by a similar factor, which takes us down to a few minutes</i><br>
<i>&gt; for a major redesign.  I expect a few hours of progress at that rate would</i><br>
<i>&gt; result in an entity capable of inventing all sorts of novel attacks that our</i><br>
<i>&gt; systems aren't designed to resist.</i><br>

<p>
OK, you are simply assuming nobody notices what appears to be a super
worm or virus on the net? A program that not just takes of processor
power, but also hogs net resources to send dense data (its thoughts)
everywhere. In a world that is obviously highly dependent on the net,
where many important systems appear to be net-connected and the
majority of people have grown up with the net and its pitfalls? 

<p>
Personally I think this is another big Hollywood meme: the
crackability of systems. Somehow it seems so simple in movies to crack
mothership computers... Sure, security will always have flaws. But are
they really so exploitable that somebody can take over a lot of
systems easily without making any fuss? (remember things like anomaly
monitoring software - I have seen a neural network classify what the
users are doing and what kind of people they are; that program would
immediately notice something amiss). Most likely you could run SATAN
and its derivatives to get a few accounts, but in the process a lot of
system operators would go on alert.


<p>
Notice that you seem to be assuming that as soon as the good AI that
so far has spent its existence single-mindedly programming better
versions of itself notices a lack of computing power, it quickly
teaches itself to crack all sorts of systems (without anybody
noticing) and then goes on with a career as engineer. Sure, it is
smart, but *that* smart, flexible and able to find applicable skills
without anybody noticing anything?


<p>
<a href="0161.html#0207qlink7">&gt; &gt; And finally, we have the assumption that the</a><br>
<i>&gt; &gt; SI will be able to outwit any human in all respects - which is based</i><br>
<i>&gt; &gt; on the idea that intelligence is completely general and the same kind</i><br>
<i>&gt; &gt; of mind that can design a better AI can fool a human into (say)</i><br>
<i>&gt; &gt; connecting an experimental computer to the net or disable other</i><br>
<i>&gt; &gt; security features.</i><br>
<i>&gt; </i><br>
<i>&gt; I don't think intelligence is entirely general - my own cognitive abilities</i><br>
<i>&gt; are too lopsided to permit me that illusion.  A merely transhuman AI, with</i><br>
<i>&gt; an effective IQ of a few hundred, might not be any better at some tasks than</i><br>
<i>&gt; your average h</i><br>
<i>&gt; An SI is a different matter.  With an effective IQ at least thousands of</i><br>
<i>&gt; times beyond human average, it should be able to invent any human cognitive</i><br>
<i>&gt; skill with relative ease.  Even its weakest abilities would rapidly surpass</i><br>
<i>&gt; anything in human experience.</i><br>

<p>
But you are assuming what you want to prove, namely that the AI can
grow into an unstoppable SI. But you have not managed to show that the
human-level AI will reach SI level just by being in the
AI-researchers' big computer. 




<pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
asa@nada.kth.se                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0208.html">[ Next ]</a><a href="0206.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0161.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0216.html">Billy Brown</a>
</ul>
</body></html>
