<!-- received="Thu Feb 25 12:17:53 1999 MDT" -->
<!-- sent="Thu, 25 Feb 1999 12:17:32 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Posthuman mind control (was RE: FAQ Additions)" -->
<!-- id="004601be60eb$1d0c77d0$352501c0@mfg130" -->
<!-- inreplyto="36D57848.7D76E359@together.net" -->
<!-- version=1.10, linesinbody=80 -->
<html><head><title>extropians: RE: Posthuman mind control (was RE: FAQ Additions)</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Posthuman mind control (was RE: FAQ Additions)</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Thu, 25 Feb 1999 12:17:32 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2326">[ date ]</a><a href="index.html#2326">[ thread ]</a><a href="subject.html#2326">[ subject ]</a><a href="author.html#2326">[ author ]</a>
<!-- next="start" -->
<li><a href="2327.html">[ Next ]</a><a href="2325.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2316.html">Michael S. Lorrey</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2343.html">Nick Bostrom</a>
</ul>
<!-- body="start" -->

<p>
<a name="2344qlink1"><a name="2343qlink1">Michael S. Lorrey wrote:</a></a>
<br>
<a href="2316.html#2326qlink1">&gt; Billy Brown wrote:</a><br>
<i>&gt;</i><br>
<i>&gt; &gt; Here we have the root of our disagreement.  The problem rests on an</i><br>
<i>&gt; &gt; implementation issue that people tend to gloss over: how exactly do you</i><br>
<i>&gt; &gt; ensure that the AI doesn't violate its moral directives?</i><br>
<i>&gt;</i><br>
<i>&gt; Its actually rather straight forward. There are well publicized</i><br>
experiments
<br>
<a href="2316.html#2326qlink2">&gt; where people were given electrical impulses to their brains, which made</a><br>
them
<br>
<a href="2316.html#2326qlink3">&gt; do something, like scratch themselves, etc. In every case, the</a><br>
<i>&gt; test subjects stated that they felt that they were the ones in control,</i><br>
that
<br>
<i>&gt; they decided to move thus, and were able to rationalize very good reasons</i><br>
why
<br>
<a href="2316.html#2326qlink4">&gt; they moved thus. There was absolutely no sensation of outside control.</a><br>
<i>&gt;</i><br>
<i>&gt; Thus, any moral directives we hardwire into an AI it will consider to be</i><br>
<i>&gt; such a part and parcel of its own existence, that it could not</i><br>
<i>&gt; conceive that it would be the same being if we took it away. It would see</i><br>
any
<br>
<a href="2316.html#2326qlink5">&gt; attempt to remove those directives as an attempt at mind control, and</a><br>
would
<br>
<a href="2316.html#2326qlink6">&gt; defend itself against such intrusion.<a name="2344qlink2"> So long as one of its directives</a><br>
were
<br>
<a href="2316.html#2326qlink7">&gt; to not itself remove any of its own prime directives, it would never</a><br>
consider
<br>
<a href="2316.html#2326qlink8">&gt; such a course of action for itself.</a><br>

<p>
Well, yes, that is the intelligent way to set up a mind control system.
However, if you read the rest of my post you'll see that this isn't what we
were talking about.

<p>
<a name="2332qlink1">Nick Bostrom was arguing in favor of programming a fundamental moral system
into the AI, and then turning it loose with complete free will.</a>  My argument
is that this is very unreliable - the more complex a mind becomes, the more
difficult it is to predict how its moral principles will translate into
actions.  Also, an intelligent entity will tend to modify its moral system
over time, which means that it will not retain an arbitrary set of
principles indefinitely.</a>

<p>
<a name="2332qlink2">Now, I don't think that ongoing mental coercion is a good idea either, but
that's a different line of argument.  I would expect that you could devise
an effective scheme for controlling any static mind, so long as it isn't too
much smarter than you are.  If you want to control something that is
self-modifying you've got big problems - how do you design a control
mechanism that will remain effective no matter what your creation evolves
into?</a>

<p>
<a href="2316.html#2326qlink9">&gt; This brings up the subject of limits. As extropians, we believe in there</a><br>
being
<br>
<a href="2316.html#2326qlink10">&gt; little or no limits on human beings, outside of a limit on interfering</a><br>
with
<br>
<a href="2316.html#2326qlink11">&gt; others harmfully. We must ask, "Does this sort of moral engineering fit</a><br>
with
<br>
<a href="2316.html#2326qlink12">&gt; extropy?" I say it does, for only one reason. We are talking about design</a><br>
specs
<br>
<a href="2316.html#2326qlink13">&gt; of beings not yet in existence, much as we could talk about possible</a><br>
genetic
<br>
<a href="2316.html#2326qlink14">&gt; codes of children we might have. We are not talking about altering beings</a><br>
<i>&gt; already in existence. Altering beings already in existence, against their</i><br>
will,
<br>
<a href="2316.html#2326qlink15">&gt; is obviously against extropy. Altering the design of a being not yet in</a><br>
<i>&gt; existence is not against extropy. Once a design altered individual comes</i><br>
into
<br>
<a href="2316.html#2326qlink16">&gt; existence, its sense of self is derived from its design. That we were able</a><br>
to
<br>
<a name="2343qlink2"><a href="2316.html#2326qlink17">&gt; finely control what type of individual came into existence is no more</a><br>
against
<br>
<a href="2316.html#2326qlink18">&gt; extropy than in controlling what the genetic code of our</a><br>
<i>&gt; children will be.</i><br>

<p>
An interesting, and rather disturbing, point.  I think I'll reserve judgment
on that one for a bit.  Does anyone else have an opinion about</a> it?

<p>
Billy Brown, MCSE+I
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2327.html">[ Next ]</a><a href="2325.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2316.html">Michael S. Lorrey</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2343.html">Nick Bostrom</a>
</ul>
</body></html>
