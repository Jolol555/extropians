<!-- received="Sun Mar 28 18:06:26 1999 MDT" -->
<!-- sent="Mon, 29 Mar 1999 02:05:20 +0000" -->
<!-- name="Nick Bostrom" -->
<!-- email="bostrom@ndirect.co.uk" -->
<!-- subject="Re: Doomsday Argument (was: reasoning under computational limita" -->
<!-- id="199903290105.CAA21229@sioux.hosts.netdirect.net.uk" -->
<!-- inreplyto="003a01be7953$2afca740$110f6fd8@kekich.cablecomm-pa.com" -->
<!-- version=1.10, linesinbody=42 -->
<html><head><title>extropians: Re: Doomsday Argument (was: reasoning under computational limita</title>
<meta name=author content="Nick Bostrom">
<link rel=author rev=made href="mailto:bostrom@ndirect.co.uk" title ="Nick Bostrom">
</head><body>
<h1>Re: Doomsday Argument (was: reasoning under computational limita</h1>
Nick Bostrom (<i>bostrom@ndirect.co.uk</i>)<br>
<i>Mon, 29 Mar 1999 02:05:20 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3741">[ date ]</a><a href="index.html#3741">[ thread ]</a><a href="subject.html#3741">[ subject ]</a><a href="author.html#3741">[ author ]</a>
<!-- next="start" -->
<li><a href="3742.html">[ Next ]</a><a href="3740.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3718.html">Peter Passaro</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Peter Passaro wrote:

<p>
<a href="3718.html#3741qlink1">&gt; You'll have to excuse my naivete, I have only a cursory understanding of the</a><br>
<i>&gt; Doomsday argument.  It seems to me though that this argument can not be</i><br>
<i>&gt; applied to living systems.</i><br>

<p>
Au contraire, it applies *only* (at least directly) to living 
systems, or rather: intelligent systems.

<p>
<i>&gt;  Because of their special status as self</i><br>
<a href="3718.html#3741qlink2">&gt; organizing systems they would seem to fall into another category altogether.</a><br>
<i>&gt; It seems that most of the points made in the doomsday arguement can only be</i><br>
<i>&gt; applied to objects which do not act on themselves.</i><br>

<p>
Even if the Doomsday argument is right, we can improve our odds by 
taking action to minimize the risks. Indeed, the DA could make such 
actions seem even more justified.

<p>
<a href="3718.html#3741qlink3">&gt;  The argument (or a</a><br>
<i>&gt; variation thereof) may actually suggest the opposite conclusion - that</i><br>
<i>&gt; humanity and life itself may reach a point where the likelihood that they</i><br>
<i>&gt; would ever be destroyed is next to nil.</i><br>

<p>
This is actually the scary part. If there is such a ponit in the 
relatively near future, then the DA would suggest that there is a 
large probability that we will go extinct before we reach that point.

<p>
<a href="3718.html#3741qlink4">&gt; The only way I can see it actually applying to the number of humans alive as</a><br>
<i>&gt; a finite number is if humanity is superseded by another organism of its own</i><br>
<i>&gt; creation.</i><br>

<p>
That *might* be a possibility that is not ruled out by the DA; it 
depends on the reference class problem, which has not been solved 
yet.



<p>
Nick Bostrom
<br>
<a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a>      n.bostrom@lse.ac.uk
Department of Philosophy, Logic and Scientific Method
London School of Economics
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3742.html">[ Next ]</a><a href="3740.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3718.html">Peter Passaro</a>
<!-- nextthread="start" -->
</ul>
</body></html>
