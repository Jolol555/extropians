<!-- received="Thu Mar 25 11:17:55 1999 MDT" -->
<!-- sent="Thu, 25 Mar 1999 19:11:45 +0100" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: Yudkowsky's AI (again)" -->
<!-- id="199903251817.KAA00477@geocities.com" -->
<!-- inreplyto="Yudkowsky's AI (again)" -->
<!-- version=1.10, linesinbody=87 -->
<html><head><title>extropians: Re: Yudkowsky's AI (again)</title>
<meta name=author content="den Otter">
<link rel=author rev=made href="mailto:neosapient@geocities.com" title ="den Otter">
</head><body>
<h1>Re: Yudkowsky's AI (again)</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Thu, 25 Mar 1999 19:11:45 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3581">[ date ]</a><a href="index.html#3581">[ thread ]</a><a href="subject.html#3581">[ subject ]</a><a href="author.html#3581">[ author ]</a>
<!-- next="start" -->
<li><a href="3582.html">[ Next ]</a><a href="3580.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3579.html">Michael S. Lorrey</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->



<hr>
<br>
&gt; From: Michael S. Lorrey &lt;mike@lorrey.com&gt;<br>

<p>
<a href="3579.html#3581qlink1">&gt; den Otter wrote:</a><br>
<i>&gt; </i><br>
<i>&gt; &gt; ----------</i><br>
<i>&gt; &gt; &gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; &gt; (Earliest estimate:  2025.  Most realistic:  2040.)</i><br>
<i>&gt; &gt; &gt; We're running close enough to the edge as it is.  It is by no means</i><br>
<i>&gt; &gt; &gt; certain that the AI Powers will be any more hostile or less friendly</i><br>
<i>&gt; &gt; &gt; than the human ones.  I really don't think we can afford to be choosy.</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; We _must_ be choosy. IMHO, a rational person will delay the Singularity</i><br>
<i>&gt; &gt; at (almost?) any cost until he can transcend himself.</i><br>
<i>&gt; </i><br>
<i>&gt; Which is not practical. Initial uploads will be expensive. </i><br>

<p>
Indeed they will, which is why I keep stressing the importance
of wealth (accumulating as much wealth as possible, by any
practical means, would arguably be the single most useful
thing any transhuman organization could do).

<p>
<a href="3579.html#3581qlink2">&gt; As cost of the</a><br>
<i>&gt; technology drops, use frequency increases thus applying economis of scale. </i><br>

<p>
I strongly suspect that true upload-grade tech will never become
mainstream. Expensive prototypes will emerge is one or several
labs, quickly followed by the Singularity (assuming that the
earth doesn't get destroyed first or that AIs transcend before any
human).

<p>
Since
<br>
<a href="3579.html#3581qlink3">&gt; you are talking about guarding against even ONE Power getting there before you,</a><br>
<i>&gt; then no one will ever upload. Someone has to be first, if it is done at all. </i><br>

<p>
And someone *will* be first; the one with the best combination of
knowledge, wealth, determination and blind luck. Of course, instead
of a single individual it could also be a (small) contract group which
transcends simultaneously (see below).

<p>
<a href="3579.html#3581qlink4">&gt; It is all a matter of trust. Who do you trust?</a><br>

<p>
No-one of course (I watch the X-Files every week, you know). Hence
the proposal to create a Singularity group; combining brain power
and resources increases everyone's chances. If all works out ok
you'll all transcend simultaneously, thus (theoretically) giving
everyone an equal chance. I have no idea what would happen
next, but that wouldn't be a human problem anymore. I think
this is just about the only solution that would have any chance 
of success. 
 
<p>
<a href="3579.html#3581qlink5">&gt; What you want to guard against is unethical persons being uploaded. </a><br>

<p>
Side note: who decides what is "ethical"? 

<p>
You must ask
<br>
<a href="3579.html#3581qlink6">&gt; yourself after careful investigation and introspection if any one of the first</a><br>
<i>&gt; could be trusted with god like powers. If not, those individuals must not be</i><br>
<i>&gt; allowed to upload. Interview each with veradicators like lie detectors, voice</i><br>
<i>&gt; stress analysers, etc. to find out a) what their own feelings and opinions about</i><br>
<i>&gt; integrity, verbal contracts, etc are, and b) have them take something like an</i><br>
<i>&gt; oath of office (the position of "god" is an office, isn't it?).</i><br>

<p>
This would probably only filter out the bad liars and openly unstable
persons, and leave the really dangerous types; any lie detection
system can be tricked given enough effort (funds, intelligence etc.),
the investigators can be bribed and why would the rich &amp; powerful
consent to such a screening (and how could you be sure that you
didn't miss anyone?) Last but not least, the ethics of a post- or even transhuman
could change in dramatic and from the human pov totally unpredictable ways. Even the
most altruistic, benevolent person could 
easily turn into a genocydal Power (and vice versa, but would you bet 
your _life_ on that?) 
 
<p>
<a href="3579.html#3581qlink7">&gt; The transhuman transition period may be the first time when we can get a</a><br>
<i>&gt; practical merit system of citizenship in place, where all who wish to belong to</i><br>
<i>&gt; the new polity must earn their place and understand their responsibilities as</i><br>
<i>&gt; well as their rights.</i><br>

<p>
You could be right of course, but think that there's at least an equally
great chance that the transition period will simply be a matter of survival 
of the fittest.
 
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3582.html">[ Next ]</a><a href="3580.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3579.html">Michael S. Lorrey</a>
<!-- nextthread="start" -->
</ul>
</body></html>
