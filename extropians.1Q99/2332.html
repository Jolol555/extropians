<!-- received="Thu Feb 25 13:41:43 1999 MDT" -->
<!-- sent="Thu, 25 Feb 1999 15:29:24 -0500" -->
<!-- name="Michael S. Lorrey" -->
<!-- email="retroman@together.net" -->
<!-- subject="Re: Posthuman mind control (was RE: FAQ Additions)" -->
<!-- id="36D5B2A3.389DF293@together.net" -->
<!-- inreplyto="Posthuman mind control (was RE: FAQ Additions)" -->
<!-- version=1.10, linesinbody=40 -->
<html><head><title>extropians: Re: Posthuman mind control (was RE: FAQ Additions)</title>
<meta name=author content="Michael S. Lorrey">
<link rel=author rev=made href="mailto:retroman@together.net" title ="Michael S. Lorrey">
</head><body>
<h1>Re: Posthuman mind control (was RE: FAQ Additions)</h1>
Michael S. Lorrey (<i>retroman@together.net</i>)<br>
<i>Thu, 25 Feb 1999 15:29:24 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2332">[ date ]</a><a href="index.html#2332">[ thread ]</a><a href="subject.html#2332">[ subject ]</a><a href="author.html#2332">[ author ]</a>
<!-- next="start" -->
<li><a href="2333.html">[ Next ]</a><a href="2331.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2326.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Billy Brown wrote:

<p>
<a href="2326.html#2332qlink1">&gt; Nick Bostrom was arguing in favor of programming a fundamental moral system</a><br>
<i>&gt; into the AI, and then turning it loose with complete free will.  My argument</i><br>
<i>&gt; is that this is very unreliable - the more complex a mind becomes, the more</i><br>
<i>&gt; difficult it is to predict how its moral principles will translate into</i><br>
<i>&gt; actions.  Also, an intelligent entity will tend to modify its moral system</i><br>
<i>&gt; over time, which means that it will not retain an arbitrary set of</i><br>
<i>&gt; principles indefinitely.</i><br>

<p>
Yes, however how the individual's principles evolve will have a direct impact on
that individual's fitness to survive. I think though, oppositely from you. The
more intelligent an AI is, the more rigorously logical it will be, and thus will
actually be far more predictably reliable than a less intelligent AI. Free will
has far more to do with how we rationalize the things we do. When we do
something, we or others wonder why we did what we did. We rationalize an
explaination for it, and thus program ourselves to respond similarly to
associated situations. An AI which is more intelligent than we are will likely
be far more logical in its rationalizations for its actions than we are.

<p>
<i>&gt;</i><br>
<a href="2326.html#2332qlink2">&gt; Now, I don't think that ongoing mental coercion is a good idea either, but</a><br>
<i>&gt; that's a different line of argument.  I would expect that you could devise</i><br>
<i>&gt; an effective scheme for controlling any static mind, so long as it isn't too</i><br>
<i>&gt; much smarter than you are.  If you want to control something that is</i><br>
<i>&gt; self-modifying you've got big problems - how do you design a control</i><br>
<i>&gt; mechanism that will remain effective no matter what your creation evolves</i><br>
<i>&gt; into?</i><br>

<p>
<a name="2334qlink1">You create a blind spot. In the blind spot is the 'concience' kernel, which
cannot be directly manipulated. It can only be programmed by the experiential
data input which it analyses for useful programming content. It colors this new
content by its existing meme set before it integrates the new content into its
accumulated database of 'dos' and 'donts'. The entire database gets to vote on
every decision, so new content cannot completely wipe out old content, except
under extremely stressful circumstances (i.e. HOT STUFF HURTS!).</a>

<p>
Mike Lorrey
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2333.html">[ Next ]</a><a href="2331.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2326.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
</body></html>
