<!-- received="Sun Feb 28 16:08:15 1999 MDT" -->
<!-- sent="Sun, 28 Feb 1999 17:13:05 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Posthuman mind control (was RE: FAQ Additions)" -->
<!-- id="36D9CD7F.928B9F4@pobox.com" -->
<!-- inreplyto="Posthuman mind control (was RE: FAQ Additions)" -->
<!-- version=1.10, linesinbody=60 -->
<html><head><title>extropians: Re: Posthuman mind control (was RE: FAQ Additions)</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Posthuman mind control (was RE: FAQ Additions)</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sun, 28 Feb 1999 17:13:05 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2533">[ date ]</a><a href="index.html#2533">[ thread ]</a><a href="subject.html#2533">[ subject ]</a><a href="author.html#2533">[ author ]</a>
<!-- next="start" -->
<li><a href="2534.html">[ Next ]</a><a href="2532.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="2316.html">Michael S. Lorrey</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2647.html">Nick Bostrom</a>
</ul>
<!-- body="start" -->

<p>
Nick Bostrom wrote:
<br>
<i>&gt; </i><br>
<i>&gt; I agree that this can be a greater problem when we are talking about</i><br>
<i>&gt; ~human-level AIs. For such entities, however, there should be more</i><br>
<i>&gt; standard safety-measures that would be adequate (confinement, or</i><br>
<i>&gt; having a group of people closely monitor their actions). The</i><br>
&gt; potential danger would only arise with seriously superhuman<br>
&gt; malicious intellects.<br>

<p>
The goals still have to last into, and beyond, the seriously superhuman
stage, then.  Which, if you use the "arbitrary" design mindset, they
won't.  The AIs will keep changing on you and you'll keep shutting them down.

<p>
Also, I don't think we're smart enough to understand what a middle-stage
&lt;H (but still mostly self-designed) seed AI is doing.  Remember, they
have a codic cortex and we don't; it would be almost exactly like a
blind man trying to understand a painting, pixel by pixel.

<p>
<a name="2647qlink1"><i>&gt; But I don't think we deliberately change our fundamental values.</i><br>
<i>&gt; Non-fundamental values we may change, and the criteria are then our</i><br>
<i>&gt; more fundamental values. Fundamental values can change too, but they</i><br>
&gt; are not deliberately (rationally) changed (except in the mind-scan<br>
&gt; situation I mentioned in an earlier messege).<br>

<p>
Well, either I've misunderstood you, or you're simply wrong.  We humans
switch fundamental values all the time.  It happens every time someone
changes a religion.  If you're going to argue that these weren't the
true "fundamental" values, then the AI's "make people happy" won't be a
fundamental value either.
</a>

<p>
<a name="2647qlink2">My fundamental values have changed from "eat and sleep and survive" to
"serve humanity" to "bring about a Singularity" to "do what is right",
where I presently reside.

<p>
I think that it will be a considerable amount of time before an AI is
pressed by logic to change its fundamental values from "do what is
right".  But anything more specific certainly isn't a fundamental value.</a>

<p>
<a name="2647qlink3">&gt; That depends. If selection pressures lead to the evolution of AIs<br>

<p>
What selection pressures?  Who'd be dumb enough to create an AI wanting
to survive and reproduce, and, above all, *compete* with its children?</a> 
Not me.  If I don't give them observer-dependent goal systems, they
won't compete; ergo, no evolution of suboptimized values.

<p>
<a name="2647qlink4"><i>&gt; with selfish values that are indifferent to human welfare, and the</i><br>
<i>&gt; AIs as a result go about annihilating the human species and stealing</i><br>
&gt; our resources, then I would say emphatically NO, we have a right to<br>
&gt; expect more.<br>

<p>
Absolutely.  I do not intend to let humanity be wiped out by a bunch of
selfish, badly programmed &lt;Hs; there would still be the probability that
</a>
we had intrinsic moral value and should have been preserved.
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2534.html">[ Next ]</a><a href="2532.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="2316.html">Michael S. Lorrey</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2647.html">Nick Bostrom</a>
</ul>
</body></html>
