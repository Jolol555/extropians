<!-- received="Thu Mar 25 11:02:59 1999 MDT" -->
<!-- sent="Thu, 25 Mar 1999 11:59:08 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Yudkowsky's AI (again)" -->
<!-- id="36FA796A.4FC90BA7@pobox.com" -->
<!-- inreplyto="Yudkowsky's AI (again)" -->
<!-- version=1.10, linesinbody=75 -->
<html><head><title>extropians: Re: Yudkowsky's AI (again)</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Yudkowsky's AI (again)</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Thu, 25 Mar 1999 11:59:08 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3582">[ date ]</a><a href="index.html#3582">[ thread ]</a><a href="subject.html#3582">[ subject ]</a><a href="author.html#3582">[ author ]</a>
<!-- next="start" -->
<li><a href="3583.html">[ Next ]</a><a href="3581.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3572.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="3584qlink1">den Otter wrote:
<br>
<i>&gt; </i><br>
<a href="3572.html#3582qlink1">&gt; Not necessarily. Not all of us anyway.</a><br>

<p>
The chance that some humans will Transcend, and have their self
preserved in that Transcendence, while others die in Singularity - is
effectively zero.<a name="3586qlink1">  (If your self is preserved, you wouldn't kill off
your fellow humans, would</a> you?)  We're all in this together.  There are
<a name="3593qlink2">no differential choices between humans.</a></a>

<p>
<a name="3593qlink3"><a href="3572.html#3582qlink2">&gt; Conclusion: we need a (space) vehicle that can move us out of harm's</a><br>
<i>&gt; way when the trouble starts. Of course it must also be able to</i><br>
<i>&gt; sustain you for at least 10 years or so. A basic colonization of</i><br>
<i>&gt; Mars immediately comes to mind. Perhaps a scaled-up version</i><br>
<i>&gt; of Zubrin's Mars Direct plan. Research aimed at uploading must</i><br>
<i>&gt; continue at full speed of course while going to, and living on, Mars</i><br>
<i>&gt; (or another extra-terrestrial location).</i><br>
</a>
<p>
Impractical.<a name="3593qlink4">  Probability effectively zero.</a>  At absolute most you might
<a name="3593qlink5">hope for an O'Neill colony capable of supporting itself given nanotech.</a>

<p>
<a name="3593qlink6">Besides, since only *you* are going to Transcend, why should *I* help
you build a Mars colony?</a>

<p>
<a href="3572.html#3582qlink3">&gt; Btw, you tend overestimate the dangers of nanotech and</a><br>
<i>&gt; conventional warfare (fairly dumb tech in the hands of fairly dumb</i><br>
<i>&gt; people), while underestimating the threat of Powers (intelligence</i><br>
<i>&gt; beyond our wildest dreams). God vs monkeys with fancy toys.</i><br>

<p>
<a name="3593qlink7">Let us say that I do not underestimate the chance of a world in which
neither exists, to wit, as close to zero as makes no difference.  Given
a choice between ravenous goo and a Power, I'll take my chances on the
benevolence of the Power.  "Unacceptable" my foot; the probability
exists and is significant,</a> *unlike* the probability of the goo deciding
not to eat you.

<p>
<a name="3593qlink8"><a href="3572.html#3582qlink4">&gt; Any kind of Power which isn't you is an unaccepable threat,</a><br>
<i>&gt; because it is completely unpredictable from the human pov.</i><br>
<i>&gt; You are 100% at its mercy, as you would be if God existed.</i><br>
<i>&gt; So, both versions are undesirable.</i><br>

<p>
So only one human can ever become a Power.  By golly, let's all start
sabotaging each other's efforts!</a>

<p>
Sheesh.  There's a reason why humans have evolved an instinct for altruism.

<p>
<a name="3593qlink9"><a href="3572.html#3582qlink5">&gt; We _must_ be choosy. IMHO, a rational person will delay the Singularity</a><br>
<i>&gt; at (almost?) any cost until he can transcend himself.</i><br>

<p>
If AI-based Powers are hostile, it is almost certain, from what I know
of the matter, that human-based Powers will be hostile as well.  So only
the first human to Transcend winds up as a Power.</a>  So your a priori
<a name="3593qlink10">chance of Transcending under these assumptions is one in six billion,</a>
<a name="3593qlink11">and no more than one can get the big prize.  So you'll try to sabotage
all the Singularity efforts, and they'll try to sabotage you.  A snake-pit.</a>

<p>
<a name="3593qlink12">If only one human can ever become a Power, your chance of being that
human cannot possibly exceed one in a hundred.</a>  Combined with the fact
<a name="3593qlink13">that AI transcendence will be possible far earlier, technologically
speaking,</a> and that delaying the Singularity greatly increases the
probability of a killing war, and that a Power version of you might be
utterly unidentifiable as being human in origin, I think that the other
branches of probability - in which AI Powers are our *good friends* and
upgrade us *gently* - outweigh the probability of making it alone.

<p>
<a name="3593qlink14">In other words, almost regardless of the relative probability of AI
hostility and AI benevolence, you have a better absolute chance of
getting whatever you want if you create an AI Power as fast as possible.</a>
<pre>
-- 
        sentience@pobox.com          Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/singul_arity.html">http://pobox.com/~sentience/singul_arity.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3583.html">[ Next ]</a><a href="3581.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3572.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
</body></html>
