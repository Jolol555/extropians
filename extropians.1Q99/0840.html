<!-- received="Mon Jan 18 00:47:45 1999 MDT" -->
<!-- sent="Mon, 18 Jan 1999 01:50:28 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Singularity: Just Say No!" -->
<!-- id="36A2E79F.86163480@pobox.com" -->
<!-- inreplyto="Singularity: Just Say No!" -->
<!-- version=1.10, linesinbody=124 -->
<html><head><title>extropians: Re: Singularity: Just Say No!</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Singularity: Just Say No!</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Mon, 18 Jan 1999 01:50:28 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#840">[ date ]</a><a href="index.html#840">[ thread ]</a><a href="subject.html#840">[ subject ]</a><a href="author.html#840">[ author ]</a>
<!-- next="start" -->
<li><a href="0841.html">[ Next ]</a><a href="0839.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0790.html">Chris Wolcomb</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0877.html">Max More</a>
</ul>
<!-- body="start" -->

<p>
Chris Wolcomb wrote:
<br>
&gt;
<br>
<a href="0790.html#0840qlink1">&gt; 3) Those who believe the singularity is not inevitable but want it to come as soon as possible.</a><br>

<p>
That's me.

<p>
<a href="0790.html#0840qlink2">&gt; As an extropian, I want to shape my future around my desires.  Any force which attempts to curb my desires will be met with the strongest resistance possible.</a><br>

<p>
As a mortal, I'd like to see humanity survive in any form at all. 
Getting the future to conform perfectly to my desires is ludicrous and
would probably result in eternal living hell, no matter how careful I was.

<p>
As a Singularitarian, I want a Singularity to happen, but I don't really
care when it happens as long as it's within the next ten thousand years
or so (a mere moment on a galactic time scale).  Of course, I could be
totally wrong about the lack of urgency; it could be that every single
second of delay is more terrible than all the pains of human history. 
But the moral scales (and my feeble understanding of How Causality
Actually Works) seem to me to tilt far more towards "maximize
probability" than "minimize time".

<p>
The question doesn't arise.
<br>
My two allegiances don't come into conflict.
It's Singularity or bust.

<p>
Okay, maybe nuclear war would just delay things by a century.  So then
we have the same option again - Abort, Retry, Singularity.  And again. 
And again.  Until either intelligent life wipes itself out or creates
its successor.

<p>
Okay, so maybe the Singularity will wipe out humanity for spare atoms. 
Maybe it won't.  I accept both possibilities.  If I knew "for certain
sure" it was option one, I'd still be a Singularitarian; but as has been
stated, I'm not sure, so the question of divided allegiance doesn't arise.

<p>
I race towards the Singularity because every delay in the Singularity
increases the chance that there will be no human Singularity, ever.  And
while I might weep over that for my own peculiar logical reasons, your
reason for weeping is that it means that you and everyone you know and
the entire human race will be extinct.

<p>
<a name="0877qlink1">Screw dynamic optimism.  The future isn't a playground, it's a
minefield.  Okay, so it's a unique minefield in that blind panic will
get you killed even faster than blind enthusiasm, but blind is blind. 
The fact is that shiny new technologies always get used for military
purposes, and that technological advance decreases the amount of cash
needed to wipe out human life.  Forgive me, but I don't see any reason
for raving optimism.</a>

<p>
&gt; According to several Singulatarians, quite literally nothing - it considers 'me' irrelevant.<br>

<p>
I don't know that, and if anyone else says they do, they're mistaken.  I
simply offer no guarantees and point out that no guarantees are necessary.

<p>
&gt; So why should I accept it or want it? If the Singulatarians are right, then the Singularity will sweep away everything we know in its rapid path to increasing complexity.<br>

<p>
Again, this is only a possibility.

<p>
<a href="0790.html#0840qlink3">&gt; I would much rather spend a few centuries in a place like Iain Banks' Culture, than become part of some blind singularity as soon as possible.</a><br>

<p>
Yes, I'd love to spend a few centuries in the Culture, especially the
new revised-history "You can Sublime off any time you get bored"
version.  But I don't see any GCUs in Earth's atmosphere, do you? 
Evidently this simply isn't an option.

<p>
Besides which, the Archive (my own invention; see _Singularity
Analysis_) beats the Culture any day of the week.

<p>
&gt; Now, what if I were to start a movement to stop the singularity from occurring? I think I'll call this new political movement - the Anti-Singularity Party.  Some might say I I'm going down the path to curbing people's freedom - like freedom to build seed AI's.<br>

<p>
I happen to think that nanotechnology could get us all killed.  Do you
see me proposing any laws to restrict Zyvex?  That trick never works. 
Not for me, not for you.  Restricting technology is always more
destructive than the technology itself.

<p>
<a href="0790.html#0840qlink4">&gt; Yet, how is this any different from *them* creating a singularity and forcing me to be either be assimilated or weeded out through cybernetic natural selection?</a><br>

<p>
Very simple.  We are not forcing you to do a damn thing.  We don't have
the right.  We are not superintelligent.  You are not superintelligent
either and have no more right than I to impose your views on your fellows.

<p>
A Culture Mind might be able to search through enough futures to safely
suppress a technology.  I can't.  The same applies to coercion.  I do
not refrain from coercion and suppression for any fundamental moral
reason, but because they ALWAYS end in disaster.

<p>
If you're really right about coercion being morally wrong, I'm sure that
our Future Friends will be able to figure it out as easily as you did.

<p>
<a href="0791.html#0840qlink5">&gt; So what if most living transhumanists do not want to be absorbed into the 'sublime plenum' that is this singularity?  I've heard Singulatarians say "too bad, you will be assimilated by the singularity anyway'.</a><br>

<p>
If that's how the cards are set up, yes.  If not, not.  It's not
something I think we can influence via initial conditions.

<p>
Look, the human brain is finite.  It's got a limited number of states. 
So you have to die, go into an eternal loop, or Transcend.  In the long
run... the really long run... mortality isn't an option.

<p>
So why not just go ahead and do it, leave the womb, for good or for
evil, while my grandparents are still alive and before the Earth gets
eaten by grey goo?

<p>
<a name="0919qlink1">Wouldn't it be ironic if you delayed the seed AI, got smothered in goo,
</a>
and all along it turned out that the Singularity would have obligingly
provided an environment hedonistic beyond the most fevered imaginings of
the AhForgetIt Tendency?

<p>
&gt; Chris Wolcomb.<br>
<a href="0791.html#0840qlink6">&gt; GSV Its Only a Matter of Time.</a><br>

<p>
eliezer yudkowsky
<br>
the excession

<p>
<a href="0790.html#0840qlink7">&gt; P.S. Eliezer, we now find ourselves on the opposite sides of a new political spectrum - those of you for the Singularity, and those of us against it. The future is shaping up to be very interesting. :-)</a><br>

<p>
Considering how hard I've been pushing against the opposition, I can't
say I'm displeased to find it coalescing, particularly on my terms.
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
--
<a name="0919qlink2">Who on this list is Culture, who is Contact... and who is SC?
</a>
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0841.html">[ Next ]</a><a href="0839.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0790.html">Chris Wolcomb</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0877.html">Max More</a>
</ul>
</body></html>
