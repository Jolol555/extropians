<!-- received="Fri Jan  8 11:32:44 1999 MDT" -->
<!-- sent="Fri, 8 Jan 1999 12:32:28 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Major Technologies" -->
<!-- id="000e01be3b35$3f531eb0$352501c0@mfg130" -->
<!-- inreplyto="36963C99.78B9E6A8@pobox.com" -->
<!-- version=1.10, linesinbody=52 -->
<html><head><title>extropians: RE: Major Technologies</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Major Technologies</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Fri, 8 Jan 1999 12:32:28 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#264">[ date ]</a><a href="index.html#264">[ thread ]</a><a href="subject.html#264">[ subject ]</a><a href="author.html#264">[ author ]</a>
<!-- next="start" -->
<li><a href="0265.html">[ Next ]</a><a href="0263.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0256.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0274.html">Anders Sandberg</a>
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky wrote:
<br>
<a href="0256.html#0264qlink1">&gt; Billy Brown wrote:</a><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; Scenario 2 - Nanotech Doomsday</i><br>
<i>&gt; &gt; Assumptions:</i><br>
<i>&gt; &gt; Automated engineering is much easier than nanotech, and will thus be</i><br>
<i>&gt; &gt; implemented substantially sooner..</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; Scenario 3 - The Hard Takeoff to Singularity</i><br>
<i>&gt; &gt; Assumptions:</i><br>
<i>&gt; &gt; Automated engineering and nanotech are problems of similar</i><br>
<i>&gt; difficulty, and</i><br>
<i>&gt; &gt; will develop together..</i><br>
<i>&gt;</i><br>
<i>&gt; I'd reverse the outcomes.  Primitive nanotech results in destabilizing</i><br>
<i>&gt; competition and wars fought with half-baked weapons (sc. 3).  Instant</i><br>
<i>&gt; omnipotent drextech lets the winner take over the world without much</i><br>
<i>&gt; fuss and even evacuate the planet in case of emergency.  Problem is, I</i><br>
<i>&gt; think nanotech will start primitive.</i><br>

<p>
Hmm.  Depend on what you mean by 'primitive' and 'advanced'.  I don't think
instant mature nanotech is probable - the computers you need to design it
can't be built without primitive nanotech or many decades of top-down
evolution.

<p>
<a name="0274qlink1">In general, however, I think you are correct.  If we consider a range of
possible innovation speeds, there is a distinct danger zone in which
technology advances faster than our institutions can adapt, but not fast
enough to allow the inventor to solve all problems himself.  This
environment is likely to panic governments and other powerful organizations,
and has a high probability of leading to irrational, cataclysmic abuse</a> of
nanotech.

<p>
On the high side of the danger zone, the leading power advances so quickly
that nothing anyone else does can threaten it.  On the low side the rate of
change is slow enough that the social order can adapt, or at least avoid
being suicidally stupid.

<p>
The scenarios I listed would fall out like this:

<pre>
slow advance                        danger zone                fast advance
&lt;------------------------------|---------------------|--------------------&gt;
</pre>
<p>
   &lt;----4----&gt;   &lt;----3----&gt;    &lt;---------2---------&gt;

<p>
I suppose we could put in a '1.5' for very fast advance, but it seems very
unlikely - if automated engineering is that easy, a seed AI should also be
feasible and we're back to scenario 1.

<p>
Billy Brown, MCSE+I
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0265.html">[ Next ]</a><a href="0263.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0256.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0274.html">Anders Sandberg</a>
</ul>
</body></html>
