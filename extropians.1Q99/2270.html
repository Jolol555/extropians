<!-- received="Wed Feb 24 14:43:47 1999 MDT" -->
<!-- sent="Wed, 24 Feb 1999 20:34:53 +0000" -->
<!-- name="Nick Bostrom" -->
<!-- email="bostrom@ndirect.co.uk" -->
<!-- subject="Re: FAQ Additions (Posthuman mind control)" -->
<!-- id="199902242037.UAA15463@sioux.hosts.netdirect.net.uk" -->
<!-- inreplyto="36D33CA8.4A5C7B08@pobox.com" -->
<!-- version=1.10, linesinbody=53 -->
<html><head><title>extropians: Re: FAQ Additions (Posthuman mind control)</title>
<meta name=author content="Nick Bostrom">
<link rel=author rev=made href="mailto:bostrom@ndirect.co.uk" title ="Nick Bostrom">
</head><body>
<h1>Re: FAQ Additions (Posthuman mind control)</h1>
Nick Bostrom (<i>bostrom@ndirect.co.uk</i>)<br>
<i>Wed, 24 Feb 1999 20:34:53 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2270">[ date ]</a><a href="index.html#2270">[ thread ]</a><a href="subject.html#2270">[ subject ]</a><a href="author.html#2270">[ author ]</a>
<!-- next="start" -->
<li><a href="2271.html">[ Next ]</a><a href="2269.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2230.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="2293qlink1">Eliezer S. Yudkowsky wrote:
 
<p>
<a href="2230.html#2270qlink1">&gt; Your posthumans will find their own goals.  In any formal goal system</a><br>
<i>&gt; that uses first-order probabilistic logic, there are lines of logic that</i><br>
<i>&gt; will crank them out, totally independent of what goals they start with. </i><br>
<i>&gt; I'm not talking theory; I'm talking a specific formal result I've</i><br>
<i>&gt; produced by manipulating a formal system.</i><br>
</a>
<p>
What?!?

<p>
<a href="2230.html#2270qlink2">&gt; It's like a heat engine.  Choices are powered by differential</a><br>
<i>&gt; desirabilities.  If you think the real, factual landscape is flat, you</i><br>
<i>&gt; can impose a set of arbitrary (or even inconsistent) choices without</i><br>
<i>&gt; objection.  But we don't *know* what the real landscape is, and the</i><br>
<i>&gt; probabilistic landscape *isn't flat*.  The qualia of joy have a higher</i><br>
<i>&gt; probability of being "good" than the qualia of pain.  Higher</i><br>
<i>&gt; intelligence is more likely to lead to an optimal future.</i><br>

<p>
<a name="2293qlink2">Well, if we allow the SIs to have completely unfettered intellects, 
then it should be all right with you if we require that they have 
respect for human rights as a fundamental value, right? For if there 
is some "objective morality" then they should discover that and 
change their fundamental values despite the default values we have 
given them. Would you be happy as long as we allow them full capacity 
to think about moral issues and (once we think they are intelligent 
enough not to make stupid mistakes) even allow them full control over 
their internal structure and motivations (i.e. make them autopotent)?</a>


<p>
<a href="2230.html#2270qlink3">&gt; Can we at least agree that you won't hedge the initial goals with</a><br>
<i>&gt; forty-seven coercions, or put in any safeguards against changing the</i><br>
<i>&gt; goals?</i><br>

<p>
<a name="2293qlink3">As indicated, yes, we could allow them to change their goals (though 
only after they are intelligent and knowledgeable enough that they 
know precisely what they are doing -- just as you wouldn't allow a 
small child to experiment with dangerous drugs).</a>

<p>
The "coercions" would only be necessary for early generations of AI 
that don't have a full understanding of what they are doing. A robot 
that is cleaning your house -- it could be useful if it has an 
internal program that monitors its actions and "coerces" it to pull 
back and shut down if it perceives that it is about to fall down a 
staircase or if it hears a human distress dry. (In fact, this kind of 
internal "coercion" is even useful in humans. Instinctive fear 
sometimes saves us from the consequences of some really stupid 
decisions.)

<p>
Nick Bostrom
<br>
<a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a>      n.bostrom@lse.ac.uk
Department of Philosophy, Logic and Scientific Method
London School of Economics
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2271.html">[ Next ]</a><a href="2269.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2230.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
