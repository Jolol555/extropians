<!-- received="Mon Jan 11 16:38:13 1999 MDT" -->
<!-- sent="Mon, 11 Jan 1999 18:39:43 -0500" -->
<!-- name="Keith M. Elis" -->
<!-- email="hagbard@ix.netcom.com" -->
<!-- subject="Re: Socialism, Intelligence, and Posthumanity" -->
<!-- id="369A8BBF.2D735C24@ix.netcom.com" -->
<!-- inreplyto="Socialism, Intelligence, and Posthumanity" -->
<!-- version=1.10, linesinbody=217 -->
<html><head><title>extropians: Re: Socialism, Intelligence, and Posthumanity</title>
<meta name=author content="Keith M. Elis">
<link rel=author rev=made href="mailto:hagbard@ix.netcom.com" title ="Keith M. Elis">
</head><body>
<h1>Re: Socialism, Intelligence, and Posthumanity</h1>
Keith M. Elis (<i>hagbard@ix.netcom.com</i>)<br>
<i>Mon, 11 Jan 1999 18:39:43 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#454">[ date ]</a><a href="index.html#454">[ thread ]</a><a href="subject.html#454">[ subject ]</a><a href="author.html#454">[ author ]</a>
<!-- next="start" -->
<li><a href="0455.html">[ Next ]</a><a href="0453.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0346.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->



<p>
<a name="0458qlink1">"Eliezer S. Yudkowsky" wrote:
<br>
<i>&gt; </i><br>
<a href="0346.html#0454qlink1">&gt; Welcome, Keith M. Elis to the very small group of people whom I guess,</a><br>
<i>&gt; from personal observation, to be relatively sane.</i><br>

<p>
How small would you say?</a> 

<p>
<i>&gt; </i><br>
<i>&gt; "Keith M. Elis" wrote:</i><br>
<i>&gt;</i><br>
<a href="0331.html#0454qlink2">&gt; &gt; [...] The advent of AI/neurohacks will surely speed this</a><br>
<i>&gt; &gt; up as the ratio of augments to gaussians increases. [...]</i><br>
<i>&gt; </i><br>
<i>&gt; Thanks for using the terminology!</i><br>

<p>
<a name="0458qlink2">It's good terminology. I remember your original post to this list a year
or two ago on 'Neutral Terminology for the Future', or something like
that. I saved it somewhere. Has Anders put it in the &gt;H vocabulary list? 
</a>

<p>
<a href="0346.html#0454qlink3">&gt; A point that far too few appreciate.  Happiness is a state that</a><br>
<i>&gt; theoretically occurs when all problems are solved;</i><br>

<p>
And coupled with an evolutionary analysis we see that the solved
problems that sufficiently cause happiness are likely very few. We are
happy when our bellies are full and we are in no pain and just about to
mate. Then happiness gives way to physical pleasure. What better way to
keep us procreating? Our current way of life tends to complicate the
'about to mate' part. Our mating rituals are intricate, and costly, and
no results are guaranteed. One might say we are the most dejected bunch
of animals ever to walk the planet. Consciousness is a burden.
  
<p>
<i>&gt; happiness is not</i><br>
<a href="0346.html#0454qlink4">&gt; necessarily the best way to solve problems.</a><br>

<p>
Any knucklehead can follow an evolutionary analysis from statements that
are nearly universally agreed-upon (such as 'happiness is good') to
'people probably can't help thinking so.' An hypothesis: the more people
agree on something, the more likely it is to be a product of evolution
rather than reason.'

<p>
<i>&gt;</i><br>
<i>&gt; From my personal experience</i><br>
<a href="0346.html#0454qlink5">&gt; I can only say that there is intelligence in sorrow, frustration, and</a><br>
<i>&gt; despair; but presumably other "negative" emotions have their uses as well.</i><br>
<i>&gt;</i><br>

<p>
They all had or have at least one use, the use to which they are
probably best suited: causing us to make affirmative efforts toward
seeing our genes into the next generation. In many cases, though, it is
suffering that gets one to self-reflect. The adage is that 'ignorance is
bliss.' I don't know about that, but I do think that bliss can breed
ignorance.

<p>
I spent some time reading Bentham and Mill, and they both nod in the
direction of Francis Hutcheson who once wrote (I forget where, offhand)
"That action is best which procures the greatest happiness for the
greatest number."

<p>
<a name="0458qlink3">This is intuitively right to nearly everyone. And that should cause us
to raise an eyebrow. Read the above and replace 'happiness' with
'sadness'. The resulting sentence seems ridiculous, but it's no more
arbitrary than the original. Is there a difference between the two
sentences other than whimsy? Maybe. If there is a difference, then what
causes this difference? I don't think it's wrong to say that we cause
the difference. We just like happiness better. I'm sorry but
that's no standard for a rational person to live by. Or if it is, then
the logic of thinking so ought to be replicable by an equally rational
person.</a>
 
<p>
<a name="0458qlink4"><a href="0346.html#0454qlink6">&gt; I have a story on the boiler where a bunch of immortal uploaded (but not</a><br>
<i>&gt; mentally upgraded) humans burn out their minds like twigs in a flame</i><br>
<i>&gt; over the course of a mere thousand years of happiness, for the very</i><br>
<i>&gt; simple reason that human minds are no more designed to be immortal and</i><br>
<i>&gt; happy than human bodies are designed to live to age three hundred and</i><br>
<i>&gt; withstand nuclear weapons.</i><br>

<p>
When you're finished, I'd like to read it. I've been working on a
first-person novella illustrating the Darwin-informed absurdity of human
behavior (I know Camus did something similar, but he suspended
judgment). I'm not really sure I'm the one that ought to be writing it
because it could be decent literature if done correctly. I'm in no
hurry, though. The opening lines are these (approx.): 

<p>
"It takes a very clever person to be a good animal. To be a human? That
must take some kind of god."</a>

<p>
I envision an implicit transhumanist theme. In defining what man was and
what man is, we define what man was not, is not, and is not yet. This
brings me to a point about transhumanism. Among transhumanists, a group
of people with models that approximate reality better than most, IMO, I
see too many animals eschewing limits and not enough humans. Biological
limits keep us from growing outward, indeed, but they also keep us from
growing inward. My consciousness, surrounded by the universe, seems to
be on the fringes of this deep, dank recess within myself, where all
sorts of stupidity lurks. Some call it the reptile brain, or animal
nature. I just call it unconsciousness. At this point, we don't have the
tech to beat a path across the galaxy, or live forever, or compile
matter, or even interface our brains with a simple calculator. With a
few possible exceptions (you know who you are), the cheerleaders here
are not going to be the ones to make it happen. But there is a lot to do
by even amateurs like myself. There's<a name="0458qlink5"> a lot to do within ourselves. I'm
more interested in man growing inward at this point. We monkeys best
attempt to get the human thing right before fancying ourselves something
greater. If we're going to spill foolishness across the galaxy, we may
as well stay here.</a>

<p>
<a href="0346.html#0454qlink7">&gt; Another point, grasped so easily by this one, impossibly elusive to</a><br>
<i>&gt; almost everyone else.  Yes, the qualia of pleasure are the most</i><br>
<i>&gt; plausible candidate for the ultimate good that I know of - but you have</i><br>
<i>&gt; to assign a rational probability to that, not assume it because it feels</i><br>
<i>&gt; good.</i><br>

<p>
<a name="0458qlink6">So then the ultimate good must be something along the lines of
'correctly assigning rational probabilities to candidates for the
ultimate good.'</a> 

<p>
<a href="0346.html#0454qlink8">&gt;  Happiness may be good, or it may be irrelevant; and the</a><br>
<i>&gt; possibility that it is good does not mean it is the only possible good.</i><br>

<p>
Logically true.


<p>
<a href="0346.html#0454qlink9">&gt; &gt; It makes my head spin.</a><br>
<i>&gt; </i><br>
<i>&gt; Cease thy disorientation and consider this:  That the logic we know is</i><br>
<i>&gt; only an approximation to the truth, as the hard rationality of Euclidean</i><br>
<i>&gt; space was only a superb but untrue physical theory.</i><br>

<p>
A great and important point. We may regard the set of that which entails
reality to be a complete description of reality, but we have no basis to
assert that the set of all logical truths is a complete description of
same. 

<p>
<i>&gt;  I think that our</i><br>
<a href="0346.html#0454qlink10">&gt; evolved logic holds only as a high-level description of most of our own</a><br>
<i>&gt; space, but not for anything deeper, such as the laws of physics or</i><br>
<i>&gt; consciousness or the meaning of life.</i><br>

<p>
It's no wonder that logic implies weird paradoxical conclusions when
applied to things that had no perceptible effect on our evolution. We
evolved in this world of 'macro-sized' objects, where a thing is itself,
and not something else. Some have said that evolution has little
philosophical import, but I can't imagine that it had nothing to do with
the way we reason. 


<p>
<i>&gt;  That's what I mean when I talk</i><br>
<a href="0346.html#0454qlink11">&gt; about non-Turing-computability, and I believe it because logic</a><br>
<i>&gt; disintegrates into self-referential observer-dependent definitions when</i><br>
<i>&gt; you try to reduce it to basic units.  In other words, I wound up with</i><br>
<i>&gt; basic elements of cognition rather than mathematics.</i><br>

<p>
I don't think I've gone this far, and I'm not sure I would know a basic
element of cognition if it thumped me on the noggin, but the idea
resonates. I recall Wittgenstein's attempt to map, principally, the
limits of linguistic ability to express intelligible meaning, and thus
map the limits of conceptual thought. His idea was that if we can't
represent it, we cannot cogitate it. His conclusions basically threw
metaphysics and ethics and aesthetics out the window as principally
meaningless from a logical standpoint, which (through some
misunderstanding it seems) spawned the Vienna Circle and eventually
logical positivism.

<p>
<a name="0458qlink7">Intelligent people, when confronted with (for lack of a better term) a
'philosophical' debate, usually start wrangling over definitions and
terms. I see this all the time, and I do it myself. In some sense, we
are searching for the perfect language of philosophy, the perfect
symbols for our meanings, with an eye toward a self-consistent and
logical body of axioms. For some reason, it's always fruitless. Maybe
you've discovered why. We're arguing about how we think, not what or
</a>why.        

<p>
<i>&gt; </i><br>
<a name="0458qlink8"><a href="0346.html#0454qlink12">&gt; There are certain circumstances under which SIs might be illogical, and</a><br>
<i>&gt; it is enough to make my head spin, and these are they:  If the logical</i><br>
<i>&gt; thing for SIs to do is wipe out the mortal race that gave them birth,</i><br>
<i>&gt; and the mortals can perceive that logic, they will not give birth to the</i><br>
<i>&gt; SI.</i><br>

<p>
If it is logical for the SI to wipe out the gaussians, it is logical for
the gaussians to wipe out the gaussians.</a> If so, and given that we
<a name="0458qlink9">haven't been wiped out, there is something interfering between this
logical conclusion and actually doing the deed. One candidate is a
strong genetic imperative to survive, and all of the attendant memes
associated therewith. Then again, maybe we have been wiped out
(forcefully uploaded) and are running as a sim in the elsewhere.</a>     

<p>
Then again, even if it is logical for an SI to wipe us out, *and* we can
perceive the logic, at least you and I (and presumably others) would
still give birth to the SI simply because logic has no claim to being
the source of objectively true answers. It may be or may not be, but
that's what the SI is for.

<p>
<a name="0458qlink10"><a href="0346.html#0454qlink13">&gt;  It would therefore be to the SI's advantage not to be visibly bound</a><br>
<i>&gt; by that logic, to be illogical.  However, once the SI is created, it can</i><br>
<i>&gt; do whatever it wants; it cannot influence the mortal reasoning used, so</i><br>
<i>&gt; how can it be bound by it?  However, the mortals know that, so whatever</i><br>
<i>&gt; illogic the mortals think the SI can use would have to transcend it.</i><br>

<p>
Ugh. This is very tricky to follow. The problem here is that what is
logical cannot be illogical, and what is illogical cannot be logical. Or
more to the point, if we know what logic is, we can deduce what illogic
is. So both logical and illogical actions are easily perceptible,
although the set of both together is infinite in scope. You're talking
about something that is neither logic or illogic. That is, logic and
illogic are not flip sides of a coin, but are actually together on one
side, and there is something else on the other side. There's nothing
very helpful about this except insofar as it requires one to admit</a> that
all bets are off.   


<hr>
<br>
Keith M. Elis
<br>
<a href="mailto:hagbard@ix.netcom.com">mailto:hagbard@ix.netcom.com</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0455.html">[ Next ]</a><a href="0453.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0346.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
