<!-- received="Wed Jan  6 14:56:04 1999 MDT" -->
<!-- sent="Wed, 6 Jan 1999 21:49:37 -0000" -->
<!-- name="Bryan Moss" -->
<!-- email="bryan.moss@dial.pipex.com" -->
<!-- subject="Major Technologies" -->
<!-- id="003301be39be$9332c4e0$555895c1@jim" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=71 -->
<html><head><title>extropians: Major Technologies</title>
<meta name=author content="Bryan Moss">
<link rel=author rev=made href="mailto:bryan.moss@dial.pipex.com" title ="Bryan Moss">
</head><body>
<h1>Major Technologies</h1>
Bryan Moss (<i>bryan.moss@dial.pipex.com</i>)<br>
<i>Wed, 6 Jan 1999 21:49:37 -0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#166">[ date ]</a><a href="index.html#166">[ thread ]</a><a href="subject.html#166">[ subject ]</a><a href="author.html#166">[ author ]</a>
<!-- next="start" -->
<li><a href="0167.html">[ Next ]</a><a href="0165.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0110.html">Bryan Moss</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
I said:

<p>
<a href="0110.html#0166qlink1">"Since we're talking about plausible future scenarios it might be fun, being</a><br>
in the midst of millennium fever, to come up with some. No dates or
predictions, just how you think the next few major technologies will
pan-out."

<p>
And only in a perfect world would it be polite to ask others to do this
without doing myself:

<p>
<a name="0190qlink1">Several months ago I tried to explain my views on Artificial Intelligence
without much success, but I shall persist: We all agree that alien
intelligence would be very different to human intelligence,</a> but tend to
liken artificial intelligence to our own. Firstly, it's important to
remember that AI will not evolve on the Savannah and will never swing
through trees. Secondly, the evolution of AI will be unique in the sense
that it is directed evolution. If man could play a part in the evolution of
an intelligent species it would evolve slaves. We have directed the
evolution of many animals to our own needs with great success. Despite many
<br>
"out of control" scenarios depicted in science fiction AI offers an<br>
unprecedented level of control, and some might argue (wrongly) an
unprecedented ability to screw up. The eventual shape of AI can be found by
weighing human need against our template for intelligence, the human brain.
Thus AI will be shaped more by human users interaction needs than any
science fiction pipe dreams; and I find invisible 'go-between' interfaces
more likely than 'in-your-face' conversational interfaces.

<p>
Is human-level AI achievable? I don't care. The majority of AI will evolve
in an environment alien to even our physical laws. The majority of AI will
not know who we are, what we do, and why we do it. An AI's dedication to
doing it's job will not be like our enjoyment of an occupation or even a
moths attraction to a flame - people can get bored and moths can evolve. AI'
s will be dedicated in the same way that we have an unquestioned dedication
to our existence in the universe. I cannot guarantee that bad AI will not be
made but that is very different from an AI "going out of control" - an event
that has such a small chance of happening as to have no chance at all. In
the laboratory simulations of human-like intelligence will be routine and no
doubt much effort will be put into improving them, I have outlined reasons
for doubting this ability in other posts. If universal super-intelligence is
achieved in the laboratory then it will replace us, but it will not 'emerge'
from our domestic appliances. Whether it replaces us or not will be a
largely social and political decision. If I'm wrong and 'in-your-face'
conversational (or social) interfaces are preferred then people may well be
ready to embrace their 'mind children' but I doubt it.

<p>
Information technology will evolve to a point where we can discover the DNA
of knowledge. Storing knowledge by deviation and linking bite size
conceptual chunks of it by mutation will not only create a network of
unprecedented power but will allow us to automate knowledge acquisition. And
who needs intelligence when knowledge discovers itself? First we will have
interfaces that display our knowledge symbolically and allow us to mutate it
through many dimensions (these may be too many for practicality) and see new
pieces of knowledge and their place amongst current accepted knowledge. Some
time after this it will not be two much of a stretch to automate the
production of knowledge about knowledge (and analogies between analogies)
that can find quicker ways to create more knowledge. Thus intelligence will
be superseded by self-replicating knowledge and the fate of sentient life
will become a question of philosophy. This small paragraph covers a lot, and
I in no way want to suggest (heaven forbid) a Singularity - I imagine this
happening over centuries, if not millennia. But I do see semantic network
and hypertext having large and far reaching social effects in the meantime,
and social change is far less conservative a prediction than technological
change.

<p>
Nanotechnology:  Thwarted by complexity issues; gets here eventually.
Neurotechnology: Will make us closer to our computers (and this will make
<pre>
                 human-like AI even less likely).

</pre>
<p>
BM
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0167.html">[ Next ]</a><a href="0165.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0110.html">Bryan Moss</a>
<!-- nextthread="start" -->
</ul>
</body></html>
