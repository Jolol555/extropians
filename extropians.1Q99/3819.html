<!-- received="Mon Mar 29 17:55:35 1999 MDT" -->
<!-- sent="Mon, 29 Mar 1999 19:01:36 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Yudkowsky's AI (again)" -->
<!-- id="37002265.91A58D36@pobox.com" -->
<!-- inreplyto="Yudkowsky's AI (again)" -->
<!-- version=1.10, linesinbody=66 -->
<html><head><title>extropians: Re: Yudkowsky's AI (again)</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Yudkowsky's AI (again)</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Mon, 29 Mar 1999 19:01:36 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3819">[ date ]</a><a href="index.html#3819">[ thread ]</a><a href="subject.html#3819">[ subject ]</a><a href="author.html#3819">[ author ]</a>
<!-- next="start" -->
<li><a href="3820.html">[ Next ]</a><a href="3818.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3814.html">Lyle Burkhead</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Lyle Burkhead wrote:
<br>
<i>&gt; </i><br>
<i>&gt; Eliezer writes,</i><br>
<i>&gt; </i><br>
<a href="3814.html#3819qlink1">&gt; &gt; Business: I believe we do have some high-financial-percentile folks</a><br>
<i>&gt; &gt; reading this list. I would like to see you post...  a list of</i><br>
<i>&gt; &gt; what you're interested in funding (... Extropian business ideas? ... )</i><br>
<i>&gt; </i><br>
<i>&gt; I tried that almost three years ago.  No response.  Extropianism isnt</i><br>
<i>&gt; about making money.</i><br>

<p>
I am afraid that I agree with you.

<p>
<a name="3875qlink1">To the extent that I can define Singularitarianism, it *is* about making
money.  It takes time, brains, and money to get to the Singularity, and
we have less money than time and brains, so presently being a
Singularitarian is about money.  Take that how you like.</a>

<p>
<a href="3814.html#3819qlink2">&gt; In another post Eliezer writes,</a><br>
<i>&gt; </i><br>
<i>&gt; &gt; The most realistic estimate for a seed AI transcendence</i><br>
<i>&gt; &gt; is 2020; nanowar, before 2015. The most optimistic estimate</i><br>
<i>&gt; &gt; for project Elisson would be 2006; the earliest nanowar, 2003.</i><br>
<i>&gt; </i><br>
<i>&gt; To which den Otter replies</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; Conclusion: we need a (space) vehicle that can</i><br>
<i>&gt; &gt; move us out of harm's way when the trouble starts.</i><br>
<i>&gt; &gt; Of course it must also be able to sustain you for</i><br>
<i>&gt; &gt; at least 10 years or so. A basic colonization of Mars</i><br>
<i>&gt; &gt; immediately comes to mind.</i><br>
<i>&gt; </i><br>
<i>&gt; You have no idea how bizarre this discussion appears to an outsider.  You</i><br>
<i>&gt; guys are as far out of touch with reality as the Scientologists.  Maybe</i><br>
<i>&gt; more.</i><br>

<p>
"You guys" I assume, includes me.  Your statement about den Otter I
agree with.  A Mars colony is effectively impossible and ludicrously
expensive compared to a L5; even L5 probably isn't practical in the time
we have.  Fantasyland.

<p>
About mine...

<p>
<a href="3814.html#3819qlink3">&gt; This kind of thinking weakens you. This is not the way to see reality</a><br>
<i>&gt; clearly.  On a battlefield, in business, or anywhere, the one who sees</i><br>
<i>&gt; clearly wins.  Our way of thinking (calibration) is exemplified by the</i><br>
<i>&gt; geniebusters site.  It strengthens us.  It does lead to clear perceptions.</i><br>

<p>
The way to see reality clearly is to accept all the possibilities.  Is
there a 1% chance of Earth being destroyed?  Obviously.  So why not a
10% chance, or a 70% chance, or a 95% chance?  There's no mysterious
protective field that will prevent us from being killed by our own dumb
mistakes, like thinking there's a protective field.  We could learn to
live with military nanotech, bring technocapitalism to the masses, get
right to the verge of creating a seed AI, and get wiped out by a comet. 
Life ain't fair.  Live with it.

<p>
I don't believe in genies, either, BTW.  That kind of AI is powerful
enough to shatter our reality, not just make free glow-in-the-dark frisbies.
<pre>
-- 
        sentience@pobox.com          Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/singul_arity.html">http://pobox.com/~sentience/singul_arity.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3820.html">[ Next ]</a><a href="3818.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3814.html">Lyle Burkhead</a>
<!-- nextthread="start" -->
</ul>
</body></html>
