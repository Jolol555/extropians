<!-- received="Fri Feb 26 09:43:25 1999 MDT" -->
<!-- sent="Fri, 26 Feb 1999 09:42:57 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Comparative AI disasters" -->
<!-- id="005801be619e$b0192ec0$352501c0@mfg130" -->
<!-- inreplyto="36D64B3B.1EC39213@pobox.com" -->
<!-- version=1.10, linesinbody=43 -->
<html><head><title>extropians: RE: Comparative AI disasters</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Comparative AI disasters</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Fri, 26 Feb 1999 09:42:57 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2393">[ date ]</a><a href="index.html#2393">[ thread ]</a><a href="subject.html#2393">[ subject ]</a><a href="author.html#2393">[ author ]</a>
<!-- next="start" -->
<li><a href="2394.html">[ Next ]</a><a href="2392.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2379.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky wrote:

<p>
<a href="2379.html#2393qlink1">&gt; Now comes the interesting point.  This very day, I realized that goal</a><br>
<i>&gt; systems are not a necessary part of the Elisson architecture.</i><br>
<i>&gt;  In fact, they represent a rather messy, rigid, and inefficient method of</i><br>
<i>&gt; "caching" decisions.  The goals I had designed bear the same relation to</i><br>
<i>&gt; the reality as first-order-logic propositions bear to a human sentence.</i><br>
<i>&gt; If you regard a "goal" as a way to cache a piece of problem-solving</i><br>
<i>&gt; logic so that it doesn't have to be recalculated for each possible</i><br>
<i>&gt; situation, then you can apply all kinds of interesting design</i><br>
<i>&gt; patterns, particularly the "reductionistic" and "horizon" patterns.</i><br>
<i>&gt; A "goal" caches (and *considerably* speeds up) a piece of logic that is</i><br>
<i>&gt; repeatedly used to link a class of facts to a class of choices.  This</i><br>
<i>&gt; may not hold true for BAI's #initial# goals, but it does hold true for</i><br>
<i>&gt; practical reasoning about a subproblem, and BAI will still notice..</i><br>

<p>
For an SI, or even a distributed consciousness, I can see real merit in this
approach.  I'm not so sure that it makes sense for a small, highly
concentrated mind like Elisson (or humans, for that matter).  But perhaps
I've misunderstood you.

<p>
I've been assuming that Elisson would have a single goal representation
system, a reasoning engine (presumably implemented as a collection of
specialized domdules), a central world-model, a sensory processing system
(which provides input for the reasoning system), an effector system (which
carries out instructions issued by the reasoning system), and various memory
subsystems.  The reasoning system has an implicit goal of "figure out what
to do next", which is what drives it to populate the initially empty goal
system.

<p>
With this kind of architecture, the centralized goal representation is what
keeps the whole system moving in the same direction.  If one part of the
reasoning system adopts a new sub-goal, everything else can immediately see
and act on that goal.  If a goal is deleted, every module that is working on
it will quickly notice the change.

<p>
What would change if you replaced the IGS with and FCCS, and how would this
be beneficial?

<p>
Billy Brown, MCSE+I
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2394.html">[ Next ]</a><a href="2392.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2379.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
