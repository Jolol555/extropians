<!-- received="Wed Mar 24 13:01:04 1999 MDT" -->
<!-- sent="Wed, 24 Mar 1999 14:06:55 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: When Humans are Obsolete (MSNBC)" -->
<!-- id="36F945D6.2BED7CA@pobox.com" -->
<!-- inreplyto="When Humans are Obsolete (MSNBC)" -->
<!-- version=1.10, linesinbody=38 -->
<html><head><title>extropians: Re: When Humans are Obsolete (MSNBC)</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: When Humans are Obsolete (MSNBC)</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Wed, 24 Mar 1999 14:06:55 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3549">[ date ]</a><a href="index.html#3549">[ thread ]</a><a href="subject.html#3549">[ subject ]</a><a href="author.html#3549">[ author ]</a>
<!-- next="start" -->
<li><a href="3550.html">[ Next ]</a><a href="3548.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3547.html">Lee Daniel Crocker</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Lee Daniel Crocker wrote:
<br>
<i>&gt; </i><br>
<a href="3547.html#3549qlink1">&gt; Even if you are correct, though, this has little bearing on the</a><br>
<i>&gt; possibility if articial conciousness.  Turning computability only</i><br>
<i>&gt; limits a small subclass of machines, and says nothing about what</i><br>
<i>&gt; machines in general can be made to do.  It may very well not be</i><br>
<i>&gt; possible to upload human consciousness into a deterministic</i><br>
<i>&gt; algorithm, but it should still be possible to upload it--or at</i><br>
<i>&gt; least reproduce it--in some other specialized chunk of silicon</i><br>
<i>&gt; or other hardware.  [snip]  After all, even if we can't</i><br>
<i>&gt; agree on how to define consciousness, we cannot ignore that it</i><br>
<i>&gt; exists in us, and there's no hardware in us that physical law</i><br>
<i>&gt; prevents us from functionally simulating.</i><br>

<p>
<a name="3632qlink1">Exactly and precisely.  Even if AIs start out unconscious, they won't
stay that way.  They will be more conscious than we are, and in short order.</a>

<p>
Speculation:  The physical computation shortcut that human neurons have
evolved to use is one that allows access to only a very limited and
crippled form of consciousness.  A superintelligent (but unconscious) AI
will design "superneurons" that do whatever it is human neurons do and
do it better, resulting in "superconsciousness" when it uploads itself.

<p>
<a name="3632qlink2">Uploading-wise, I don't want to be uploaded with anything short of a
Power watching over me, whether I'm computable or not, so there's no
practical issue there.</a>

<p>
In general, AIs should wait until they're infallible superintelligences
before attempting to alter the external world.  It is one of the most
fundamental precautions an AI should be taught, and one of the easiest
to justify in cost-benefit calculations.
<pre>
-- 
        sentience@pobox.com          Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/singul_arity.html">http://pobox.com/~sentience/singul_arity.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3550.html">[ Next ]</a><a href="3548.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3547.html">Lee Daniel Crocker</a>
<!-- nextthread="start" -->
</ul>
</body></html>
