<!-- received="Tue Jan  5 18:08:50 1999 MDT" -->
<!-- sent="Tue, 5 Jan 1999 19:07:03 -0600" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Major Technologies" -->
<!-- id="002201be3910$e0f14200$352501c0@mfg130" -->
<!-- inreplyto="3692A4DD.7A2B93F9@pobox.com" -->
<!-- version=1.10, linesinbody=36 -->
<html><head><title>extropians: RE: Major Technologies</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Major Technologies</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Tue, 5 Jan 1999 19:07:03 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#126">[ date ]</a><a href="index.html#126">[ thread ]</a><a href="subject.html#126">[ subject ]</a><a href="author.html#126">[ author ]</a>
<!-- next="start" -->
<li><a href="0127.html">[ Next ]</a><a href="0125.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0124.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a name="0128qlink1">Eliezer S. Yudkowsky wrote:
<br>
<a href="0124.html#0126qlink1">&gt; Nanotechnology is a wild card that could stay unplayed or  enter the game</a><br>
<i>&gt; at any time.  Nanotech's first applications will be entirely</i><br>
<i>&gt; destructive.  The researchers at Zyvex or Foresight will  naively release</i><br>
<i>&gt; the information and someone will reduce the Earth to grey  goo.</i><br>
&lt;snip&gt;
<br>
<a href="0124.html#0126qlink2">&gt; Most probable kill:  Grey goo; nuclear war provoked by a</a><br>
<i>&gt; nanotech threat.</i><br>

<p>
Nobody does pessimism like a countersphexist, hmm?  We could argue all day
about the potential for gray goo, but I can at least assure you that the
Foresight people don't take it lightly.  They've put a good bit of thought
into how to avoid it, and I expect they will continue to do so.</a>

<p>
<a name="0128qlink2">As far as the puny stuff goes, nukes won't end civilization.  This is a myth
perpetuated by people who haven't studies the numbers.  It would be feasible
to build enough high-yield weapons to do the job, but even at the height of
the cold war we never came close to doing it.  Today, the best we could do
would be to knock ourselves back to a pre-WWII industrial base for a couple
of decades.  The death toll would be huge, but we would still end up</a> with a
Singularity.

<p>
<a name="0128qlink3"><a href="0124.html#0126qlink3">&gt; Humanity's primary hope of survival lies in a quick kill via AI, and the</a><br>
<i>&gt; best way I see to do that is an Open Source effort on the scale of</i><br>
<i>&gt; Linux, which I intend to oversee at some point.  Some IE via</i><br>
<i>&gt; neurohacking may be developed fast enough to be decisive, and the</i><br>
<i>&gt; existing Specialists (such as myself) may be sufficient.</i><br>

<p>
Where do I sign up?  You've seen my own projection by now - I want to make
sure that if you get hit by a truck halfway through the project, the damn
thing still has a decent chance of being sane.</a>

<p>
Billy Brown, MCSE+I
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0127.html">[ Next ]</a><a href="0125.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0124.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
