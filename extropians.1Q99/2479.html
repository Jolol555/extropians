<!-- received="Sat Feb 27 17:49:04 1999 MDT" -->
<!-- sent="Sat, 27 Feb 1999 18:50:08 -0600" -->
<!-- name="arnaldo" -->
<!-- email="arnaldo@apex.net" -->
<!-- subject="Re: FAQ Additions (Posthuman mind control)" -->
<!-- id="36D89294.8DFBBF70@apex.net" -->
<!-- inreplyto="FAQ Additions (Posthuman mind control)" -->
<!-- version=1.10, linesinbody=55 -->
<html><head><title>extropians: Re: FAQ Additions (Posthuman mind control)</title>
<meta name=author content="arnaldo">
<link rel=author rev=made href="mailto:arnaldo@apex.net" title ="arnaldo">
</head><body>
<h1>Re: FAQ Additions (Posthuman mind control)</h1>
arnaldo (<i>arnaldo@apex.net</i>)<br>
<i>Sat, 27 Feb 1999 18:50:08 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2479">[ date ]</a><a href="index.html#2479">[ thread ]</a><a href="subject.html#2479">[ subject ]</a><a href="author.html#2479">[ author ]</a>
<!-- next="start" -->
<li><a href="2480.html">[ Next ]</a><a href="2478.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2464.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Speaking of morals being subjective, what is your opinion, transhumanists and
extropians, about what is gonna happen when (if ever) a machine reaches a high level of
artificial intelligence - a level in which it can learn like we do, but much faster, in
the order of the 100 or more MIPS?
<br>
Will it have a mind of its own, like us? A biologist once told me that Mind is a
function of the Brain, like Digestion is a function of the Digestive System and no one
dares to attibute a Soul to the Digestive System.
Will such a machine will be trying to find god the creator? If it does, it will have
better luck than we do for sure... The creator(s) will be easily find by the machine...

<p>
arnaldo

<p>
den Otter wrote:

<p>
<a href="2464.html#2479qlink1">&gt; ----------</a><br>
<i>&gt; &gt; From: Michael S. Lorrey &lt;retroman@together.net&gt;</i><br>
<i>&gt;</i><br>
<i>&gt; &gt; den Otter wrote:</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; &gt; Morals are subjective, but the most practical (=rational) course of action</i><br>
<i>&gt; &gt; &gt; would be not to create AIs with SI potential and work on uploading instead.</i><br>
<i>&gt; &gt; &gt; Again, our prime directive should _always_ be survival. Survival is the</i><br>
<i>&gt; &gt; &gt; prerequisite for _all_ other actions. Any philosophy that does not value</i><br>
<i>&gt; &gt; &gt; personal survival [in an optimal state] above everything else is by definition</i><br>
<i>&gt; &gt; &gt; irrational. Thus follows that transhumanism (with an immortalist element)</i><br>
<i>&gt; &gt; &gt; is the best philosophy currently available to us.</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; Still beating the dead horse of subjective morals I see.</i><br>
<i>&gt;</i><br>
<i>&gt; Is it really that dead? ;-)</i><br>
<i>&gt;</i><br>
<i>&gt; &gt; Survival is not as</i><br>
<i>&gt; &gt; important as evolution.</i><br>
<i>&gt;</i><br>
<i>&gt; ???? Evolution is just a tool to increase your chances of survival,</i><br>
<i>&gt; and a possible source of entertainment. There can be life without</i><br>
<i>&gt; evolution (even quality life for that matter -- one could imagine that</i><br>
<i>&gt; an advanced SI could live happily forever without evolving in any</i><br>
<i>&gt; significant manner), but there can't be evolution without life.</i><br>
<i>&gt;</i><br>
<i>&gt; &gt; It is not extropic to want to live a million years, but to</i><br>
<i>&gt; &gt; not learn anything or change one whit the whole time. The Prime Directive of</i><br>
<i>&gt; &gt; Extropy is that all individuals should have the freedom to learn, to experience,</i><br>
<i>&gt; &gt; to change as the individual sees fit. Being able to live long enough to fill that</i><br>
<i>&gt; &gt; to the individuals satisfaction is a part of that freedom, but not a prerequisite.</i><br>
<i>&gt;</i><br>
<i>&gt; Yes it is; without life no [personal] extropy.</i><br>
<i>&gt;</i><br>
<i>&gt; &gt; "Live Free or Die, Death is not the Worst of Evils."</i><br>
<i>&gt; &gt;                             - General John Stark</i><br>
<i>&gt;</i><br>
<i>&gt; "Death is the ultimate oppressor".</i><br>
<i>&gt;                                        -den Otter</i><br>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2480.html">[ Next ]</a><a href="2478.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2464.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
</body></html>
