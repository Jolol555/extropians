<!-- received="Wed Feb 24 18:48:30 1999 MDT" -->
<!-- sent="Wed, 24 Feb 1999 19:53:08 -0600" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: FAQ Additions (Posthuman mind control)" -->
<!-- id="36D4ACFA.CEF9E458@pobox.com" -->
<!-- inreplyto="FAQ Additions (Posthuman mind control)" -->
<!-- version=1.10, linesinbody=105 -->
<html><head><title>extropians: Re: FAQ Additions (Posthuman mind control)</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: FAQ Additions (Posthuman mind control)</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Wed, 24 Feb 1999 19:53:08 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2293">[ date ]</a><a href="index.html#2293">[ thread ]</a><a href="subject.html#2293">[ subject ]</a><a href="author.html#2293">[ author ]</a>
<!-- next="start" -->
<li><a href="2294.html">[ Next ]</a><a href="2292.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2270.html">Nick Bostrom</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2338.html">Nick Bostrom</a>
</ul>
<!-- body="start" -->

<p>
<a name="2338qlink1">Nick Bostrom wrote:
<br>
<i>&gt; </i><br>
<a href="2270.html#2293qlink1">&gt; Eliezer S. Yudkowsky wrote:</a><br>
<i>&gt; </i><br>
<i>&gt; &gt; Your posthumans will find their own goals.  In any formal goal system</i><br>
<i>&gt; &gt; that uses first-order probabilistic logic, there are lines of logic that</i><br>
<i>&gt; &gt; will crank them out, totally independent of what goals they start with.</i><br>
<i>&gt; &gt; I'm not talking theory; I'm talking a specific formal result I've</i><br>
<i>&gt; &gt; produced by manipulating a formal system.</i><br>
<i>&gt; </i><br>
<i>&gt; What?!?</i><br>

<p>
Sigh... here we go again.  Take a look at "Coding a Transhuman AI";
there's a section in there about goal systems.  That's got the full
cognitive architecture.</a>

<p>
Let's say you're starting up a goal-reasoning system with a blank slate.


<OL>
  <li>  Push P&amp;~P pair:


</OL>
<p>
 P  = [Exist goal G: (G.value != 0)]; P.probability = Unknown$1
 ~P = [All goal G:  (G.value == 0)]; ~P.probability = 1 - Unknown$1

<p>
2.  Fork proposition and evaluate; ~P cancels out.
 In any particular instance, it is a trivial consequence of the branch
of reality containing ~P that all choices in that branch of reality have
equal valence.  Deducing the general rule takes reflexive reasoning, but
it is not strictly necessary to deduce the general rule.  The factor
Unknown$1, being present in all goals in the other branch of reality,
cancels out of the renormalized equation.

<p>
The present result is that the AI can always _act as if_ (not _assume_)
it is in the branch of reality containing the proposition P, or in other
words a goal G with Unknown$2 world-state and Unknown$3 != 0 value.

<p>
That's where the basic differential desirability comes from.  How that
differential expresses itself in particular choices depends on the
system knowledge base.  Obviously, some knowledge is needed before
choices can be formulated that apply to a specific world-state; I am
attempting to demonstrate that the knowledge required to describe
choices and solve subproblems will almost always bind supergoals to the
goal G.

<p>
A generalized AI with a knowledge base, abstract heuristics (and so on)
will be enough; such a generalized AI is almost certain to contain (or
formulate!) heuristics whose specification operates on generic goal
objects.  For example, "thinking about goal X is a positive subgoal Y of
X".  Well, then you have the positively-valued goal "think about G". 
Again, all without any initial goals whatsoever.  A trivial case, but it
demonstrates the problem.

<p>
Likewise, any initial goal stating "make humans happy" and containing
sufficient specification of "humans" and "happy", plus the implicit
knowledge "your programmers have added the assertion 'make humans
happy'", will be enough to generate independent Interim values for that
goal, probably but not necessarily positive, and almost certainly with
an at least slightly different set of relative values (priorities).

<p>
I don't see any way to have an AI that reasons reflexively and learns
from observation, without also permitting it to form heuristics that
operate on generic goals; once that happens, Interim goal values can
come into existence and conflict with any initially established goals.

<p>
<a href="2270.html#2293qlink2">&gt; Well, if we allow the SIs to have completely unfettered intellects,</a><br>
<i>&gt; then it should be all right with you if we require that they have</i><br>
<i>&gt; respect for human rights as a fundamental value, right? For if there</i><br>
<i>&gt; is some "objective morality" then they should discover that and</i><br>
<i>&gt; change their fundamental values despite the default values we have</i><br>
<i>&gt; given them. Would you be happy as long as we allow them full capacity</i><br>
<i>&gt; to think about moral issues and (once we think they are intelligent</i><br>
<i>&gt; enough not to make stupid mistakes) even allow them full control over</i><br>
<i>&gt; their internal structure and motivations (i.e. make them autopotent)?</i><br>

<p>
As a rational outcome of the debate, I'd be happy.  Strictly speaking,
I'd be a lot happier if you manipulated the heuristics and knowledge
base to get the Interim goals you wanted.  With initial goals, I'd worry
about the AI going insane - even over such a trivial issue as a priority
conflict between initial and Interim versions of the same goals!

<p>
<a href="2270.html#2293qlink3">&gt; As indicated, yes, we could allow them to change their goals (though</a><br>
<i>&gt; only after they are intelligent and knowledgeable enough that they</i><br>
<i>&gt; know precisely what they are doing -- just as you wouldn't allow a</i><br>
<i>&gt; small child to experiment with dangerous drugs).</i><br>

<p>
Certainly a simple, rational cost-of-failure model, with respect to
self-alteration (failure: system shutdown) and goal alteration (failure:
unbounded catastrophe), should suffice to keep them cautious until
superintelligence is reached and fallibility is no longer an issue. 
Again, this can be done entirely in Interim (consequence-of-knowledge)
goals rather than Arbitrary (imposed-at-startup) goals.

<p>
It may seem like a trivial distinction, but it's a very fundamental
difference in architecture.  You enforce Arbitrary goals with
special-case code and other coercions; you enforce Interim goals by
explaining benefits and failure scenarios to the AI.  You protect
Arbitrary goals by piling coercions and untouchable code sections and
monitoring code on top of coercions; you protect Interim goals by
explaining them in greater detail.
<pre>
-- 
        sentience@pobox.com         Eliezer S. Yudkowsky
         <a href="http://pobox.com/~sentience/AI_design.temp.html">http://pobox.com/~sentience/AI_design.temp.html</a>
          <a href="http://pobox.com/~sentience/sing_analysis.html">http://pobox.com/~sentience/sing_analysis.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2294.html">[ Next ]</a><a href="2292.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2270.html">Nick Bostrom</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2338.html">Nick Bostrom</a>
</ul>
</body></html>
