<!-- received="Sat Jan  2 14:09:33 1999 MDT" -->
<!-- sent="Sat, 2 Jan 1999 15:22:37 +0100 (CET)" -->
<!-- name="Eugene Leitl" -->
<!-- email="root@lrz.uni-muenchen.de" -->
<!-- subject="Re: Paths to Uploading (was: RE: clones/'perfect'-mates/self-image)" -->
<!-- id="Pine.LNX.3.96.990102132029.371A-100000@lrz.uni-muenchen.de" -->
<!-- inreplyto="001501be360e$4c364660$352501c0@mfg130" -->
<!-- version=1.10, linesinbody=132 -->
<html><head><title>extropians: Re: Paths to Uploading (was: RE: clones/'perfect'-mates/self-image)</title>
<meta name=author content="Eugene Leitl">
<link rel=author rev=made href="mailto:root@lrz.uni-muenchen.de" title ="Eugene Leitl">
</head><body>
<h1>Re: Paths to Uploading (was: RE: clones/'perfect'-mates/self-image)</h1>
Eugene Leitl (<i>root@lrz.uni-muenchen.de</i>)<br>
<i>Sat, 2 Jan 1999 15:22:37 +0100 (CET)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#36">[ date ]</a><a href="index.html#36">[ thread ]</a><a href="subject.html#36">[ subject ]</a><a href="author.html#36">[ author ]</a>
<!-- next="start" -->
<li><a href="0037.html">[ Next ]</a><a href="0035.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0026.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
On Fri, 1 Jan 1999, Billy Brown wrote:

<p>
<a href="0026.html#0036qlink1">&gt; You guys have some interesting thoughts about uploading, but I think we're</a><br>
<i>&gt; still talking past each other a bit when we get down to actual scenarios.  I</i><br>

<p>
I think this is only natural. At the end of the millennium, we have surely
a lot more hard data at our disposal than Lem &amp; Co-visionaries in 1960's,
but nevertheless this is still highly speculative terrain. If at all,
uploading is thought to become feasible in a few decades, where future
histories are already very warped by the curvature of the nearby
prediction horizont.

<p>
<a href="0026.html#0036qlink2">&gt; [premises of some assumptions self-contradictory?]</a><br>
<i>&gt; </i><br>
<i>&gt; To do an upload requires advanced computers, advanced sensors, and knowledge</i><br>
<i>&gt; about how the brain works.  To get a reasonably upload scenario you have to</i><br>
<i>&gt; project advances in all three of these fields at the same time, and see what</i><br>
<i>&gt; you come up with.</i><br>

<p>
Our ability to model dynamics of (macro)molecular systems at low to medium
energies far outstrips our capabilities to create, and, especially, to
mass-produce them.  Due to the bootstrap bottleneck and the intrinsic
simplicity of optimal hardware as artefacted by basic physical laws
constraints the design space could be already very well sampled prior to
the advent of the very first assembler. If analogies to compiler bootstrap
are valid, the second-generation assembler (in extreme case, only few
fabrication hours away) will be already very useful, and due to the
immediate availability of very formidable computational resources as
product of the second-generation assembler, the almost immediately (years
to months) avaliable third-generation systems should be truly optimal. Of
course heavy regulation (nanotechnology simulation software and assemblers
to be declared munitions, with a simultaneous implementation of an
executive strong enough to enforce that) could delay that, which may or
may not result in a prognosis from beneficial to the catastrophic. 

<p>
I doubt that new sensorics is at all necessary for a feasible destructive
scan: recently available methods as cryo AFM of freeze-fracture vitrified
tissue cryosections already allows imaging at near-molecular or molecular
resolution, and in principle, the technique should be scalable to imaging
in the bulk through introduction of abrasion, automation and massive
parallelism to attain adequate processivity. I think problems like
creation and tight integration of sufficiently dense and fast memories for
interim voxelset storage and algorithms for the processing of the latter
into the scanning pipeline are significantly more complicated.

<p>
The current state of the art of computational neurobiology is not exactly
negligeable, and due to the advent of sufficiently large computer
performance fully bottom-up automatical knowledge extraction should become
feasible, using 'ab initio' methods utilizing total genome sequence data,
accurate structures from protein folding prediction and abovementioned
molecular-resolution maps of vitrifed animal cells.  The same applies to
top-down approaches with multiple-million channels microelectrode arrays
for in vivo recording and manipulation, and sufficient computational
performance for their analysis as made possible with the advent of
molecular manufacturing of any flavour. 
 
<p>
<a href="0026.html#0036qlink3">&gt; Now, the traditional proof-of-principle for uploading is obviously never</a><br>
<i>&gt; going to actually be used.  It assumes no knowledge at all about how the</i><br>

<p>
Oh, perhaps the Cyberworm gang will eventually produce a killer demo good
enough to warrant further funding, in a really focused program. If it
wasn't for difficulties to patch-clamp the tiny critters, C. elegans is
the prime candidate for a POP. 

<p>
<a href="0026.html#0036qlink4">&gt; brain works, which results in enormous computation requirements.  Unless you</a><br>
<i>&gt; think the Omega hardware will be built tomorrow, and everyone in the biotech</i><br>

<p>
For the reasons I mentioned above, I do indeed think that the Omega
hardware will become avalable relatively early, i.e. in a few decades, if
things will indeed pan out as expected (but nobody never expects the
Spanish Inquisition, of course).

<p>
<a href="0026.html#0036qlink5">&gt; industry is about to jump off a cliff, that doesn't make sense.</a><br>

<p>
The whole of the humanity could jump off the cliff in a hard-edge
Singularity if somebody is foolish enough to create the boundary
conditions for a SI before we can do uploads on a broad scale. You need a
lot less ops and knowledge to grow an alife Golem with excellent
juggernaut potential. (Yes, Dr. Scott, an accident has made it happen).

<p>
<a href="0026.html#0036qlink6">&gt; A simulation at the cellular level, relying purely on advanced knowledge of</a><br>
<i>&gt; biochemistry, lets you reduce the computational burden by several orders of</i><br>
<i>&gt; magnitude.  It still isn't very likely, however, because it matches a modest</i><br>

<p>
I agree with you that such a model is extremely valuable, especially for
bootstrap purposes, as the lowest, or second-lowest tier in an
automagically progressive learning hierarchic simulation. (I wouldn't want
to define a framework for such a tour de force in software design though,
perhaps no one who goes on two legs could).

<p>
<a href="0026.html#0036qlink7">&gt; increase in medical knowledge with a fantastic improvement in computers and</a><br>
<i>&gt; sensor technology.</i><br>

<p>
For about 10-20 k$, using off-shelf commodity components, you could now
build a conventional MD system capable of probing the dynamics of
biological system roughly one million atoms large in a time window few ns
long. Even if lacking breakthrough to reduce the computational task for
PFP, forcefields will grow a little faster and great deal more accurate in
the coming decade or two, while Moore's law should not yet run into
saturation yet, particularly if development of molecular circuitry will
start early enough to become smoothly available when progress in
semiconductor photolitho will fail suddenly, having run out of steam.

<p>
The problem with imaging is less a sensor problem than a problem of
scaling existing technologies (nanorobotics/abrasive AFM, SNOM, vacuum
sublimation, excimer and plasma etch) into micro (MEMS) and meso
(diamondoid systems)  domain, using massive parallelism and automation. 
In a sense, the task is much tougher than simply coming up with new
sensors. 

<p>
<a href="0026.html#0036qlink8">&gt; A much more probable scenario would project medical advances forward until</a><br>
<i>&gt; there is hardware fast enough to run the sim, and sensors good enough to</i><br>
<i>&gt; gather the data.  That implies at least a moderately good understanding of</i><br>

<p>
The impetus for new imaging comes from basic research, some of which is of
course medical. Medical funding could increase dramatically once the
potential of nanomedicine is fully understood by the mainstream. Hardware
good and fast enough will come from the mainstream, probably specifically
from the multimedia demands, and then perhaps consumer and service
robotics, the next big thing after industry automation. 

<p>
<a href="0026.html#0036qlink9">&gt; the brain - something better than just an understanding of biochemistry, but</a><br>
<i>&gt; probably not good enough to just model the brain's data processing.</i><br>

<p>
I don't quite follow you here. If you can model the neural tissue in
machina, all you need is too watch the movies and to abstract. The process
of abstraction can be made automatical. What is the problem, then? 

<p>
ciao,
<br>
'gene
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0037.html">[ Next ]</a><a href="0035.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0026.html">Billy Brown</a>
<!-- nextthread="start" -->
</ul>
</body></html>
