<!-- received="Wed Feb 24 09:49:10 1999 MDT" -->
<!-- sent="Wed, 24 Feb 1999 12:45:09 +0100" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: FAQ Additions (Posthuman mind control)" -->
<!-- id="199902241647.IAA07440@geocities.com" -->
<!-- inreplyto="FAQ Additions (Posthuman mind control)" -->
<!-- version=1.10, linesinbody=46 -->
<html><head><title>extropians: Re: FAQ Additions (Posthuman mind control)</title>
<meta name=author content="den Otter">
<link rel=author rev=made href="mailto:neosapient@geocities.com" title ="den Otter">
</head><body>
<h1>Re: FAQ Additions (Posthuman mind control)</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Wed, 24 Feb 1999 12:45:09 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2253">[ date ]</a><a href="index.html#2253">[ thread ]</a><a href="subject.html#2253">[ subject ]</a><a href="author.html#2253">[ author ]</a>
<!-- next="start" -->
<li><a href="2254.html">[ Next ]</a><a href="2252.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2219.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2258.html">Nick Bostrom</a>
</ul>
<!-- body="start" -->



<hr>
<br>
<i>&gt; From: Billy Brown &lt;bbrown@conemsco.com&gt;</i><br>
<a href="2219.html#2253qlink1">&gt; I see two fatal criticisms of the idea suggested in the FAQ:</a><br>
<i>&gt; </i><br>
<i>&gt; First, it is mind control.  Remember, posthumans are by definition fully</i><br>
<i>&gt; sentient beings.  Programming them to abide by a preordained moral code is</i><br>
<i>&gt; no different than doing the same thing to our own children, or to each</i><br>
<i>&gt; other. I can see no possible way to justify such an action on moral</i><br>
<i>&gt; grounds.</i><br>

<p>
Depends on your morals, right? For example, a rational Egoist shouldn't 
have too much moral trouble with such a solution (of course, he may still
reject it on practical grounds, as the idea of controlling a posthuman
is questionable). In fact, the way we raise our kids and indoctrinate 
people to keep them from anti-social and criminal behaviour is a 
universally accepted and indeed necessary form of programming. 
So if you accept this, then there is no *moral* reason not to accept 
putting some basic moral rules into an AI. 
 
<p>
<a href="2219.html#2253qlink2">&gt; I suggest that this passage be amended to remove the advocation of mass mind</a><br>
<i>&gt; control.  Perhaps something like this:</i><br>
<i>&gt; </i><br>
<i>&gt;   In the first case, we could make sure that the first such entities possess</i><br>
<i>&gt;   a thorough understanding of, and respect for, existing human moral codes.</i><br>

<p>
<a name="2258qlink1">Better yet:

<p>
In the first case, we should make sure that such entities are *us*.</a> Creating
separate SIs (from AI) is a BIG mistake. We can certainly make sure
that the new entities thoroughly understand us, but that by no means
guarantees their respect for our moral codes.
 
<p>
<a href="2219.html#2253qlink3">&gt; That would be enough to ensure that they at least think the whole thing</a><br>
<i>&gt; through before adopting some completely alien viewpoint.  It is also the</i><br>
<i>&gt; strongest measure that I can see either a moral or a practical basis for.</i><br>

<p>
<a name="2281qlink1"><a name="2274qlink1">Morals are subjective, but the most practical (=rational) course of action
would be not to create AIs with SI potential and work on uploading instead.
Again, our prime directive should _always_ be survival. Survival is the
prerequisite for _all_ other actions. Any philosophy that does not value
personal survival [in an optimal state] above everything else is by definition
irrational. Thus follows that transhumanism (with an immortalist element)
is the best philosophy currently available to us.
</a>
</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2254.html">[ Next ]</a><a href="2252.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2219.html">Billy Brown</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="2258.html">Nick Bostrom</a>
</ul>
</body></html>
