<!-- received="Thu Feb 25 14:59:07 1999 MDT" -->
<!-- sent="25 Feb 99 22:54:51 +0100" -->
<!-- name="J. Eric Mortensen" -->
<!-- email="eric@episteme.no" -->
<!-- subject="Moral behavior of SIs" -->
<!-- id="199902252158.WAA29063@login-2.eunet.no" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=57 -->
<html><head><title>extropians: Moral behavior of SIs</title>
<meta name=author content="J. Eric Mortensen">
<link rel=author rev=made href="mailto:eric@episteme.no" title ="J. Eric Mortensen">
</head><body>
<h1>Moral behavior of SIs</h1>
J. Eric Mortensen (<i>eric@episteme.no</i>)<br>
<i>25 Feb 99 22:54:51 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2342">[ date ]</a><a href="index.html#2342">[ thread ]</a><a href="subject.html#2342">[ subject ]</a><a href="author.html#2342">[ author ]</a>
<!-- next="start" -->
<li><a href="2343.html">[ Next ]</a><a href="2341.html">[ Previous ]</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Here's an idea:

<p>
The short version: 
<br>
Assuming that SI's are similar to humans in kind of sentience,
they will possibly develop (among themselves) norms not too different from our norms
<br>
(do not kill, do not steal etc.). They will possibly also develop an empathy for other<br>
intelligent beings, given enough time of interaction with us. The way to instill the 
best moral valus (both seen from their own perspective and ours) in SIs, might then
be to have them "genetically" evolve cooperative (tit-for-tat) strategies which will incline
them to be nice to us (and each other). Such evolution might constitute the pre-programming
of each SI. An SI will, when born,  be the product of a simulated evolution, which will in theory
give him or her the guiding intuition/instinct that he/she needs to live a good life.
Whether or not he fully comprehends his own behavior is not important, in the same way it 
hasn't been very important for civilization to grow that we humans know why murder is bad. (We 
just know that if we kill our neighbor it's more than likely that his friends and family will kill us.)

<p>
A longer version:
<br>
Resarch  in cooperation theory (Axelrod etc.) indicates that being nice to others
<br>
(following a variant of the tit-for-tat-strategy) is a very likely strategy to emerge<br>
in a population of interacting beings that each are in a prisoner's dilemma 
toward the others. Human norms and culture (and to an extent its implication, 
human morality) can be thought of as the result of such developments.
True, humans are not perfect and they can
be brutal towards one another, but I still know of no society where murdering your friendly 
neighbor is not in some way punished. Ditto for theft, kidnapping etc.

<p>
If SI's are sentient in a way not to dissimilar to the way humans are sentient, I think it's
reasonable to think that superintelligent beings will develop much the same norms as
humans wrt how to behave toward each others. After all, if they are self-reflective, rational,
optimizing, moderately egoist, empathizing individuals, they will see each other much the
same way we humans see each others, as members of a fellow race, with moderately 
predictable behavior with whom they can have valuable relationships, but also with whom they
can have costly conflicts if not checked carefully. 

<p>
Furthermore, it seems that the more we humans remove ourselves from our brutish origins the 
more we seem to develop empathy towards beings of other (animal) races. 
Witness how the question of animal rights is becoming more and more important for every 
decade. As long as SI's are not overwhelmingly powerful or different (so as to make 
communication difficult) compared to humans, they should also see us as valuable social 
partners. Among all the conceivable strategies that SI's and humans might play against each 
others, the fact that we are fairly similar (by assumption) and the fact that we will tend to be 
social to members of our own group, should lead to both races discovering mutually optimal
strategies, i.e., cooperation.

<p>
<a name="2351qlink1">I'm probably too anthropomorphic in my thinking, but the basic idea</a> is that if human behavior is 
determined more by evolution of adapted strategies than rational optimization, and if SI's are preprogrammed to behave in much the same way, the outcome should be that SIs will develop
the wanted moral values (be nice to others etc.).

<p>
Any comments?

<p>
Regards,
<br>
Eric
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2343.html">[ Next ]</a><a href="2341.html">[ Previous ]</a>
<!-- nextthread="start" -->
</ul>
</body></html>
