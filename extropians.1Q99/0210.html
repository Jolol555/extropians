<!-- received="Thu Jan  7 11:40:22 1999 MDT" -->
<!-- sent="07 Jan 1999 19:40:16 +0100" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: Paths to Uploading" -->
<!-- id="b4990ff6i5r.fsf@void.nada.kth.se" -->
<!-- inreplyto="Wed, 06 Jan 1999 13:20:28 -0600" -->
<!-- version=1.10, linesinbody=172 -->
<html><head><title>extropians: Re: Paths to Uploading</title>
<meta name=author content="Anders Sandberg">
<link rel=author rev=made href="mailto:asa@nada.kth.se" title ="Anders Sandberg">
</head><body>
<h1>Re: Paths to Uploading</h1>
Anders Sandberg (<i>asa@nada.kth.se</i>)<br>
<i>07 Jan 1999 19:40:16 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#210">[ date ]</a><a href="index.html#210">[ thread ]</a><a href="subject.html#210">[ subject ]</a><a href="author.html#210">[ author ]</a>
<!-- next="start" -->
<li><a href="0211.html">[ Next ]</a><a href="0209.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0160.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
"Eliezer S. Yudkowsky" &lt;sentience@pobox.com&gt; writes:

<p>
<a href="0160.html#0210qlink1">&gt; Anders Sandberg wrote:</a><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; OK, this is the standard SI apotheosis scenario. But note that it is</i><br>
<i>&gt; &gt; based on a lot of unsaid assumptions: that it is just hardware</i><br>
<i>&gt; &gt; resources that distinguish a human level AI from an SI (i.e, the</i><br>
<i>&gt; &gt; software development is fairly trivial for the AI and can be done very</i><br>
<i>&gt; </i><br>
<i>&gt; That's not what Billy Brown is assuming.  He's assuming software</i><br>
<i>&gt; development is _harder_ than hardware development, so that by the time</i><br>
<i>&gt; we have a working seed AI the available processing power is considerably</i><br>
<i>&gt; larger than human-equivalent.  I happen to agree with this, by the way.</i><br>

<p>
Yes, that is not unlikely. But note that this is a problem for the
scenario, since if you assume that software development is hard, then
the AI will have a hard problem to solve (obviously on the order of
many man-years with a diverse skillset and diverse outlooks)

<p>
<a href="0160.html#0210qlink2">&gt; &gt; fast, and adding more processor power will make the AI *smarter*),</a><br>
<i>&gt; </i><br>
<i>&gt; I'll speak for this one.  If we could figure out how to add neurons to</i><br>
<i>&gt; the human brain and integrate them, people would get smarter.</i><br>

<p>
&lt;ROFL!&gt; Sorry, but I disagree as a neuroscientist. The number of
neurons obviously place a ceiling on information content in our
brains, but it is the connection structure that makes us
smart. "Integrating" them is the problem, since I assume what you are
meaning is equivalent to "connecting them in the *right* way" instead
of just connecting them randomly (the later is quite possible; brain
transplants are being researched, and there is a gene in the mouse
that causes enormous brain growth - I wonder if they called it
Algernon? :-)

<p>
<a href="0160.html#0210qlink3">&gt; (I deduce</a><br>
<i>&gt; this from the existence of Specialists; adding cognitive resources does</i><br>
<i>&gt; produce an improvement.)</i><br>

<p>
Does it? In what tasks? In what ways? What resources? This is not so simple.

<p>
<i>&gt;  Similarly, I expect that substantial amounts</i><br>
<a href="0160.html#0210qlink4">&gt; of the AI will be working on sufficiently guided search trees that</a><br>
<i>&gt; additional processing power can produce results of substantially higher</i><br>
<i>&gt; quality, without requiring exponential amounts of power.</i><br>

<p>
What about combinatorical explosions? 

<p>
<a href="0160.html#0210qlink5">&gt; &gt; that this process has a time constant shorter than days (why just this</a><br>
<i>&gt; &gt; figure? why not milliseconds or centuries?),</i><br>
<i>&gt; </i><br>
<i>&gt; Actually, I think milliseconds.  I just say "hours or days" because</i><br>
<i>&gt; otherwise my argument gets tagged as hyperbole by anyone who lives on</i><br>
<i>&gt; the human timescale.</i><br>

<p>
Actually, I doubt milliseconds because of the hardware limitations
even on nanocomputers, unless it turns out that self-enhancement is
well suited for massive parallelism without too much internal
communication.
 
<p>
<a href="0160.html#0210qlink6">&gt; Centuries is equally plausible.  It's called a "bottleneck".</a><br>

<p>
Hmm, it took mankind hundreds of thousands of years to go from "Oook!"
to Linux, and he calls a puny century a bottleneck... :-)

<p>
<a href="0160.html#0210qlink7">&gt; &gt; that there will be no</a><br>
<i>&gt; &gt; systems able to interfere with it - note that one of your original</i><br>
<i>&gt; &gt; assumptions was the existence of human-level AI; if AI can get faster</i><br>
<i>&gt; &gt; (not even smarter) by adding more processor power, "tame" AI could</i><br>
<i>&gt; &gt; keep the growing AI under control</i><br>
<i>&gt; </i><br>
<i>&gt; 1)  This is an Asimov Law.  That trick never works.</i><br>
<i>&gt; 2)  I bet your tame AI has to be smarter than whatever it's keeping</i><br>
<i>&gt; under control.  Halting problem...</i><br>

<p>
As the other response indicates, the watchdogs do not need to be
asimovs to do their job. You can have loyal, barking and biting
programs defending your system without having to worry about them
starting to discuss abolition. 

<p>
And yes, an intelligent invader will still have a problem with the
security measures since it will likely (if it is designed well) not
have enough information about them, they have the advantages of a home
game and they can protect their system by cutting the net connection
if things turn out bad.

<p>
<a href="0160.html#0210qlink8">&gt; &gt; - that this SI is able to invent</a><br>
<i>&gt; &gt; anything it needs to (where does it get the skills?)</i><br>
<i>&gt; </i><br>
<a name="0227qlink1"><i>&gt; If you dropped back a hundred thousand years, how long would it take you</i><br>
<i>&gt; to out-invent hunters who had been using spears all their life?  Skill</i><br>
<i>&gt; is a poor substitute for smartness.</a></i><br>

<p>
<a name="0227qlink2">I wonder how well I could build all those waterwheels, metal melting,
steam engines and Volta cells. Have you tried to recreate technology?
And the interesting thing in this example is that in the end it hinges
not on me being a super-genius, but on me knowing things already (and
then needing to somehow implement them, which is the hard part!).</a> 

<p>
<a name="0227qlink3">It would be interesting to drop you off on an isolated island together
with a randomly selected but stupid survivalist. Would your superior
intellect bring you more food?</a> 

<p>
<a name="0227qlink4">I would rather say smartness is a poor substitute for skill, which is
why we tend to rely on learned skills rather than problem solving for
most tasks we do.</a>

<p>
<a name="0227qlink5"><a href="0160.html#0210qlink9">&gt; And if it's stumped, it can get the answers off the Internet, just like</a><br>
<i>&gt; I do.</a></i><br>

<p>
<a name="0227qlink6">OK, here is a question: how do I design a good robot body? Assume no
prior knowledge beyond a common sense database about the world,
including no experience with being physical, no scientific education,
no engineering education.</a>


<p>
<a href="0160.html#0210qlink10">&gt; &gt; why are you</a><br>
<i>&gt; &gt; assuming the AI is able to hack any system,</i><br>
<i>&gt; </i><br>
<i>&gt; There are bloody _humans_ who can hack any system.</i><br>

<p>
*Any* system?

<p>
<a href="0160.html#0210qlink11">&gt; &gt; especially given the</a><br>
<i>&gt; &gt; presence of other AI?).</i><br>
<i>&gt; </i><br>
<i>&gt; In a supersaturated solution, the first crystal wins.</i><br>

<p>
But in a solution filled with small crystals, none can grow. 

<p>
<a href="0160.html#0210qlink12">&gt; If the intelligence is roughly human-equivalent, then there will be</a><br>
<i>&gt; specialties at which it excels and gaping blind spots.  If the</i><br>
<i>&gt; intelligence is far transhuman, it will still have specialties and blind</i><br>
<i>&gt; spots, but not that we can perceive.</i><br>

<p>
Why assume we can't see them? There doesn't seem to be any reason for
one way or another.

<p>
&gt; So yes, I make that assumption: <br>
<a href="0160.html#0210qlink13">&gt; An SI will be able to outwit any human in all respects. </a><br>

<p>
Why do you make this assumption? What evidence or arguments do you have?

<p>
<a href="0160.html#0210qlink14">&gt; &gt; As you can tell, I don't quite buy this scenario. To me, it sounds</a><br>
<i>&gt; &gt; more like a Hollywood meme.</i><br>
<i>&gt; </i><br>
<i>&gt; Not really.  Hollywood is assuming that conflicts occur on a humanly</i><br>
<i>&gt; understandable level - not to mention that the hero always wins. </i><br>

<p>
You mean like in "The Lawnmover Man"?

<p>
<a href="0160.html#0210qlink15">&gt; The forces involved in a Singularity are Vast.  I don't know how the</a><br>
<i>&gt; forces will work out, but I do predict that the result will be extreme</i><br>
<i>&gt; (from our perspective), and not much influenced by our actions or by</i><br>
<i>&gt; initial conditions.  There won't be the kind of balance we evolved in. </i><br>
<i>&gt; Too much positive feedback, not enough negative feedback.</i><br>

<p>
I agree with the word Vast. But remember, positive feedback
*amplifies* small differences, which means that the result is
influenced by our every action - the problem is whether it is chaotic
or just exponential. In the first case, we cannot predict anything. In
the later case, we can try to aim in a direction and hope things go
right.

<p>
Personally I prefer setting up the feedback loops instead. Why bet on
the outcome when you can try to write the rules?

<pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
asa@nada.kth.se                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0211.html">[ Next ]</a><a href="0209.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0160.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
