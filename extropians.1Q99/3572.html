<!-- received="Thu Mar 25 07:34:27 1999 MDT" -->
<!-- sent="Thu, 25 Mar 1999 15:32:22 +0100" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: Yudkowsky's AI (again)" -->
<!-- id="199903251434.GAA08117@geocities.com" -->
<!-- inreplyto="Yudkowsky's AI (again)" -->
<!-- version=1.10, linesinbody=79 -->
<html><head><title>extropians: Re: Yudkowsky's AI (again)</title>
<meta name=author content="den Otter">
<link rel=author rev=made href="mailto:neosapient@geocities.com" title ="den Otter">
</head><body>
<h1>Re: Yudkowsky's AI (again)</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Thu, 25 Mar 1999 15:32:22 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3572">[ date ]</a><a href="index.html#3572">[ thread ]</a><a href="subject.html#3572">[ subject ]</a><a href="author.html#3572">[ author ]</a>
<!-- next="start" -->
<li><a href="3573.html">[ Next ]</a><a href="3571.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3561.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->



<hr>
<br>
<a name="3579qlink1"><a href="1669.html#3572qlink1">&gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</a><br>
</a>

<p>
<a href="3561.html#3572qlink2">&gt; The whole altruistic argument is intended as a supplement to the basic</a><br>
<i>&gt; and very practical theory of the Singularity:  If we don't get some kind</i><br>
<i>&gt; of transhuman intelligence around *real soon*, we're dead meat.</i><br>

<p>
<a name="3582qlink1">Not necessarily. Not all of us anyway.</a>
 
<p>
<a href="3561.html#3572qlink3">&gt; My current estimate, as of right now, is that humanity has no more than</a><br>
<i>&gt; a 30% chance of making it, probably less.  The most realistic estimate</i><br>
<i>&gt; for a seed AI transcendence is 2020; nanowar, before 2015.  The most</i><br>
<i>&gt; optimistic estimate for project Elisson would be 2006; the earliest</i><br>
<i>&gt; nanowar, 2003.</i><br>

<p>
<a name="3814qlink3"><a name="3582qlink2">Conclusion: we need a (space) vehicle that can move us out of harm's 
way when the trouble starts. Of course it must also be able to
sustain you for at least 10 years or so. A basic colonization of
Mars immediately comes to mind.</a> Perhaps a scaled-up version
of Zubrin's Mars Direct plan. Research aimed at uploading must 
continue at full speed of course while going to, and living on, Mars
(or another extra-terrestrial location).</a>


<p>
<a name="3582qlink3">Btw, you tend overestimate the dangers of nanotech and
conventional warfare (fairly dumb tech in the hands of fairly dumb
people), while underestimating the threat of Powers (intelligence
beyond our wildest dreams). God vs monkeys with fancy toys.</a>

<p>
<a href="3561.html#3572qlink4">&gt; So we have a chance, but do you see why I'm not being picky about what</a><br>
<i>&gt; kind of Singularity I'll accept?</i><br>

<p>
No. Only a very specific kind of Singularity (the kind where you personally
transcend) is acceptable. I'd rather have no Singularity than one where
I'm placed at the mercy of posthuman Gods (I think all Libertarians,
anarchists, individualists and other freedom-loving individuals will have
to agree here).
 
<p>
<a href="3561.html#3572qlink5">&gt; The point is - are you so utterly, absolutely, unflinchingly certain</a><br>
<i>&gt; that (1) morality is subjective </i><br>

<p>
Probably, but who cares? Whether it's objective or subjective, seeking
to live (indefinitely) and prosper is *always* a good decision (if only
because it buys you time to consider philosophical issues such as the
one above). If "objective morality" tells me to die, it can go and kiss
my ass.

<p>
(2) your morality is correct 

<p>
Maybe(?) not perfect, but certainly good enough. 

<p>
(3)
<br>
<a href="3561.html#3572qlink6">&gt; AI-based Powers would kill you and </a><br>
(4) human Powers would be your
<br>
<a href="3561.html#3572qlink7">&gt; friends - that you would try to deliberately avoid an AI-based Singularity?</a><br>

<p>
<a name="3582qlink4">Any kind of Power which isn't you is an unaccepable threat, 
because it is completely unpredictable from the human pov.
You are 100% at its mercy, as you would be if God existed. 
So, both versions are undesirable.</a>

<p>
<a href="3561.html#3572qlink8">&gt; It will take *incredibly* sophisticated nanotechnology before a human</a><br>
<i>&gt; can become the first Power - *far* beyond that needed for one guy to</i><br>
<i>&gt; destroy the world.  </i><br>

<p>
Hence, to space, asap svp.

<p>
<a name="3579qlink2"><i>&gt; (Earliest estimate:  2025.  Most realistic:  2040.) </i><br>
<a href="3561.html#3572qlink9">&gt; We're running close enough to the edge as it is.  It is by no means</a><br>
<i>&gt; certain that the AI Powers will be any more hostile or less friendly</i><br>
<i>&gt; than the human ones.  I really don't think we can afford to be choosy.</i><br>

<p>
<a name="3582qlink5">We _must_ be choosy. IMHO, a rational person will delay the Singularity
at (almost?) any cost until he can transcend himself.
</a>
</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3573.html">[ Next ]</a><a href="3571.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3561.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
