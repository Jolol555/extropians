<!-- received="Wed Mar  3 10:51:37 1999 MDT" -->
<!-- sent="Wed, 3 Mar 1999 17:48:51 +0000" -->
<!-- name="Nick Bostrom" -->
<!-- email="bostrom@ndirect.co.uk" -->
<!-- subject="Re: Posthuman mind control (was RE: FAQ Additions)" -->
<!-- id="199903031751.RAA31248@sioux.hosts.netdirect.net.uk" -->
<!-- inreplyto="36DB8CB1.94EB0CD6@pobox.com" -->
<!-- version=1.10, linesinbody=100 -->
<html><head><title>extropians: Re: Posthuman mind control (was RE: FAQ Additions)</title>
<meta name=author content="Nick Bostrom">
<link rel=author rev=made href="mailto:bostrom@ndirect.co.uk" title ="Nick Bostrom">
</head><body>
<h1>Re: Posthuman mind control (was RE: FAQ Additions)</h1>
Nick Bostrom (<i>bostrom@ndirect.co.uk</i>)<br>
<i>Wed, 3 Mar 1999 17:48:51 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2754">[ date ]</a><a href="index.html#2754">[ thread ]</a><a href="subject.html#2754">[ subject ]</a><a href="author.html#2754">[ author ]</a>
<!-- next="start" -->
<li><a href="2755.html">[ Next ]</a><a href="2753.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2688.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky wrpte:

<p>
<a href="2688.html#2754qlink1">&gt; Nick Bostrom wrote:</a><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; My answer to this is a superposition of three points: (1) I</i><br>
<i>&gt; &gt; explicitly allowed that fundamental values could change, only (except</i><br>
<i>&gt; &gt; for the mind-scan) the change wouldn't be rationally brought about.</i><br>
<i>&gt; &gt; For example, at puberty peoples' values may change, but it's not</i><br>
<i>&gt; &gt; because of a rational choice they made.</i><br>
<i>&gt; </i><br>
<a name="2793qlink1"><i>&gt; Are you arguing that, say, someone who was brought up as a New Age</i><br>
<i>&gt; believer and switches to being an agnostic is not making a rational choice?</i><br>

<p>
Believing in the healing powers of crystals is not a value, it's a 
mistaken factual opinion. The New Ager, provided he has the same data 
as we do, will be rational to give up his New Agey beliefs.</a>

<p>
<a href="2688.html#2754qlink2">&gt; &gt; (2) Just because somebody</a><br>
<i>&gt; &gt; calls a certain value fundamental doesn't mean it actually is</i><br>
<i>&gt; &gt; fundamental.</i><br>
<i>&gt; </i><br>
<i>&gt; So fundamental values are... whatever values don't change?</i><br>

<p>
Not exactly. They can change in a non-rational way (i.e. not as a 
result of a well-informed deliberate choice) or in a rational way 
(for example in the mind-scan scenario).

<p>
<a href="2688.html#2754qlink3">&gt;Please</a><br>
<i>&gt; clarify by defining the cognitive elements constituting "fundamental</i><br>
<i>&gt; values".  I make the following assertion:  "There are no cognitive</i><br>
<i>&gt; elements which are both invariant and given the highest priority in</i><br>
<i>&gt; choosing goals."</i><br>

<p>
<a name="2793qlink2">It's hard to give a precise definition of fundamental value, just as 
it is hard to give a precise definition of what it means to believe 
</a>in a proposition.<a name="2793qlink3"> But let me try to explain by giving a simplified 
example. Suppose RatioBot is a robot that moves aroung in a finite
two-dimensional universe (a computer screen). RatioBot contains two 
components: (1) a long list, where each line contains a description 
of a possible state of the universe together with a real number (that 
state's "value");</a>  (2) a module that simulates all possible sequences 
of actions the RatioBot can make, 10 moves ahead. The simulated state 
of the world is compared to the list, and RatioBot then performs the 
sequence of actions leading to the state with the highest achievable 
value.

<p>
Imagine RatioBotII, which is slightly smarter than RatioBot. In order 
to use work more efficiently, RatioBotII has a third 
module, that tries to plan ahead. It looks at possibilities more than 
10 moves ahead, and figures out suitable subgoals, that module (2) 
then tries to approximate. The goals that module (3) gives to module 
(2) are intermediary goals, and they represent one type of 
non-fundamental values (emotions would be another type). On the other 
hand, the values expressed by the list (1) could be said to be 
fundamental.

<p>
<a href="2688.html#2754qlink4">&gt; Another question:  What are *your* "fundamental values" and at what age</a><br>
<i>&gt; did you discover them?</i><br>

<p>
I don't know (in the reflective, abstract sense) exactly what my 
fundamental values are; the human mind is, as we know, far from 
transparent to itself. To the extent that I do know them, I have 
discovered them gradually. My values may also have changed somewhat 
on some occasions, mostly in a non-rational (not "irrational") way.

<p>
<a name="2793qlink4">I think I know approximately what my fundamental values are: I want 
everybody to have the chance to prosper, to be healty and happy, to 
develop and mature, and to live as long as they want in a physically 
youthful and vigorous state, free to experience states of 
consciousness deeper, clearer and more sublime and blissful than 
anything heard of before; to transform themselves into new kinds of 
entities and to explore new real and artificial realities, equipt 
with intellects incommensurably more encompassing than any human 
brain, and with much richer emotional sensibilities. I want very much 
that everybody or as many as possible get a chance to do this. 
However, if I absolutely had to make a choice I would rather give 
this to my friends and those I love (and myself of course) than to 
people I haven't met, and I would (other things equal) prefer to give 
it to people now existing than only to potential future people.</a>


<p>
<a href="2688.html#2754qlink5">&gt; &gt; (3) With</a><br>
<i>&gt; &gt; imperfectly rational beings (such as humans) their might be conflicts</i><br>
<i>&gt; &gt; between what they think are their fundamental values. When they</i><br>
<i>&gt; &gt; discover that that is the case, they have to redefine their</i><br>
<i>&gt; &gt; fundamental values as the preferred weighted sum of the conflicting</i><br>
<i>&gt; &gt; values (which thereby turned out not to be truely fundamental after</i><br>
<i>&gt; &gt; all).</i><br>
<i>&gt; </i><br>
<i>&gt; Why wouldn't this happen to one of your AIs?</i><br>

<p>
<a name="2793qlink5">With human-level AIs, unless they have a very clear and unambigous 
value-structure, it could perhaps happen. That's why we need to be on 
our guard against unexpected consequences.</a>

<p>
Nick Bostrom
<br>
<a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a>      n.bostrom@lse.ac.uk
Department of Philosophy, Logic and Scientific Method
London School of Economics
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2755.html">[ Next ]</a><a href="2753.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2688.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
