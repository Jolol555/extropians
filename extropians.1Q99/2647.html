<!-- received="Mon Mar  1 18:21:21 1999 MDT" -->
<!-- sent="Tue, 2 Mar 1999 01:18:37 +0000" -->
<!-- name="Nick Bostrom" -->
<!-- email="bostrom@ndirect.co.uk" -->
<!-- subject="Re: Posthuman mind control (was RE: FAQ Additions)" -->
<!-- id="199903020121.BAA24047@sioux.hosts.netdirect.net.uk" -->
<!-- inreplyto="36D9CD7F.928B9F4@pobox.com" -->
<!-- version=1.10, linesinbody=71 -->
<html><head><title>extropians: Re: Posthuman mind control (was RE: FAQ Additions)</title>
<meta name=author content="Nick Bostrom">
<link rel=author rev=made href="mailto:bostrom@ndirect.co.uk" title ="Nick Bostrom">
</head><body>
<h1>Re: Posthuman mind control (was RE: FAQ Additions)</h1>
Nick Bostrom (<i>bostrom@ndirect.co.uk</i>)<br>
<i>Tue, 2 Mar 1999 01:18:37 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2647">[ date ]</a><a href="index.html#2647">[ thread ]</a><a href="subject.html#2647">[ subject ]</a><a href="author.html#2647">[ author ]</a>
<!-- next="start" -->
<li><a href="2648.html">[ Next ]</a><a href="2646.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2533.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Eliezer S. Yudkowsky wrote:


<p>
<a href="2533.html#2647qlink1">&gt; &gt; But I don't think we deliberately change our fundamental values.</a><br>
<i>&gt; &gt; Non-fundamental values we may change, and the criteria are then our</i><br>
<i>&gt; &gt; more fundamental values. Fundamental values can change too, but they</i><br>
<i>&gt; &gt; are not deliberately (rationally) changed (except in the mind-scan</i><br>
<i>&gt; &gt; situation I mentioned in an earlier messege).</i><br>
<i>&gt; </i><br>
<i>&gt; Well, either I've misunderstood you, or you're simply wrong.  We humans</i><br>
<i>&gt; switch fundamental values all the time.  It happens every time someone</i><br>
<i>&gt; changes a religion.  If you're going to argue that these weren't the</i><br>
<i>&gt; true "fundamental" values, then the AI's "make people happy" won't be a</i><br>
<i>&gt; fundamental value either.</i><br>

<p>
<a name="2688qlink1">My answer to this is a superposition of three points: (1) I 
explicitly allowed that fundamental values could change, only (except 
for the mind-scan) the change wouldn't be rationally brought about. 
For example, at puberty peoples' values may change, but it's not 
because of a rational choice they made.</a> (2) Just because somebody 
<a name="2688qlink2">calls a certain value fundamental doesn't mean it actually is 
</a>fundamental. Especially ideological values are, for humans, often 
better described as their official policies rather than their true 
driving force. Otherwise everybody would be perfectly willing to 
suffer torment and death for the sake of their professed "fundamental 
values"; but since they don't, those values are either not 
fundamental or are just part of their fundamental values. (3) With 
<a name="2688qlink3">imperfectly rational beings (such as humans) their might be conflicts 
between what they think are their fundamental values. When they 
discover that that is the case, they have to redefine their 
fundamental values as the preferred weighted sum of the conflicting 
values (which thereby turned out not to be truely fundamental after 
</a>all).

<p>
<a href="2533.html#2647qlink2">&gt; My fundamental values have changed from "eat and sleep and survive" to</a><br>
<i>&gt; "serve humanity" to "bring about a Singularity" to "do what is right",</i><br>
<i>&gt; where I presently reside.</i><br>
<i>&gt; </i><br>
<i>&gt; I think that it will be a considerable amount of time before an AI is</i><br>
<i>&gt; pressed by logic to change its fundamental values from "do what is</i><br>
<i>&gt; right".  But anything more specific certainly isn't a fundamental value.</i><br>

<p>
<a name="2688qlink4">"Do what is right" sounds almost like "Do what is the best thing to 
do", which is entirely vacuous.</a>

<p>
<a href="2533.html#2647qlink3">&gt; &gt; That depends. If selection pressures lead to the evolution of AIs</a><br>
<i>&gt; </i><br>
<i>&gt; What selection pressures?  Who'd be dumb enough to create an AI wanting</i><br>
<i>&gt; to survive and reproduce, and, above all, *compete* with its children? </i><br>

<p>
<a name="2688qlink5">I suspect there would be many humans who would do exactly that. Even 
if none did, such a mindset could still evolve if there were 
heritable variation.</a>

 
<p>
<a href="2533.html#2647qlink4">&gt; &gt; with selfish values that are indifferent to human welfare, and the</a><br>
<i>&gt; &gt; AIs as a result go about annihilating the human species and stealing</i><br>
<i>&gt; &gt; our resources, then I would say emphatically NO, we have a right to</i><br>
<i>&gt; &gt; expect more.</i><br>
<i>&gt; </i><br>
<i>&gt; Absolutely.  I do not intend to let humanity be wiped out by a bunch of</i><br>
<i>&gt; selfish, badly programmed &lt;Hs</i><br>

<p>
<a name="2688qlink6">I'm glad to hear that. But do you hold the same if we flip the 
inequality sign? I don't want to be wiped out by &gt;Hs either.
</a>

<p>
Nick Bostrom
<br>
<a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a>      n.bostrom@lse.ac.uk
Department of Philosophy, Logic and Scientific Method
London School of Economics
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="2648.html">[ Next ]</a><a href="2646.html">[ Previous ]</a>
<b>In reply to:</b> <a href="2533.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
