<!-- received="Tue Jan 12 11:40:23 1999 MDT" -->
<!-- sent="Tue, 12 Jan 1999 07:44:44 -0600" -->
<!-- name="Scott Badger" -->
<!-- email="wbadger@psyberlink.net" -->
<!-- subject="Re: Singularity Mind-Benders" -->
<!-- id="004b01be3e31$dca25ba0$d89112cf@wbadger" -->
<!-- inreplyto="Singularity Mind-Benders" -->
<!-- version=1.10, linesinbody=75 -->
<html><head><title>extropians: Re: Singularity Mind-Benders</title>
<meta name=author content="Scott Badger">
<link rel=author rev=made href="mailto:wbadger@psyberlink.net" title ="Scott Badger">
</head><body>
<h1>Re: Singularity Mind-Benders</h1>
Scott Badger (<i>wbadger@psyberlink.net</i>)<br>
<i>Tue, 12 Jan 1999 07:44:44 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#499">[ date ]</a><a href="index.html#499">[ thread ]</a><a href="subject.html#499">[ subject ]</a><a href="author.html#499">[ author ]</a>
<!-- next="start" -->
<li><a href="0500.html">[ Next ]</a><a href="0498.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0457.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
<a href="0467.html#0499qlink1">&gt; From: Eliezer S. Yudkowsky &lt;sentience@pobox.com&gt;</a><br>
<i>&gt; Fun Problems for Singularitarians:</i><br>
<i>&gt;</i><br>
<i>&gt; ==</i><br>
<i>&gt;</i><br>
<i>&gt; If mortal life is totally meaningless, it would be logical to</i><br>
<i>&gt; exterminate them for their spare atoms.  Mortals, knowing this, will</i><br>
<i>&gt; refuse to create Singularities.  If mortals could bargain with the</i><br>
<i>&gt; Singularity, it would obviously be to the Singularity's advantage to set</i><br>
<i>&gt; aside a quadrillionth of computing power for Permutation-City-style</i><br>
<i>&gt; accomodations, in return for existing at all.  But we can't bargain with</i><br>
<i>&gt; the Singularity until after we've created it and our hold is gone.</i><br>
<i>&gt; Bearing this in mind, how can you bind the Singularity to the bargain?</i><br>
<i>&gt; What is the Singularity's logical course of action?</i><br>


<p>
Sounds to me like your premise is strained.  A completely
logical SI would recognize that it's own existence has no
more "meaning" than does human existence.  It would have
to self-destruct if it determined that this was a sufficient
criterion for extermination.  IMO, there's no such thing as
meaning.  There's only relevance.  And if it came to the
point where our existence became irrelevant to the SI,
why would it logically follow that it would then go out of
it's way to exterminate us?


<p>
<a href="0457.html#0499qlink2">&gt; Bonus question one:  Suppose you have a time machine that can ONLY</a><br>
<i>&gt; convey the information as to whether or not the Singularity will happen.</i><br>
<i>&gt;  If you change your plans as a result of the indicator, it resends.</i><br>
<i>&gt; This is one bit of information and thus can be modulated to convey</i><br>
<i>&gt; messages from the future.  How do you negotiate?</i><br>


<p>
This riddle is too fuzzy to me.  Sorry.

<p>
<a href="0457.html#0499qlink3">&gt; Bonus question two:  In both cases above, it is necessary to plausibly</a><br>
<i>&gt; threaten not to create a Singularity.  The only other option, in the</i><br>
<i>&gt; long run, is exterminating the human race.  This has to be able to</i><br>
<i>&gt; plausibly happen, either as a logical consequence or as an alternate</i><br>
<i>&gt; future.  How do you force yourself to destroy the Earth?</i><br>
Dunno, still working on my logical skills
<br>
<i>&gt;</i><br>
<i>&gt;</i><br>
<a href="0457.html#0499qlink4">&gt; If a Singularity is a good thing, why haven't earlier Singularities sent</a><br>
<i>&gt; robot probes to help it happen?  If SIs commit suicide, why isn't the</i><br>
<i>&gt; whole Universe full of mortals?</i><br>


<p>
I agree with Justin Jones who succinctly said:

<p>
"I dont care if people say there is a certain probability of other
intelligent life out there, I see no evidence of past singularities or other
civilizations so I don't assume there have been any."

<p>
<a href="0457.html#0499qlink5">&gt; How can you fight your future self, who automatically knows all of your</a><br>
<i>&gt; plans, including the ones you're making right now?  What if the future</i><br>
<i>&gt; self is a transhuman?</i><br>


<p>
You enlist the aid of comrades whose actions will be unknowable by
your future self.  If my future self is a transhuman . . . I'll leave him
alone!  Whatever he's doing will be in my best interest regardless
of how it appears.  :-)

<p>
<a href="0457.html#0499qlink6">&gt; Is there any way to oppose a Power running a simulation of you?</a><br>


<p>
Isn't there a sci-fi novel about this?  "Permutation City" perhaps?



<p>
Scott Badger
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0500.html">[ Next ]</a><a href="0498.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0457.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
</ul>
</body></html>
