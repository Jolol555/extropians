<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Otter vs. Yudkowsky</TITLE>
<META NAME="Author" CONTENT="sayke (sayke@gmx.net)">
<META NAME="Subject" CONTENT="Re: Otter vs. Yudkowsky">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Otter vs. Yudkowsky</H1>
<!-- received="Mon Mar 13 05:45:50 2000" -->
<!-- isoreceived="20000313124550" -->
<!-- sent="Mon, 13 Mar 2000 04:40:14 -0800" -->
<!-- isosent="20000313124014" -->
<!-- name="sayke" -->
<!-- email="sayke@gmx.net" -->
<!-- subject="Re: Otter vs. Yudkowsky" -->
<!-- id="3.0.6.32.20000313044014.007b4e40@pop.gmx.net" -->
<!-- inreplyto="38CC5F86.F73183AF@pobox.com" -->
<STRONG>From:</STRONG> sayke (<A HREF="mailto:sayke@gmx.net?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;3.0.6.32.20000313044014.007b4e40@pop.gmx.net&gt;"><EM>sayke@gmx.net</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon Mar 13 2000 - 05:40:14 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4725.html">Anders Sandberg: "Re: Emotion and Cognitive Science (ws) What is intelligence(Ws: neuro mods......"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4723.html">Anders Sandberg: "near anything box design"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4683.html">Eliezer S. Yudkowsky: "Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4843.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4843.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4724">[ date ]</A>
<A HREF="index.html#4724">[ thread ]</A>
<A HREF="subject.html#4724">[ subject ]</A>
<A HREF="author.html#4724">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
At 09:25 PM 3/12/00 -0600, someone with the password to the
<BR>
<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;3.0.6.32.20000313044014.007b4e40@pop.gmx.net&gt;">sentience@pobox.com</A> mailbox used said mailbox to write, to den otter, in
<BR>
particular, the following:
<BR>
<P><EM>&gt;In particular, I think you're prejudiced against AIs.  I see humans and
</EM><BR>
<EM>&gt;AIs as occupying a continuum of cognitive architectures, while you seem
</EM><BR>
<EM>&gt;to be making several arbitrary distinctions between humans and AIs.  You
</EM><BR>
<EM>&gt;write, for example, of gathering a group of humans for the purpose of
</EM><BR>
<EM>&gt;being uploaded simultaneously, where I would say that if you can use a
</EM><BR>
<EM>&gt;human for that purpose, you can also build an AI that will do the same thing.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i find my conclusions to be similar to den otter's, but of course i do not
<BR>
speak of him. that said, i agree with you in that humans and AIs can in
<BR>
theory occupy a continuum of cognitive architectures. but... i think that,
<BR>
given an intelligence capable of hacking itself, the middle ground of the
<BR>
continuum goes away. i see a few basic peaks in intelligence phase space,
<BR>
if i can be so loose: a peak for those not capable of hacking anything, one
<BR>
for those capable of hacking some things but not themselves, and one for
<BR>
those capable of hacking themselves.
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;granted, the lines between the categories are fuzzy, but that's beside the
<BR>
point.
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the crux of my point of view is that, if a self-tweaking intelligence,
<BR>
that is not me, occurs anywhere near me, and i am running on my current
<BR>
substrate, i will quite likely die. i think this is to be avoided; i can go
<BR>
into why i think that if you really want me to, but i think you could
<BR>
probably argue my pov quite well. therefore, i should strive to be one of
<BR>
the first, if not the first, self-hacking intelligence.
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;teehee... will i eat the solar system in my ruthless quest for
<BR>
self-transparency? who knows! should it matter to me? true, the
<BR>
2-people-in-lifeboat-in-ocean thought experiment might very well apply to a
<BR>
multiple-Powers-ascending-in-close-proximity situation, but so what? why
<BR>
should that affect my current course of action? i'll deal with the problems
<BR>
involved in hitting people with oars when i have several orders of
<BR>
magnitude more thinking power to work with. ;)
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;but i'll play devils advocate for a second. if the lifeboat thought
<BR>
experiment possibly applies, then should i be attempting to remove/kill off
<BR>
my potential competition? no. for one thing, i need my potential
<BR>
competition, because most of them are badass specialists who i probably
<BR>
couldn't ascend without. for that pragmatic reason, and others which i'm
<BR>
sure you can divine, i think that attempting to kill off my potential
<BR>
competition is a baaaaaad plan.
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if i recall correctly, you didnt dig den otter acknowledging that he
<BR>
might, say, eat the planet, if he ascends. given that the really scary
<BR>
kinds of nanowar are technically more difficult then uploading (i think it
<BR>
was billy brown who persuasively argued this... or am i smoking crack?),
<BR>
and that the inevitable first superintelligences will quite possibly be
<BR>
ruthlessly selfish, should my first priority not be to become once of said
<BR>
superintelligences?
<BR>
<P><EM>&gt;As you know, I don't think that any sufficiently advanced mind can be
</EM><BR>
<EM>&gt;coerced.  If there is any force that would tend to act on Minds, any
</EM><BR>
<EM>&gt;inevitability in the ultimate goals a Mind adopts, then there is
</EM><BR>
<EM>&gt;basically nothing we can do about it.  And this would hold for both
</EM><BR>
<EM>&gt;humanborn and synthetic Minds; if it didn't hold for humans, then it
</EM><BR>
<EM>&gt;would not-hold for some class of AIs as well.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i'm not concerned about committing suicide after eons of life after as a
<BR>
Power. i am concerned by any situation that involves me being consumed for
<BR>
the yummy carbon i currently run on.
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and do you really think it would be easier to write an upload-managing ai,
<BR>
then to just upload? i really really doubt Minds can be given, by us, the
<BR>
goal-system momentum you speak of. to do so seems way tougher then mere
<BR>
moravecian uploading.
<BR>
<P>sayke, v2.3.05 #brought to you by the Coalition for the Removal of Capital
<BR>
Letters ;)
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4725.html">Anders Sandberg: "Re: Emotion and Cognitive Science (ws) What is intelligence(Ws: neuro mods......"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4723.html">Anders Sandberg: "near anything box design"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4683.html">Eliezer S. Yudkowsky: "Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4843.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4843.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4724">[ date ]</A>
<A HREF="index.html#4724">[ thread ]</A>
<A HREF="subject.html#4724">[ subject ]</A>
<A HREF="author.html#4724">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:04:59 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
