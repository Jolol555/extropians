<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Otter vs. Yudkowsky</TITLE>
<META NAME="Author" CONTENT="sayke (sayke@gmx.net)">
<META NAME="Subject" CONTENT="Re: Otter vs. Yudkowsky">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Otter vs. Yudkowsky</H1>
<!-- received="Tue Mar 14 06:50:18 2000" -->
<!-- isoreceived="20000314135018" -->
<!-- sent="Tue, 14 Mar 2000 05:47:23 -0800" -->
<!-- isosent="20000314134723" -->
<!-- name="sayke" -->
<!-- email="sayke@gmx.net" -->
<!-- subject="Re: Otter vs. Yudkowsky" -->
<!-- id="3.0.6.32.20000314054723.00793e50@pop.gmx.net" -->
<!-- inreplyto="38CDE95E.12BAF519@pobox.com" -->
<STRONG>From:</STRONG> sayke (<A HREF="mailto:sayke@gmx.net?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;3.0.6.32.20000314054723.00793e50@pop.gmx.net&gt;"><EM>sayke@gmx.net</EM></A>)<BR>
<STRONG>Date:</STRONG> Tue Mar 14 2000 - 06:47:23 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4868.html">Robert J. Bradbury: "GeneEng: Pigs Cloned..."</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4866.html">sayke: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4837.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4825.html">Spike Jones: "both denOtter and Yudkowsky: possible singularity scenarios"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4867">[ date ]</A>
<A HREF="index.html#4867">[ thread ]</A>
<A HREF="subject.html#4867">[ subject ]</A>
<A HREF="author.html#4867">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
At 01:26 AM 3/14/00 -0600, <A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;3.0.6.32.20000314054723.00793e50@pop.gmx.net&gt;">sentience@pobox.com</A> wrote:
<BR>
<EM>&gt;sayke wrote:
</EM><BR>
<EM>&gt;&gt; 
</EM><BR>
<EM>&gt;&gt;         do terms like &quot;dumb&quot; kinda lose meaning in the absence of personal
</EM><BR>
<EM>&gt;&gt; control? i think so.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;Oh, bull.  You have no personal control over your quarks, your neurons,
</EM><BR>
<EM>&gt;or your environment.  There is not one tool you can use which has a 100%
</EM><BR>
<EM>&gt;chance of working.  You are at the mercy of the random factors and the
</EM><BR>
<EM>&gt;hidden variables.  &quot;Maintaining control&quot; consists of using the tool with
</EM><BR>
<EM>&gt;the highest probability of working.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;maintaining &quot;personal control&quot; is not the same as &quot;maintaining (generic)
<BR>
control&quot;. as you can see by looking at what i wrote above, i was not
<BR>
talking about generic control... aw well. argument about this serves no
<BR>
purpose, methinks. its beside the point. moving on...
<BR>
<P><EM>&gt;&gt;         how kind of the sysop. theocracy might sound nifty, but i don't
</EM><BR>
think it
<BR>
<EM>&gt;&gt; would be stable, let alone doable, from a monkey point of view.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;How fortunate that the Sysop is not a monkey.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;but monkeys will be making it, and operating under it, i presume. that was
<BR>
my point.
<BR>
<P><EM>&gt;&gt;         an omniscient ai is pretty much inscrutable, right? i don't know
</EM><BR>
how we
<BR>
<EM>&gt;&gt; can evaluate the inscrutable's chances of becoming what we would call
</EM><BR>
<EM>&gt;&gt; &quot;corrupt&quot;. i think the least inscrutable thing about an omniscient
</EM><BR>
<EM>&gt;&gt; intelligence would be its need for resources. other then that... i dunno.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;Yes, its need for resources in order to make humans happy.  Munching on
</EM><BR>
<EM>&gt;the humans to get the resources to make the humans happy is not valid
</EM><BR>
<EM>&gt;logic even for SHRDLU.  Inscrutability is one thing, stupidity another.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shizat, man, we're talkin right past each other. let me rephrase: i don't
<BR>
think you can make a sysop. i don't think any monkeys can. i doubt that
<BR>
suitable momentum can be imparted to something inscrutable. complex systems
<BR>
are not molded by shoving in ones fingers and stirring... and thats the
<BR>
least of the difficulties, i think...
<BR>
<P><EM>&gt;&gt;         i fail to see how it could not get tangled up... even in a case
</EM><BR>
like &quot;in
<BR>
<EM>&gt;&gt; order to maximize greeness, the resources over there should be used in this
</EM><BR>
<EM>&gt;&gt; manner&quot; (which has no self-subject implied) a distenction must be made
</EM><BR>
<EM>&gt;&gt; between resources more directly controlled (what i would call &quot;my stuff&quot;)
</EM><BR>
<EM>&gt;&gt; and resources more indirectly controlled (what i would call &quot;other stuff&quot;),
</EM><BR>
<EM>&gt;&gt; etc... and as soon as that distenction is made, degrees of
</EM><BR>
<EM>&gt;&gt; ownership/beingness/whatever is implied, and from there promptly gets mixed
</EM><BR>
<EM>&gt;&gt; up in the goal system...
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;Wrong.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;What else can I say?  You, as a human, have whole symphonies of
</EM><BR>
<EM>&gt;emotional tones that automatically bind to a cognitive structure with
</EM><BR>
<EM>&gt;implications of ownership.  Seeds don't.  End of story.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;it is quite possible that my opinion is being corrupted by my evolutionary
<BR>
programming. but i don't think things are nearly that simple... aw well.
<BR>
for the sake of argument, and because i don't think either of us can really
<BR>
lecture the other on the architecture of transcendent minds, i will concede
<BR>
the point. lets say that Powers don't need a self-subject; they do not find
<BR>
them useful. what does that change? it does not make the task of sysop
<BR>
creation any easier.
<BR>
<P><EM>&gt;&gt;         necessary? in the sense that such an arrangement will increase
</EM><BR>
my odds of
<BR>
<EM>&gt;&gt; survival, etc? i doubt it, if only because the odds against my survival
</EM><BR>
<EM>&gt;&gt; must be dire indeed (understatement) to justify the massive amount of work
</EM><BR>
<EM>&gt;&gt; that would be required to make a sysop; effort that could rather be
</EM><BR>
<EM>&gt;&gt; invested towards, say, getting off this planet; where getting off the
</EM><BR>
<EM>&gt;&gt; planet would be a better stopgap anyway.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;Getting off the planet will protect you from China.  It will not protect
</EM><BR>
<EM>&gt;you from me.  And you can't get off the planet before I get access to a
</EM><BR>
<EM>&gt;nanocomputer, anyway.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i don't think your last statement is supportable. i don't think either of
<BR>
us knows nearly enough about future event sequences to have a say on that.
<BR>
<P><EM>&gt;&gt;         unless, of course, you come up with a well thought out essay on
</EM><BR>
the order
<BR>
<EM>&gt;&gt; of &quot;coding a transhuman ai&quot; discussing the creation of a specialized sysop
</EM><BR>
<EM>&gt;&gt; ai.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;If the problem is solvable, it should be comparatively trivial. 
</EM><BR>
<EM>&gt;Extremely hard, you understand, but not within an order of magnitude of
</EM><BR>
<EM>&gt;the problem of intelligence itself.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;um. i beg to differ with this. in the first case, you create a
<BR>
self-hacking intelligence, which is by nature incomprehensible to you once
<BR>
it has emerged and tweaked itself for a bit. in the other case, your task
<BR>
is to give the incomprehensible a specific, arbitrary form of motivational
<BR>
momentum.
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;best of luck to you. either that, or no luck at all.
<BR>
<P><EM>&gt;&gt;         i trend towards advocating a very dumb sysop, if it can be
</EM><BR>
called that...
<BR>
<EM>&gt;&gt; a &quot;simple&quot; upload manager...
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;Probably not technologically possible.  Even a mind as relatively
</EM><BR>
<EM>&gt;&quot;simple&quot; as Eurisko was held together mostly by the fact of
</EM><BR>
self-modification.
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the &quot;simple&quot; upload manager i was talking about is not nearly mind-level.
<BR>
<P><EM>&gt;&gt; &gt;You and a thousand other Mind-wannabes wish to
</EM><BR>
<EM>&gt;&gt; &gt;ensure your safety and survival.  One course of action is to upload,
</EM><BR>
<EM>&gt;&gt; &gt;grow on independent hardware, and then fight it out in space.
</EM><BR>
<EM>&gt;&gt; 
</EM><BR>
<EM>&gt;&gt;         or just run the fuck away, and hopefully not fight it out for a
</EM><BR>
very, very
<BR>
<EM>&gt;&gt; long time, if ever. dibs on alpha centauri... ;)
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;One of the things Otter and I agree on is that you can't run away from a
</EM><BR>
<EM>&gt;Power.  Nano, yes.  Not a Power.  Andromeda wouldn't be far enough.  The
</EM><BR>
<EM>&gt;only defense against a malevolent Power is to be a Power yourself. 
</EM><BR>
<EM>&gt;Otter got that part.  The part Otter doesn't seem to get is that if a
</EM><BR>
<EM>&gt;thousand people want to be Powers, then synchronization is probably
</EM><BR>
<EM>&gt;physically impossible and fighting it out means your chance of winning
</EM><BR>
<EM>&gt;is 0.1%; the only solution with a non-negligible probability of working
</EM><BR>
<EM>&gt;is creating a trusted Sysop Mind.  Maybe it only has a 30% chance of
</EM><BR>
<EM>&gt;working, but that's better than 0.1%.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;well... being insignificant probably helps. i take that to me the case
<BR>
with respect to our continued existence. given that powers probably exist,
<BR>
somewhere, they must not be completely voracious, because we have not been
<BR>
eaten yet...
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;running away would be a stopgap. being that i think your sysop is not
<BR>
doable, i'm left with stopgaps like running away.  shrug. whaddya gonna do...
<BR>
<P>[snippage snipped because i don't think a sysop is doable]
<BR>
<EM>&gt;&gt;         mutually assured destruction seems more clever then a sysop.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;It won't work for nano and it sure won't work for Minds.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nano, yea. the old lone-gunman-who-does-not-value-existance thing...
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;but do you really think that Minds don't value their continued existence?
<BR>
<P>[minor snippage]
<BR>
<EM>&gt;&gt;         what if i want to *be* said Pact?
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;I don't trust you.  I can't see your source code, and if I could, I
</EM><BR>
<EM>&gt;almost certainly wouldn't trust it.  den Otter doesn't trust you either.
</EM><BR>
<EM>&gt; You're an agent, not a tool.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;you seem to forget that you would be incapable of reading your sysop's
<BR>
source code, and that it would be an agent in and of itself. you can trust
<BR>
my self-interest, etc... den otter can too... and i thought you thought (as
<BR>
i think) that &quot;transcendent tool&quot; is an oxymoron, but, that seems to be
<BR>
what you want for a sysop.
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i'm not followin, man...
<BR>
<P>sayke, v2.3.05
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4868.html">Robert J. Bradbury: "GeneEng: Pigs Cloned..."</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4866.html">sayke: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4837.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4825.html">Spike Jones: "both denOtter and Yudkowsky: possible singularity scenarios"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4867">[ date ]</A>
<A HREF="index.html#4867">[ thread ]</A>
<A HREF="subject.html#4867">[ subject ]</A>
<A HREF="author.html#4867">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:05:08 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
