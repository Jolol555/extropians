<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Otter vs. Yudkowsky</TITLE>
<META NAME="Author" CONTENT="D.den Otter (neosapient@geocities.com)">
<META NAME="Subject" CONTENT="Re: Otter vs. Yudkowsky">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Otter vs. Yudkowsky</H1>
<!-- received="Fri Mar 24 16:22:46 2000" -->
<!-- isoreceived="20000324232246" -->
<!-- sent="Sat, 25 Mar 2000 00:14:35 +0100" -->
<!-- isosent="20000324231435" -->
<!-- name="D.den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: Otter vs. Yudkowsky" -->
<!-- id="38DBF6DB.29C6@geocities.com" -->
<!-- inreplyto="Otter vs. Yudkowsky" -->
<STRONG>From:</STRONG> D.den Otter (<A HREF="mailto:neosapient@geocities.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38DBF6DB.29C6@geocities.com&gt;"><EM>neosapient@geocities.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Fri Mar 24 2000 - 16:14:35 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5734.html">hal@finney.org: "Re: a to-do list for the next century"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5732.html">Replicant00@aol.com: "Re: The Value of Extremists"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="4683.html">Eliezer S. Yudkowsky: "Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5891.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5891.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5733">[ date ]</A>
<A HREF="index.html#5733">[ thread ]</A>
<A HREF="subject.html#5733">[ subject ]</A>
<A HREF="author.html#5733">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
----------
<BR>
<EM>&gt; From: Eliezer S. Yudkowsky &lt;<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38DBF6DB.29C6@geocities.com&gt;">sentience@pobox.com</A>&gt;
</EM><BR>
<P><EM>&gt; &gt; Yes, that's the *practical* side of the dispute. There's also the
</EM><BR>
<EM>&gt; &gt; philosophical issue of whether personal survival is more important
</EM><BR>
<EM>&gt; &gt; than the creation of superintelligent successors, &quot;egoism&quot; vs
</EM><BR>
<EM>&gt; &gt; &quot;altruism&quot; etc., of course. This inevitably adds an element of
</EM><BR>
<EM>&gt; &gt; bias to the above debate.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I have no trouble seeing your point of view.  I am not attempting to
</EM><BR>
<EM>&gt; persuade you to relinquish your selfishness; I am attempting to persuade
</EM><BR>
<EM>&gt; you that the correct action is invariant under selfishness, altruism,
</EM><BR>
<EM>&gt; and Externalism.
</EM><BR>
<P>If nano is relatively easy, if it quickly becomes dangerous and 
<BR>
uncontrollable and if strong AI is much easier than human 
<BR>
upgrading then your approach does indeed make sense, as a last 
<BR>
desperate effort of a dying race. However, if the situation
<BR>
turns out to be less urgent, then (gradual) uploading is definitely 
<BR>
the way to go from a &quot;selfish&quot; perspective because, instead
<BR>
of being forced to passively await a very uncertain future, one
<BR>
can significantly improve one's odds by stepping up one's efforts.
<BR>
Initiative &amp; creativity are rewarded, as they should be. You can
<BR>
outsmart (or understand and make deals with, for that matter)
<BR>
other uploads, but you can't do that with a transhuman AI if
<BR>
you're merely human or an upload running on its system. It's 
<BR>
not just an instinctive, irrational illusion of control, you
<BR>
really *have* more control. 
<BR>
<P>[corrected piece copied from other post]
<BR>
<EM>&gt; &gt; No, that estimate is definitely incorrect.  Using a value of less than
</EM><BR>
<EM>&gt; &gt; 10% or more than 70% would be unjustifiable.  30% was &quot;pulled out of the
</EM><BR>
<EM>&gt; &gt; air&quot;; I'll happily defend the range itself.
</EM><BR>
<EM>&gt; &gt; 
</EM><BR>
<EM>&gt; &gt; More than 70% would be unjustifiable due to the Fermi Paradox and unknowability.
</EM><BR>
<EM>&gt; &gt; 
</EM><BR>
<EM>&gt; &gt; Since, if we can create a Sysop with specifiable stable goals, we win,
</EM><BR>
<EM>&gt; &gt; to assert that the probability is less than 10% would require
</EM><BR>
<EM>&gt; &gt; demonstrating that the probability of (A) External goals (and hostile
</EM><BR>
<EM>&gt; &gt; ones, at that), or (B) the probability that stable arbitrary goals can
</EM><BR>
<EM>&gt;                                                                      ^^^
</EM><BR>
<EM>&gt; should be 'cannot'
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &gt; be produced, are one or the other above 90%, or that their product is
</EM><BR>
<EM>&gt; &gt; above 90%; which requires a degree of definite knowledge about these
</EM><BR>
<EM>&gt; &gt; issues that nobody possesses.  Even if it were possible to rationally
</EM><BR>
<EM>&gt; &gt; estimate the resulting &quot;specifiable stable goals&quot; probability as being
</EM><BR>
<EM>&gt; &gt; below 10%, which I do not think is the case, then it would be absurd to
</EM><BR>
<EM>&gt; &gt; argue it as being 1%.  To say that a 99% probability of &quot;no specifiable
</EM><BR>
<EM>&gt; &gt; goals&quot; holds is to imply definite knowledge, which neither of us has.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Or in other words, I'm sure den Otter would agree that 70% is a
</EM><BR>
<EM>&gt; reasonable upper bound on our chance of success given our current
</EM><BR>
<EM>&gt; knowledge (although I'm sure he thinks it's too optimistic).  
</EM><BR>
<P>Well yes, ok, I do think it's somewhat optimistic, but, strictly 
<BR>
speaking, the 10-70% range is reasonable enough. Needless
<BR>
to say, when you look at the average we're dealing with some
<BR>
pretty bad odds here. I mean, if those were your odds to
<BR>
survive a particular operation, you'd probably want your cryonics
<BR>
organization to do a [rescue team] standby at the hospital. 
<BR>
Unfortunately, in the Singularity Game there are no backups
<BR>
and no second chances.
<BR>
<P><EM>&gt; It is
</EM><BR>
<EM>&gt; equally possible to set a reasonable upper bound on our chance of failure.
</EM><BR>
<P>Fairy nuff...
<BR>
[end insert]
<BR>
<P><EM>&gt; &gt; The longer you exist, the more opportunities there will be for
</EM><BR>
<EM>&gt; &gt; something to go wrong. That's pretty much a mathematical
</EM><BR>
<EM>&gt; &gt; certainty, afaik.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; While I view the growth of knowledge and intelligence as an open-ended
</EM><BR>
<EM>&gt; process, essentially because I am an optimist, I do expect that all
</EM><BR>
<EM>&gt; reasoning applicable to basic goals will have been identified and
</EM><BR>
<EM>&gt; produced within a fairly small amount of time, with any remaining
</EM><BR>
<EM>&gt; revision taking place within the sixth decimal place.  I expect the same
</EM><BR>
<EM>&gt; to hold of the True Basic Ultimate Laws of Physics as well.  The problem
</EM><BR>
<EM>&gt; is finite; the applications may be infinite, and the variations may be
</EM><BR>
<EM>&gt; infinite, but the basic rules of reasoning, and any specific structure,
</EM><BR>
<EM>&gt; are finite.
</EM><BR>
<P>Well, I hope you're right. The biggest risk is probably, as you 
<BR>
already suggested, at the very beginning; that's when human error
<BR>
(i.e. bad programming/bad hardware/some freak mishap)
<BR>
could thoroughly mess up the AI's mental structure, making
<BR>
it utterly unpredictable and potentially very dangerous. Or
<BR>
just useless, of course. This possibility should by no means
<BR>
be underestimated.
<BR>
&nbsp;
<BR>
[Sysop]
<BR>
<EM>&gt; Don't think of it as an enemy; think of it as an Operating System.
</EM><BR>
<P>Operating Systems can be your enemy too; as a Win95 user
<BR>
I know what I'm talking about...
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Natural evolution may have made some pretty bad mistakes, but
</EM><BR>
<EM>&gt; &gt; that doesn't necessarily mean that *all* of our programming will become
</EM><BR>
<EM>&gt; &gt; obsolete. If the SIs want to do something, they will have to stay
</EM><BR>
<EM>&gt; &gt; alive to do it (unless of course they decide to kill themselves, but
</EM><BR>
<EM>&gt; &gt; let's assume for the sake of argument that this won't be the case).
</EM><BR>
<EM>&gt; &gt; Basic logic. So some sort of self-preservation &quot;instinct&quot; will be
</EM><BR>
<EM>&gt; &gt; required(*) to keep the forces of entropy at bay. Survival requires
</EM><BR>
<EM>&gt; &gt; control --the more the better-- over one's surroundings. Other
</EM><BR>
<EM>&gt; &gt; intelligent entities represent by definition an area of deminished
</EM><BR>
<EM>&gt; &gt; control, and must be studied and then placed in a threat/benefit
</EM><BR>
<EM>&gt; &gt; hierarchy which will help to determine future actions. And voila,
</EM><BR>
<EM>&gt; &gt; your basic social hierachy is born. The &quot;big happy egoless
</EM><BR>
<EM>&gt; &gt; cosmic family model&quot; only works when the other sentients
</EM><BR>
<EM>&gt; &gt; are either evolutionary dead-ends which are &quot;guaranteed&quot; to
</EM><BR>
<EM>&gt; &gt; remain insignificant, or completely and permanently like-minded.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Nonsense.  If the other sentients exist within a trustworthy Operating
</EM><BR>
<EM>&gt; System - I do think that a small Power should be able to design a
</EM><BR>
<EM>&gt; super-Java emulation that even a big Power shouldn't be able to break
</EM><BR>
<EM>&gt; out of; the problem is finite - then the other sentients pose no threat.
</EM><BR>
<P>Yes, but what if the other sentients *aren't* part of the 
<BR>
simulation (alien life forms from distant galaxies, uploads
<BR>
or AIs that have ascended more or less simultaneously but
<BR>
independently). By &quot;surroundings&quot; I meant the &quot;real world&quot; 
<BR>
outside the SI technosphere, not the (semi-autonomous)
<BR>
simulations it is running. I agree that containing the latter
<BR>
shouldn't be too much of a problem, but that's not the
<BR>
issue here. 
<BR>
<P><EM>&gt;  Even if they do pose a threat, then your argument is analogous to
</EM><BR>
<EM>&gt; saying that a rational operating system, which views its goal as
</EM><BR>
<EM>&gt; providing the best possible environment for its subprocesses, will kill
</EM><BR>
<EM>&gt; off all processes because they are untrustworthy.  As a logical chain,
</EM><BR>
<EM>&gt; this is simply stupid.
</EM><BR>
<P>It would be stupid, yes, but that's not what I was saying (see 
<BR>
above). 
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; No, no, no! It's exactly the other way around; goals are
</EM><BR>
<EM>&gt; &gt; observer dependent by default. As far as we know this is
</EM><BR>
<EM>&gt; &gt; the only way they *can* be.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I should correct my terminology; I should say that observer-*biased*
</EM><BR>
<EM>&gt; goals are simply evolutionary artifacts.  Even if only
</EM><BR>
<EM>&gt; observer-dependent goals are possible, this doesn't rule out the
</EM><BR>
<EM>&gt; possibility of creating a Sysop with observer-unbiased goals.
</EM><BR>
<P>That's a lot better. I still think that a totally &quot;selfless&quot;
<BR>
being is a lower form of life no matter how smart it
<BR>
is otherwise (a giant leap backwards on the ladder of
<BR>
evolution, really), but perhaps this is just an &quot;irrational&quot;
<BR>
matter of personal taste. In any case, the prudent 
<BR>
approach is to keep your ego until you're certain, beyond 
<BR>
all reasonable doubt, that it is not an asset but a handicap 
<BR>
(or useless appendage). In other words, you'll have to
<BR>
become a SI first.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Evolution represents, among other things, some basic rules
</EM><BR>
<EM>&gt; &gt; for survival. No matter how smart the SIs will become, they'll
</EM><BR>
<EM>&gt; &gt; still have to play by the rules of this reality to live &amp; prosper.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Your statement is simply incorrect.  The possibility of a super-Java
</EM><BR>
<EM>&gt; encapsulation, which I tend to view as the default possibility - human
</EM><BR>
<EM>&gt; Java can be broken because humans make mistakes, humans make mistakes
</EM><BR>
<EM>&gt; because they're running with a high-level four-item short-term memory
</EM><BR>
<EM>&gt; and no codic cortex, and a superintelligence which knows all the laws of
</EM><BR>
<EM>&gt; physics and has a codic cortex should be able to design security a Power
</EM><BR>
<EM>&gt; couldn't crack; the problem is finite - directly contradicts the
</EM><BR>
<EM>&gt; necessity of all the survival activities you postulate.
</EM><BR>
<P>Is it your aim to &quot;encapsulate&quot; the whole known universe with
<BR>
everything in it? Regardless of your answer, if there *are* other
<BR>
sentients in this universe, you will eventually have to deal
<BR>
with them. Either you come to them -swallowing galaxies
<BR>
as you go along- or they come to you. Then you'll have to
<BR>
deal with them, not in Java-land, but in reality-as-we-know-it
<BR>
where the harsh rules of evolution (may) still apply.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; You can't deny self-evident truths like &quot;might makes right&quot;
</EM><BR>
<EM>&gt; &gt; without paying the price (decreased efficiency, possibly
</EM><BR>
<EM>&gt; &gt; serious damage or even annihilation) at some point. And
</EM><BR>
<EM>&gt; &gt; yes, I also belief that suicide is fundamentally stupid,
</EM><BR>
<EM>&gt; &gt; *especially* for a Power which could always alter its mind
</EM><BR>
<EM>&gt; &gt; and bliss out forever if there's nothing better to do.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Only if the Power is set up to view this as desirable, and why would it
</EM><BR>
<EM>&gt; be?   My current goal-system design plans don't even call for &quot;pleasure&quot;
</EM><BR>
<EM>&gt; as a separate module, just selection of actions on the basis of their
</EM><BR>
<EM>&gt; outcomes. And despite your anthropomorphism, this does not consist of
</EM><BR>
<EM>&gt; pleasure.  Pleasure is a complex functional adaptation which responds to
</EM><BR>
<EM>&gt; success by reinforcing skills used, raising the level of mental energy,
</EM><BR>
<EM>&gt; and many other subtle and automatic effects that I see no reason to
</EM><BR>
<EM>&gt; preserve in an entity capable of consciously deciding how to modify
</EM><BR>
<EM>&gt; itself. 
</EM><BR>
<P>No, it wouldn't need the pleasure/pain mechanism for 
<BR>
self-modification, but that doesn't mean that the pure 
<BR>
emotion &quot;pleasure&quot; will become redundant; if your machine
<BR>
seeks the &quot;meaning of life&quot;, it might very well re-discover
<BR>
this particular emotion. If it's inherently logical, it *will* 
<BR>
eventually happen, or so you say.
<BR>
<P><EM>&gt; In particular, your logic implies that the *real* supergoal is
</EM><BR>
<EM>&gt; get-success-feedback, 
</EM><BR>
<P>Forget success-feedback, we're talking about an &quot;untangled&quot;
<BR>
emotion for its own sake. The supergoal would be &quot;to have
<BR>
fun&quot;, and the best way to do this is to have a separate
<BR>
module for this, and let &quot;lower&quot; autonomous systems sort
<BR>
out the rest. The power would be happy &amp; gay all the time,
<BR>
no matter what happened, without being cognitively impaired
<BR>
as ecstatic humans tend to be.
<BR>
<P><EM>&gt; and that the conditions for success feedback are
</EM><BR>
<EM>&gt; modifiable; this is not, however, an inevitable consequence of system
</EM><BR>
<EM>&gt; architecture, and would in fact be spectacularly idiotic; it would
</EM><BR>
<EM>&gt; require a deliberate effort by the system programmer to represent
</EM><BR>
<EM>&gt; success-feedback as a declarative goal on the same level as the other
</EM><BR>
<EM>&gt; initial supergoals, which would be simply stupid.
</EM><BR>
<P>If you don't allow an entitity to experience the emotion 
<BR>
&quot;pleasure&quot;, you may have robbed it of something &quot;inherently&quot;
<BR>
good. Not because emotions are needed for functioning
<BR>
and expanding, but because pure, un-attached, freely
<BR>
controllable pleasure is a *bonus*. You have 3 basic
<BR>
levels: -1 is &quot;suffering&quot;, 0 is the the absence of emotions, 
<BR>
good or bad (death is one such state) and 1 is 
<BR>
&quot;pleasure&quot;. Why would a fully freethinking SI want
<BR>
to remain on level 0 when it can move, without sacrificing 
<BR>
anything, to level 1, which is &quot;good&quot; by defintion
<BR>
(I think I'm happy, therefore I *am* happy or something
<BR>
like that). Think of it as a SI doing crack, but without all 
<BR>
the nasty side-effects. Would there be any logical reason 
<BR>
_not_ to &quot;do drugs&quot;, i.e. bliss out, if it didn't impair your 
<BR>
overall functioning in any way? Bottom line: it doesn't
<BR>
matter whether you include pleasure in the original
<BR>
design; if your machine seeks the ultimate good and
<BR>
has the ability to modify itself accordingly, it may
<BR>
introduce a pleasure module at some point because
<BR>
it concludes that nothing else makes (more) sense. 
<BR>
I think it's very likely, you may think the opposite, but 
<BR>
that the possibility exists is beyond any reasonable 
<BR>
doubt. 
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; The only
</EM><BR>
<EM>&gt; &gt; logical excuse for killing yourself is when one knows for pretty
</EM><BR>
<EM>&gt; &gt; damn sure, beyond all reasonable doubt, that the alternative
</EM><BR>
<EM>&gt; &gt; is permanent, or &quot;indefinite&quot;, hideous suffering.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Nonsense; this is simply den Otter's preconditions.  I thought you had
</EM><BR>
<EM>&gt; admitted - firmly and positively asserted, in fact - that this sort of
</EM><BR>
<EM>&gt; thing was arbitrary?
</EM><BR>
<P>Yes, IMO everything is arbitrary in the end, but not everything 
<BR>
is equally arbitrary in the context of our current situation. If
<BR>
you strip everything away you're left with a pleasure-pain
<BR>
mechanism, the carrot and the stick. From our current pov, 
<BR>
it makes sense to enhance the carrot and to get rid of the 
<BR>
stick altogether (while untangling the reward system from 
<BR>
our survival mechanism and making the latter more or
<BR>
less &quot;autonomous&quot;, for obvious reasons). AIs could still 
<BR>
be useful as semi-independent brain modules that take
<BR>
care of system while the &quot;I&quot; is doing its bliss routine.
<BR>
&quot;Mindless&quot; Sysops that still have to answer *fully* to 
<BR>
the superintelligent &quot;I&quot;, whose line of consciousness can
<BR>
be traced back directly to one or several human uploads.
<BR>
For these beings, personal identity would be much less
<BR>
of an illusion than it is to us, for a SIs &quot;never sleep&quot; and
<BR>
actually understand and control their inner systems.
<BR>
But that aside... 
<BR>
<P><EM>&gt; &gt; In other words, objective morality will always be just an educated
</EM><BR>
<EM>&gt; &gt; guess. Will there be a limit to evolution anyway? One would be
</EM><BR>
<EM>&gt; &gt; inclined to say &quot;yes, of course&quot;, but if this isn't the case, then
</EM><BR>
<EM>&gt; &gt; the quest for objective morality will go on forever.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Well, you see &quot;objective morality&quot; as a romantic, floating label.  I see
</EM><BR>
<EM>&gt; it as a finite and specifiable problem which, given true knowledge of
</EM><BR>
<EM>&gt; the ultimate laws of physics, can be immediately labeled as either
</EM><BR>
<EM>&gt; &quot;existent&quot; or &quot;nonexistent&quot; within the permissible system space.
</EM><BR>
<P>You're still driven by &quot;arbitrary&quot; emotions, attaching value to
<BR>
random items (well, not completely random in an evolutionary
<BR>
context; seeking &quot;perfection&quot; can be good for survival). At the
<BR>
very least you should recognize that your desire to get rid of &quot;all
<BR>
human suffering&quot; is just an emotional, &quot;evolved&quot; monkey
<BR>
hangup (the whole altruism thing, of which this is clearly 
<BR>
a part, is just another survival strategy. Nothing more, nothing
<BR>
less). But that's ok, we're all still merely human after all.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; I'm sure you could make some pov-less freak in the lab, and
</EM><BR>
<EM>&gt; &gt; keep it alive under &quot;ideal&quot;, sterile conditions, but I doubt that
</EM><BR>
<EM>&gt; &gt; it would be very effective in the real world. As I see it, we have
</EM><BR>
<EM>&gt; &gt; two options: a) either the mind really has no &quot;self&quot; and no &quot;bias&quot;
</EM><BR>
<EM>&gt; &gt; when it comes to motivation, in which case it will probably just
</EM><BR>
<EM>&gt; &gt; sit there and do nothing, or b) it *does* have a &quot;self&quot;, or creates
</EM><BR>
<EM>&gt; &gt; one as a logical result of some pre-programmed goal(s), in
</EM><BR>
<EM>&gt; &gt; which case it is likely to eventually become completely
</EM><BR>
<EM>&gt; &gt; &quot;selfish&quot; due to a logical line of reasoning.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Again, nonsense.  The Sysop would be viewable - would view itself -
</EM><BR>
<EM>&gt; simply as an intelligent process that acted to maintain maximum freedom
</EM><BR>
<EM>&gt; for the inhabitants, an operating system intended to provide equal
</EM><BR>
<EM>&gt; services for the human species, its user base.  Your argument that
</EM><BR>
<EM>&gt; subgoals could interfere with these supergoals amounts to postulating
</EM><BR>
<EM>&gt; simple stupidity on the part of the Sysop.  Worries about other
</EM><BR>
<EM>&gt; supergoals interfering are legitimate, and I acknowledge that, but your
</EM><BR>
<EM>&gt; alleged chain of survival logic is simply bankrupt.
</EM><BR>
<P>You said yourself that your Sysop would have power-like
<BR>
abilities, and could reprogram itself completely if so desired.
<BR>
I mean, an entity that can't even free itself from its original
<BR>
programming can hardly be called a Power, can it? Perhaps
<BR>
you could more or less guarantee the reliability (as a
<BR>
defender of mankind) of a &quot;dumb&quot; Sysop (probably too
<BR>
difficult to complete in time, if at all possible), but not
<BR>
that of truly intelligent system which just happens to
<BR>
have been given some initial goals. Even humans can
<BR>
change their supergoals &quot;just like that&quot;, let alone SIs.
<BR>
And how would the search for ultimate truth fit into
<BR>
this picture, anyway? Does &quot;the truth&quot; have a lower, 
<BR>
equal or higher priority than protecting humans?
<BR>
<P><EM>&gt; &gt; [snakes &amp; rodents compared to AIs &amp; humans]
</EM><BR>
<EM>&gt; &gt; &gt; It would be very much different.  Both snakes and rodents evolved.
</EM><BR>
<EM>&gt; &gt; &gt; Humans may have evolved, but AIs haven't.
</EM><BR>
<EM>&gt; &gt; 
</EM><BR>
<EM>&gt; &gt; But they will have to evolve in order to become SIs.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; No, they wouldn't.  &quot;Evolution&quot; is an extremely specific term yielding
</EM><BR>
<EM>&gt; phenomena such as selection pressures, adaptations, competition for
</EM><BR>
<EM>&gt; mates, and so on.  An AI would need to improve itself to become Sysop;
</EM><BR>
<EM>&gt; this is quite a different proposition than evolution.
</EM><BR>
<P>Unless it encounters outside competition. It may not have to
<BR>
compete for mates, but resources and safety issues can be
<BR>
expected to remain significant even for Powers. Also, even
<BR>
if we assume no outside competition (&quot;empty skies&quot;), it
<BR>
still could make a *really* bad judgement call while upgrading 
<BR>
itself. Evolution of any sort will involve some trial and error,
<BR>
and some mistakes can have serious consequences (certainly
<BR>
when messing with your mental structure, and moving into
<BR>
completely uncharted territory).
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; I'd take the former, of course, but that's because the odds in this
</EM><BR>
<EM>&gt; &gt; particular example are extremely (and quite unrealistically so)
</EM><BR>
<EM>&gt; &gt; bad. In reality, it's not you vs the rest of humanity, but you vs
</EM><BR>
<EM>&gt; &gt; a relative small financial/technological elite, many (most) of
</EM><BR>
<EM>&gt; &gt; whom don't even fully grasp the potential of the machines they're
</EM><BR>
<EM>&gt; &gt; working on. Most people will simply never know what hit them.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Even so, your chances are still only one in a thousand, tops - 0.1%, as
</EM><BR>
<EM>&gt; I said before.
</EM><BR>
<P>Again, only in the worst-case &quot;highlander&quot; scenario and only 
<BR>
if the upload group would really be that big (it would probably 
<BR>
be a lot smaller). 
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Anyway, there are no certainties. AI is not a &quot;sure shot&quot;, but
</EM><BR>
<EM>&gt; &gt; just another blind gamble, so the whole analogy sort of
</EM><BR>
<EM>&gt; &gt; misses the point.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Not at all; my point is that AI is a gamble with a {10%..70%} chance of
</EM><BR>
<EM>&gt; getting 10^47 particles to compute with, while uploading is a gamble
</EM><BR>
<EM>&gt; with a {0.0000001%..0.1%} of getting 10^56.  If you count in the rest of
</EM><BR>
<EM>&gt; the galaxy, 10^58 particles vs. 10^67.
</EM><BR>
<P>Think bigger. Anyway, this is not (just) about getting as much
<BR>
particles as possible for your computations (&quot;greed&quot;), but rather
<BR>
about threat control (&quot;survival&quot;). There is an &quot;unknown&quot; chance
<BR>
that the Sysop will turn against you (for whatever reason), in
<BR>
which case you're at a horrible disadvantage, much more so
<BR>
than in the case of a MAD failure and subsequent battle between
<BR>
more or less equally developed Powers. I'd rather fight a 
<BR>
thousand peers than one massively superior Power anytime. 
<BR>
Uploads basically have two chances to survive: 1) You make 
<BR>
a deal, most likely based on MAD, and no-one fights, everyone 
<BR>
lives. 2) If this fails, you still have a fighting 0.1% chance 
<BR>
even in the worst case scenario, i.e. when everyone fights to 
<BR>
the death (*which isn't likely*; SIs ain't stupid so they're 
<BR>
more likely to compromize than to fight a battle with such bad
<BR>
odds). 
<BR>
<P>Therefore, I have to conclude that an upload's
<BR>
chance would be considerably better than your 0.1%
<BR>
figure. 10-70%, i.e. Sysop range, would be a lot more 
<BR>
realistic, IMO. SIs may not like competition, but they
<BR>
are no *retards*. I'd be surprised indeed if they just 
<BR>
started bashing eachother like a bunch of cavemen.
<BR>
If MAD can even work for tribes of highly irrational
<BR>
monkeys, it sure as hell should work for highly rational
<BR>
Powers. 
<BR>
<P><EM>&gt; &gt; Power corrupts, and absolute power...this may not apply just
</EM><BR>
<EM>&gt; &gt; to humans. Better have an Assembly of Independent Powers.
</EM><BR>
<EM>&gt; &gt; Perhaps the first thing they'd do is try to assassinate eachother,
</EM><BR>
<EM>&gt; &gt; that would be pretty funny.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &quot;Funny&quot; is an interesting term for it.  You're anthropomorphizing again.
</EM><BR>
<P>Of course, I *am* still human, after all. It would be &quot;funny&quot; from
<BR>
my current perspective, nothing more and nothing less.
<BR>
<P><EM>&gt;  What can you, as a cognitive designer, do with a design for a group of
</EM><BR>
<EM>&gt; minds that you cannot do with a design for a single mind?  I think the
</EM><BR>
<EM>&gt; very concept that this constitutes any sort of significant innovation,
</EM><BR>
<EM>&gt; that it contributes materially to complexity in any way whatsover, is
</EM><BR>
<EM>&gt; evolved-mind anthropomorphism in fee simple.  
</EM><BR>
<P>Just plain old multiple redundancy. Seemed like a good idea
<BR>
when there's so much at stake and humans are doing the
<BR>
initial programming.
<BR>
<P><EM>&gt; As I recall, you thought
</EM><BR>
<EM>&gt; approximately the same thing, back when you, I, and Nick Bostrum were
</EM><BR>
<EM>&gt; tearing apart Anders Sandberg's idea that an optimized design for a
</EM><BR>
<EM>&gt; Power could involve humanlike subprocesses.
</EM><BR>
<P>Ah, those were the days. Was that before or after we smashed
<BR>
Max More's idea that a SI would need others to interact with
<BR>
(for economical or social reasons? Anyway, I still agree that
<BR>
messy human subprocesses should be kept out of a SI's 
<BR>
mental frame, of course. No disagreement here. But what 
<BR>
exactly has this got to do with multiple redundancy for
<BR>
Sysops?
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; &gt; I *would* just forget about the Singularity, if it was necessary.
</EM><BR>
<EM>&gt; &gt; 
</EM><BR>
<EM>&gt; &gt; Necessary for what?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Serving the ultimate good.
</EM><BR>
<P>Oh yes, of course. I suppose this is what some call &quot;God&quot;...
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Aren't you really just rationalizing
</EM><BR>
<EM>&gt; &gt; an essentially &quot;irrational&quot; choice (supergoal) like the rest of
</EM><BR>
<EM>&gt; &gt; humanity?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; No.  I'm allowing the doubting, this-doesn't-make-sense part of my mind
</EM><BR>
<EM>&gt; total freedom over every part of myself and my motivations; selfishness,
</EM><BR>
<EM>&gt; altruism, and all.  I'm not altruistic because my parents told me to be,
</EM><BR>
<EM>&gt; because I'm under the sway of some meme, or because I'm the puppet of my
</EM><BR>
<EM>&gt; romantic emotions; I'm altruistic because of a sort of absolute
</EM><BR>
<EM>&gt; self-cynicism under which selfishness makes even less sense than
</EM><BR>
<EM>&gt; altruism.  Or at least that's how I'd explain things to a cynic.
</EM><BR>
<P>I've done some serious doubting myself, but extreme (self)
<BR>
cynicism invariably lead to nihilism, not altruism. Altruism 
<BR>
is just one of many survival strategies for &quot;selfish&quot; genes, 
<BR>
*clearly* just a means to an evolved end. Altruism is a sub-
<BR>
goal if there ever was one, and one that becomes utterly 
<BR>
redundant when a self-sufficient entity (a SI) gets into a 
<BR>
position where it can safely eliminate all competition. 
<BR>
<P>Altruism is *compromize*. A behaviour born out of necessity 
<BR>
on a planet with weak, mortal, interdependent, evolved 
<BR>
creatures. To use it out of context, i.e. when it has
<BR>
become redundant, is at least as arbitrary as the preference
<BR>
for some particular flavor of ice-cream. At the very least, 
<BR>
it is just as arbitrary as selfishness (its original &quot;master&quot;). 
<BR>
So, unless you just flipped a coin to determine your guideline 
<BR>
in life (and a likely guideline for SIs), what exactly *is* the 
<BR>
logic behind altruism? Why on earth would it hold in a SI world,
<BR>
assuming that SIs can really think for themselves and aren't
<BR>
just blindly executing some original program?
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; &gt; If it's an irrational prejudice, then let it go.
</EM><BR>
<EM>&gt; &gt; 
</EM><BR>
<EM>&gt; &gt; Then you'd have to stop thinking altogether, I'm afraid.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Anxiety!  Circular logic!  If you just let *go*, you'll find that your
</EM><BR>
<EM>&gt; mind continues to function, except that you don't have to rationalize
</EM><BR>
<EM>&gt; falsehoods for fear of what will happen if you let yourself see the
</EM><BR>
<EM>&gt; truth.  Your mind will go on as before, just a little cleaner.
</EM><BR>
<P>A bit of untangling is fine, but getting rid of the &quot;I&quot; is not
<BR>
acceptable until I have (much) more data. Such is the prudent
<BR>
approach...Upload, expand, reconsider. 
<BR>
&nbsp;
<BR>
<EM>&gt; Still:  (1)  You'll notice that Angel hasn't committed suicide or
</EM><BR>
<EM>&gt; ditched his soul, both actions which he knows perfectly well how to
</EM><BR>
<EM>&gt; execute.  
</EM><BR>
<P>Ditching his soul would be more or less the same as 
<BR>
committing suicide (he exits, Angelus enters). His innate 
<BR>
fear of death combined with a rather irrational sense of 
<BR>
&quot;duty&quot; (to do penance for crimes he didn't even commit)
<BR>
is what keeps him going, IMO. Also, one would expect
<BR>
that he, subconsciously or not, expects to -eventually- be 
<BR>
rewarded in one way or another for his good behaviour.
<BR>
The latter certainly isn't unlikely is *his* reality...
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5734.html">hal@finney.org: "Re: a to-do list for the next century"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5732.html">Replicant00@aol.com: "Re: The Value of Extremists"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="4683.html">Eliezer S. Yudkowsky: "Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5891.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5891.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5733">[ date ]</A>
<A HREF="index.html#5733">[ thread ]</A>
<A HREF="subject.html#5733">[ subject ]</A>
<A HREF="author.html#5733">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:06:17 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
