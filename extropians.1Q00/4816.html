<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Otter vs. Yudkowsky</TITLE>
<META NAME="Author" CONTENT="sayke (sayke@gmx.net)">
<META NAME="Subject" CONTENT="Re: Otter vs. Yudkowsky">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Otter vs. Yudkowsky</H1>
<!-- received="Mon Mar 13 20:44:59 2000" -->
<!-- isoreceived="20000314034459" -->
<!-- sent="Mon, 13 Mar 2000 19:44:32 -0800" -->
<!-- isosent="20000314034432" -->
<!-- name="sayke" -->
<!-- email="sayke@gmx.net" -->
<!-- subject="Re: Otter vs. Yudkowsky" -->
<!-- id="3.0.6.32.20000313194432.007b5a60@pop.gmx.net" -->
<!-- inreplyto="38CD8B79.9B837411@pobox.com" -->
<STRONG>From:</STRONG> sayke (<A HREF="mailto:sayke@gmx.net?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;3.0.6.32.20000313194432.007b5a60@pop.gmx.net&gt;"><EM>sayke@gmx.net</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon Mar 13 2000 - 20:44:32 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4817.html">Ian Goddard: "Re: Prozac &amp; Violence - New Study"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4815.html">Mike Linksvayer: "Re: [Fwd: Corel Intel Deal in the making]"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4786.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4837.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4837.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4816">[ date ]</A>
<A HREF="index.html#4816">[ thread ]</A>
<A HREF="subject.html#4816">[ subject ]</A>
<A HREF="author.html#4816">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
At 06:45 PM 3/13/00 -0600, <A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;3.0.6.32.20000313194432.007b5a60@pop.gmx.net&gt;">sentience@pobox.com</A> wrote:
<BR>
<P>[much snippage of good stuff that does not need me ranting at it]
<BR>
<EM>&gt;And here we come to the true crux of the problem.  You don't want to be
</EM><BR>
<EM>&gt;at someone else's mercy.  You don't want to entrust your fate to the
</EM><BR>
<EM>&gt;hidden variables.  You want to choose a course of action that puts you
</EM><BR>
<EM>&gt;in the driver's seat, even if it kills you.  You're prejudiced in favor
</EM><BR>
<EM>&gt;of plans that include what look like forceful actions against those
</EM><BR>
<EM>&gt;yucky possibilities, even if the actions are ineffective and have awful
</EM><BR>
<EM>&gt;side effects.  This is the same intuitive underpinning that underlies
</EM><BR>
<EM>&gt;Welfare, bombing Kosovo and the War on Drugs.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;Screw personal independence and all such slogans; the fundamental
</EM><BR>
<EM>&gt;principle of Transhumanism is *rationality*.  If maintaining personal
</EM><BR>
<EM>&gt;control is dumb, then you shouldn't do it.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;do terms like &quot;dumb&quot; kinda lose meaning in the absence of personal
<BR>
control? i think so.
<BR>
<P><EM>&gt;&gt; Who monitors the Sysop?
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;I've considered the utility of including a &quot;programmer override&quot;, but my
</EM><BR>
<EM>&gt;current belief is that the social anxiety generated by planning to
</EM><BR>
<EM>&gt;include such an override has a negative utility that exceeds the danger
</EM><BR>
<EM>&gt;of not having an override.  We'll just have to get it right the first
</EM><BR>
<EM>&gt;time (meaning not flawlessness but flaw tolerance, of course).
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;&gt; Self-defense excluded, I hope. Otherwise the OtterMind would
</EM><BR>
<EM>&gt;&gt; be a sitting duck.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;No, the Sysop Mind would defend you.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;how kind of the sysop. theocracy might sound nifty, but i don't think it
<BR>
would be stable, let alone doable, from a monkey point of view.
<BR>
<P><EM>&gt;&gt; Let's look at it this way: what if the government proposed a
</EM><BR>
<EM>&gt;&gt; system like this, i.e. everyone gets a chip implant that will
</EM><BR>
<EM>&gt;&gt; monitor his/her behaviour, and correct it if necessary so that
</EM><BR>
<EM>&gt;&gt; people no longer can (intentionally) harm eachother. How
</EM><BR>
<EM>&gt;&gt; would the public react? How would the members of this list
</EM><BR>
<EM>&gt;&gt; react? Wild guess: most wouldn't be too happy about it
</EM><BR>
<EM>&gt;&gt; (to use a titanic understatement). Blatant infringement of
</EM><BR>
<EM>&gt;&gt; fundamental rights and all that. Well, right they are. Now,
</EM><BR>
<EM>&gt;&gt; what would make this system all of a sudden &quot;acceptable&quot;
</EM><BR>
<EM>&gt;&gt; in a SI future? Does an increase in intelligence justify
</EM><BR>
<EM>&gt;&gt; this kind of coercion?
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;What makes the system unacceptable, if implemented by humans, is that
</EM><BR>
<EM>&gt;the humans have evolved to be corruptible and have an incredibly bad
</EM><BR>
<EM>&gt;track record at that sort of thing.  All the antigovernmental heuristics
</EM><BR>
<EM>&gt;of transhumanism have evolved from the simple fact that, historically,
</EM><BR>
<EM>&gt;government doesn't work.  However, an omniscient AI is no more likely to
</EM><BR>
<EM>&gt;become corrupt than a robot is likely to start lusting after human women.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;an omniscient ai is pretty much inscrutable, right? i don't know how we
<BR>
can evaluate the inscrutable's chances of becoming what we would call
<BR>
&quot;corrupt&quot;. i think the least inscrutable thing about an omniscient
<BR>
intelligence would be its need for resources. other then that... i dunno.
<BR>
<P><EM>&gt;&gt; And something else: you belief that a SI can do with
</EM><BR>
<EM>&gt;&gt; us as it pleases because of its massively superior
</EM><BR>
<EM>&gt;&gt; intelligence. Superior intelligence = superior morality,
</EM><BR>
<EM>&gt;&gt; correct?
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;No.  I believe that, for some level of intelligence above X - where X is
</EM><BR>
<EM>&gt;known to be higher than the level attained by modern humans in modern
</EM><BR>
<EM>&gt;civilization - it becomes possible to see the objectively correct moral
</EM><BR>
<EM>&gt;decisions.  It has nothing to do with who, or what, the SIs are.  Their
</EM><BR>
<EM>&gt;&quot;right&quot; is not a matter of social dominance due to superior
</EM><BR>
<EM>&gt;formidability, but a form of reasoning that both you or I would
</EM><BR>
<EM>&gt;inevitably agree with if we were only smart enough.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;That human moral reasoning is observer-dependent follows from the
</EM><BR>
<EM>&gt;historical fact that the dominant unit of evolutionary selection was the
</EM><BR>
<EM>&gt;individual.  There is no reason to expect similar effects to arise in a
</EM><BR>
<EM>&gt;system that be programmed to conceptualize itself as a design component
</EM><BR>
<EM>&gt;as easily as an agent or an individual, and more likely would simply
</EM><BR>
<EM>&gt;have not have any moral &quot;self&quot; at all.  I mean, something resembling an
</EM><BR>
<EM>&gt;&quot;I&quot; will probably evolve whether we design it or not, but that doesn't
</EM><BR>
<EM>&gt;imply that the &quot;I&quot; gets tangled up in the goal system.  Why would it?
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i fail to see how it could not get tangled up... even in a case like &quot;in
<BR>
order to maximize greeness, the resources over there should be used in this
<BR>
manner&quot; (which has no self-subject implied) a distenction must be made
<BR>
between resources more directly controlled (what i would call &quot;my stuff&quot;)
<BR>
and resources more indirectly controlled (what i would call &quot;other stuff&quot;),
<BR>
etc... and as soon as that distenction is made, degrees of
<BR>
ownership/beingness/whatever is implied, and from there promptly gets mixed
<BR>
up in the goal system...
<BR>
<P>[mad snippage...]
<BR>
<EM>&gt;&gt; Well, see above. This would only make sense in an *acutely*
</EM><BR>
<EM>&gt;&gt; desperate situation. By all means, go ahead with your research,
</EM><BR>
<EM>&gt;&gt; but I'd wait with the final steps until we know for sure
</EM><BR>
<EM>&gt;&gt; that uploading/space escape isn't going to make it. In that
</EM><BR>
<EM>&gt;&gt; case I'd certainly support a (temporary!) Sysop arrangement.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;I think that we have enough concrete knowledge of the social situation,
</EM><BR>
<EM>&gt;and of the pace of technological development, to say that a Sysop
</EM><BR>
<EM>&gt;arrangement will almost certainly become necessary.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;necessary? in the sense that such an arrangement will increase my odds of
<BR>
survival, etc? i doubt it, if only because the odds against my survival
<BR>
must be dire indeed (understatement) to justify the massive amount of work
<BR>
that would be required to make a sysop; effort that could rather be
<BR>
invested towards, say, getting off this planet; where getting off the
<BR>
planet would be a better stopgap anyway.
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unless, of course, you come up with a well thought out essay on the order
<BR>
of &quot;coding a transhuman ai&quot; discussing the creation of a specialized sysop
<BR>
ai. if you see a way to make it more doable then uploading, space travel,
<BR>
&quot;normal&quot; transcendent ai, and nanowar, more power to ya... but that sounds
<BR>
way tougher then &quot;coding&quot; was. maybe i'll trend towards advocating sysop
<BR>
creation when i think its doable in the relevent time frame.
<BR>
<P>[more snippage]
<BR>
<EM>&gt;When I say that the increment of utility is low, what I mean is that you
</EM><BR>
<EM>&gt;and your cohorts will inevitably decide to execute a Sysop-like
</EM><BR>
<EM>&gt;arrangement in any case.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i trend towards advocating a very dumb sysop, if it can be called that...
<BR>
a &quot;simple&quot; upload manager...
<BR>
<P><EM>&gt;You and a thousand other Mind-wannabes wish to
</EM><BR>
<EM>&gt;ensure your safety and survival.  One course of action is to upload,
</EM><BR>
<EM>&gt;grow on independent hardware, and then fight it out in space.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or just run the fuck away, and hopefully not fight it out for a very, very
<BR>
long time, if ever. dibs on alpha centauri... ;)
<BR>
<P><EM>&gt;If defense turns out to have an absolute, laws-of-physics advantage over
</EM><BR>
<EM>&gt;offense, then you'll all be safe.  I think this is extraordinarily
</EM><BR>
<EM>&gt;unlikely to be the case, given the historical trend.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;historical trend is better then nothing, of course, but i dunno if
<BR>
historical trend really cuts it in this case...
<BR>
<P><EM>&gt;If offense has an advantage over defense, you'll all fight it out until
</EM><BR>
<EM>&gt;only one Mind remains with a monopoly on available resources.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or we all will go Elsewhere... or we will all stalemate... or we will all
<BR>
borgify... or we will all decide to commit suicide... or (insert
<BR>
possibilities here that only a Power could think of).
<BR>
<P><EM>&gt;However, is the utility
</EM><BR>
<EM>&gt;of having the whole Solar System to yourself really a thousand times the
</EM><BR>
<EM>&gt;utility, the &quot;fun&quot;, of having a thousandth of the available resources? 
</EM><BR>
<EM>&gt;No.  You cannot have a thousand times as much fun with a thousand times
</EM><BR>
<EM>&gt;as much mass.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i don't see how we can know that. what if, just for example, we need the
<BR>
entire solar system to make a very special kind of black hole? geez...
<BR>
[insert rant about the incomperhensibility of Powers here].
<BR>
<P><EM>&gt;You need a peace treaty.  You need a system, a process, which ensures
</EM><BR>
<EM>&gt;your safety.  Humans (and the then-hypothetical human-derived Minds) are
</EM><BR>
<EM>&gt;not knowably transparent or trustworthy, and your safety cannot be
</EM><BR>
<EM>&gt;trusted to either a human judge or a process composed of humans.  The
</EM><BR>
<EM>&gt;clever thing to do would be to create a Sysop which ensures that the
</EM><BR>
<EM>&gt;thousand uploadees do not harm each other, which divides resources
</EM><BR>
<EM>&gt;equally and executes other commonsense rules.  Offense may win over
</EM><BR>
<EM>&gt;defense in physical reality, but not in software.  But now you're just
</EM><BR>
<EM>&gt;converging straight back to the same method I proposed...
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mutually assured destruction seems more clever then a sysop.
<BR>
<P><EM>&gt;The other half of the &quot;low utility&quot; part is philosophical; if there are
</EM><BR>
<EM>&gt;objective goals, you'll converge to them too, thus accomplishing exactly
</EM><BR>
<EM>&gt;the same thing as if some other Mind converged to those goals.  Whether
</EM><BR>
<EM>&gt;or not the Mind happens to be &quot;you&quot; is an arbitrary prejudice; if the
</EM><BR>
<EM>&gt;Otterborn Mind is bit-by-bit indistinguishable from an Eliezerborn or
</EM><BR>
<EM>&gt;AIborn Mind, but you take an action based on the distinction which
</EM><BR>
<EM>&gt;decreases your over-all-branches probability of genuine personal
</EM><BR>
<EM>&gt;survival, it's a stupid prejudice.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;what if the objective goal is to attain as much &quot;individuality&quot; (whatever
<BR>
that turns out to be) as possible... granted, borgification might happen.
<BR>
or not. i just wanna keep as many options open as possible.
<BR>
<P><EM>&gt;&gt; That's a tough one. Is &quot;survival&quot; an objective (super)goal? One
</EM><BR>
<EM>&gt;&gt; must be alive to have (other) goals, that's for sure, but this
</EM><BR>
<EM>&gt;&gt; makes it a super-subgoal rather than a supergoal. Survival
</EM><BR>
<EM>&gt;&gt; for its own sake is rather pointless. In the end it still comes
</EM><BR>
<EM>&gt;&gt; down to arbitrary, subjective choices IMO.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;Precisely; and, in this event, it's possible to construct a Living Pact
</EM><BR>
<EM>&gt;which runs on available hardware and gives you what you want at no
</EM><BR>
<EM>&gt;threat to anyone else, thus maximizing the social and technical
</EM><BR>
<EM>&gt;plausibility of the outcome.
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;what if i want to *be* said Pact?
<BR>
<P>sayke, v2.3.05
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4817.html">Ian Goddard: "Re: Prozac &amp; Violence - New Study"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4815.html">Mike Linksvayer: "Re: [Fwd: Corel Intel Deal in the making]"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4786.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4837.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4837.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4816">[ date ]</A>
<A HREF="index.html#4816">[ thread ]</A>
<A HREF="subject.html#4816">[ subject ]</A>
<A HREF="author.html#4816">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:05:05 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
