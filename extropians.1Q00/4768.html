<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Otter vs. Yudkowsky</TITLE>
<META NAME="Author" CONTENT="D.den Otter (neosapient@geocities.com)">
<META NAME="Subject" CONTENT="Re: Otter vs. Yudkowsky">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Otter vs. Yudkowsky</H1>
<!-- received="Mon Mar 13 15:41:43 2000" -->
<!-- isoreceived="20000313224143" -->
<!-- sent="Mon, 13 Mar 2000 23:32:34 +0100" -->
<!-- isosent="20000313223234" -->
<!-- name="D.den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: Otter vs. Yudkowsky" -->
<!-- id="38CD6C82.2BD@geocities.com" -->
<!-- inreplyto="Otter vs. Yudkowsky" -->
<STRONG>From:</STRONG> D.den Otter (<A HREF="mailto:neosapient@geocities.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38CD6C82.2BD@geocities.com&gt;"><EM>neosapient@geocities.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon Mar 13 2000 - 15:32:34 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4769.html">Joe E. Dees: "Re: near anything boxes allowed to br in the hands of the public?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4767.html">Michael S. Lorrey: "Re: extro wise day"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="4683.html">Eliezer S. Yudkowsky: "Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4786.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4786.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4768">[ date ]</A>
<A HREF="index.html#4768">[ thread ]</A>
<A HREF="subject.html#4768">[ subject ]</A>
<A HREF="author.html#4768">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
----------
<BR>
<EM>&gt; From: Eliezer S. Yudkowsky &lt;<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38CD6C82.2BD@geocities.com&gt;">sentience@pobox.com</A>&gt;
</EM><BR>
<P><EM>&gt; Otter, you and I were having an interesting discussion on the topic of
</EM><BR>
<EM>&gt; AI, nano, uploading, and navigation a few months back, trading 30K posts
</EM><BR>
<EM>&gt; back and forth, when the discussion suddenly halted for no particular
</EM><BR>
<EM>&gt; reason.  
</EM><BR>
<P>Stalemate? I dunno...
<BR>
<P><EM>&gt; Can we reopen the topic?
</EM><BR>
<P>The game is afoot.
<BR>
&nbsp;
<BR>
<EM>&gt; In particular, I think you're prejudiced against AIs.  I see humans and
</EM><BR>
<EM>&gt; AIs as occupying a continuum of cognitive architectures, while you seem
</EM><BR>
<EM>&gt; to be making several arbitrary distinctions between humans and AIs.  
</EM><BR>
<P>I don't have anything against AIs per se, and I agree that they are
<BR>
part of the cognitive continuum. I wouldn't distrust AIs more than I
<BR>
would distrust humans (&gt;H uploads). The problem with &gt;H AI is
<BR>
that, unlike for example nanotech and upload technology in general, 
<BR>
it isn't just another tool to help us overcome the limitations of 
<BR>
our current condition, but lirerally has a &quot;mind of its own&quot;. It's
<BR>
unpredictable, unreliable and therefore *bad* tech from the 
<BR>
traditional transhuman perspective. An powerful genie that, once
<BR>
released from its bottle, could grant a thousand wishes or send 
<BR>
you straight to Hell.
<BR>
Russian roulette.
<BR>
<P>If you see your personal survival as a mere bonus, and the 
<BR>
Singularity as a goal in itself, then of course &gt;H AI is a great
<BR>
tool for the job, but if you care about your survival and freedom --
<BR>
as, I belief, is one of the core tenets of Transhumanism/Extropianism--
<BR>
then &gt;H AI is only useful as a last resort in an utterly desperate
<BR>
situation. Example: grey goo is eating the planet and there's no
<BR>
way of stopping it, or a small group of survivors is in a space
<BR>
station, life support is slowly failing and is beyond repair etc.
<BR>
In cases like these it makes sense to say: &quot;hey, nothing to
<BR>
lose, let's activate the AI and hope for the best&quot;.
<BR>
<P><EM>&gt; You
</EM><BR>
<EM>&gt; write, for example, of gathering a group of humans for the purpose of
</EM><BR>
<EM>&gt; being uploaded simultaneously, where I would say that if you can use a
</EM><BR>
<EM>&gt; human for that purpose, you can also build an AI that will do the same thing.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; As you know, I don't think that any sufficiently advanced mind can be
</EM><BR>
<EM>&gt; coerced.  
</EM><BR>
<P>Yes, and as you may know I agree. That's why this isn't a viable
<BR>
option (again, from the traditional &gt;H perspective). What I suggest
<BR>
is that after sufficient testing with rodents, dogs &amp; monkeys you
<BR>
use &quot;dumb&quot; AI (fancy computer) to run the opload sequence. The
<BR>
participants might be uploaded into separate self-sufficient &quot;escape
<BR>
capsules&quot; which are then shot into space, which means that if
<BR>
a conflict follows it will be fought IRL, or they could be uploaded
<BR>
into a single medium, and fight it out (or merge or whatever) in
<BR>
VR. Though maybe not a very appealing choice to us individualists, 
<BR>
the hive mind could ultimately be a reasonable (necessary?) 
<BR>
comprimize. 15-30 years down the road we'll all probably be
<BR>
much more &quot;connected&quot; anyway, so presumably the threshold
<BR>
for this sort of thing wouldn't be as high as it currently is. Mixed 
<BR>
scenarios would also be possible, of course.
<BR>
&nbsp;
<BR>
<EM>&gt; If there is any force that would tend to act on Minds, any
</EM><BR>
<EM>&gt; inevitability in the ultimate goals a Mind adopts, then there is
</EM><BR>
<EM>&gt; basically nothing we can do about it.  And this would hold for both
</EM><BR>
<EM>&gt; humanborn and synthetic Minds; if it didn't hold for humans, then it
</EM><BR>
<EM>&gt; would not-hold for some class of AIs as well.
</EM><BR>
<P>True...
<BR>
&nbsp;
<BR>
<EM>&gt; If the ultimate goal of a Mind doesn't converge to a single point,
</EM><BR>
<EM>&gt; however, then it should be possible - although, perhaps, nontrivial - to
</EM><BR>
<EM>&gt; construct a synthetic Mind which possesses momentum.  Not &quot;coercions&quot;,
</EM><BR>
<EM>&gt; not elaborate laws, just a set of instructions which it carries out for
</EM><BR>
<EM>&gt; lack of anything better to do.  Which, as an outcome, would include
</EM><BR>
<EM>&gt; extending den Otter the opportunity to upload and upgrade.  It would
</EM><BR>
<EM>&gt; also include the instruction not to allow the OtterMind the opportunity
</EM><BR>
<EM>&gt; to harm others; 
</EM><BR>
<P>Self-defense excluded, I hope. Otherwise the OtterMind would
<BR>
be a sitting duck.
<BR>
<P><EM>&gt; this, in turn, would imply that the Sysop Mind must
</EM><BR>
<EM>&gt; maintain a level of intelligence in advance of OtterMind, and that it
</EM><BR>
<EM>&gt; must either maintain physical defenses undeniably more powerful than
</EM><BR>
<EM>&gt; that of the OtterMind, or that the OtterMind may only be allowed access
</EM><BR>
<EM>&gt; to external reality through a Sysop API (probably the latter).
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Which may *sound* constricting, but I really doubt it would *feel*
</EM><BR>
<EM>&gt; constricting.  Any goal that does not directly contradict the Sysop
</EM><BR>
<EM>&gt; Goals should be as easily and transparently accomplishable as if you
</EM><BR>
<EM>&gt; were the Sysop Mind yourself.
</EM><BR>
<P>Well, it's certainly better than nothing, but the fact remains that
<BR>
the Sysop mind could, at any time and for any reason, decide
<BR>
that it has better things to do than babysitting the OtterMind,
<BR>
and terminate/adapt the latter. Being completely at someone's/
<BR>
something's mercy is never a good idea. The Sysop mind, having
<BR>
evolved from a human design, could easily have some flaws
<BR>
that could eventually cause it to mutate into just about
<BR>
anything, including a vicious death star. Who would stop
<BR>
it then (surely not the relatively &quot;dumb&quot; and restricted 
<BR>
OtterMind)? Who monitors the Sysop?
<BR>
<P>Let's look at it this way: what if the government proposed a 
<BR>
system like this, i.e. everyone gets a chip implant that will
<BR>
monitor his/her behaviour, and correct it if necessary so that
<BR>
people no longer can (intentionally) harm eachother. How
<BR>
would the public react? How would the members of this list
<BR>
react? Wild guess: most wouldn't be too happy about it
<BR>
(to use a titanic understatement). Blatant infringement of 
<BR>
fundamental rights and all that. Well, right they are. Now,
<BR>
what would make this system all of a sudden &quot;acceptable&quot;
<BR>
in a SI future? Does an increase in intelligence justify
<BR>
this kind of coercion? 
<BR>
<P>And something else: you belief that a SI can do with
<BR>
us as it pleases because of its massively superior
<BR>
intelligence. Superior intelligence = superior morality,
<BR>
correct? This would have some interesting implications
<BR>
in the present (like it's cool to kill/torture animals, 
<BR>
infants, retards etc), but that aside. Point is, by coercing
<BR>
the &quot;ex-human&quot; SI (OtterMind in this case) by means
<BR>
of morally rigid Sysop, you'd implicitly assume that you, 
<BR>
a mere neurohack human, already know &quot;what's right&quot;. 
<BR>
You'd apparently just &quot;know&quot; that harming others goes 
<BR>
against Objective Morality. But since, according to
<BR>
your doctrine, it takes an unbound SI to find the true
<BR>
meaning of life (if there is any), this would violate the 
<BR>
rules of your own game, and ruin your Singularity.
<BR>
<P>There is an obvious compromize, though (and perhaps
<BR>
this is what you meant all along): the synthetic Minds
<BR>
make sure that everyone uploads and reaches (approx.)
<BR>
the same level of development (this means boosting 
<BR>
some Minds while slowing down others), and then they
<BR>
shut themselves down, or simply merge with the 
<BR>
&quot;human&quot; Minds. The latter are then free to find the
<BR>
true meaning of it all, and perhaps kill eachother in
<BR>
the process (or maybe not). It remains a tricky
<BR>
situation, of course; the synth(s) could mutate before
<BR>
shutdown, and kill/take over everyone. Well, no-one
<BR>
said that surviving the Singularity is going to be *easy*...
<BR>
&nbsp;
<BR>
<EM>&gt; The objection you frequently offer, Otter, is that any set of goals
</EM><BR>
<EM>&gt; requires &quot;survival&quot; as a subgoal; since humans can themselves become
</EM><BR>
<EM>&gt; Minds and thereby compete for resources, you reason, any Mind will
</EM><BR>
<EM>&gt; regard all Minds or potential Minds as threats.  However, the simple
</EM><BR>
<EM>&gt; requirement of survival or even superiority, as a momentum-goal, does
</EM><BR>
<EM>&gt; not imply the monopolization of available resources.  
</EM><BR>
<P>Yes it does, assuming the Mind is fully rational and doesn't
<BR>
like loose ends. The more control one has over one's surroundings,
<BR>
the better one's chances of survival are. Also we don't know
<BR>
how much resources a Mind would actually need, or how much
<BR>
it could actually use. But even if we assume that scarcity 
<BR>
will never be a problem, the security issue would still remain.
<BR>
Minds could have conflicts for other reasons than control
<BR>
of resources.
<BR>
<P><EM>&gt; In fact, if one of
</EM><BR>
<EM>&gt; the momentum-goals is to ensure that all humans have the opportunity to
</EM><BR>
<EM>&gt; be all they can be, then monopolization of resources would directly
</EM><BR>
<EM>&gt; interfere with that goal.  As a formal chain of reasoning, destroying
</EM><BR>
<EM>&gt; humans doesn't make sense.
</EM><BR>
<P>But where would this momentum goal come from? It's not a
<BR>
logical goal (like &quot;survival&quot; or &quot;fun&quot;) but an arbitrary goal, 
<BR>
a random preference (like one's favorite color, for example).
<BR>
Sure, a Mind could decide that it wants to keep, or develop,
<BR>
a sentimental attachment to humans, but I'd be very
<BR>
surprised indeed if this turned out to be the rule.
<BR>
&nbsp;
<BR>
<EM>&gt; Given both the technological difficulty of achieving simultaneous
</EM><BR>
<EM>&gt; perfect uploading before both nanowar and AI Transcendence, and the
</EM><BR>
<EM>&gt; sociological difficulty of doing *anything* in a noncooperative
</EM><BR>
<EM>&gt; environment, it seems to me that the most rational choice given the
</EM><BR>
<EM>&gt; Otter Goals - whatever they are - is to support the creation of a Sysop
</EM><BR>
<EM>&gt; Mind with a set of goals that permit the accomplishment of the Otter Goals.
</EM><BR>
<P>Well, see above. This would only make sense in an *acutely*
<BR>
desperate situation. By all means, go ahead with your research, 
<BR>
but I'd wait with the final steps until we know for sure 
<BR>
that uploading/space escape isn't going to make it. In that
<BR>
case I'd certainly support a (temporary!) Sysop arrangement.
<BR>
<P><EM>&gt; Yes, there's a significant probability that the momentum-goals will be
</EM><BR>
<EM>&gt; overridden by God knows what, but (1) as I understand it, you don't
</EM><BR>
<EM>&gt; believe in objectively existent supergoals 
</EM><BR>
<P>That's a tough one. Is &quot;survival&quot; an objective (super)goal? One
<BR>
must be alive to have (other) goals, that's for sure, but this
<BR>
makes it a super-subgoal rather than a supergoal. Survival
<BR>
for its own sake is rather pointless. In the end it still comes
<BR>
down to arbitrary, subjective choices IMO.
<BR>
<P>In any case, there's no need for &quot;objectively existent 
<BR>
supergoals&quot; to change the Sysop's mind; a simple
<BR>
glitch in the system could have the same result, for
<BR>
example.
<BR>
<P><EM>&gt; and (2) the creation of a
</EM><BR>
<EM>&gt; seed AI with Sysop instructions before 2015 is a realistic possibility;
</EM><BR>
<EM>&gt; achieving simultaneous uploading of a thousand-person group before 2015
</EM><BR>
<EM>&gt; is not.
</EM><BR>
<P>Well, that's your educated (and perhaps a wee bit biased) 
<BR>
guess, anyway. We'll see.
<BR>
<P>P.s: do you watch _Angel_ too?
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4769.html">Joe E. Dees: "Re: near anything boxes allowed to br in the hands of the public?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4767.html">Michael S. Lorrey: "Re: extro wise day"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="4683.html">Eliezer S. Yudkowsky: "Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4786.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4786.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4768">[ date ]</A>
<A HREF="index.html#4768">[ thread ]</A>
<A HREF="subject.html#4768">[ subject ]</A>
<A HREF="author.html#4768">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:05:03 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
