<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Otter vs. Yudkowsky</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: Otter vs. Yudkowsky">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Otter vs. Yudkowsky</H1>
<!-- received="Sat Mar 18 19:40:05 2000" -->
<!-- isoreceived="20000319024005" -->
<!-- sent="Sat, 18 Mar 2000 20:41:49 -0600" -->
<!-- isosent="20000319024149" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Otter vs. Yudkowsky" -->
<!-- id="38D43E63.360900C5@pobox.com" -->
<!-- inreplyto="38D3FC73.29E8@geocities.com" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38D43E63.360900C5@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Sat Mar 18 2000 - 19:41:49 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5322.html">Robert Owen: "A Satiric Ode to Cryonemesis"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5320.html">James Wetterau: "Re: Patent breakthrough- maybe we don't need them after all?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5300.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5327.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5327.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5328.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5321">[ date ]</A>
<A HREF="index.html#5321">[ thread ]</A>
<A HREF="subject.html#5321">[ subject ]</A>
<A HREF="author.html#5321">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
&quot;D.den Otter&quot; wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Yes, that's the *practical* side of the dispute. There's also the
</EM><BR>
<EM>&gt; philosophical issue of whether personal survival is more important
</EM><BR>
<EM>&gt; than the creation of superintelligent successors, &quot;egoism&quot; vs
</EM><BR>
<EM>&gt; &quot;altruism&quot; etc., of course. This inevitably adds an element of
</EM><BR>
<EM>&gt; bias to the above debate.
</EM><BR>
<P>I have no trouble seeing your point of view.  I am not attempting to
<BR>
persuade you to relinquish your selfishness; I am attempting to persuade
<BR>
you that the correct action is invariant under selfishness, altruism,
<BR>
and Externalism.
<BR>
<P><EM>&gt; &gt; &gt; Ok, you say something like 30% vs 0,1%, but how exactly
</EM><BR>
<EM>&gt; &gt; &gt; did you get these figures? Is there a particular passage in
</EM><BR>
<EM>&gt; &gt; &gt; _Coding..._ or _Plan to.._ that deals with this issue?
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; 30% I pulled out of thin air.  0.1% is a consequence of symmetry and the
</EM><BR>
<EM>&gt; &gt; fact that only one unique chance for success exists.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Out of thin air. Ok, so it could be really anything, including
</EM><BR>
<EM>&gt; some value &lt;0.1%.
</EM><BR>
<P>No, that estimate is definitely incorrect.  Using a value of less than
<BR>
10% or more than 70% would be unjustifiable.  30% was &quot;pulled out of the
<BR>
air&quot;; I'll happily defend the range itself.
<BR>
<P>More than 70% would be unjustifiable due to the Fermi Paradox and unknowability.
<BR>
<P>Since, if we can create a Sysop with specifiable stable goals, we win,
<BR>
to assert that the probability is less than 10% would require
<BR>
demonstrating that the probability of (A) External goals (and hostile
<BR>
ones, at that), or (B) the probability that stable arbitrary goals can
<BR>
be produced, are one or the other above 90%, or that their product is
<BR>
above 90%; which requires a degree of definite knowledge about these
<BR>
issues that nobody possesses.  Even if it were possible to rationally
<BR>
estimate the resulting &quot;specifiable stable goals&quot; probability as being
<BR>
below 10%, which I do not think is the case, then it would be absurd to
<BR>
argue it as being 1%.  To say that a 99% probability of &quot;no specifiable
<BR>
goals&quot; holds is to imply definite knowledge, which neither of us has.
<BR>
&nbsp;
<BR>
<EM>&gt; The longer you exist, the more opportunities there will be for
</EM><BR>
<EM>&gt; something to go wrong. That's pretty much a mathematical
</EM><BR>
<EM>&gt; certainty, afaik.
</EM><BR>
<P>While I view the growth of knowledge and intelligence as an open-ended
<BR>
process, essentially because I am an optimist, I do expect that all
<BR>
reasoning applicable to basic goals will have been identified and
<BR>
produced within a fairly small amount of time, with any remaining
<BR>
revision taking place within the sixth decimal place.  I expect the same
<BR>
to hold of the True Basic Ultimate Laws of Physics as well.  The problem
<BR>
is finite; the applications may be infinite, and the variations may be
<BR>
infinite, but the basic rules of reasoning, and any specific structure,
<BR>
are finite.
<BR>
<P><EM>&gt; Those are two different kinds of &quot;freedom&quot;. In the former case
</EM><BR>
<EM>&gt; you can go everywhere, but you carry your own partial prison
</EM><BR>
<EM>&gt; around in your head (like the guy from _Clockwork Orange_),
</EM><BR>
<EM>&gt; while in the latter case you may not be able to go anywhere
</EM><BR>
<EM>&gt; you want, but you *are* master of your own domain. I think
</EM><BR>
<EM>&gt; I prefer the latter, not only because it is more &quot;dignified&quot;
</EM><BR>
<EM>&gt; (silly human concept), but because it's wise to put as much
</EM><BR>
<EM>&gt; distance and defences between you and a potential
</EM><BR>
<EM>&gt; enemy as possible. When that enemy is sharing your
</EM><BR>
<EM>&gt; body, you have a real problem.
</EM><BR>
<P>Don't think of it as an enemy; think of it as an Operating System.
<BR>
<P><EM>&gt; &gt; An intelligent,
</EM><BR>
<EM>&gt; &gt; self-improving AI should teach itself to be above programmer error and
</EM><BR>
<EM>&gt; &gt; even hardware malfunction.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Yes, *if* you can get the basics right.
</EM><BR>
<P>I'll answer for that.
<BR>
<P><EM>&gt; Natural evolution may have made some pretty bad mistakes, but
</EM><BR>
<EM>&gt; that doesn't necessarily mean that *all* of our programming will become
</EM><BR>
<EM>&gt; obsolete. If the SIs want to do something, they will have to stay
</EM><BR>
<EM>&gt; alive to do it (unless of course they decide to kill themselves, but
</EM><BR>
<EM>&gt; let's assume for the sake of argument that this won't be the case).
</EM><BR>
<EM>&gt; Basic logic. So some sort of self-preservation &quot;instinct&quot; will be
</EM><BR>
<EM>&gt; required(*) to keep the forces of entropy at bay. Survival requires
</EM><BR>
<EM>&gt; control --the more the better-- over one's surroundings. Other
</EM><BR>
<EM>&gt; intelligent entities represent by definition an area of deminished
</EM><BR>
<EM>&gt; control, and must be studied and then placed in a threat/benefit
</EM><BR>
<EM>&gt; hierarchy which will help to determine future actions. And voila,
</EM><BR>
<EM>&gt; your basic social hierachy is born. The &quot;big happy egoless
</EM><BR>
<EM>&gt; cosmic family model&quot; only works when the other sentients
</EM><BR>
<EM>&gt; are either evolutionary dead-ends which are &quot;guaranteed&quot; to
</EM><BR>
<EM>&gt; remain insignificant, or completely and permanently like-minded.
</EM><BR>
<P>Nonsense.  If the other sentients exist within a trustworthy Operating
<BR>
System - I do think that a small Power should be able to design a
<BR>
super-Java emulation that even a big Power shouldn't be able to break
<BR>
out of; the problem is finite - then the other sentients pose no threat.
<BR>
&nbsp;Even if they do pose a threat, then your argument is analogous to
<BR>
saying that a rational operating system, which views its goal as
<BR>
providing the best possible environment for its subprocesses, will kill
<BR>
off all processes because they are untrustworthy.  As a logical chain,
<BR>
this is simply stupid.
<BR>
<P><EM>&gt; No, no, no! It's exactly the other way around; goals are
</EM><BR>
<EM>&gt; observer dependent by default. As far as we know this is
</EM><BR>
<EM>&gt; the only way they *can* be.
</EM><BR>
<P>I should correct my terminology; I should say that observer-*biased*
<BR>
goals are simply evolutionary artifacts.  Even if only
<BR>
observer-dependent goals are possible, this doesn't rule out the
<BR>
possibility of creating a Sysop with observer-unbiased goals.
<BR>
<P><EM>&gt; &gt; &gt; Fat comfort to *them* that the
</EM><BR>
<EM>&gt; &gt; &gt; Almighty have decided so in their infinite wisdom. I wouldn't
</EM><BR>
<EM>&gt; &gt; &gt; be much surprised if one of the &quot;eternal truths&quot; turns out to
</EM><BR>
<EM>&gt; &gt; &gt; be &quot;might makes right&quot;.
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; *I* would.  &quot;Might makes right&quot; is an evolutionary premise which I
</EM><BR>
<EM>&gt; &gt; understand in its evolutionary context.  To find it in a Mind would be
</EM><BR>
<EM>&gt; &gt; as surprising as discovering an inevitable taste for chocolate.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Evolution represents, among other things, some basic rules
</EM><BR>
<EM>&gt; for survival. No matter how smart the SIs will become, they'll
</EM><BR>
<EM>&gt; still have to play by the rules of this reality to live &amp; prosper.
</EM><BR>
<P>Your statement is simply incorrect.  The possibility of a super-Java
<BR>
encapsulation, which I tend to view as the default possibility - human
<BR>
Java can be broken because humans make mistakes, humans make mistakes
<BR>
because they're running with a high-level four-item short-term memory
<BR>
and no codic cortex, and a superintelligence which knows all the laws of
<BR>
physics and has a codic cortex should be able to design security a Power
<BR>
couldn't crack; the problem is finite - directly contradicts the
<BR>
necessity of all the survival activities you postulate.
<BR>
<P><EM>&gt; You can't deny self-evident truths like &quot;might makes right&quot;
</EM><BR>
<EM>&gt; without paying the price (decreased efficiency, possibly
</EM><BR>
<EM>&gt; serious damage or even annihilation) at some point. And
</EM><BR>
<EM>&gt; yes, I also belief that suicide is fundamentally stupid,
</EM><BR>
<EM>&gt; *especially* for a Power which could always alter its mind
</EM><BR>
<EM>&gt; and bliss out forever if there's nothing better to do.
</EM><BR>
<P>Only if the Power is set up to view this as desirable, and why would it
<BR>
be?  My current goal-system design plans don't even call for &quot;pleasure&quot;
<BR>
as a separate module, just selection of actions on the basis of their
<BR>
outcomes.  And despite your anthropomorphism, this does not consist of
<BR>
pleasure.  Pleasure is a complex functional adaptation which responds to
<BR>
success by reinforcing skills used, raising the level of mental energy,
<BR>
and many other subtle and automatic effects that I see no reason to
<BR>
preserve in an entity capable of consciously deciding how to modify
<BR>
itself.  In particular, your logic implies that the *real* supergoal is
<BR>
get-success-feedback, and that the conditions for success feedback are
<BR>
modifiable; this is not, however, an inevitable consequence of system
<BR>
architecture, and would in fact be spectacularly idiotic; it would
<BR>
require a deliberate effort by the system programmer to represent
<BR>
success-feedback as a declarative goal on the same level as the other
<BR>
initial supergoals, which would be simply stupid.
<BR>
<P><EM>&gt; The only
</EM><BR>
<EM>&gt; logical excuse for killing yourself is when one knows for pretty
</EM><BR>
<EM>&gt; damn sure, beyond all reasonable doubt, that the alternative
</EM><BR>
<EM>&gt; is permanent, or &quot;indefinite&quot;, hideous suffering.
</EM><BR>
<P>Nonsense; this is simply den Otter's preconditions.  I thought you had
<BR>
admitted - firmly and positively asserted, in fact - that this sort of
<BR>
thing was arbitrary?
<BR>
<P><EM>&gt; &gt; &gt; I don't know, it's rather difficult to imagine an untangled &quot;I&quot;.
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; So do I, but I can take a shot at it!
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; But you don't, more or less by defintition, really know what
</EM><BR>
<EM>&gt; you're doing!
</EM><BR>
<P>Of course I do.  I can see what a tangled &quot;I&quot; looks like, in detail,
<BR>
which enables me to take a shot at imagining what an untangled &quot;I&quot; would
<BR>
look like.  And in particular, I can trace particular visualizations on
<BR>
your part to features of that tangled &quot;I&quot; that would not be present in
<BR>
an untangled system.  I certainly understand enough to label specific
<BR>
arguments as anthropomorphic and false, regardless of whether I really
<BR>
have a constructive understanding of the design or behavior of an
<BR>
untangled system.
<BR>
<P><EM>&gt; &gt; When it reaches the point where any objective morality that exists would
</EM><BR>
<EM>&gt; &gt; probably have been discovered; i.e. when the lack of that discovery
</EM><BR>
<EM>&gt; &gt; counts as input to the Bayesian Probability Theorem.  When continuing to
</EM><BR>
<EM>&gt; &gt; act on the possibility would be transparently stupid even to you and I,
</EM><BR>
<EM>&gt; &gt; we might expect it to be transparently stupid to the AI.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; In other words, objective morality will always be just an educated
</EM><BR>
<EM>&gt; guess. Will there be a limit to evolution anyway? One would be
</EM><BR>
<EM>&gt; inclined to say &quot;yes, of course&quot;, but if this isn't the case, then
</EM><BR>
<EM>&gt; the quest for objective morality will go on forever.
</EM><BR>
<P>Well, you see &quot;objective morality&quot; as a romantic, floating label.  I see
<BR>
it as a finite and specifiable problem which, given true knowledge of
<BR>
the ultimate laws of physics, can be immediately labeled as either
<BR>
&quot;existent&quot; or &quot;nonexistent&quot; within the permissible system space.
<BR>
<P><EM>&gt; I'm sure you could make some pov-less freak in the lab, and
</EM><BR>
<EM>&gt; keep it alive under &quot;ideal&quot;, sterile conditions, but I doubt that
</EM><BR>
<EM>&gt; it would be very effective in the real world. As I see it, we have
</EM><BR>
<EM>&gt; two options: a) either the mind really has no &quot;self&quot; and no &quot;bias&quot;
</EM><BR>
<EM>&gt; when it comes to motivation, in which case it will probably just
</EM><BR>
<EM>&gt; sit there and do nothing, or b) it *does* have a &quot;self&quot;, or creates
</EM><BR>
<EM>&gt; one as a logical result of some pre-programmed goal(s), in
</EM><BR>
<EM>&gt; which case it is likely to eventually become completely
</EM><BR>
<EM>&gt; &quot;selfish&quot; due to a logical line of reasoning.
</EM><BR>
<P>Again, nonsense.  The Sysop would be viewable - would view itself -
<BR>
simply as an intelligent process that acted to maintain maximum freedom
<BR>
for the inhabitants, an operating system intended to provide equal
<BR>
services for the human species, its user base.  Your argument that
<BR>
subgoals could interfere with these supergoals amounts to postulating
<BR>
simple stupidity on the part of the Sysop.  Worries about other
<BR>
supergoals interfering are legitimate, and I acknowledge that, but your
<BR>
alleged chain of survival logic is simply bankrupt.
<BR>
<P><EM>&gt; [snakes &amp; rodents compared to AIs &amp; humans]
</EM><BR>
<EM>&gt; &gt; It would be very much different.  Both snakes and rodents evolved.
</EM><BR>
<EM>&gt; &gt; Humans may have evolved, but AIs haven't.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; But they will have to evolve in order to become SIs.
</EM><BR>
<P>No, they wouldn't.  &quot;Evolution&quot; is an extremely specific term yielding
<BR>
phenomena such as selection pressures, adaptations, competition for
<BR>
mates, and so on.  An AI would need to improve itself to become Sysop;
<BR>
this is quite a different proposition than evolution.
<BR>
<P><EM>&gt; I'd take the former, of course, but that's because the odds in this
</EM><BR>
<EM>&gt; particular example are extremely (and quite unrealistically so)
</EM><BR>
<EM>&gt; bad. In reality, it's not you vs the rest of humanity, but you vs
</EM><BR>
<EM>&gt; a relative small financial/technological elite, many (most) of
</EM><BR>
<EM>&gt; whom don't even fully grasp the potential of the machines they're
</EM><BR>
<EM>&gt; working on. Most people will simply never know what hit them.
</EM><BR>
<P>Even so, your chances are still only one in a thousand, tops - 0.1%, as
<BR>
I said before.
<BR>
<P><EM>&gt; Anyway, there are no certainties. AI is not a &quot;sure shot&quot;, but
</EM><BR>
<EM>&gt; just another blind gamble, so the whole analogy sort of
</EM><BR>
<EM>&gt; misses the point.
</EM><BR>
<P>Not at all; my point is that AI is a gamble with a {10%..70%} chance of
<BR>
getting 10^47 particles to compute with, while uploading is a gamble
<BR>
with a {0.0000001%..0.1%} of getting 10^56.  If you count in the rest of
<BR>
the galaxy, 10^58 particles vs. 10^67.
<BR>
<P><EM>&gt; Power corrupts, and absolute power...this may not apply just
</EM><BR>
<EM>&gt; to humans. Better have an Assembly of Independent Powers.
</EM><BR>
<EM>&gt; Perhaps the first thing they'd do is try to assassinate eachother,
</EM><BR>
<EM>&gt; that would be pretty funny.
</EM><BR>
<P>&quot;Funny&quot; is an interesting term for it.  You're anthropomorphizing again.
<BR>
&nbsp;What can you, as a cognitive designer, do with a design for a group of
<BR>
minds that you cannot do with a design for a single mind?  I think the
<BR>
very concept that this constitutes any sort of significant innovation,
<BR>
that it contributes materially to complexity in any way whatsover, is
<BR>
evolved-mind anthropomorphism in fee simple.  As I recall, you thought
<BR>
approximately the same thing, back when you, I, and Nick Bostrum were
<BR>
tearing apart Anders Sandberg's idea that an optimized design for a
<BR>
Power could involve humanlike subprocesses.
<BR>
<P><EM>&gt; &gt; &gt; To ask of me to just forget about continuity is like asking
</EM><BR>
<EM>&gt; &gt; &gt; you to just forget about the Singularity.
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; I *would* just forget about the Singularity, if it was necessary.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Necessary for what?
</EM><BR>
<P>Serving the ultimate good.
<BR>
<P><EM>&gt; In what context (form your pov, not mine) is the Singularity
</EM><BR>
<EM>&gt; rational, and why isn't this motivation just another irrational
</EM><BR>
<EM>&gt; prejudice that just happens to have strongly linked itself to
</EM><BR>
<EM>&gt; your pleasure centers? Aren't you really just rationalizing
</EM><BR>
<EM>&gt; an essentially &quot;irrational&quot; choice (supergoal) like the rest of
</EM><BR>
<EM>&gt; humanity?
</EM><BR>
<P>No.  I'm allowing the doubting, this-doesn't-make-sense part of my mind
<BR>
total freedom over every part of myself and my motivations; selfishness,
<BR>
altruism, and all.  I'm not altruistic because my parents told me to be,
<BR>
because I'm under the sway of some meme, or because I'm the puppet of my
<BR>
romantic emotions; I'm altruistic because of a sort of absolute
<BR>
self-cynicism under which selfishness makes even less sense than
<BR>
altruism.  Or at least that's how I'd explain things to a cynic.
<BR>
<P><EM>&gt; &gt; If it's an irrational prejudice, then let it go.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Then you'd have to stop thinking altogether, I'm afraid.
</EM><BR>
<P>Anxiety!  Circular logic!  If you just let *go*, you'll find that your
<BR>
mind continues to function, except that you don't have to rationalize
<BR>
falsehoods for fear of what will happen if you let yourself see the
<BR>
truth.  Your mind will go on as before, just a little cleaner.
<BR>
<P><PRE>
--
<P>&gt; Sure, but what's really interesting is that his conscience
&gt; (aka &quot;soul&quot;) is a *curse*, specifically designed to make him
&gt; suffer (*).
<P>I wasn't too impressed with the ethical reasoning of the Calderash
either, nor Angel's for that matter.  But - and totally new (to me)
insights like this one are one of the primary reasons I watch _Buffy_ -
real people, in real life, can totally ignore the structural ethics when
strong emotions are in play.  Giles and Xander blaming the returned
Angel for Angelus's deeds; Buffy torturing herself for sending someone
to Hell who would have gone there anyway - along with everyone else - if
she hadn't done a thing; and so on.
<P>Still:  (1)  You'll notice that Angel hasn't committed suicide or
ditched his soul, both actions which he knows perfectly well how to
execute.  (2)  Why should I care what the Calderash gypsies think?  Does
Joss Whedon (Buffy-creator) know more about this than I do?  
-- 
       <A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38D43E63.360900C5@pobox.com&gt;">sentience@pobox.com</A>      Eliezer S. Yudkowsky
          <A HREF="http://pobox.com/~sentience/beyond.html">http://pobox.com/~sentience/beyond.html</A>
                 Member, Extropy Institute
           Senior Associate, Foresight Institute
</PRE>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5322.html">Robert Owen: "A Satiric Ode to Cryonemesis"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5320.html">James Wetterau: "Re: Patent breakthrough- maybe we don't need them after all?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5300.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5327.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5327.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5328.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5321">[ date ]</A>
<A HREF="index.html#5321">[ thread ]</A>
<A HREF="subject.html#5321">[ subject ]</A>
<A HREF="author.html#5321">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:05:47 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
