<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Ethics as Science</TITLE>
<META NAME="Author" CONTENT="Dan Fabulich (daniel.fabulich@yale.edu)">
<META NAME="Subject" CONTENT="Re: Ethics as Science">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Ethics as Science</H1>
<!-- received="Fri Mar  3 11:47:43 2000" -->
<!-- isoreceived="20000303184743" -->
<!-- sent="Fri, 3 Mar 2000 13:47:41 -0500 (EST)" -->
<!-- isosent="20000303184741" -->
<!-- name="Dan Fabulich" -->
<!-- email="daniel.fabulich@yale.edu" -->
<!-- subject="Re: Ethics as Science" -->
<!-- id="Pine.GSO.4.10.10003031144120.11990-100000@mercury.cis.yale.edu" -->
<!-- inreplyto="4.2.0.58.20000303093844.00a7cf00@mason.gmu.edu" -->
<STRONG>From:</STRONG> Dan Fabulich (<A HREF="mailto:daniel.fabulich@yale.edu?Subject=Re:%20Ethics%20as%20Science&In-Reply-To=&lt;Pine.GSO.4.10.10003031144120.11990-100000@mercury.cis.yale.edu&gt;"><EM>daniel.fabulich@yale.edu</EM></A>)<BR>
<STRONG>Date:</STRONG> Fri Mar 03 2000 - 11:47:41 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4052.html">hal@finney.org: "Fair division"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4050.html">Dan Fabulich: "RE: ECON: Eliezer's calls"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4042.html">Robin Hanson: "Re: Ethics as Science"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4058.html">Robin Hanson: "Re: Ethics as Science"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4051">[ date ]</A>
<A HREF="index.html#4051">[ thread ]</A>
<A HREF="subject.html#4051">[ subject ]</A>
<A HREF="author.html#4051">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
<EM>&gt; &gt;The fact that I'm going to believe X *normally* doesn't provide any argument
</EM><BR>
<EM>&gt; &gt;at all for believing X now, but the fact that I'll believe X at the end of
</EM><BR>
<EM>&gt; &gt;inquiry *does* provide me with reason to believe X.  ... your scientific
</EM><BR>
<EM>&gt; &gt;process ... argument ... it's a dismal failure if it fails at all, because
</EM><BR>
<EM>&gt; &gt;*nothing* like the argument from the best answer applies his cloudier crystal
</EM><BR>
<EM>&gt; &gt;ball.  ... [the same claim is made several more times in other words]
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I think this is just wrong.  The difference between your beliefs now and at
</EM><BR>
<EM>&gt; the &quot;end&quot; of inquiry is made up of a bunch of little differences between
</EM><BR>
<EM>&gt; nearby points in time.  Anything that informs you about your beliefs at any
</EM><BR>
<EM>&gt; future time implicitly informs you about your beliefs at the &quot;end&quot;.  I could
</EM><BR>
<EM>&gt; prove this in a Bayesian framework if you would find that informative.
</EM><BR>
<P>Go ahead.  Right off the bat, I posit that you can't provide me with such
<BR>
a proof at all if I'm going to have an infinite number of logically
<BR>
independent thoughts about ethics (as at an Omega point, if any).
<BR>
<P>Before you begin, however, take note of an interesting fact.  (You jammed
<BR>
this into [argument] brackets, without, as I see it, answering the point.)  
<BR>
The probability that I'll change my beliefs in light of what your
<BR>
computation tells me provides an upper limit on the certainty of your
<BR>
prediction; in particular, if it is 100% likely that I will change my
<BR>
beliefs in light of your report, your report can have no certainty at all,
<BR>
since even a process which attempts to predict what beliefs I'm LIKELY to
<BR>
have will have to revise its judgment based on how I'll change my beliefs
<BR>
in light of its report.  So its computation will have to take into account
<BR>
the result of its own computation before yielding its answer.  This
<BR>
argument holds for arbitrarily weak certainty on the part of the computer;
<BR>
any certainty greater than &quot;no certainty whatsoever&quot; suffers from this
<BR>
problem if the chance that I'll change my beliefs is unity.
<BR>
<P>Let me add that I'm not suggesting changing my beliefs based on what your
<BR>
computation tells me *simply* to be difficult, but because I'm hoping that
<BR>
it will give me the *right* answers, and that, upon encountering them,
<BR>
I'll want to change my beliefs to those straightaway.
<BR>
<P>But consider ANOTHER interesting fact.  I'm either going to change my
<BR>
beliefs in light of your computation's report, or I'm not.  If I'm not,
<BR>
then the computation hasn't told me anything new.  (Nor will the fact that
<BR>
I'll never change my belief that X give me any additional reason to
<BR>
believe that X.)  But if I AM going to change my beliefs, in the hope of
<BR>
making my beliefs converge on the right answers, then the computation
<BR>
can't tell me anything with any certainty whatsoever.
<BR>
<P>All this is just another way of saying that I can't know what I'm going to
<BR>
decide until I decide to do it, that I, as a computer, can't predict the
<BR>
results of my own computation.
<BR>
<P><EM>&gt; Let's consider a physics analogy.  [...] You want the mass of the
</EM><BR>
<EM>&gt; particular atom you have in mind, and no you aren't going to tell me
</EM><BR>
<EM>&gt; which one that is.
</EM><BR>
<P>This is a totally faulty analogy.  Totally unlike this example, the
<BR>
question of what ethical beliefs I'm going to have is entirely well-posed
<BR>
and completely verifiable after the fact.  (So long as we take
<BR>
functionalism to be right, and I do.)
<BR>
<P><EM>&gt; The analogy should be obvious: Your exact moral beliefs at any point in
</EM><BR>
<EM>&gt; time will be determined by your brain state, some of which will be
</EM><BR>
<EM>&gt; unobservable by the rest of us.  That doesn't mean that we can't learn
</EM><BR>
<EM>&gt; most everything there is to know about morals by learning how brains
</EM><BR>
<EM>&gt; evolve their moral beliefs.
</EM><BR>
<P>Look, I'm sure this machine could tell me a lot about you and it might
<BR>
tell you a lot about me, (it would be quite a feat if I ever came to know
<BR>
what you're going to do as well as you did!) but the whole point of this
<BR>
machine is to tell ME about me (or us about us) so that we can get the
<BR>
machine to tell us what our ethical beliefs will/should be.  That's what
<BR>
this process can't do.
<BR>
<P>-Dan
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-unless you love someone-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;-nothing else makes any sense-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e.e. cummings
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4052.html">hal@finney.org: "Fair division"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4050.html">Dan Fabulich: "RE: ECON: Eliezer's calls"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4042.html">Robin Hanson: "Re: Ethics as Science"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4058.html">Robin Hanson: "Re: Ethics as Science"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4051">[ date ]</A>
<A HREF="index.html#4051">[ thread ]</A>
<A HREF="subject.html#4051">[ subject ]</A>
<A HREF="author.html#4051">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:04:30 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
