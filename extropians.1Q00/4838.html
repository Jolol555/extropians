<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Luddites are everywhere!</TITLE>
<META NAME="Author" CONTENT="hal@finney.org (hal@finney.org)">
<META NAME="Subject" CONTENT="Re: Luddites are everywhere!">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Luddites are everywhere!</H1>
<!-- received="Tue Mar 14 00:36:52 2000" -->
<!-- isoreceived="20000314073652" -->
<!-- sent="Mon, 13 Mar 2000 23:38:52 -0800" -->
<!-- isosent="20000314073852" -->
<!-- name="hal@finney.org" -->
<!-- email="hal@finney.org" -->
<!-- subject="Re: Luddites are everywhere!" -->
<!-- id="200003140738.XAA04383@finney.org" -->
<!-- inreplyto="Luddites are everywhere!" -->
<STRONG>From:</STRONG> <A HREF="mailto:hal@finney.org?Subject=Re:%20Luddites%20are%20everywhere!&In-Reply-To=&lt;200003140738.XAA04383@finney.org&gt;"><EM>hal@finney.org</EM></A><BR>
<STRONG>Date:</STRONG> Tue Mar 14 2000 - 00:38:52 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4839.html">hal@finney.org: "Re: both denOtter and Yudkowsky: possible singularity scenarios"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4837.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="4833.html">John Clark: "Luddites are everywhere!"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4873.html">Robin Hanson: "Re: Luddites are everywhere!"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4873.html">Robin Hanson: "Re: Luddites are everywhere!"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4838">[ date ]</A>
<A HREF="index.html#4838">[ thread ]</A>
<A HREF="subject.html#4838">[ subject ]</A>
<A HREF="author.html#4838">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
John Clark, &lt;<A HREF="mailto:jonkc@worldnet.att.net?Subject=Re:%20Luddites%20are%20everywhere!&In-Reply-To=&lt;200003140738.XAA04383@finney.org&gt;">jonkc@worldnet.att.net</A>&gt;, writes:
<BR>
<EM>&gt;
</EM><BR>
<EM>&gt; In the current issue of Wired magazine is an article I did not expect
</EM><BR>
<EM>&gt; to see, the author expresses great fear over Nanotechnology, computers
</EM><BR>
<EM>&gt; smarter than humans, and of course genetic engineering.  After expressing
</EM><BR>
<EM>&gt; sympathy for the ideas if not the methods of Ted Kaczynski, the mad
</EM><BR>
<EM>&gt; Unibomber, the author advises &quot;limiting our pursuit of certain kinds
</EM><BR>
<EM>&gt; of knowledge&quot;. The really strange thing is not the content, it's pretty
</EM><BR>
<EM>&gt; standard Luddite stuff, the bizarre thing is that the author is Bill Joy,
</EM><BR>
<EM>&gt; chief scientist and cofounder of Sun Microsystems.  It's a little sad
</EM><BR>
<EM>&gt; to see an attitude like that in the very heart of Silicon Valley.
</EM><BR>
<P>This is now the third mention of this article in the last two days.
<BR>
<P>I have had a chance to read it now, and I was unhappy with it, for
<BR>
several reasons.
<BR>
<P>For one thing, it is a one sided polemic.  However it is written in a
<BR>
calm, dispassionate style which somewhat disguises that fact.  There is
<BR>
little development of the advantages that technology could offer, and
<BR>
considerably more extensive writing about the dangers.
<BR>
<P>However, even with this emphasis, the dangers aren't really spelled out
<BR>
very clearly.  For robotics, we are given only a quote from Moravec about
<BR>
how they will inevitably out-compete humans.  We have discussed here in
<BR>
the past the principle of comparative advantage, which gives an economic
<BR>
rationale for even uncompetitive parties to carve out an economic niche,
<BR>
so that the outcome is not as one-sided as Moravec (who is no economist)
<BR>
forecasts.
<BR>
<P>For nanotech it is the standard grey goo scenario, which we have also
<BR>
debated at length.  The upshot is that it is not that easy to devise an
<BR>
effective gray goo that cannot be defeated by comparable technology.
<BR>
<P>For genetic engineering it is any of the scary frankenfood scenarios
<BR>
now being harped upon by the greens.  Joy quotes his friends Amory
<BR>
and Hunter Lovins, long-time politically active environmentalists,
<BR>
in opposition to genetic engineering.  Their article, available at
<BR>
<A HREF="http://www.rmi.org/biotechnology/twobotanies.html">http://www.rmi.org/biotechnology/twobotanies.html</A>, is IMO very poorly
<BR>
reasoned and full of cheap rhetoric.
<BR>
<P>With this rather sketchy outline of the dangers, Joy fleshes out his
<BR>
article with a rambling passage about the early days of the nuclear arms
<BR>
race, with the implication that the world would have been a better place
<BR>
had the technology somehow been turned over to an international body.
<BR>
<P>He looks for essentially the same solution for the three dangerous
<BR>
technologies: &quot;relinquishment&quot;.  Meaning, you give up on them and stop
<BR>
working on them.  And of course an important element of this has to be
<BR>
&quot;transparency&quot;, i.e. universal monitoring.  (Oddly enough he doesn't
<BR>
mention David Brin, although he manages to drag in the Doomsday Effect.)
<BR>
<P>There is no discussion of the dangers of setting up such a universal
<BR>
monitoring system.  There is no discussion of the suffering and deaths of
<BR>
poor people around the world if economic growth is thwarted by preventing
<BR>
technological development.  In short there is no discussion of the risks
<BR>
and costs associated with his recommended path.
<BR>
<P>Instead we are told of the advice of the Dalai Lama, who tells us to
<BR>
conduct our lives with love and compassion for others.  We hear about
<BR>
Jacques Attali, who supposedly inspired Joy's approach to the Java and
<BR>
Jini architectures, telling us that the highest utopian goal is altruism.
<BR>
This alone &quot;associates individual happiness with the happiness of
<BR>
others, affording the promise of self-sustainment.&quot;  (As the Randians
<BR>
love to point out, this circular definition is useless; if everyone
<BR>
adopts altruism, the only way to make someone happy is to make other
<BR>
people happy.  But then you don't know how to make anyone happy unless
<BR>
you already know how to make someone happy.  It's a philosophy without
<BR>
a foundation, and in practice the only reason it appears to work is
<BR>
because no one is truly altruistic.  Altruism is a philosophy which
<BR>
would collapse into meaninglessness if everyone truly followed it.)
<BR>
<P>Given that he literally wants to shift the path of the entire world
<BR>
economy, he needs to come up with a much better explanation of the
<BR>
problems and why the solutions we have planned for can't work.  In his
<BR>
discussion of the early days of the nuclear arms race, he makes the
<BR>
analogy that we are in the same position today as those early nuclear
<BR>
scientists.  But in fact, nuclear weapons have not been the end of the
<BR>
world as many predicted.  Is it right to give up our future just on the
<BR>
basis of fears?
<BR>
<P>Last year, Tim May used to post on usenet with a signature line
<BR>
which read, Y2K: It's not the odds, it's the stakes.  But I think
<BR>
that we all learned that in fact the odds are pretty important too.
<BR>
The sheer magnitude of the Y2K non-event suggests to me that the odds
<BR>
of a catastrophe actually were pretty near zero.  I didn't see that;
<BR>
like a lot of other people, I was fooled.
<BR>
<P>But it seems that Joy is making something of the same mistake.  The stakes
<BR>
are high, yes; survival of humanity, survival of life.  But the odds
<BR>
must be evaluated as well.  Every course carries risk, consequences,
<BR>
and costs.  We must look at the big picture, and a one sided analysis
<BR>
does not do that.
<BR>
<P>I found the last page of the article to be reminiscent in tone of
<BR>
Douglass Hofstadter's sanctimonious essay on his decision to join
<BR>
the nuclear freeze movement of the 1980s.  In hindsight this was a
<BR>
misguided effort, which if it had succeeded would only have enabled
<BR>
the corrupt and evil Soviet empire to stay intact a few years longer.
<BR>
(And don't get me started on the tainted science behind the nuclear
<BR>
winter scenario from the same period, one of the worst examples of
<BR>
politics masquerading as science, and heavily supported by Carl Sagan,
<BR>
whom Joy quotes approvingly.)
<BR>
<P>Like Hofstadter before him, Joy can't quite understand why all the smart
<BR>
people he associates with, apparently largely tree-hugging Birkenstock
<BR>
liberals like himself, aren't seized by the need to Save Humanity
<BR>
by adopting the Grand Project that he sees so clearly.  He's puzzled,
<BR>
and slightly hurt.  Disappointed, more.  He thought humanity was so
<BR>
much _better_ than that.
<BR>
<P>Of course, Hofstadter only wanted us to stop building weapons.
<BR>
Joy expects us to halt economic progress itself.  I think it's pretty
<BR>
clear who's crazier.
<BR>
<P>A final quote from Joy:
<BR>
<P>&quot;I believe we must find alternative outputs for our creative forces,
<BR>
beyond the culture of perpetual economic growth; this growth has largely
<BR>
been a blessing for several hundred years, but it has not brought us
<BR>
unalloyed happiness, and we must now choose between the pursuit of
<BR>
unrestricted and undirected growth through science and technology and
<BR>
the clear accompanying dangers.&quot;
<BR>
<P>Fractured syntax aside, Bill Joy as a co-founder of Sun Microsystems is
<BR>
probably worth hundreds of millions of dollars.  He is sitting awfully
<BR>
pretty to be calling for a halt to economic growth.  Let him give away
<BR>
his vast wealth, and go to live as a subsistence farmer in Zaire for a
<BR>
few years.  Then he will be in a better moral position to lecture us on
<BR>
the need to stop growth.
<BR>
<P>I haven't had time to even mention the scary pictures which decorate
<BR>
the article, genetically engineered foods, nuclear explosions (lots of
<BR>
those), Strangelovian equipment, biohazard suits, a boy swimming in a
<BR>
polluted lake.  It all adds up to technology out of control.
<BR>
<P>In short, it's a powerful but one sided argument, strong on attack but
<BR>
short on defense.  Rebuttals and counter arguments are certainly possible.
<BR>
But in practice, the ideas seem so outlandish and impossible to implement
<BR>
that it seems unlikely to go anywhere.  (Of course, that's what I thought
<BR>
about David Brin as well, and almost every day I read something more
<BR>
about how good it would be to get rid of privacy once and for all...)
<BR>
<P>Hal
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4839.html">hal@finney.org: "Re: both denOtter and Yudkowsky: possible singularity scenarios"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4837.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="4833.html">John Clark: "Luddites are everywhere!"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4873.html">Robin Hanson: "Re: Luddites are everywhere!"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4873.html">Robin Hanson: "Re: Luddites are everywhere!"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4838">[ date ]</A>
<A HREF="index.html#4838">[ thread ]</A>
<A HREF="subject.html#4838">[ subject ]</A>
<A HREF="author.html#4838">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:05:07 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
