<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Otter vs. Yudkowsky</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: Otter vs. Yudkowsky">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Otter vs. Yudkowsky</H1>
<!-- received="Sun Mar 26 18:51:18 2000" -->
<!-- isoreceived="20000327015118" -->
<!-- sent="Sun, 26 Mar 2000 19:54:34 -0600" -->
<!-- isosent="20000327015434" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Otter vs. Yudkowsky" -->
<!-- id="38DEBF4A.AFC2452B@pobox.com" -->
<!-- inreplyto="38DBF6DB.29C6@geocities.com" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38DEBF4A.AFC2452B@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Sun Mar 26 2000 - 18:54:34 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5892.html">EvMick@aol.com: "Re: some humor from a website that 'translates's your name!"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5890.html">EvMick@aol.com: "Meme Infection"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5733.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5891">[ date ]</A>
<A HREF="index.html#5891">[ thread ]</A>
<A HREF="subject.html#5891">[ subject ]</A>
<A HREF="author.html#5891">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
&quot;D.den Otter&quot; wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; If nano is relatively easy, if it quickly becomes dangerous and
</EM><BR>
<EM>&gt; uncontrollable and if strong AI is much easier than human
</EM><BR>
<EM>&gt; upgrading then your approach does indeed make sense, as a last
</EM><BR>
<EM>&gt; desperate effort of a dying race.
</EM><BR>
<P>Well, right to begin with, I think this *is* the situation.  My
<BR>
*particular* scenario is not beyond reasonable doubt, but the
<BR>
impossibility of the perfect-uploading-on-the-first-try scenario *is*. 
<BR>
The *only* way you have a chance of segueing directly from nanotech to
<BR>
uploading is if you can obtain and hack the nanomeds to attempt BCI
<BR>
(Brain-Computer Interface) upgrading, followed by uploading of mild
<BR>
transhumans with enough power to figure out how to upload themselves. 
<BR>
This itself is improbable; neurocommunicative nanotech is considerably
<BR>
more advanced than that needed for nanowar.
<BR>
<P>The thing to remember, though, is that upgrading is an inherently
<BR>
runaway process, and therefore inherently unsynchronized.  What actually
<BR>
happens is that the first transhuman who gets that far immediately sees
<BR>
the crushing necessity of using vis nanocomputer to write vis own Sysop,
<BR>
so as not to run the risk of anyone else writing a Sysop first.  Even if
<BR>
your transhumans stay as themselves all the way, the first one to reach
<BR>
the point of rewriting vis own source code winds up as the One Power. 
<BR>
Even if you had absolutely no competitors, if it's possible to become a
<BR>
Power and maintain any shred of yourself, then it was possible to write
<BR>
a Sysop all along.  If you do not maintain any shred of your thoughts,
<BR>
emotions, and initial motivations, then it is *not you* and you are
<BR>
*dead*, as clearly as if you'd put a bullet through your head.
<BR>
<P><EM>&gt; However, if the situation
</EM><BR>
<EM>&gt; turns out to be less urgent, then (gradual) uploading is definitely
</EM><BR>
<EM>&gt; the way to go from a &quot;selfish&quot; perspective because, instead
</EM><BR>
<EM>&gt; of being forced to passively await a very uncertain future, one
</EM><BR>
<EM>&gt; can significantly improve one's odds by stepping up one's efforts.
</EM><BR>
<EM>&gt; Initiative &amp; creativity are rewarded, as they should be. You can
</EM><BR>
<EM>&gt; outsmart (or understand and make deals with, for that matter)
</EM><BR>
<EM>&gt; other uploads, but you can't do that with a transhuman AI if
</EM><BR>
<EM>&gt; you're merely human or an upload running on its system. It's
</EM><BR>
<EM>&gt; not just an instinctive, irrational illusion of control, you
</EM><BR>
<EM>&gt; really *have* more control.
</EM><BR>
<P>I get the feeling that maintaining control is a Ruling Argument for you.
<BR>
&nbsp;One of my Ruling Arguments, for example, is the thought that we're all
<BR>
going to wind up looking like complete idiots for attempting to control
<BR>
*any* of this; that we do not understand what's Really Going On any more
<BR>
than Neanderthals sitting around the campfire, and that all our punches
<BR>
will connect only with air.  And moreover, this is a *happy* ending. 
<BR>
One of the thoughts I find most cheering, when it comes to imagining
<BR>
what happens on that Last Day when I finally do get uploaded, is that I
<BR>
will *not* be spending the next billion years on an endless round of
<BR>
simulated physical pleasures.  I don't *know* what will happen next.  It
<BR>
really is a step into the true unknown.  And that's what keeps it exciting.
<BR>
<P>Nonetheless, with so much at stake, I can override my own worries about
<BR>
winding up looking like an idiot, and think about Sysop instructions
<BR>
*anyway*, just in case we *do* understand what's going on.  The prospect
<BR>
of failing to properly appreciate the mystery, no matter how much I hate
<BR>
it, does not change the fact that I cannot reasonably state the chance
<BR>
that we *are* right to be less than 10%.  That possibility must
<BR>
therefore be prepared for, even though there is a 90% probability that I
<BR>
will wind up looking like an idiot.
<BR>
<P>When cold, quantitative logic gives unambiguous recommendations, I can
<BR>
override a Ruling Argument.  Your Ruling Argument about loss of control
<BR>
is not in accordance with the simple, quantitative logic dictating the
<BR>
best probability of survival.  You yourself have laid down the rules for
<BR>
what is, to you, the rational action.  It is the action that maximizes
<BR>
your probability of survival with pleasurable experiences.  Maintaining
<BR>
control is a *subgoal* of that necessity.  It is illogical to optimize
<BR>
all actions for maintaining control if that involves *defiance* of the supergoal.
<BR>
<P>Subgoals are not physical laws.  They are heuristics.  They have
<BR>
exceptions.  In exceptional cases, it is necessary to calculate the
<BR>
supergoal directly.  To do otherwise is to abandon intelligence in favor
<BR>
of blind inertia.
<BR>
<P><EM>&gt; &gt; Or in other words, I'm sure den Otter would agree that 70% is a
</EM><BR>
<EM>&gt; &gt; reasonable upper bound on our chance of success given our current
</EM><BR>
<EM>&gt; &gt; knowledge (although I'm sure he thinks it's too optimistic).
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Well yes, ok, I do think it's somewhat optimistic, but, strictly
</EM><BR>
<EM>&gt; speaking, the 10-70% range is reasonable enough. Needless
</EM><BR>
<EM>&gt; to say, when you look at the average we're dealing with some
</EM><BR>
<EM>&gt; pretty bad odds here. I mean, if those were your odds to
</EM><BR>
<EM>&gt; survive a particular operation, you'd probably want your cryonics
</EM><BR>
<EM>&gt; organization to do a [rescue team] standby at the hospital.
</EM><BR>
<EM>&gt; Unfortunately, in the Singularity Game there are no backups
</EM><BR>
<EM>&gt; and no second chances.
</EM><BR>
<P>I quite agree.  So what?  That 10%-70% is the best available chance
<BR>
which is affected by my actions.  That's all I care about.
<BR>
<P><EM>&gt; &gt; [...] I do expect that all
</EM><BR>
<EM>&gt; &gt; reasoning applicable to basic goals will have been identified and
</EM><BR>
<EM>&gt; &gt; produced within a fairly small amount of time, with any remaining
</EM><BR>
<EM>&gt; &gt; revision taking place within the sixth decimal place.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Well, I hope you're right. The biggest risk is probably, as you
</EM><BR>
<EM>&gt; already suggested, at the very beginning; that's when human error
</EM><BR>
<EM>&gt; (i.e. bad programming/bad hardware/some freak mishap)
</EM><BR>
<EM>&gt; could thoroughly mess up the AI's mental structure, making
</EM><BR>
<EM>&gt; it utterly unpredictable and potentially very dangerous. Or
</EM><BR>
<EM>&gt; just useless, of course. This possibility should by no means
</EM><BR>
<EM>&gt; be underestimated.
</EM><BR>
<P>I agree.  Do not underestimate the possibility of bad programming, bad
<BR>
hardware, and some freak mishap combining to mess up a *human* in the
<BR>
process of *uploading* to new hardware and then *upgrading* to new
<BR>
levels of intelligence.  Leaving aside your Ruling Argument, the most
<BR>
rational course of action might be to write a Sysop and have *it*
<BR>
upgrade *you*, solely on the grounds that undertaking the journey
<BR>
yourself gives you a less than 10%-70% chance.
<BR>
<P>I've never been an upload, of course, but I'm being a neurohack, writing
<BR>
&quot;Coding a Transhuman AI&quot;, and trying to figure out how to write a goal
<BR>
system has given me some insight as to what happens when an evolved mind
<BR>
starts to acquire fine-grained self-awareness.  You (den Otter) are, of
<BR>
course, allowed to tell me to shut up, but I think that if you went
<BR>
through simply what *I've* been through as the result of being brilliant
<BR>
and introspecting with the aid of cognitive science, your personality
<BR>
would wind up looking like something that went through a shredder.  If
<BR>
it happened gradually, you'd build up new stuff to replace the old.  If
<BR>
it happened fast - if more than one thing happened simultaneously -
<BR>
there'd be a serious risk of insanity or an unknown circular-logic
<BR>
effect embedding itself.  In either case, I'm really don't know what
<BR>
your motivations would be like afterwards.
<BR>
<P>A slow transition, if events occurred in the right order, would have a
<BR>
fairly high chance of giving rise to another Yudkowsky *when* you were
<BR>
at *this point* on the curve - I don't know what happens afterwards.  If
<BR>
you had the ability to observe and manipulate your emotions completely
<BR>
before you started work on upgrading intelligence, as seems likely, then
<BR>
I really don't know what would happen.  I can predict the human
<BR>
emotional sequiturs of increasing intelligence up to my own level, but
<BR>
not if the intelligence itself is choosing them.
<BR>
<P><EM>&gt; [Sysop]
</EM><BR>
<EM>&gt; &gt; Don't think of it as an enemy; think of it as an Operating System.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Operating Systems can be your enemy too; as a Win95 user
</EM><BR>
<EM>&gt; I know what I'm talking about...
</EM><BR>
<P>You're personifying again.  Think of a Sysop as an archetypal OS, one
<BR>
that does what you tell it to, invisibly and transparently unless you
<BR>
ask it to behave otherwise.
<BR>
<P><EM>&gt; Yes, but what if the other sentients *aren't* part of the
</EM><BR>
<EM>&gt; simulation (alien life forms from distant galaxies, uploads
</EM><BR>
<EM>&gt; or AIs that have ascended more or less simultaneously but
</EM><BR>
<EM>&gt; independently). By &quot;surroundings&quot; I meant the &quot;real world&quot;
</EM><BR>
<EM>&gt; outside the SI technosphere, not the (semi-autonomous)
</EM><BR>
<EM>&gt; simulations it is running. I agree that containing the latter
</EM><BR>
<EM>&gt; shouldn't be too much of a problem, but that's not the
</EM><BR>
<EM>&gt; issue here.
</EM><BR>
<P>Why is it safer to be on the front lines yourself than it is to let the
<BR>
Sysop do it, aside from the emotional anxiety-binding you're using as a
<BR>
Ruling Argument?  Two Powers in the same situation are presumably going
<BR>
to come up with the same plans of attack or defense; your actual chance
<BR>
of survival is completely unaltered.
<BR>
<P><EM>&gt; &gt; I should correct my terminology; I should say that observer-*biased*
</EM><BR>
<EM>&gt; &gt; goals are simply evolutionary artifacts.  Even if only
</EM><BR>
<EM>&gt; &gt; observer-dependent goals are possible, this doesn't rule out the
</EM><BR>
<EM>&gt; &gt; possibility of creating a Sysop with observer-unbiased goals.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; That's a lot better. I still think that a totally &quot;selfless&quot;
</EM><BR>
<EM>&gt; being is a lower form of life no matter how smart it
</EM><BR>
<EM>&gt; is otherwise (a giant leap backwards on the ladder of
</EM><BR>
<EM>&gt; evolution, really), but perhaps this is just an &quot;irrational&quot;
</EM><BR>
<EM>&gt; matter of personal taste. In any case, the prudent
</EM><BR>
<EM>&gt; approach is to keep your ego until you're certain, beyond
</EM><BR>
<EM>&gt; all reasonable doubt, that it is not an asset but a handicap
</EM><BR>
<EM>&gt; (or useless appendage). In other words, you'll have to
</EM><BR>
<EM>&gt; become a SI first.
</EM><BR>
<P>Unless you don't start out with an ego.  And I should note, by the way,
<BR>
that the emotional bindings backing up that &quot;beyond all reasonable
<BR>
doubt&quot; might be one of the first things to dissolve if you started
<BR>
upgrading; instead of &quot;anxiety about making a mistake&quot; being a Ruling
<BR>
Argument, you might just quietly multiply probabilities together.  One
<BR>
of the universals of my own journey has been that the more you practice
<BR>
self-alteration, the less anxious you become about altering other things.
<BR>
<P><EM>&gt; Is it your aim to &quot;encapsulate&quot; the whole known universe with
</EM><BR>
<EM>&gt; everything in it? Regardless of your answer, if there *are* other
</EM><BR>
<EM>&gt; sentients in this universe, you will eventually have to deal
</EM><BR>
<EM>&gt; with them. Either you come to them -swallowing galaxies
</EM><BR>
<EM>&gt; as you go along- or they come to you. Then you'll have to
</EM><BR>
<EM>&gt; deal with them, not in Java-land, but in reality-as-we-know-it
</EM><BR>
<EM>&gt; where the harsh rules of evolution (may) still apply.
</EM><BR>
<P>Okay, but if it's possible for a Solar-originating Power to either win
<BR>
or manage to hold off the enemy, we're safe inside the Sysop; it is
<BR>
nonthinkable for the behaviors necessary on the *outside* to alter the
<BR>
behaviors executed on the *inside*.  A Sysop is not an evolved mind.  It
<BR>
does not have *habits*.
<BR>
<P><EM>&gt; No, it wouldn't need the pleasure/pain mechanism for
</EM><BR>
<EM>&gt; self-modification, but that doesn't mean that the pure
</EM><BR>
<EM>&gt; emotion &quot;pleasure&quot; will become redundant; if your machine
</EM><BR>
<EM>&gt; seeks the &quot;meaning of life&quot;, it might very well re-discover
</EM><BR>
<EM>&gt; this particular emotion. If it's inherently logical, it *will*
</EM><BR>
<EM>&gt; eventually happen, or so you say.
</EM><BR>
<P>I think of objective morality as being more analogous to a discoverable
<BR>
physical substance, although admittedly the formalization is as an
<BR>
inevitable chain of logic.  Anyway, this truly strikes me as being
<BR>
spectacularly unlikely.  Pleasure has no logical value; it's simply a
<BR>
highly inefficient and error-prone way to implement a system.  It is
<BR>
nonthinkable for a better architecture to deliberately cause itself to
<BR>
transition to a pleasure-based architecture; the pleasure-based
<BR>
architecture might approve, but it isn't *there* yet; the *previous*
<BR>
system has to approve, and there is no rational, supergoal-serving
<BR>
reason for it to do so.
<BR>
<P><EM>&gt; &gt; In particular, your logic implies that the *real* supergoal is
</EM><BR>
<EM>&gt; &gt; get-success-feedback,
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Forget success-feedback, we're talking about an &quot;untangled&quot;
</EM><BR>
<EM>&gt; emotion for its own sake.
</EM><BR>
<P>I don't get it.  What is pleasure, if not success feedback?  I know what
<BR>
pleasure does in the human brain.  It's not magic.  An ever-mounting
<BR>
number stored inside a LISP atom labeled &quot;pleasure&quot; is no more
<BR>
&quot;pleasure&quot; than the system clock unless it has side effects.  What are
<BR>
the side effects?  And which parts define &quot;pleasure&quot;?  Why would a
<BR>
protective, supergoal-based Sysop decide that it served those goals to
<BR>
recenter itself around the sole goal of increasing some number labeled
<BR>
&quot;pleasure&quot;?  Heck, why would you?
<BR>
<P><EM>&gt; The supergoal would be &quot;to have
</EM><BR>
<EM>&gt; fun&quot;, and the best way to do this is to have a separate
</EM><BR>
<EM>&gt; module for this, and let &quot;lower&quot; autonomous systems sort
</EM><BR>
<EM>&gt; out the rest. The power would be happy &amp; gay all the time,
</EM><BR>
<EM>&gt; no matter what happened, without being cognitively impaired
</EM><BR>
<EM>&gt; as ecstatic humans tend to be.
</EM><BR>
<P>Why doesn't the Sysop worry that the autonomous system would take over,
<BR>
crush the Sysop, and institute its own pleasure-having module?  Why
<BR>
can't you view the Sysop as the autonomous system that is a part of
<BR>
yourself?  Why couldn't the Sysop view itself as a part of you, to the
<BR>
extent that it was necessary to &quot;view itself&quot; as anything at all?  All
<BR>
your instincts that have anything to do with dividing the Universe into
<BR>
&quot;agents&quot; are, in a fundamental sense, arbitrary.
<BR>
<P><EM>&gt; If you don't allow an entitity to experience the emotion
</EM><BR>
<EM>&gt; &quot;pleasure&quot;, you may have robbed it of something &quot;inherently&quot;
</EM><BR>
<EM>&gt; good. Not because emotions are needed for functioning
</EM><BR>
<EM>&gt; and expanding, but because pure, un-attached, freely
</EM><BR>
<EM>&gt; controllable pleasure is a *bonus*. You have 3 basic
</EM><BR>
<EM>&gt; levels: -1 is &quot;suffering&quot;, 0 is the the absence of emotions,
</EM><BR>
<EM>&gt; good or bad (death is one such state) and 1 is
</EM><BR>
<EM>&gt; &quot;pleasure&quot;. Why would a fully freethinking SI want
</EM><BR>
<EM>&gt; to remain on level 0 when it can move, without sacrificing
</EM><BR>
<EM>&gt; anything, to level 1, which is &quot;good&quot; by defintion
</EM><BR>
<EM>&gt; (I think I'm happy, therefore I *am* happy or something
</EM><BR>
<EM>&gt; like that).
</EM><BR>
<P>It is *not* good by the only definition that matters - the Sysop's. 
<BR>
System A does not transition to System B because System B thinks it's a
<BR>
good idea, but because System A thinks it's a good idea.
<BR>
<P><EM>&gt; Think of it as a SI doing crack, but without all
</EM><BR>
<EM>&gt; the nasty side-effects. Would there be any logical reason
</EM><BR>
<EM>&gt; _not_ to &quot;do drugs&quot;, i.e. bliss out, if it didn't impair your
</EM><BR>
<EM>&gt; overall functioning in any way?
</EM><BR>
<P>But it would.  If blissing out is anything for *us* to worry about, than
<BR>
it's something for a protective Sysop to worry about.  In any case, what
<BR>
exactly *is* pleasure, on the source-code level, and what distinguishes
<BR>
it from, say, modeling the three-body problem?  Why is a Power
<BR>
intrinsically more likely to devote more space to one than to the other?
<BR>
<P><EM>&gt; Bottom line: it doesn't
</EM><BR>
<EM>&gt; matter whether you include pleasure in the original
</EM><BR>
<EM>&gt; design; if your machine seeks the ultimate good and
</EM><BR>
<EM>&gt; has the ability to modify itself accordingly, it may
</EM><BR>
<EM>&gt; introduce a pleasure module at some point because
</EM><BR>
<EM>&gt; it concludes that nothing else makes (more) sense.
</EM><BR>
<EM>&gt; I think it's very likely, you may think the opposite, but
</EM><BR>
<EM>&gt; that the possibility exists is beyond any reasonable
</EM><BR>
<EM>&gt; doubt.
</EM><BR>
<P>If the goal system is improperly designed and fragile, there is the
<BR>
possibility of that happening.  There is also the possibility that no
<BR>
matter *how* I design a goal system, human or otherwise, it will come to
<BR>
view itself on a level where the indicator is the supergoal, then
<BR>
short-circuit.  The first possibility is Something I Control, and while
<BR>
I don't know that I could sit down and write a non-fragile goal system,
<BR>
I do think that once we can write the prehuman architecture we'll
<BR>
understand what to put in it.  From an engineering standpoint, I am not
<BR>
intrinsically more worried about nonfragile goals than about getting the
<BR>
seed AI and the subsequent Power to do nonfragile physics or nonfragile mathematics.
<BR>
<P>In the later event, we're talking about the possibility that all Powers
<BR>
do something that I would regard as sterile and essentially insane.  If
<BR>
so, then we're all doomed to wind up dead or sterile, and there's very
<BR>
little I can do about it.
<BR>
<P><EM>&gt; Yes, IMO everything is arbitrary in the end, but not everything
</EM><BR>
<EM>&gt; is equally arbitrary in the context of our current situation. If
</EM><BR>
<EM>&gt; you strip everything away you're left with a pleasure-pain
</EM><BR>
<EM>&gt; mechanism, the carrot and the stick.
</EM><BR>
<P>Au contraire.  Carrots and sticks are one of the first things that get
<BR>
stripped away.
<BR>
<P>If you strip *everything* away, you're left with the system's behavior,
<BR>
i.e. the list of choices that it makes.  On a higher level, you can
<BR>
hopefully view those choices as tending to maximize or minimize the
<BR>
presence or absence of various labels which describe the physical
<BR>
Universe; these labels are the behavioral supergoals.  If the choices
<BR>
are made by a process which is internally consistent in certain ways,
<BR>
and the process declaratively represents those labels, then those labels
<BR>
are cognitive supergoals.  To some extent regarding a physical system as
<BR>
a cognitive process is an observer-dependent decision, but it makes
<BR>
sense as an engineering assumption.
<BR>
<P><EM>&gt;From a crystalline standpoint, all you need for that basic assumption is
</EM><BR>
a representation of external reality, a set of supergoals (as a function
<BR>
acting on representations), a set of available choices, and a model of
<BR>
those choices' effects within the representation, and a piece of code
<BR>
which actually makes choices by comparing the degree of supergoal
<BR>
fulfillment in the represented outcomes.
<BR>
<P>The human system, if it were a lot more crystalline than it is, would be
<BR>
regarded as implementing the code via an indirection, which we call a
<BR>
pleasure-and-pain system.  Pleasure and pain are determined by supergoal
<BR>
fulfillment, and choices are made on the basis of pleasure and pain;
<BR>
therefore, the possibility exists for the system to short-circuit by
<BR>
changing the conditions for supergoal fulfillment.  But pleasure and
<BR>
pain are not an intrinsic part of the system.
<BR>
<P>Because a pleasure-and-pain system can do anything a direct-supergoal
<BR>
system can do, it will always be possible to anthropomorphize a
<BR>
direct-supergoal system as &quot;doing what makes it happy&quot;, but the code for
<BR>
a pleasure-and-pain system will be physically absent, along with the
<BR>
chance of short-circuiting.
<BR>
<P><EM>&gt; From our current pov,
</EM><BR>
<EM>&gt; it makes sense to enhance the carrot and to get rid of the
</EM><BR>
<EM>&gt; stick altogether (while untangling the reward system from
</EM><BR>
<EM>&gt; our survival mechanism and making the latter more or
</EM><BR>
<EM>&gt; less &quot;autonomous&quot;, for obvious reasons). AIs could still
</EM><BR>
<EM>&gt; be useful as semi-independent brain modules that take
</EM><BR>
<EM>&gt; care of system while the &quot;I&quot; is doing its bliss routine.
</EM><BR>
<EM>&gt; &quot;Mindless&quot; Sysops that still have to answer *fully* to
</EM><BR>
<EM>&gt; the superintelligent &quot;I&quot;, whose line of consciousness can
</EM><BR>
<EM>&gt; be traced back directly to one or several human uploads.
</EM><BR>
<P>And why aren't these &quot;Mindless&quot; Sysops subject to all the what-iffery
<BR>
you unloaded on my living peace treaties?  If you can safely build a
<BR>
Sysop which is fully responsible to you, you can build a Sysop which is
<BR>
a safe protector of humanity.  Who the Sysop is responsible to is not a
<BR>
basic architectural variable and does not affect whether or not it works.
<BR>
<P>All you're really pointing out is that, if you're a Power, you might be
<BR>
able to compartmentalize your mind.  Doesn't really affect the choices
<BR>
from our perspective, except to point up the basic subjectivity of what
<BR>
constitutes a &quot;compartment&quot;.
<BR>
<P><EM>&gt; &gt; Well, you see &quot;objective morality&quot; as a romantic, floating label.  I see
</EM><BR>
<EM>&gt; &gt; it as a finite and specifiable problem which, given true knowledge of
</EM><BR>
<EM>&gt; &gt; the ultimate laws of physics, can be immediately labeled as either
</EM><BR>
<EM>&gt; &gt; &quot;existent&quot; or &quot;nonexistent&quot; within the permissible system space.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; You're still driven by &quot;arbitrary&quot; emotions, attaching value to
</EM><BR>
<EM>&gt; random items (well, not completely random in an evolutionary
</EM><BR>
<EM>&gt; context; seeking &quot;perfection&quot; can be good for survival). At the
</EM><BR>
<EM>&gt; very least you should recognize that your desire to get rid of &quot;all
</EM><BR>
<EM>&gt; human suffering&quot; is just an emotional, &quot;evolved&quot; monkey
</EM><BR>
<EM>&gt; hangup (the whole altruism thing, of which this is clearly
</EM><BR>
<EM>&gt; a part, is just another survival strategy. Nothing more, nothing
</EM><BR>
<EM>&gt; less). But that's ok, we're all still merely human after all.
</EM><BR>
<P>It is not a survival strategy.  It is an adaptation.  Humans are not
<BR>
fitness-maximizers.  We are adaptation-executors.  All I care about is
<BR>
whether those adaptations serve my current purpose.  I don't care what
<BR>
their &quot;purpose&quot; was in the ancestral environment, except insofar as
<BR>
knowledge of evolutionary psychology helps me maintain control of my own
<BR>
mind.  I do not care if a pocket calculator grew on a tree and evolved
<BR>
arithmetical ability in response to selection pressures, as long as it
<BR>
gives the right answers.
<BR>
<P>Yes, part of my altruism-for-fellow-humans is evolved; although there's
<BR>
a genuine &quot;what if pleasure qualia are objectively moral&quot; train of
<BR>
thought in there, in the event that there's no objective purpose to the
<BR>
Universe, altruism is what I'd use for a goal.
<BR>
<P>Any objective purpose would take precedence over the altruism, because
<BR>
the objective purpose is real and the altruism is only a shadow of
<BR>
evolution (unless the objective purpose verifies the altruism, of
<BR>
course).  The same would hold for you, whenever you became smart enough
<BR>
for the pressure of logic to become irresistable.
<BR>
<P><EM>&gt; You said yourself that your Sysop would have power-like
</EM><BR>
<EM>&gt; abilities, and could reprogram itself completely if so desired.
</EM><BR>
<EM>&gt; I mean, an entity that can't even free itself from its original
</EM><BR>
<EM>&gt; programming can hardly be called a Power, can it? Perhaps
</EM><BR>
<EM>&gt; you could more or less guarantee the reliability (as a
</EM><BR>
<EM>&gt; defender of mankind) of a &quot;dumb&quot; Sysop (probably too
</EM><BR>
<EM>&gt; difficult to complete in time, if at all possible), but not
</EM><BR>
<EM>&gt; that of truly intelligent system which just happens to
</EM><BR>
<EM>&gt; have been given some initial goals. Even humans can
</EM><BR>
<EM>&gt; change their supergoals &quot;just like that&quot;, let alone SIs.
</EM><BR>
<P>But why would the Sysop do so?  There has to be a reason.  You wouldn't
<BR>
hesitate to designate as worthlessly &quot;improbable&quot; the scenario in which
<BR>
the Sysop reprograms itself as a pepperoni pizza.
<BR>
<P><EM>&gt; And how would the search for ultimate truth fit into
</EM><BR>
<EM>&gt; this picture, anyway? Does &quot;the truth&quot; have a lower,
</EM><BR>
<EM>&gt; equal or higher priority than protecting humans?
</EM><BR>
<P>The truth is what the Sysop *is*.  A Mind is a reflection of the truth
<BR>
contained in a cognitive representation.  If that truth dictates
<BR>
actions, it dictates actions.  If that truth does not dictate actions,
<BR>
then the actions are dictated by a separate system of supergoals.  If
<BR>
supergoals can be &quot;true&quot; or &quot;false&quot;, and they are false, then they
<BR>
disappear from the mirror.
<BR>
<P><EM>&gt; Unless it encounters outside competition. It may not have to
</EM><BR>
<EM>&gt; compete for mates, but resources and safety issues can be
</EM><BR>
<EM>&gt; expected to remain significant even for Powers.
</EM><BR>
<P>Evolution requires population.  From our perspective, all that's
<BR>
significant is the Power we start out in, which either lives or dies.
<BR>
<P><EM>&gt; Also, even
</EM><BR>
<EM>&gt; if we assume no outside competition (&quot;empty skies&quot;), it
</EM><BR>
<EM>&gt; still could make a *really* bad judgement call while upgrading
</EM><BR>
<EM>&gt; itself.
</EM><BR>
<P>So could you.  So could I.  Of the three, I'll trust the seed AI.  The
<BR>
seed AI is designed to tolerate change.  We are not.  I would regard any
<BR>
other judgement call as suspect, probably biased by that evolved-mind
<BR>
observer-biased tendency to trust oneself, a function of selection
<BR>
pressures in ancestral politics and ignorance of brain-damage case histories.
<BR>
<P>Do you know that damage to the right side of the brain can result in
<BR>
anosognosia, in which left-side appendage(s) (an arm, a leg, both) are
<BR>
paralyzed, but the patient doesn't know it?  They'll rationalize away
<BR>
their inability to move it.  Try to tell them otherwise, and they'll
<BR>
became angry.  It could happen to *you*.  Do you really trust yourself
<BR>
to survive uploading and upgrading without running haywire?
<BR>
<P><EM>&gt; &gt; Even so, your chances are still only one in a thousand, tops - 0.1%, as
</EM><BR>
<EM>&gt; &gt; I said before.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Again, only in the worst-case &quot;highlander&quot; scenario and only
</EM><BR>
<EM>&gt; if the upload group would really be that big (it would probably
</EM><BR>
<EM>&gt; be a lot smaller).
</EM><BR>
<P>The smaller the group, the less likely you are to obtain the necessary
<BR>
nanotechnology and take a shot at it.  I regard the Highlander Scenario
<BR>
as the default, since it only requires *one* attack-beats-defense or
<BR>
runaway-feedback-desynchronizes situation at *any* point along the
<BR>
self-enhancement curve, and my current engineering knowledge leads me to
<BR>
think that *both* are likely to hold at *all* points along the curve. 
<BR>
The uploading scenario is far, far, far more tenuous than the Sysop
<BR>
scenario.  The number of non-default assumptions necessary for the
<BR>
project to be executed, for the utility to be significant, and for there
<BR>
to be more than one survivor places it into the category of non-useful probabilities.
<BR>
<P><EM>&gt; &gt; Not at all; my point is that AI is a gamble with a {10%..70%} chance of
</EM><BR>
<EM>&gt; &gt; getting 10^47 particles to compute with, while uploading is a gamble
</EM><BR>
<EM>&gt; &gt; with a {0.0000001%..0.1%} of getting 10^56.  If you count in the rest of
</EM><BR>
<EM>&gt; &gt; the galaxy, 10^58 particles vs. 10^67.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Think bigger.
</EM><BR>
<P>Okay.  Knuthillions vs. knuthillions.
<BR>
<P><EM>&gt; Anyway, this is not (just) about getting as much
</EM><BR>
<EM>&gt; particles as possible for your computations (&quot;greed&quot;), but rather
</EM><BR>
<EM>&gt; about threat control (&quot;survival&quot;).
</EM><BR>
<P>I agree, except that I'd delete the &quot;just&quot;.  The greed
<BR>
utility-differentials are small enough to be irrelevant.
<BR>
<P><EM>&gt; There is an &quot;unknown&quot; chance
</EM><BR>
<EM>&gt; that the Sysop will turn against you (for whatever reason), in
</EM><BR>
<EM>&gt; which case you're at a horrible disadvantage, much more so
</EM><BR>
<EM>&gt; than in the case of a MAD failure and subsequent battle between
</EM><BR>
<EM>&gt; more or less equally developed Powers. I'd rather fight a
</EM><BR>
<EM>&gt; thousand peers than one massively superior Power anytime.
</EM><BR>
<P>If you have to fight, you lose.  Period.  
<BR>
<P><EM>&gt; Uploads basically have two chances to survive: 1) You make
</EM><BR>
<EM>&gt; a deal, most likely based on MAD, and no-one fights, everyone
</EM><BR>
<EM>&gt; lives.
</EM><BR>
<P>As an end-state, this is physically plausible; i.e. either defense beats
<BR>
attack, or any unstoppable attack can be detected in time to launch an
<BR>
unstoppable retaliation.  I wouldn't really argue with a &quot;10%-70%&quot;
<BR>
probability of stability for the physical situation you describe,
<BR>
although I'd make it more like &quot;20%-40%&quot;.  The problem is maintaining
<BR>
perfect synchronization until Power level is attained - without invoking
<BR>
a Sysop, of course.  How do you get from your basement laboratory to a
<BR>
group of independent Powers running on independent hardware?  Without
<BR>
allowing runaway self-enhancement or first-strike capability on anyone's
<BR>
part at any point along the curve?
<BR>
<P>Even if that happens, the social and technological tenuousities involved
<BR>
with postulating uploading on your part (*before* Sysop achievement on
<BR>
mine) are more than enough to place your scenario beyond the realm of
<BR>
the possible.  And if you managed to deal even with *that*, you'd still
<BR>
have to deal with the philosophical problems of &quot;destructive upgrading&quot;,
<BR>
which should, by your declared rational rules, decrease the perceived
<BR>
utility-increment of your scenario (from the den Otter perspective) over
<BR>
the Sysop scenario (from the den Otter perspective).
<BR>
<P>It's not just one objection, it's *all* the objections.
<BR>
<P><EM>&gt; 2) If this fails, you still have a fighting 0.1% chance
</EM><BR>
<EM>&gt; even in the worst case scenario, i.e. when everyone fights to
</EM><BR>
<EM>&gt; the death
</EM><BR>
<P>This chance is trivial.  0.1%, nonconsequential, as stated.  It should
<BR>
not factor into rational calculations.
<BR>
<P><EM>&gt; (*which isn't likely*; SIs ain't stupid so they're
</EM><BR>
<EM>&gt; more likely to compromize than to fight a battle with such bad
</EM><BR>
<EM>&gt; odds).
</EM><BR>
<P>Only if compromise is enforceable by counterstrikes, or if treaty is
<BR>
enforceable by Hofstadterian superrationality.  I think the second
<BR>
scenario is nonthinkable, and the first scenario is dependent on physics.
<BR>
<P><EM>&gt; Therefore, I have to conclude that an upload's
</EM><BR>
<EM>&gt; chance would be considerably better than your 0.1%
</EM><BR>
<EM>&gt; figure. 10-70%, i.e. Sysop range, would be a lot more
</EM><BR>
<EM>&gt; realistic, IMO. SI may not like competition, but they
</EM><BR>
<EM>&gt; are no *retards*. I'd be surprised indeed if they just
</EM><BR>
<EM>&gt; started bashing each other like a bunch of cavemen.
</EM><BR>
<EM>&gt; If MAD can even work for tribes of highly irrational
</EM><BR>
<EM>&gt; monkeys, it sure as hell should work for highly rational
</EM><BR>
<EM>&gt; Powers.
</EM><BR>
<P>Again, I'll okay the possible physical plausibility of your end-state,
<BR>
but I really don't think you can get there from here.
<BR>
<P><EM>&gt; &gt; What can you, as a cognitive designer, do with a design for a group of
</EM><BR>
<EM>&gt; &gt; minds that you cannot do with a design for a single mind?  I think the
</EM><BR>
<EM>&gt; &gt; very concept that this constitutes any sort of significant innovation,
</EM><BR>
<EM>&gt; &gt; that it contributes materially to complexity in any way whatsover, is
</EM><BR>
<EM>&gt; &gt; evolved-mind anthropomorphism in fee simple.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Just plain old multiple redundancy. Seemed like a good idea
</EM><BR>
<EM>&gt; when there's so much at stake and humans are doing the
</EM><BR>
<EM>&gt; initial programming.
</EM><BR>
<P>If you're gonna do multiple redundancy, do it with multiple versions of
<BR>
internal modules in a single individual.  Don't do it with multiple
<BR>
individuals all using the same architecture.  That's just asking for trouble.
<BR>
<P><EM>&gt; &gt; As I recall, you thought
</EM><BR>
<EM>&gt; &gt; approximately the same thing, back when you, I, and Nick Bostrum were
</EM><BR>
<EM>&gt; &gt; tearing apart Anders Sandberg's idea that an optimized design for a
</EM><BR>
<EM>&gt; &gt; Power could involve humanlike subprocesses.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Ah, those were the days.  Was that before or after we smashed
</EM><BR>
<EM>&gt; Max More's idea that a SI would need others to interact with
</EM><BR>
<EM>&gt; (for economical or social reasons?
</EM><BR>
<P>Yep, those were the days.
<BR>
<P><EM>&gt; Anyway, I still agree that
</EM><BR>
<EM>&gt; messy human subprocesses should be kept out of a SI's
</EM><BR>
<EM>&gt; mental frame, of course. No disagreement here. But what
</EM><BR>
<EM>&gt; exactly has this got to do with multiple redundancy for
</EM><BR>
<EM>&gt; Sysops?
</EM><BR>
<P>I'm just saying that the concept that a committee is more reliable than
<BR>
an individual is anthropomorphic, having to do with the idea that
<BR>
competing areas of willful blindness will sum to overall
<BR>
trustworthiness.  If you're gonna have redundancy, it seems to me that
<BR>
one should sprinkle it through multiple hiearchical levels.
<BR>
<P>Anyway, redundancy is a programmer's issue, and not really germane.
<BR>
<P><EM>&gt; &gt; &gt; &gt; I *would* just forget about the Singularity, if it was necessary.
</EM><BR>
<EM>&gt; &gt; &gt;
</EM><BR>
<EM>&gt; &gt; &gt; Necessary for what?
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; Serving the ultimate good.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Oh yes, of course. I suppose this is what some call &quot;God&quot;...
</EM><BR>
<P>Yes, den Otter, and some people call the laws of physics &quot;God&quot;.  We both
<BR>
know better than to care.  Right?
<BR>
<P><EM>&gt; &gt; No.  I'm allowing the doubting, this-doesn't-make-sense part of my mind
</EM><BR>
<EM>&gt; &gt; total freedom over every part of myself and my motivations; selfishness,
</EM><BR>
<EM>&gt; &gt; altruism, and all.  I'm not altruistic because my parents told me to be,
</EM><BR>
<EM>&gt; &gt; because I'm under the sway of some meme, or because I'm the puppet of my
</EM><BR>
<EM>&gt; &gt; romantic emotions; I'm altruistic because of a sort of absolute
</EM><BR>
<EM>&gt; &gt; self-cynicism under which selfishness makes even less sense than
</EM><BR>
<EM>&gt; &gt; altruism.  Or at least that's how I'd explain things to a cynic.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I've done some serious doubting myself, but extreme (self)
</EM><BR>
<EM>&gt; cynicism invariably lead to nihilism, not altruism.
</EM><BR>
<P>??  How so?  There's no logical, rational sequitur from &quot;life is
<BR>
meaningless&quot; to &quot;go on a shooting spree&quot;.  It's just a habit of thought
<BR>
that comes from using &quot;life is meaningful&quot; to justify &quot;I will not go on
<BR>
a shooting spree&quot;; it's just as easy to organize your thoughts so that
<BR>
&quot;I will not go on a shooting spree&quot; is justified by &quot;that would be even
<BR>
*more* pointless&quot;.
<BR>
<P><EM>&gt; Altruism
</EM><BR>
<EM>&gt; is just one of many survival strategies for &quot;selfish&quot; genes,
</EM><BR>
<EM>&gt; *clearly* just a means to an evolved end. Altruism is a sub-
</EM><BR>
<EM>&gt; goal if there ever was one, and one that becomes utterly
</EM><BR>
<EM>&gt; redundant when a self-sufficient entity (a SI) gets into a
</EM><BR>
<EM>&gt; position where it can safely eliminate all competition.
</EM><BR>
<P>Adaptation-executers, not fitness-maximizers.  Evolution is, in a sense,
<BR>
subjective.  There's no true mind behind it.  What do I care what
<BR>
evolution's &quot;purpose&quot; was in giving me the ability to see that 2 and 2
<BR>
make 4?  I'm here now, making the adaptations serve *my* purpose.
<BR>
<P>Of course altruism is a subgoal-from-an-engineering-standpoint of
<BR>
evolution.  So is selfishness.  So is pleasure, and pain, and rational
<BR>
thought.  The substance of our mind can put those building blocks
<BR>
together however we like, or favor the ones which are least arbitrary. 
<BR>
Why would an SI that eliminated altruism not also eliminate the tendency
<BR>
to eliminate competition?  One is as arbitrary as the other.
<BR>
<P><EM>&gt; Altruism is *compromize*. A behaviour born out of necessity
</EM><BR>
<EM>&gt; on a planet with weak, mortal, interdependent, evolved
</EM><BR>
<EM>&gt; creatures. To use it out of context, i.e. when it has
</EM><BR>
<EM>&gt; become redundant, is at least as arbitrary as the preference
</EM><BR>
<EM>&gt; for some particular flavor of ice-cream. At the very least,
</EM><BR>
<EM>&gt; it is just as arbitrary as selfishness (its original &quot;master&quot;).
</EM><BR>
<P>This is *precisely* where you are wrong.  Altruism and selfishness are
<BR>
independent adaptations, subgoals of the relentlessly selfish *gene*,
<BR>
which does not give the vaguest damn how long you you survive as long as
<BR>
you have a lot of grandchildren.  Yes, some types of altruism serve
<BR>
selfish purposes, just as selfishness, in a capitalist economy, can
<BR>
serve the altruistic purposes of providing goods and services at low prices.
<BR>
<P><EM>&gt; So, unless you just flipped a coin to determine your guideline
</EM><BR>
<EM>&gt; in life (and a likely guideline for SIs), what exactly *is* the
</EM><BR>
<EM>&gt; logic behind altruism? Why on earth would it hold in a SI world,
</EM><BR>
<EM>&gt; assuming that SIs can really think for themselves and aren't
</EM><BR>
<EM>&gt; just blindly executing some original program?
</EM><BR>
<P>Are we talking about objective morality, or altruism?  Altruism is, at
<BR>
the core, inertia, unless it turns out to be validated by objective
<BR>
morality.  Objective morality would be a physical truth, which would
<BR>
show up in any mirror that reflects external reality, any mind that
<BR>
represents the Universe.
<BR>
<P><EM>&gt; &gt; Anxiety!  Circular logic!  If you just let *go*, you'll find that your
</EM><BR>
<EM>&gt; &gt; mind continues to function, except that you don't have to rationalize
</EM><BR>
<EM>&gt; &gt; falsehoods for fear of what will happen if you let yourself see the
</EM><BR>
<EM>&gt; &gt; truth.  Your mind will go on as before, just a little cleaner.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; A bit of untangling is fine, but getting rid of the &quot;I&quot; is not
</EM><BR>
<EM>&gt; acceptable until I have (much) more data. Such is the prudent
</EM><BR>
<EM>&gt; approach...Upload, expand, reconsider.
</EM><BR>
<P>den Otter, letting go of this particular irrational prejudice is not a
<BR>
philosophical move.  It is the *minimum* required to execute the
<BR>
*reasoning* you must use *now* in order to *survive*.
<BR>
<P>==
<BR>
<P><EM>&gt; Also, one would expect
</EM><BR>
<EM>&gt; that he, subconsciously or not, expects to -eventually- be
</EM><BR>
<EM>&gt; rewarded in one way or another for his good behaviour.
</EM><BR>
<EM>&gt; The latter certainly isn't unlikely in *his* reality...
</EM><BR>
<P>I doubt it.
<BR>
One:  Adaptation-executers, not fitness-maximizers.
<BR>
Two:  Have you *seen* all the episodes this season?  Including the one
<BR>
where Buffy showed up?
<BR>
<PRE>
-- 
       <A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38DEBF4A.AFC2452B@pobox.com&gt;">sentience@pobox.com</A>      Eliezer S. Yudkowsky
          <A HREF="http://pobox.com/~sentience/beyond.html">http://pobox.com/~sentience/beyond.html</A>
                 Member, Extropy Institute
           Senior Associate, Foresight Institute
</PRE>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5892.html">EvMick@aol.com: "Re: some humor from a website that 'translates's your name!"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5890.html">EvMick@aol.com: "Meme Infection"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5733.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5891">[ date ]</A>
<A HREF="index.html#5891">[ thread ]</A>
<A HREF="subject.html#5891">[ subject ]</A>
<A HREF="author.html#5891">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:06:34 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
