<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Otter vs. Yudkowsky</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: Otter vs. Yudkowsky">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Otter vs. Yudkowsky</H1>
<!-- received="Wed Mar 15 12:43:00 2000" -->
<!-- isoreceived="20000315194300" -->
<!-- sent="Wed, 15 Mar 2000 13:44:11 -0600" -->
<!-- isosent="20000315194411" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Otter vs. Yudkowsky" -->
<!-- id="38CFE802.9E6BFC9A@pobox.com" -->
<!-- inreplyto="38CF97D0.3C07@geocities.com" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38CFE802.9E6BFC9A@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Wed Mar 15 2000 - 12:44:11 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4985.html">Zero Powers: "Re: [GUNS\ Re: g*n c*ntr*l"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4983.html">Zero Powers: "Re: Nanomilitary policy"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4967.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5167.html">Xiaoguang Li: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4984">[ date ]</A>
<A HREF="index.html#4984">[ thread ]</A>
<A HREF="subject.html#4984">[ subject ]</A>
<A HREF="author.html#4984">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
&quot;D.den Otter&quot; wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; ----------
</EM><BR>
<EM>&gt; &gt; From: Eliezer S. Yudkowsky &lt;<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38CFE802.9E6BFC9A@pobox.com&gt;">sentience@pobox.com</A>&gt;
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; [snip agreement]
</EM><BR>
<EM>&gt; &gt; The points in dispute are these:
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; 1)  From a strictly selfish perspective, does the likely utility of
</EM><BR>
<EM>&gt; &gt; attempting to upload yourself outweigh the utility of designing a Sysop
</EM><BR>
<EM>&gt; &gt; Mind?  Sub-disputes include (2) whether it's practically possible to
</EM><BR>
<EM>&gt; &gt; develop perfect uploading before China initiates a nanowar or Eliezer
</EM><BR>
<EM>&gt; &gt; runs a seed AI; (3) whether the fact that humans can be trusted no more
</EM><BR>
<EM>&gt; &gt; than AIs will force your group to adopt a Sysop Mind approach in any
</EM><BR>
<EM>&gt; &gt; case;
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Yes, this pretty much sums it up.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; (4) whether telling others that the Transtopians are going to
</EM><BR>
<EM>&gt; &gt; upload and then erase the rest of humanity
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; This is optional. Also, this is not specifically a &quot;transtopian&quot; thing;
</EM><BR>
<EM>&gt; if it's the most logical course of action, *all* ascending Minds
</EM><BR>
<EM>&gt; would erase the rest of humanity. Wiping out humanity for
</EM><BR>
<EM>&gt; its own sake is hardly interesting, and can be done more
</EM><BR>
<EM>&gt; conveniently in simulations anyway, if so desired.
</EM><BR>
<P>I accept the correction.
<BR>
<P><EM>&gt; &gt; will generate opposition
</EM><BR>
<EM>&gt; &gt; making it impossible for you to gain access to uploading prerequisite
</EM><BR>
<EM>&gt; &gt; technologies.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Opposition in the form of government bans should be no
</EM><BR>
<EM>&gt; problem for a dedicated group. Besides, look who's
</EM><BR>
<EM>&gt; talking; your pages are bound to scare the living crap
</EM><BR>
<EM>&gt; out of many people, and not just the fanatical luddites
</EM><BR>
<EM>&gt; (see KPJ's recent post regarding the Bill Joy poll at
</EM><BR>
<EM>&gt; <A HREF="http://cgi.zdnet.com/zdpoll/question.html?pollid=17054&action=a">http://cgi.zdnet.com/zdpoll/question.html?pollid=17054&action=a</A>),
</EM><BR>
<EM>&gt; for example). People are duly getting worried, and
</EM><BR>
<EM>&gt; statements like &quot;there's a significant chance that AIs
</EM><BR>
<EM>&gt; will indeed wipe us out, but hey, that's cool as long as
</EM><BR>
<EM>&gt; they find the meaning of life&quot; aren't likely to calm them
</EM><BR>
<EM>&gt; down.
</EM><BR>
<P>Actually, the statement I use is &quot;there's a significant chance that AIs
<BR>
will indeed wipe us out, but we can't avoid confronting that chance
<BR>
sooner or later, and the sooner we confront it the less chance we have
<BR>
of being wiped out by something else&quot;.
<BR>
<P>Our point of dispute is whether uploading presents any real difference
<BR>
to personal-survival-probability than AI.  From my perspective, humans
<BR>
are simply a special case of AIs, and synchronization is physically
<BR>
implausible.  If uploading presents a difference from AI, therefore, it
<BR>
can only do so for one (1) human.  If you, den Otter, are part of a
<BR>
group of a thousand individuals, your chance of deriving any benefit
<BR>
from this scenario is not more than 0.1%.
<BR>
<P><EM>&gt; &gt; &gt; Well, it's certainly better than nothing, but the fact remains that
</EM><BR>
<EM>&gt; &gt; &gt; the Sysop mind could, at any time and for any reason, decide
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; If it doesn't happen in the first few hours, you're safe forever.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &quot;Objective&quot; hours? That could be thousands if not millions
</EM><BR>
<EM>&gt; of subjective years from the Sysop's pov. That's plenty of
</EM><BR>
<EM>&gt; time to mutate beyond recognition.
</EM><BR>
<P>My point is that there is not an infinite number of chances for things
<BR>
to go wrong.  If it doesn't decide to off you after a couple of hours,
<BR>
you don't need to spend the rest of eternity living in fear.
<BR>
<P><EM>&gt; Damn right. This could be the only chance we'll ever get to
</EM><BR>
<EM>&gt; become truly &quot;free&quot; (as far as mental structures allow), and
</EM><BR>
<EM>&gt; it would be a shame to waste it.
</EM><BR>
<P>My point is that existing inside a Sysop wouldn't &quot;feel&quot; any less free
<BR>
than being a Lone Power, unless your heart's desire is to take actions
<BR>
that would infringe on the rights of others.  Certainly, it would be no
<BR>
less free - with freedom measured in terms of available actions - than
<BR>
existing in a Universe where other Powers had their own physically
<BR>
inviolable areas of control, and it would be a great deal more free than
<BR>
the Universe you'd live in if a non-Sysop gained the upper hand.
<BR>
<P><EM>&gt; Handing over control to AIs may initially offer an advantage, i.e.
</EM><BR>
<EM>&gt; a greater chance of surviving the early phases of the Singularity,
</EM><BR>
<EM>&gt; but may very well be a long-term terminal error. Uploading
</EM><BR>
<EM>&gt; with the help of &quot;dumb&quot; computers on the other hand may
</EM><BR>
<EM>&gt; increase your initial risks, but *if* you make it you've made
</EM><BR>
<EM>&gt; it good. The key issue is: how big are these chances *really*?
</EM><BR>
<EM>&gt; Ok, you say something like 30% vs 0,1%, but how exactly
</EM><BR>
<EM>&gt; did you get these figures? Is there a particular passage in
</EM><BR>
<EM>&gt; _Coding..._ or _Plan to.._ that deals with this issue?
</EM><BR>
<P>30% I pulled out of thin air.  0.1% is a consequence of symmetry and the
<BR>
fact that only one unique chance for success exists.
<BR>
<P><EM>&gt; So the Sysop is God. Ok, let's hope Murphy's Law doesn't
</EM><BR>
<EM>&gt; apply *too* much to coding AIs...
</EM><BR>
<P>Intelligence is the force that opposes Murphy's Law.  An intelligent,
<BR>
self-improving AI should teach itself to be above programmer error and
<BR>
even hardware malfunction.
<BR>
<P><EM>&gt; Actually, we just don't know what forces would be at work in
</EM><BR>
<EM>&gt; an AI that has reached the &quot;omniscient&quot; level. Maybe some
</EM><BR>
<EM>&gt; form of terminal corruption is inevitable somewhere along
</EM><BR>
<P>Aside from your social paranoia instinct, which is *very* far from home,
<BR>
there is no more reason to believe this *is* the case than there is to
<BR>
believe that the AI would inevitably evolve to believe itself a chicken.
<BR>
<P><EM>&gt; the line, which would certainly explain the apparent lack of
</EM><BR>
<EM>&gt; SI activity in the known universe. Bottom line: there are
</EM><BR>
<EM>&gt; absolutely no guarantees. A dynamic, evolving system
</EM><BR>
<EM>&gt; can't be trusted by defintion.
</EM><BR>
<P>Said the dynamic, evolving system to the other dynamic, evolving system...
<BR>
<P><EM>&gt; &gt; It has nothing to do with who, or what, the SIs are.  Their
</EM><BR>
<EM>&gt; &gt; &quot;right&quot; is not a matter of social dominance due to superior
</EM><BR>
<EM>&gt; &gt; formidability, but a form of reasoning that both you or I would
</EM><BR>
<EM>&gt; &gt; inevitably agree with if we were only smart enough.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Yes, *we* could &quot;inevitably&quot; agree that it makes perfect sense
</EM><BR>
<EM>&gt; to disassemble all lower life forms.
</EM><BR>
<P>You're anthropomorphizing - social hierarchies are human-evolutionary artifacts.
<BR>
<P>I'm not talking about an observer-dependent process.  I mean that if you
<BR>
and I were Minds and an Objective third Mind decided we needed killing,
<BR>
we would agree and commit suicide.  Again, there is absolutely no
<BR>
a-priori reason to expect that goals would converge to some kind of
<BR>
observer-dependent set of answers.  If goals converge at all, I'd expect
<BR>
them to behave like all other known &quot;convergent mental structures which
<BR>
do not depend on initial opinions&quot;, which we usually call &quot;facts&quot;.
<BR>
<P>Any other scenario may be a theoretical possibility due to
<BR>
&quot;inscrutability&quot; but it's as likely as convergence to chickenhood.
<BR>
<P><EM>&gt; Fat comfort to *them* that the
</EM><BR>
<EM>&gt; Almighty have decided so in their infinite wisdom. I wouldn't
</EM><BR>
<EM>&gt; be much surprised if one of the &quot;eternal truths&quot; turns out to
</EM><BR>
<EM>&gt; be &quot;might makes right&quot;.
</EM><BR>
<P>*I* would.  &quot;Might makes right&quot; is an evolutionary premise which I
<BR>
understand in its evolutionary context.  To find it in a Mind would be
<BR>
as surprising as discovering an inevitable taste for chocolate.
<BR>
<P><EM>&gt; &gt; That human moral reasoning is observer-dependent follows from the
</EM><BR>
<EM>&gt; &gt; historical fact that the dominant unit of evolutionary selection was the
</EM><BR>
<EM>&gt; &gt; individual.  There is no reason to expect similar effects to arise in a
</EM><BR>
<EM>&gt; &gt; system that be programmed to conceptualize itself as a design component
</EM><BR>
<EM>&gt; &gt; as easily as an agent or an individual, and more likely would simply
</EM><BR>
<EM>&gt; &gt; have not have any moral &quot;self&quot; at all.  I mean, something resembling an
</EM><BR>
<EM>&gt; &gt; &quot;I&quot; will probably evolve whether we design it or not, but that doesn't
</EM><BR>
<EM>&gt; &gt; imply that the &quot;I&quot; gets tangled up in the goal system.  Why would it?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I don't know, it's rather difficult to imagine an untangled &quot;I&quot;.
</EM><BR>
<P>So do I, but I can take a shot at it!
<BR>
<P><EM>&gt; This is basically an alien form of life...
</EM><BR>
<P>Yes, it is!
<BR>
<P><EM>&gt; &gt; I would regard this as good, we're fundamentally and
</EM><BR>
<EM>&gt; &gt; mortally opposed, and fortunately neither of us has any influence
</EM><BR>
<EM>&gt; &gt; whatsoever on how it turns out.)  But while the seed AI isn't at the
</EM><BR>
<EM>&gt; &gt; level where it can be *sure* that no objective meaning exists,
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; How can the AI be sure that it has reached this mystical
</EM><BR>
<EM>&gt; plateau of true objective meaning? Because it &quot;just knows&quot;?
</EM><BR>
<P>When it reaches the point where any objective morality that exists would
<BR>
probably have been discovered; i.e. when the lack of that discovery
<BR>
counts as input to the Bayesian Probability Theorem.  When continuing to
<BR>
act on the possibility would be transparently stupid even to you and I,
<BR>
we might expect it to be transparently stupid to the AI.
<BR>
<P><EM>&gt; &gt; it has to
</EM><BR>
<EM>&gt; &gt; take into account the possibility that it does.  The seed would tend to
</EM><BR>
<EM>&gt; &gt; reason:  &quot;Well, I'm not sure whether or not this is the right thing to
</EM><BR>
<EM>&gt; &gt; do, but if I just upgrade myself a bit farther, then I'll be sure.&quot;
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Ad infinitum. A bit like chasing the horizon, IMO.
</EM><BR>
<P>Not at all, as stated above.
<BR>
<P><EM>&gt; &gt; The only way the Pause would be a valid suggestion is if there's such a
</EM><BR>
<EM>&gt; &gt; good reason for doing it that the seed itself would come up with the
</EM><BR>
<EM>&gt; &gt; suggestion independently.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Again, it's all a matter of perspective; what's good from the AI's
</EM><BR>
<EM>&gt; pov doesn't have to be good from *our* pov.
</EM><BR>
<P>Sigh.
<BR>
<P>This is the basic anthropomorphization.
<BR>
<P>What you call a &quot;point-of-view&quot; is an evolutionary artifact.  I can see
<BR>
it from where I stand, a great tangled mess of instincts and intuitions
<BR>
and cached experience about observer-dependent models of the Universe,
<BR>
observer-dependent utility functions, the likelihood that others will
<BR>
&quot;cheat&quot; or tend to calculate allegedly group-based utility functions in
<BR>
ways that enhance their own benefit, and so on.
<BR>
<P>I can also see that it won't exist in an AI.  An AI *does not have* a
<BR>
pov, and your whole don't-trust-the-AIs scenario rests on that basic
<BR>
anxiety, that the AI will make decisions biased toward itself.  There is
<BR>
just no reason for that to happen.  An AI doesn't have a pov.  It's
<BR>
every bit as likely to make decisions biased towards you, or towards the
<BR>
&quot;viewpoint&quot; of some quark somewhere, as it is to make decisions biased
<BR>
toward itself.  The tendency that exists in humans is the product of
<BR>
evolutionary selection and nothing else.
<BR>
<P>Having a pov is natural to evolved organisms, not to minds in general.
<BR>
<P><EM>&gt; There may be a
</EM><BR>
<EM>&gt; fundamental conflict between the two without one being
</EM><BR>
<EM>&gt; more (ojectively) &quot;right&quot; than the other. Compare it to the
</EM><BR>
<EM>&gt; situation of, for example, a snake and a rodent. The snake
</EM><BR>
<EM>&gt; has to feed in order to survive, while the rodent obviously
</EM><BR>
<EM>&gt; needs to avoid capture to achieve the same goal. Has one
</EM><BR>
<EM>&gt; more &quot;right&quot; to live than the other? No, they are both &quot;right&quot;
</EM><BR>
<EM>&gt; from their own pov. It would probably be no different in a
</EM><BR>
<EM>&gt; human-AI conflict of interest.
</EM><BR>
<P>It would be very much different.  Both snakes and rodents evolved. 
<BR>
Humans may have evolved, but AIs haven't.
<BR>
<P><EM>&gt; &gt; In which case,
</EM><BR>
<EM>&gt; &gt; delaying Sysop deployment involves many definite risks.
</EM><BR>
<EM>&gt; [snip risks, on which we agree]
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &gt; You and a thousand other Mind-wannabes wish to
</EM><BR>
<EM>&gt; &gt; ensure your safety and survival.  One course of action is to upload,
</EM><BR>
<EM>&gt; &gt; grow on independent hardware, and then fight it out in space.  If
</EM><BR>
<EM>&gt; &gt; defense turns out to have an absolute, laws-of-physics advantage over
</EM><BR>
<EM>&gt; &gt; offense, then you'll all be safe.  I think this is extraordinarily
</EM><BR>
<EM>&gt; &gt; unlikely to be the case, given the historical trend. If offense has an
</EM><BR>
<EM>&gt; &gt; advantage over defense, you'll all fight it out until only one Mind
</EM><BR>
<EM>&gt; &gt; remains with a monopoly on available resources.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; True, but who knows what the ascending Minds will decide
</EM><BR>
<EM>&gt; in their &quot;infinite&quot; wisdom? Perhaps they'll come up with some
</EM><BR>
<EM>&gt; really clever solution. Perhaps they'll even consider the needs
</EM><BR>
<EM>&gt; of the rest of humanity. And yes, maybe the first thing they'll
</EM><BR>
<EM>&gt; do is make some really big guns and try to blast eachother.
</EM><BR>
<EM>&gt; Only one way to find out...
</EM><BR>
<P>If you're going to invoke inscrutability, I can invoke it too.  If the
<BR>
opportunity for many peaceful happy free Minds exists, then the Sysop
<BR>
should be able to see it and set us all free. 
<BR>
<P><EM>&gt; &gt; However, is the utility
</EM><BR>
<EM>&gt; &gt; of having the whole Solar System to yourself really a thousand times the
</EM><BR>
<EM>&gt; &gt; utility, the &quot;fun&quot;, of having a thousandth of the available resources?
</EM><BR>
<EM>&gt; &gt; No.  You cannot have a thousand times as much fun with a thousand times
</EM><BR>
<EM>&gt; &gt; as much mass.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I think you'd have to be a SI to know that for sure.
</EM><BR>
<P>The question is not what the SI thinks.  It is what *you* think, you,
<BR>
den Otter and your organization - or whatever other organization fills
<BR>
that slot in the script - that will determine whether *you* think you
<BR>
need a peace treaty.
<BR>
<P>Now, would *you* - right here, right now - rather have the certainty of
<BR>
getting one six-billionth the mass of the Solar System, or would you
<BR>
prefer a one-in-six-billion chance of getting it all?
<BR>
<P><EM>&gt; &gt; You need a peace treaty.  You need a system, a process, which ensures
</EM><BR>
<EM>&gt; &gt; your safety.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; MAD. Not a very stable system, perhaps, but neither are
</EM><BR>
<EM>&gt; superintelligent, evolving Sysops.
</EM><BR>
<P>Who said anything about a plural?  One Sysop.  One Power to enforce The
<BR>
Rules that keep all the other Powers from playing cops-and-robbers with
<BR>
the Solar System.  That's the point.
<BR>
<P>As for MAD, it won't work for nano and it won't work for Powers.  It's
<BR>
not just the first-strike problem; we're talking about a system at
<BR>
extrema with no balancing factors.
<BR>
<P><EM>&gt; In a way yes, for the time being anyway. The main issue now
</EM><BR>
<EM>&gt; is what level of &quot;intelligence&quot; the Sysop should have.
</EM><BR>
<P>By the time non-self-improving AI becomes possible, self-improving AI
<BR>
will have advanced well past the point where I can build one in a basement.
<BR>
<P>Given enough intelligence to be useful, the AI would have enough
<BR>
intelligence to independently see the necessity of improving itself further.
<BR>
<P>I wouldn't trust an unPowered Sysop to upload me, anyway.
<BR>
<P>In short, I doubt the possibility of building a dumb one.
<BR>
<P><EM>&gt; &gt; The other half of the &quot;low utility&quot; part is philosophical; if there are
</EM><BR>
<EM>&gt; &gt; objective goals, you'll converge to them too, thus accomplishing exactly
</EM><BR>
<EM>&gt; &gt; the same thing as if some other Mind converged to those goals.  Whether
</EM><BR>
<EM>&gt; &gt; or not the Mind happens to be &quot;you&quot; is an arbitrary prejudice; if the
</EM><BR>
<EM>&gt; &gt; Otterborn Mind is bit-by-bit indistinguishable from an Eliezerborn or
</EM><BR>
<EM>&gt; &gt; AIborn Mind, but you take an action based on the distinction which
</EM><BR>
<EM>&gt; &gt; decreases your over-all-branches probability of genuine personal
</EM><BR>
<EM>&gt; &gt; survival, it's a stupid prejudice.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; To me genuine personal survival is not just the destination,
</EM><BR>
<EM>&gt; (if there even is such a thing) but also the journey. Call it an
</EM><BR>
<EM>&gt; irrational prejudice, but hey, I happen to like the illusion of
</EM><BR>
<EM>&gt; continuity. A solution is only acceptable if it respects this
</EM><BR>
<EM>&gt; pov. To ask of me to just forget about continuity is like asking
</EM><BR>
<EM>&gt; you to just forget about the Singularity.
</EM><BR>
<P>I *would* just forget about the Singularity, if it was necessary. 
<BR>
Getting to my current philosophical position required that I let go of a
<BR>
number of irrational prejudices, some of which I was very much attached
<BR>
to.  I let go of my irrational prejudice in favor of the
<BR>
Singularity-outcome, and admitted the possibility of nanowar.  I let go
<BR>
of my irrational prejudice in favor of a dramatic Singularity, one that
<BR>
would let me play the hero, in favor of a fast, quiet seed AI.
<BR>
<P>If it's an irrational prejudice, then let it go.
<BR>
<P><EM>&gt; &gt; &gt; P.s: do you watch _Angel_ too?
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; Of course.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Ah yes, same here. Nice altruistic chap, isn't he?
</EM><BR>
<P>And very realistically so, actually.  If Angel's will is strong enough
<BR>
to balance and overcome the built-in predatory instincts of a vampire,
<BR>
then he's rather unlikely to succumb to lesser moral compromises.  He's
<BR>
also a couple of hundred years old, which is another force operating to
<BR>
move him to the extrema of whatever position he takes.
<BR>
<P>Gotta develop an instinct for those non-gaussians if you're gonna think
<BR>
about Minds...
<BR>
<PRE>
-- 
       <A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38CFE802.9E6BFC9A@pobox.com&gt;">sentience@pobox.com</A>      Eliezer S. Yudkowsky
          <A HREF="http://pobox.com/~sentience/beyond.html">http://pobox.com/~sentience/beyond.html</A>
                 Member, Extropy Institute
           Senior Associate, Foresight Institute
</PRE>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4985.html">Zero Powers: "Re: [GUNS\ Re: g*n c*ntr*l"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4983.html">Zero Powers: "Re: Nanomilitary policy"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4967.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5167.html">Xiaoguang Li: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4984">[ date ]</A>
<A HREF="index.html#4984">[ thread ]</A>
<A HREF="subject.html#4984">[ subject ]</A>
<A HREF="author.html#4984">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:05:17 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
