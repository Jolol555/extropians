<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: regarding the Eliezer vrs. Otter debate...</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: regarding the Eliezer vrs. Otter debate...">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: regarding the Eliezer vrs. Otter debate...</H1>
<!-- received="Thu Mar 16 16:42:10 2000" -->
<!-- isoreceived="20000316234210" -->
<!-- sent="Thu, 16 Mar 2000 17:43:29 -0600" -->
<!-- isosent="20000316234329" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: regarding the Eliezer vrs. Otter debate..." -->
<!-- id="38D1717E.89B134BF@pobox.com" -->
<!-- inreplyto="20000316230059.8706.qmail@hotmail.com" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20regarding%20the%20Eliezer%20vrs.%20Otter%20debate...&In-Reply-To=&lt;38D1717E.89B134BF@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Thu Mar 16 2000 - 16:43:29 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5106.html">Technotranscendence: "LEF Update"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5104.html">john grigg: "I have seen the show _God, the Devil and Bob_"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5101.html">john grigg: "regarding the Eliezer vrs. Otter debate..."</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5105">[ date ]</A>
<A HREF="index.html#5105">[ thread ]</A>
<A HREF="subject.html#5105">[ subject ]</A>
<A HREF="author.html#5105">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
john grigg wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Eliezer wrote:
</EM><BR>
<EM>&gt; My point is that there is not an infinite number of chances for things to go
</EM><BR>
<EM>&gt; wrong. If it doesn't decide to off you after a couple of hours, you don't
</EM><BR>
<EM>&gt; need to spend the rest of eternity living in fear.
</EM><BR>
<EM>&gt; End)
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Why is that?  I would think it COULD take years/decades/centuries before our
</EM><BR>
<P>Subjective millennia, maybe.  But the kind of Minds we're talking about
<BR>
are very very very fast.
<BR>
<P><EM>&gt; AI get to the point of deciding for whatever reasons to turn against
</EM><BR>
<EM>&gt; humanity.  As time went by we would trust them more and give them more and
</EM><BR>
<EM>&gt; more responsibilities and power over society.  I would think that even if
</EM><BR>
<EM>&gt; some 'rebellion' was harbored within the 'heart' of the first AI that they
</EM><BR>
<EM>&gt; would keep it to themselves until the time was right to strike.
</EM><BR>
<P>Go reread _Staring into the Singularity_.  We are not talking about
<BR>
players on a human scale.  We are not talking about automated factory
<BR>
workers.  We're talking about a Sysop, an operating system for all
<BR>
matter in the Solar System with the possible exception of any humans
<BR>
that opt to be left alone on Earth with relatively harmless modern-day
<BR>
technologies.  If a Mind wants to strike, it can strike and there is
<BR>
absolutely nothing we can do about it.  This is one of the things that
<BR>
den Otter and I agree about completely.
<BR>
<P><EM>&gt; I don't understand how you could say AI would not have a point of view.
</EM><BR>
<EM>&gt; Perhaps initially, but then as AI hacked itself and upgraded, it would
</EM><BR>
<EM>&gt; probably develop a pov at a certain level of sophistication. Right?
</EM><BR>
<P>Wrong.
<BR>
<P><EM>&gt; The
</EM><BR>
<EM>&gt; self-development of AI would just be another form of evolutionary progress.
</EM><BR>
<P>Wrong.
<BR>
<P><EM>&gt; I don't fully grasp the nature of this debate(I admit).  How would AI really
</EM><BR>
<EM>&gt; save us from nanotech destruction?  Would it be where nano is about to
</EM><BR>
<EM>&gt; overrun the planet and human researchers lack the time to find the solution
</EM><BR>
<EM>&gt; but a lightning fast AI with labs at its control comes up with the save in a
</EM><BR>
<EM>&gt; matter of only minutes or hours?  I could see that happening.
</EM><BR>
<P>Basically, although leaving it to the last minute is really asking for it.
<BR>
<P><EM>&gt; It appears that AI, uploading, sysop and nano each have their own plusses
</EM><BR>
<EM>&gt; and minusses.  It looks like you are all trying to come up with the best
</EM><BR>
<EM>&gt; balancing act to offset the dangers of the other.  It seems to me that a
</EM><BR>
<EM>&gt; hostile AI with control over nano would have us beat hands down!
</EM><BR>
<P>Yep.  There is absolutely no doubt about that.  And who says they'll
<BR>
stop at nanotech, anyway?
<BR>
<P><EM>&gt; Even a
</EM><BR>
<EM>&gt; nuke hit that destroyed the AI would not stop the unrelenting 'engines of
</EM><BR>
<EM>&gt; destruction' it had unleashed on us.  Hopefully an AI on our team would
</EM><BR>
<EM>&gt; quickly nullify the threat.
</EM><BR>
<P>No teams.  Even if it's theoretically possible to have advanced Minds
<BR>
with conflicting goals, the first AI on the field would enhance itself
<BR>
and become Sysop.
<BR>
<P><EM>&gt; But if all the AI were to defect we would be in
</EM><BR>
<EM>&gt; serious trouble.
</EM><BR>
<P>What it comes down, in the end, after you've eliminated all the
<BR>
anthropomorphisms about selfish and skewed viewpoints on the part of
<BR>
AIs, is that either humanity has a destiny, or it doesn't.  If we really
<BR>
are nothing except cosmic dust, debris in the path of the Minds that
<BR>
truly matter, then there is not and never was any hope for our species. 
<BR>
It's a question we'll have to face evontually.  If we don't want to
<BR>
destroy ourselves in the next few years, and lose any destiny and chance
<BR>
for survival we might have had, we need to face the question as soon as possible.
<BR>
<P><EM>&gt; When it comes to uploading I must admit the classic film _Lawnmower Man_
</EM><BR>
<EM>&gt; comes to mind.  Would you want the angry Jeff Fahy character as the
</EM><BR>
<EM>&gt; 'cyberchrist' of the world?  That film will become to uploading what 2001 is
</EM><BR>
<EM>&gt; to first contact.
</EM><BR>
<P>Never seen it.  Sounds like a real yawner.
<BR>
<P><EM>&gt; I think I might prefer cool-headed and unbiased AI(if they really turn out
</EM><BR>
<EM>&gt; that way) to uploaded human personalities that could hold 'inner demons'
</EM><BR>
<EM>&gt; which could drive them to do some bad things, though at the time of
</EM><BR>
<EM>&gt; uploading we thought them psychologically healthy individuals.  I suppose
</EM><BR>
<EM>&gt; that AI and uploads will co-mingle and hopefully get along!  I think that AI
</EM><BR>
<EM>&gt; will be here first so we will be the 'new neighbors.'  Some AI may be
</EM><BR>
<EM>&gt; designed on info taken from uploads to make them more human-like.  Is that a
</EM><BR>
<EM>&gt; good idea?
</EM><BR>
<P>Not unless either (1) we can't avoid it and disaster is closing in or
<BR>
(2) we understand absolutely what we're putting in there.
<BR>
<P><EM>&gt; Based on the various rates of technological advancement, I would say that AI
</EM><BR>
<EM>&gt; may(?) be here before we have really effective nano.  Probably.  And I
</EM><BR>
<EM>&gt; suppose that it would be a good thing for it to work out that way.
</EM><BR>
<P>If not, we can always hope that nanocomputers good enough to brute-force
<BR>
the problem will be among the first applications.
<BR>
<PRE>
-- 
       <A HREF="mailto:sentience@pobox.com?Subject=Re:%20regarding%20the%20Eliezer%20vrs.%20Otter%20debate...&In-Reply-To=&lt;38D1717E.89B134BF@pobox.com&gt;">sentience@pobox.com</A>      Eliezer S. Yudkowsky
          <A HREF="http://pobox.com/~sentience/beyond.html">http://pobox.com/~sentience/beyond.html</A>
                 Member, Extropy Institute
           Senior Associate, Foresight Institute
</PRE>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5106.html">Technotranscendence: "LEF Update"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5104.html">john grigg: "I have seen the show _God, the Devil and Bob_"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5101.html">john grigg: "regarding the Eliezer vrs. Otter debate..."</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5105">[ date ]</A>
<A HREF="index.html#5105">[ thread ]</A>
<A HREF="subject.html#5105">[ subject ]</A>
<A HREF="author.html#5105">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:05:27 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
