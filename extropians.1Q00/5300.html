<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Otter vs. Yudkowsky</TITLE>
<META NAME="Author" CONTENT="D.den Otter (neosapient@geocities.com)">
<META NAME="Subject" CONTENT="Re: Otter vs. Yudkowsky">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Otter vs. Yudkowsky</H1>
<!-- received="Sat Mar 18 15:09:07 2000" -->
<!-- isoreceived="20000318220907" -->
<!-- sent="Sat, 18 Mar 2000 23:00:19 +0100" -->
<!-- isosent="20000318220019" -->
<!-- name="D.den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: Otter vs. Yudkowsky" -->
<!-- id="38D3FC73.29E8@geocities.com" -->
<!-- inreplyto="Otter vs. Yudkowsky" -->
<STRONG>From:</STRONG> D.den Otter (<A HREF="mailto:neosapient@geocities.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38D3FC73.29E8@geocities.com&gt;"><EM>neosapient@geocities.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Sat Mar 18 2000 - 15:00:19 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5301.html">Robert Owen: "Re: American Imperialism?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5299.html">Michael S. Lorrey: "Re: American Imperialism?"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="4683.html">Eliezer S. Yudkowsky: "Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5321.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5321.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5300">[ date ]</A>
<A HREF="index.html#5300">[ thread ]</A>
<A HREF="subject.html#5300">[ subject ]</A>
<A HREF="author.html#5300">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
----------
<BR>
<EM>&gt; From: Eliezer S. Yudkowsky &lt;<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38D3FC73.29E8@geocities.com&gt;">sentience@pobox.com</A>&gt;
</EM><BR>
<P><EM>&gt; &quot;D.den Otter&quot; wrote:
</EM><BR>
<P><EM>&gt; &gt; People are duly getting worried, and
</EM><BR>
<EM>&gt; &gt; statements like &quot;there's a significant chance that AIs
</EM><BR>
<EM>&gt; &gt; will indeed wipe us out, but hey, that's cool as long as
</EM><BR>
<EM>&gt; &gt; they find the meaning of life&quot; aren't likely to calm them
</EM><BR>
<EM>&gt; &gt; down.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Actually, the statement I use is &quot;there's a significant chance that AIs
</EM><BR>
<EM>&gt; will indeed wipe us out, but we can't avoid confronting that chance
</EM><BR>
<EM>&gt; sooner or later, and the sooner we confront it the less chance we have
</EM><BR>
<EM>&gt; of being wiped out by something else&quot;.
</EM><BR>
<P><EM>&gt; Our point of dispute is whether uploading presents any real difference
</EM><BR>
<EM>&gt; to personal-survival-probability than AI.  
</EM><BR>
<P>Yes, that's the *practical* side of the dispute. There's also the
<BR>
philosophical issue of whether personal survival is more important
<BR>
than the creation of superintelligent successors, &quot;egoism&quot; vs
<BR>
&quot;altruism&quot; etc., of course. This inevitably adds an element of 
<BR>
bias to the above debate.
<BR>
<P><EM>&gt; From my perspective, humans
</EM><BR>
<EM>&gt; are simply a special case of AIs, and synchronization is physically
</EM><BR>
<EM>&gt; implausible.  If uploading presents a difference from AI, therefore, it
</EM><BR>
<EM>&gt; can only do so for one (1) human.  If you, den Otter, are part of a
</EM><BR>
<EM>&gt; group of a thousand individuals, your chance of deriving any benefit
</EM><BR>
<EM>&gt; from this scenario is not more than 0.1%.
</EM><BR>
<P>If there can be only one, then you're right. Still, it's better to
<BR>
have 0.1% control over a situation than 0%. Now add to
<BR>
this the following:
<BR>
<P><EM>&gt; &gt; Ok, you say something like 30% vs 0,1%, but how exactly
</EM><BR>
<EM>&gt; &gt; did you get these figures? Is there a particular passage in
</EM><BR>
<EM>&gt; &gt; _Coding..._ or _Plan to.._ that deals with this issue?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; 30% I pulled out of thin air.  0.1% is a consequence of symmetry and the
</EM><BR>
<EM>&gt; fact that only one unique chance for success exists.
</EM><BR>
<P>Out of thin air. Ok, so it could be really anything, including
<BR>
some value &lt;0.1%. At least with uploading you know more
<BR>
or less what you can expect.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; &quot;Objective&quot; hours? That could be thousands if not millions
</EM><BR>
<EM>&gt; &gt; of subjective years from the Sysop's pov. That's plenty of
</EM><BR>
<EM>&gt; &gt; time to mutate beyond recognition.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; My point is that there is not an infinite number of chances for things
</EM><BR>
<EM>&gt; to go wrong. 
</EM><BR>
<P>The longer you exist, the more opportunities there will be for
<BR>
something to go wrong. That's pretty much a mathematical
<BR>
certainty, afaik.
<BR>
<P><EM>&gt; If it doesn't decide to off you after a couple of hours,
</EM><BR>
<EM>&gt; you don't need to spend the rest of eternity living in fear.
</EM><BR>
<P>I don't think so; what's a couple of hours compared to
<BR>
the billions of years that lie ahead. It's just a baby whose
<BR>
journey to maturity has just begun. Who knows how
<BR>
many Singularities will follow. The AI could &quot;malfunction&quot;
<BR>
or discover the meaning of life right away, or it could take
<BR>
aeons. There's just no way of knowing that.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Damn right. This could be the only chance we'll ever get to
</EM><BR>
<EM>&gt; &gt; become truly &quot;free&quot; (as far as mental structures allow), and
</EM><BR>
<EM>&gt; &gt; it would be a shame to waste it.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; My point is that existing inside a Sysop wouldn't &quot;feel&quot; any less free
</EM><BR>
<EM>&gt; than being a Lone Power, unless your heart's desire is to take actions
</EM><BR>
<EM>&gt; that would infringe on the rights of others.  Certainly, it would be no
</EM><BR>
<EM>&gt; less free - with freedom measured in terms of available actions - than
</EM><BR>
<EM>&gt; existing in a Universe where other Powers had their own physically
</EM><BR>
<EM>&gt; inviolable areas of control, 
</EM><BR>
<P>Those are two different kinds of &quot;freedom&quot;. In the former case
<BR>
you can go everywhere, but you carry your own partial prison
<BR>
around in your head (like the guy from _Clockwork Orange_),
<BR>
while in the latter case you may not be able to go anywhere
<BR>
you want, but you *are* master of your own domain. I think
<BR>
I prefer the latter, not only because it is more &quot;dignified&quot; 
<BR>
(silly human concept), but because it's wise to put as much 
<BR>
distance and defences between you and a potential 
<BR>
enemy as possible. When that enemy is sharing your 
<BR>
body, you have a real problem.
<BR>
<P><EM>&gt; and it would be a great deal more free than
</EM><BR>
<EM>&gt; the Universe you'd live in if a non-Sysop gained the upper hand.
</EM><BR>
<P>Probably...Initially, at least.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; So the Sysop is God. Ok, let's hope Murphy's Law doesn't
</EM><BR>
<EM>&gt; &gt; apply *too* much to coding AIs...
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Intelligence is the force that opposes Murphy's Law.  
</EM><BR>
<P>Usually with lots of trial and error...
<BR>
<P><EM>&gt; An intelligent,
</EM><BR>
<EM>&gt; self-improving AI should teach itself to be above programmer error and
</EM><BR>
<EM>&gt; even hardware malfunction.
</EM><BR>
<P>Yes, *if* you can get the basics right.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; the line, which would certainly explain the apparent lack of
</EM><BR>
<EM>&gt; &gt; SI activity in the known universe. Bottom line: there are
</EM><BR>
<EM>&gt; &gt; absolutely no guarantees. A dynamic, evolving system
</EM><BR>
<EM>&gt; &gt; can't be trusted by defintion.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Said the dynamic, evolving system to the other dynamic, evolving system...
</EM><BR>
<P>I don't claim that I (or humans in general) can be trusted, 
<BR>
do I? I know that humans can't be trusted, but neiter can
<BR>
evolving AIs, so there is no particular reason to favor them
<BR>
as Sysops for mankind.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; &gt; It has nothing to do with who, or what, the SIs are.  Their
</EM><BR>
<EM>&gt; &gt; &gt; &quot;right&quot; is not a matter of social dominance due to superior
</EM><BR>
<EM>&gt; &gt; &gt; formidability, but a form of reasoning that both you or I would
</EM><BR>
<EM>&gt; &gt; &gt; inevitably agree with if we were only smart enough.
</EM><BR>
<EM>&gt; &gt; 
</EM><BR>
<EM>&gt; &gt; Yes, *we* could &quot;inevitably&quot; agree that it makes perfect sense
</EM><BR>
<EM>&gt; &gt; to disassemble all lower life forms.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; You're anthropomorphizing - social hierarchies are human-evolutionary artifacts.
</EM><BR>
<P>Natural evolution may have made some pretty bad mistakes, but
<BR>
that doesn't necessarily mean that *all* of our programming will become
<BR>
obsolete. If the SIs want to do something, they will have to stay
<BR>
alive to do it (unless of course they decide to kill themselves, but
<BR>
let's assume for the sake of argument that this won't be the case). 
<BR>
Basic logic. So some sort of self-preservation &quot;instinct&quot; will be 
<BR>
required(*) to keep the forces of entropy at bay. Survival requires 
<BR>
control --the more the better-- over one's surroundings. Other 
<BR>
intelligent entities represent by definition an area of deminished
<BR>
control, and must be studied and then placed in a threat/benefit
<BR>
hierarchy which will help to determine future actions. And voila,
<BR>
your basic social hierachy is born. The &quot;big happy egoless
<BR>
cosmic family model&quot; only works when the other sentients 
<BR>
are either evolutionary dead-ends which are &quot;guaranteed&quot; to 
<BR>
remain insignificant, or completely and permanently like-minded.
<BR>
<P>(*)this is one of those things that I expect to evolve &quot;spontaneously&quot; 
<BR>
in a truly intelligent system that has some sort of motivation. A system 
<BR>
without goals will just sit on its ass for all eternity, IMO). 
<BR>
&nbsp;
<BR>
<EM>&gt; I'm not talking about an observer-dependent process.  I mean that if you
</EM><BR>
<EM>&gt; and I were Minds and an Objective third Mind decided we needed killing,
</EM><BR>
<EM>&gt; we would agree and commit suicide.  Again, there is absolutely no
</EM><BR>
<EM>&gt; a-priori reason to expect that goals would converge to some kind of
</EM><BR>
<EM>&gt; observer-dependent set of answers.  If goals converge at all, I'd expect
</EM><BR>
<EM>&gt; them to behave like all other known &quot;convergent mental structures which
</EM><BR>
<EM>&gt; do not depend on initial opinions&quot;, which we usually call &quot;facts&quot;.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Any other scenario may be a theoretical possibility due to
</EM><BR>
<EM>&gt; &quot;inscrutability&quot; but it's as likely as convergence to chickenhood.
</EM><BR>
<P>No, no, no! It's exactly the other way around; goals are
<BR>
observer dependent by default. As far as we know this is
<BR>
the only way they *can* be. Is there any evidence to the
<BR>
contrary? Not afaik. It is the whole objective morality idea
<BR>
that is wild speculation, out there in highly-unlikely-land
<BR>
with the convergence to chickenhood &amp; the tooth fairy.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Fat comfort to *them* that the
</EM><BR>
<EM>&gt; &gt; Almighty have decided so in their infinite wisdom. I wouldn't
</EM><BR>
<EM>&gt; &gt; be much surprised if one of the &quot;eternal truths&quot; turns out to
</EM><BR>
<EM>&gt; &gt; be &quot;might makes right&quot;.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; *I* would.  &quot;Might makes right&quot; is an evolutionary premise which I
</EM><BR>
<EM>&gt; understand in its evolutionary context.  To find it in a Mind would be
</EM><BR>
<EM>&gt; as surprising as discovering an inevitable taste for chocolate.
</EM><BR>
<P>Evolution represents, among other things, some basic rules
<BR>
for survival. No matter how smart the SIs will become, they'll
<BR>
still have to play by the rules of this reality to live &amp; prosper. 
<BR>
You can't deny self-evident truths like &quot;might makes right&quot;
<BR>
without paying the price (decreased efficiency, possibly
<BR>
serious damage or even annihilation) at some point. And
<BR>
yes, I also belief that suicide is fundamentally stupid,
<BR>
*especially* for a Power which could always alter its mind 
<BR>
and bliss out forever if there's nothing better to do. The only
<BR>
logical excuse for killing yourself is when one knows for pretty 
<BR>
damn sure, beyond all reasonable doubt, that the alternative 
<BR>
is permanent, or &quot;indefinite&quot;, hideous suffering. Now how
<BR>
likely is *that*?
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; I don't know, it's rather difficult to imagine an untangled &quot;I&quot;.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; So do I, but I can take a shot at it!
</EM><BR>
<P>But you don't, more or less by defintition, really know what 
<BR>
you're doing!
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; This is basically an alien form of life...
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Yes, it is!
</EM><BR>
<P>Yes indeed; we are inviting aliens to our planet. Not
<BR>
just any aliens, mind you, but *superintelligent* ones.
<BR>
Can't help feeling that this is a very big mistake. Is
<BR>
this just irrational monkey paranoia, or the voice of 
<BR>
reason itself? Well, I guess we're about to find out...
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; How can the AI be sure that it has reached this mystical
</EM><BR>
<EM>&gt; &gt; plateau of true objective meaning? Because it &quot;just knows&quot;?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; When it reaches the point where any objective morality that exists would
</EM><BR>
<EM>&gt; probably have been discovered; i.e. when the lack of that discovery
</EM><BR>
<EM>&gt; counts as input to the Bayesian Probability Theorem.  When continuing to
</EM><BR>
<EM>&gt; act on the possibility would be transparently stupid even to you and I,
</EM><BR>
<EM>&gt; we might expect it to be transparently stupid to the AI.
</EM><BR>
<P>In other words, objective morality will always be just an educated
<BR>
guess. Will there be a limit to evolution anyway? One would be
<BR>
inclined to say &quot;yes, of course&quot;, but if this isn't the case, then
<BR>
the quest for objective morality will go on forever.
<BR>
&nbsp;
<BR>
<EM>&gt; What you call a &quot;point-of-view&quot; is an evolutionary artifact.  
</EM><BR>
<P>A rather useful one too. The creatures with the strongest
<BR>
point-of-view (i.e. humans) dominate the planet utterly, and
<BR>
are the only ones that have developed culture and technology.
<BR>
One would logically expect our successors to have *more* 
<BR>
&quot;pov&quot;, not less. 
<BR>
<P><EM>&gt; I can see
</EM><BR>
<EM>&gt; it from where I stand, a great tangled mess of instincts and intuitions
</EM><BR>
<EM>&gt; and cached experience about observer-dependent models of the Universe,
</EM><BR>
<EM>&gt; observer-dependent utility functions, the likelihood that others will
</EM><BR>
<EM>&gt; &quot;cheat&quot; or tend to calculate allegedly group-based utility functions in
</EM><BR>
<EM>&gt; ways that enhance their own benefit, and so on.
</EM><BR>
<P>Needs a bit (ok, a lot) of tweaking, perhaps, but the basic 
<BR>
components for functioning are all there. It works! No idle
<BR>
speculation, but a system that's been approved by this
<BR>
reality.
<BR>
&nbsp;
<BR>
<EM>&gt; I can also see that it won't exist in an AI.  An AI *does not have* a
</EM><BR>
<EM>&gt; pov, and your whole don't-trust-the-AIs scenario rests on that basic
</EM><BR>
<EM>&gt; anxiety, that the AI will make decisions biased toward itself.  There is
</EM><BR>
<EM>&gt; just no reason for that to happen.  An AI doesn't have a pov.  It's
</EM><BR>
<EM>&gt; every bit as likely to make decisions biased towards you, or towards the
</EM><BR>
<EM>&gt; &quot;viewpoint&quot; of some quark somewhere, as it is to make decisions biased
</EM><BR>
<EM>&gt; toward itself.  The tendency that exists in humans is the product of
</EM><BR>
<EM>&gt; evolutionary selection and nothing else.
</EM><BR>
<P>Evolutionary selection for *survival*, yes. And, once again,
<BR>
you really, really need to be alive to do something. Whether
<BR>
those actions are altruistic or egoistic in nature is besides
<BR>
the point. 
<BR>
&nbsp;
<BR>
<EM>&gt; Having a pov is natural to evolved organisms, not to minds in general.
</EM><BR>
<P>I'm sure you could make some pov-less freak in the lab, and
<BR>
keep it alive under &quot;ideal&quot;, sterile conditions, but I doubt that
<BR>
it would be very effective in the real world. As I see it, we have
<BR>
two options: a) either the mind really has no &quot;self&quot; and no &quot;bias&quot;
<BR>
when it comes to motivation, in which case it will probably just
<BR>
sit there and do nothing, or b) it *does* have a &quot;self&quot;, or creates
<BR>
one as a logical result of some pre-programmed goal(s), in
<BR>
which case it is likely to eventually become completely 
<BR>
&quot;selfish&quot; due to a logical line of reasoning.
<BR>
<P>[snakes &amp; rodents compared to AIs &amp; humans] 
<BR>
<EM>&gt; It would be very much different.  Both snakes and rodents evolved. 
</EM><BR>
<EM>&gt; Humans may have evolved, but AIs haven't.
</EM><BR>
<P>But they will have to evolve in order to become SIs.
<BR>
<P><EM>&gt; Now, would *you* - right here, right now - rather have the certainty of
</EM><BR>
<EM>&gt; getting one six-billionth the mass of the Solar System, or would you
</EM><BR>
<EM>&gt; prefer a one-in-six-billion chance of getting it all?
</EM><BR>
<P>I'd take the former, of course, but that's because the odds in this
<BR>
particular example are extremely (and quite unrealistically so)
<BR>
bad. In reality, it's not you vs the rest of humanity, but you vs
<BR>
a relative small financial/technological elite, many (most) of
<BR>
whom don't even fully grasp the potential of the machines they're
<BR>
working on. Most people will simply never know what hit them.
<BR>
<P>Anyway, there are no certainties. AI is not a &quot;sure shot&quot;, but
<BR>
just another blind gamble, so the whole analogy sort of
<BR>
misses the point.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; &gt; You need a peace treaty.  You need a system, a process, which ensures
</EM><BR>
<EM>&gt; &gt; &gt; your safety.
</EM><BR>
<EM>&gt; &gt; 
</EM><BR>
<EM>&gt; &gt; MAD. Not a very stable system, perhaps, but neither are
</EM><BR>
<EM>&gt; &gt; superintelligent, evolving Sysops.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Who said anything about a plural?  One Sysop.  
</EM><BR>
<P>Ok, one then. Doesn't change the argument much though;
<BR>
it's still an unstable solution. In fact, a system with 
<BR>
multple independent Sysops could arguably be safer, providing
<BR>
checks &amp; balances like, for example, the multiple main
<BR>
computers in passenger jets or the Space Shuttle. Only 
<BR>
those commands which receive a majority &quot;vote&quot; are carried 
<BR>
out. Works pretty well, afaik. So if you're gonna make
<BR>
an AI (Sysop or otherwise) anyway, better make several
<BR>
at once (say, 3-10, depending on the costs etc.).
<BR>
<P><EM>&gt; One Power to enforce The
</EM><BR>
<EM>&gt; Rules that keep all the other Powers from playing cops-and-robbers with
</EM><BR>
<EM>&gt; the Solar System.  That's the point.
</EM><BR>
<P>Power corrupts, and absolute power...this may not apply just
<BR>
to humans. Better have an Assembly of Independent Powers.
<BR>
Perhaps the first thing they'd do is try to assassinate eachother,
<BR>
that would be pretty funny. 
<BR>
<P><EM>&gt; By the time non-self-improving AI becomes possible, self-improving AI
</EM><BR>
<EM>&gt; will have advanced well past the point where I can build one in a basement.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Given enough intelligence to be useful, the AI would have enough
</EM><BR>
<EM>&gt; intelligence to independently see the necessity of improving itself further.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I wouldn't trust an unPowered Sysop to upload me, anyway.
</EM><BR>
<P>Perhaps you're overestimating the difficulty of uploading; 
<BR>
the (destructive) scan method may require some kind of intelligent
<BR>
AI, but more gradual procedures involving neural implants and
<BR>
the like should be quite feasible for smart humans with &quot;dumb&quot; 
<BR>
but powerful computers. It's just fancy neurosurgery, really,
<BR>
which could be perfected in simulations and with various 
<BR>
animals (and eventually some humans). 
<BR>
<P><EM>&gt; &gt; To ask of me to just forget about continuity is like asking
</EM><BR>
<EM>&gt; &gt; you to just forget about the Singularity.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I *would* just forget about the Singularity, if it was necessary. 
</EM><BR>
<P>Necessary for what? 
<BR>
<P><EM>&gt; Getting to my current philosophical position required that I let go of a
</EM><BR>
<EM>&gt; number of irrational prejudices, some of which I was very much attached
</EM><BR>
<EM>&gt; to.  I let go of my irrational prejudice 
</EM><BR>
<P>Irrational by what standards -- those imposed by another randomly 
<BR>
picked irrational prejudice? 
<BR>
<P><EM>&gt; in favor of the
</EM><BR>
<EM>&gt; Singularity-outcome, 
</EM><BR>
<P>In what context (form your pov, not mine) is the Singularity
<BR>
rational, and why isn't this motivation just another irrational 
<BR>
prejudice that just happens to have strongly linked itself to 
<BR>
your pleasure centers? Aren't you really just rationalizing
<BR>
an essentially &quot;irrational&quot; choice (supergoal) like the rest of 
<BR>
humanity?
<BR>
<P><EM>&gt; and admitted the possibility of nanowar.  I let go
</EM><BR>
<EM>&gt; of my irrational prejudice in favor of a dramatic Singularity, one that
</EM><BR>
<EM>&gt; would let me play the hero, in favor of a fast, quiet seed AI.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; If it's an irrational prejudice, then let it go.
</EM><BR>
<P>Then you'd have to stop thinking altogether, I'm afraid.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; &gt; &gt; P.s: do you watch _Angel_ too?
</EM><BR>
<EM>&gt; &gt; &gt; Of course.
</EM><BR>
<EM>&gt; &gt; Ah yes, same here. Nice altruistic chap, isn't he?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; And very realistically so, actually.  If Angel's will is strong enough
</EM><BR>
<EM>&gt; to balance and overcome the built-in predatory instincts of a vampire,
</EM><BR>
<EM>&gt; then he's rather unlikely to succumb to lesser moral compromises.  He's
</EM><BR>
<EM>&gt; also a couple of hundred years old, which is another force operating to
</EM><BR>
<EM>&gt; move him to the extrema of whatever position he takes.
</EM><BR>
<P>Sure, but what's really interesting is that his conscience
<BR>
(aka &quot;soul&quot;) is a *curse*, specifically designed to make him 
<BR>
suffer (*). The implicit(?) message is that altruism isn't an 
<BR>
asset, but a handicap. The enlightened self-interest-based ethics 
<BR>
of &quot;normal&quot; vampires certainly make a lot more sense, rationally
<BR>
speaking; they usually have just one major rule, something along 
<BR>
the lines of &quot;do what you want as long as you don't harm your 
<BR>
own kind&quot;. Straightforward, logical, honest, robust &amp; efficient. 
<BR>
Compared to this elegant setup, altruism-based ethics (which
<BR>
glorify &quot;terminal&quot; self-sacrifice etc.) are just fragile bubbles, 
<BR>
ready to be popped by the sharp needle of reality.
<BR>
&nbsp;
<BR>
(*)Not exactly fair; it was the demon Angelus and not Angel himself
<BR>
who did all the killing etc. They are, afaik, two different persons
<BR>
who happen to use the same body. Those Gypsies were/are torturing
<BR>
an innocent man.
<BR>
<P><EM>&gt; Gotta develop an instinct for those non-gaussians if you're gonna think
</EM><BR>
<EM>&gt; about Minds...
</EM><BR>
<P>I'm pretty sure I've got the basics right, but of course so are you.
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5301.html">Robert Owen: "Re: American Imperialism?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5299.html">Michael S. Lorrey: "Re: American Imperialism?"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="4683.html">Eliezer S. Yudkowsky: "Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5321.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5321.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5300">[ date ]</A>
<A HREF="index.html#5300">[ thread ]</A>
<A HREF="subject.html#5300">[ subject ]</A>
<A HREF="author.html#5300">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:05:45 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
