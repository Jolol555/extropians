<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Otter vs. Yudkowsky</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: Otter vs. Yudkowsky">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Otter vs. Yudkowsky</H1>
<!-- received="Mon Mar 13 17:44:06 2000" -->
<!-- isoreceived="20000314004406" -->
<!-- sent="Mon, 13 Mar 2000 18:45:18 -0600" -->
<!-- isosent="20000314004518" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Otter vs. Yudkowsky" -->
<!-- id="38CD8B79.9B837411@pobox.com" -->
<!-- inreplyto="38CD6C82.2BD@geocities.com" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38CD8B79.9B837411@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon Mar 13 2000 - 17:45:18 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4787.html">Robert J. Bradbury: "Crime and Safety engineering [was: Ooh a gun fight!!]"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4785.html">john grigg: "Congratulations to Damien!"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4768.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4816.html">sayke: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4816.html">sayke: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4825.html">Spike Jones: "both denOtter and Yudkowsky: possible singularity scenarios"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4786">[ date ]</A>
<A HREF="index.html#4786">[ thread ]</A>
<A HREF="subject.html#4786">[ subject ]</A>
<A HREF="author.html#4786">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Otter and I appear to agree on the following:
<BR>
<P>1)  AI motivations are a coinflip:  They might be nice, they might wipe
<BR>
us out.  This is the consequence of several assertions on which we both
<BR>
agree, such as (2) you can't coerce a sufficiently advanced mind, even
<BR>
if you get to write the source code; (3) it's not known whether
<BR>
sufficiently advanced minds acquire motivations of their own, or whether
<BR>
they simply operate on momentum.
<BR>
<P>The points in dispute are these:
<BR>
<P>1)  From a strictly selfish perspective, does the likely utility of
<BR>
attempting to upload yourself outweigh the utility of designing a Sysop
<BR>
Mind?  Sub-disputes include (2) whether it's practically possible to
<BR>
develop perfect uploading before China initiates a nanowar or Eliezer
<BR>
runs a seed AI; (3) whether the fact that humans can be trusted no more
<BR>
than AIs will force your group to adopt a Sysop Mind approach in any
<BR>
case; (4) whether telling others that the Transtopians are going to
<BR>
upload and then erase the rest of humanity will generate opposition
<BR>
making it impossible for you to gain access to uploading prerequisite technologies.
<BR>
<P>I think that enough of the disputed points are dependent upon concrete
<BR>
facts to establish an unambiguous rational answer in favor of seed AI.
<BR>
<P>&quot;D.den Otter&quot; wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Well, it's certainly better than nothing, but the fact remains that
</EM><BR>
<EM>&gt; the Sysop mind could, at any time and for any reason, decide
</EM><BR>
<P>If it doesn't happen in the first few hours, you're safe forever.
<BR>
<P><EM>&gt; that it has better things to do than babysitting the OtterMind,
</EM><BR>
<EM>&gt; and terminate/adapt the latter.  Being completely at someone's
</EM><BR>
<EM>&gt; something's mercy is never a good idea.
</EM><BR>
<P>And here we come to the true crux of the problem.  You don't want to be
<BR>
at someone else's mercy.  You don't want to entrust your fate to the
<BR>
hidden variables.  You want to choose a course of action that puts you
<BR>
in the driver's seat, even if it kills you.  You're prejudiced in favor
<BR>
of plans that include what look like forceful actions against those
<BR>
yucky possibilities, even if the actions are ineffective and have awful
<BR>
side effects.  This is the same intuitive underpinning that underlies
<BR>
Welfare, bombing Kosovo and the War on Drugs.
<BR>
<P>Screw personal independence and all such slogans; the fundamental
<BR>
principle of Transhumanism is *rationality*.  If maintaining personal
<BR>
control is dumb, then you shouldn't do it.
<BR>
<P><EM>&gt; Who monitors the Sysop?
</EM><BR>
<P>I've considered the utility of including a &quot;programmer override&quot;, but my
<BR>
current belief is that the social anxiety generated by planning to
<BR>
include such an override has a negative utility that exceeds the danger
<BR>
of not having an override.  We'll just have to get it right the first
<BR>
time (meaning not flawlessness but flaw tolerance, of course).
<BR>
<P><EM>&gt; Self-defense excluded, I hope. Otherwise the OtterMind would
</EM><BR>
<EM>&gt; be a sitting duck.
</EM><BR>
<P>No, the Sysop Mind would defend you.
<BR>
<P><EM>&gt; Let's look at it this way: what if the government proposed a
</EM><BR>
<EM>&gt; system like this, i.e. everyone gets a chip implant that will
</EM><BR>
<EM>&gt; monitor his/her behaviour, and correct it if necessary so that
</EM><BR>
<EM>&gt; people no longer can (intentionally) harm eachother. How
</EM><BR>
<EM>&gt; would the public react? How would the members of this list
</EM><BR>
<EM>&gt; react? Wild guess: most wouldn't be too happy about it
</EM><BR>
<EM>&gt; (to use a titanic understatement). Blatant infringement of
</EM><BR>
<EM>&gt; fundamental rights and all that. Well, right they are. Now,
</EM><BR>
<EM>&gt; what would make this system all of a sudden &quot;acceptable&quot;
</EM><BR>
<EM>&gt; in a SI future? Does an increase in intelligence justify
</EM><BR>
<EM>&gt; this kind of coercion?
</EM><BR>
<P>What makes the system unacceptable, if implemented by humans, is that
<BR>
the humans have evolved to be corruptible and have an incredibly bad
<BR>
track record at that sort of thing.  All the antigovernmental heuristics
<BR>
of transhumanism have evolved from the simple fact that, historically,
<BR>
government doesn't work.  However, an omniscient AI is no more likely to
<BR>
become corrupt than a robot is likely to start lusting after human women.
<BR>
<P><EM>&gt; And something else: you belief that a SI can do with
</EM><BR>
<EM>&gt; us as it pleases because of its massively superior
</EM><BR>
<EM>&gt; intelligence. Superior intelligence = superior morality,
</EM><BR>
<EM>&gt; correct?
</EM><BR>
<P>No.  I believe that, for some level of intelligence above X - where X is
<BR>
known to be higher than the level attained by modern humans in modern
<BR>
civilization - it becomes possible to see the objectively correct moral
<BR>
decisions.  It has nothing to do with who, or what, the SIs are.  Their
<BR>
&quot;right&quot; is not a matter of social dominance due to superior
<BR>
formidability, but a form of reasoning that both you or I would
<BR>
inevitably agree with if we were only smart enough.
<BR>
<P>That human moral reasoning is observer-dependent follows from the
<BR>
historical fact that the dominant unit of evolutionary selection was the
<BR>
individual.  There is no reason to expect similar effects to arise in a
<BR>
system that be programmed to conceptualize itself as a design component
<BR>
as easily as an agent or an individual, and more likely would simply
<BR>
have not have any moral &quot;self&quot; at all.  I mean, something resembling an
<BR>
&quot;I&quot; will probably evolve whether we design it or not, but that doesn't
<BR>
imply that the &quot;I&quot; gets tangled up in the goal system.  Why would it?
<BR>
<P><EM>&gt; but that aside. Point is, by coercing
</EM><BR>
<EM>&gt; the &quot;ex-human&quot; SI (OtterMind in this case) by means
</EM><BR>
<EM>&gt; of morally rigid Sysop, you'd implicitly assume that you,
</EM><BR>
<EM>&gt; a mere neurohack human, already know &quot;what's right&quot;.
</EM><BR>
<EM>&gt; You'd apparently just &quot;know&quot; that harming others goes
</EM><BR>
<EM>&gt; against Objective Morality.
</EM><BR>
<P>The suggestions I make are just that, suggestions.  The design function
<BR>
of the suggestions is to provide a default and maximally happy scenario
<BR>
for the human species in the event that the Mind fails to discover
<BR>
different motivations; in which case my scenario, by definition, is not
<BR>
Objectively inferior to any other scenario.
<BR>
<P><EM>&gt; There is an obvious compromize, though (and perhaps
</EM><BR>
<EM>&gt; this is what you meant all along): the synthetic Minds
</EM><BR>
<EM>&gt; make sure that everyone uploads and reaches (approx.)
</EM><BR>
<EM>&gt; the same level of development (this means boosting
</EM><BR>
<EM>&gt; some Minds while slowing down others), and then they
</EM><BR>
<EM>&gt; shut themselves down, or simply merge with the
</EM><BR>
<EM>&gt; &quot;human&quot; Minds. The latter are then free to find the
</EM><BR>
<EM>&gt; true meaning of it all, and perhaps kill eachother in
</EM><BR>
<EM>&gt; the process (or maybe not).
</EM><BR>
<P>I've considered the possibility of a seed AI designed to pause itself
<BR>
before it reached the point of being able to discover an objective
<BR>
morality, upload humanity, give us a couple of thousand subjective
<BR>
millennia of hiatus, and then continue.  This way, regardless of how the
<BR>
ultimate answers turn out, everyone could have a reasonable amount of
<BR>
fun.  I'm willing to plan to waste a few objective hours if that plan
<BR>
relieves a few anxieties.
<BR>
<P>The problem with this picture is that I don't think it's a plausible
<BR>
&quot;suggestion&quot;.  The obvious historical genesis of the suggestion is your
<BR>
fear that the Mind will discover objective meaning.  (You would regard
<BR>
this as bad, I would regard this as good, we're fundamentally and
<BR>
mortally opposed, and fortunately neither of us has any influence
<BR>
whatsoever on how it turns out.)  But while the seed AI isn't at the
<BR>
level where it can be *sure* that no objective meaning exists, it has to
<BR>
take into account the possibility that it does.  The seed would tend to
<BR>
reason:  &quot;Well, I'm not sure whether or not this is the right thing to
<BR>
do, but if I just upgrade myself a bit farther, then I'll be sure.&quot;  And
<BR>
in fact, this *is* the correct chain of reasoning, and I'm not sure I or
<BR>
anyone else could contradict it.
<BR>
<P>The only way the Pause would be a valid suggestion is if there's such a
<BR>
good reason for doing it that the seed itself would come up with the
<BR>
suggestion independently.  That, after all, is the design heuristic I'm
<BR>
using:  No suggestion is truly stable unless it's so much a natural
<BR>
result of the entire system that it would regenerate if removed.  I hope
<BR>
to make the seed AI smart enough, and sufficiently in tune with the
<BR>
causality behind the specific suggestions, that even if I fail to come
<BR>
up with the correct suggestions, the seed will do so.  I'm not talking
<BR>
about objective morality, here, just &quot;the correct set of suggestions for
<BR>
creating a maximally happy outcome for humanity&quot;.  Ideally the seed,
<BR>
once it gets a bit above the human level, will be perfectly capable of
<BR>
understanding the desires behind the suggestions, and the rules we use
<BR>
to determine which desires are sensible and altruistic and which should
<BR>
be ignored, so that even if *I* miss a point, it won't.
<BR>
<P><EM>&gt; &gt; the simple
</EM><BR>
<EM>&gt; &gt; requirement of survival or even superiority, as a momentum-goal, does
</EM><BR>
<EM>&gt; &gt; not imply the monopolization of available resources.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Yes it does, assuming the Mind is fully rational and doesn't
</EM><BR>
<EM>&gt; like loose ends.
</EM><BR>
<P>Humans, even OtterMinds - EliezerMinds are nice and cooperative - don't
<BR>
need to be &quot;loose ends&quot;.  Specifically, if there is no significant
<BR>
probability of them &quot;breaking loose&quot; (in the sense of successfully
<BR>
executing an action outside the Sysop API which threatens the Sysop Goal
<BR>
of protecting humanity), and if the elimination of those &quot;loose ends&quot;
<BR>
would prevent the Sysop from attaining other goals (i.e. humans being
<BR>
all that they can be), destroying humanity would be an irrational
<BR>
action.  The chain of reasoning you're proposing is &quot;destroying humans
<BR>
because they pose a potential threat to the goal of protecting humans&quot;. 
<BR>
I mean, &quot;destroying humans because they pose a potential threat to the
<BR>
goal of manufacturing shoes&quot; might be a &quot;valid&quot; chain of logic, but not
<BR>
destroying humans to protect them.
<BR>
<P><EM>&gt; But where would this momentum goal come from? It's not a
</EM><BR>
<EM>&gt; logical goal (like &quot;survival&quot; or &quot;fun&quot;) but an arbitrary goal
</EM><BR>
<P>We've covered this territory.  Like you say below, &quot;survival&quot; may be a
<BR>
ubiquitous subgoal but it is no less arbitrary than &quot;green&quot; or &quot;purple&quot;
<BR>
as a supergoal.
<BR>
<P><EM>&gt; Well, see above. This would only make sense in an *acutely*
</EM><BR>
<EM>&gt; desperate situation. By all means, go ahead with your research,
</EM><BR>
<EM>&gt; but I'd wait with the final steps until we know for sure
</EM><BR>
<EM>&gt; that uploading/space escape isn't going to make it. In that
</EM><BR>
<EM>&gt; case I'd certainly support a (temporary!) Sysop arrangement.
</EM><BR>
<P>I think that we have enough concrete knowledge of the social situation,
<BR>
and of the pace of technological development, to say that a Sysop
<BR>
arrangement will almost certainly become necessary.  In which case,
<BR>
delaying Sysop deployment involves many definite risks.  From your
<BR>
perspective, a surprise attack on your headquarters (insufficient
<BR>
warning to achieve takeoff, Transcendence, and unbeatable hardware);
<BR>
from my perspective, the unnecessary death of large sectors of the human
<BR>
race; from both of our perspectives, the possibility that untested
<BR>
software fails (with missiles on the way!), or that bans on
<BR>
self-improving architectures prevent Sysop development before it's too
<BR>
late.  If the technological and social chance of a non-Sysop arrangement
<BR>
is small, and the increment of utility is low, then the rational choice
<BR>
is to maximize Sysop development speed and deploy any developed Sysop
<BR>
immediately.  Even if you don't give a damn about the 150,000 humans who
<BR>
die every day, you do need to worry about being hit by a truck, or about
<BR>
waking up to find that your lab has been disassembled by the secret
<BR>
Chinese nanotechnology project.
<BR>
<P>When I say that the increment of utility is low, what I mean is that you
<BR>
and your cohorts will inevitably decide to execute a Sysop-like
<BR>
arrangement in any case.  You and a thousand other Mind-wannabes wish to
<BR>
ensure your safety and survival.  One course of action is to upload,
<BR>
grow on independent hardware, and then fight it out in space.  If
<BR>
defense turns out to have an absolute, laws-of-physics advantage over
<BR>
offense, then you'll all be safe.  I think this is extraordinarily
<BR>
unlikely to be the case, given the historical trend.  If offense has an
<BR>
advantage over defense, you'll all fight it out until only one Mind
<BR>
remains with a monopoly on available resources.  However, is the utility
<BR>
of having the whole Solar System to yourself really a thousand times the
<BR>
utility, the &quot;fun&quot;, of having a thousandth of the available resources? 
<BR>
No.  You cannot have a thousand times as much fun with a thousand times
<BR>
as much mass.
<BR>
<P>You need a peace treaty.  You need a system, a process, which ensures
<BR>
your safety.  Humans (and the then-hypothetical human-derived Minds) are
<BR>
not knowably transparent or trustworthy, and your safety cannot be
<BR>
trusted to either a human judge or a process composed of humans.  The
<BR>
clever thing to do would be to create a Sysop which ensures that the
<BR>
thousand uploadees do not harm each other, which divides resources
<BR>
equally and executes other commonsense rules.  Offense may win over
<BR>
defense in physical reality, but not in software.  But now you're just
<BR>
converging straight back to the same method I proposed...
<BR>
<P>The other half of the &quot;low utility&quot; part is philosophical; if there are
<BR>
objective goals, you'll converge to them too, thus accomplishing exactly
<BR>
the same thing as if some other Mind converged to those goals.  Whether
<BR>
or not the Mind happens to be &quot;you&quot; is an arbitrary prejudice; if the
<BR>
Otterborn Mind is bit-by-bit indistinguishable from an Eliezerborn or
<BR>
AIborn Mind, but you take an action based on the distinction which
<BR>
decreases your over-all-branches probability of genuine personal
<BR>
survival, it's a stupid prejudice.
<BR>
<P><EM>&gt; That's a tough one. Is &quot;survival&quot; an objective (super)goal? One
</EM><BR>
<EM>&gt; must be alive to have (other) goals, that's for sure, but this
</EM><BR>
<EM>&gt; makes it a super-subgoal rather than a supergoal. Survival
</EM><BR>
<EM>&gt; for its own sake is rather pointless. In the end it still comes
</EM><BR>
<EM>&gt; down to arbitrary, subjective choices IMO.
</EM><BR>
<P>Precisely; and, in this event, it's possible to construct a Living Pact
<BR>
which runs on available hardware and gives you what you want at no
<BR>
threat to anyone else, thus maximizing the social and technical
<BR>
plausibility of the outcome.
<BR>
<P><EM>&gt; In any case, there's no need for &quot;objectively existent
</EM><BR>
<EM>&gt; supergoals&quot; to change the Sysop's mind; a simple
</EM><BR>
<EM>&gt; glitch in the system could have the same result, for
</EM><BR>
<EM>&gt; example.
</EM><BR>
<P>I acknowledge your right to hold me fully responsible for any failure to
<BR>
make an unbiased engineering estimate of the probability of such a glitch.
<BR>
<P><EM>&gt; Well, that's your educated (and perhaps a wee bit biased)
</EM><BR>
<EM>&gt; guess, anyway. We'll see.
</EM><BR>
<P>Perhaps.  I do try to be very careful about that sort of thing, though.
<BR>
<P><EM>&gt; P.s: do you watch _Angel_ too?
</EM><BR>
<P>Of course.
<BR>
<PRE>
-- 
       <A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38CD8B79.9B837411@pobox.com&gt;">sentience@pobox.com</A>      Eliezer S. Yudkowsky
          <A HREF="http://pobox.com/~sentience/beyond.html">http://pobox.com/~sentience/beyond.html</A>
                 Member, Extropy Institute
           Senior Associate, Foresight Institute
</PRE>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4787.html">Robert J. Bradbury: "Crime and Safety engineering [was: Ooh a gun fight!!]"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4785.html">john grigg: "Congratulations to Damien!"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4768.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4816.html">sayke: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4816.html">sayke: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4825.html">Spike Jones: "both denOtter and Yudkowsky: possible singularity scenarios"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4786">[ date ]</A>
<A HREF="index.html#4786">[ thread ]</A>
<A HREF="subject.html#4786">[ subject ]</A>
<A HREF="author.html#4786">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:05:04 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
