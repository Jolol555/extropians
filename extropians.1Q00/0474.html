<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: &gt;H Re: transhuman-digest V1 #562</TITLE>
<META NAME="Author" CONTENT="D.den Otter (neosapient@geocities.com)">
<META NAME="Subject" CONTENT="Re: &gt;H Re: transhuman-digest V1 #562">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: &gt;H Re: transhuman-digest V1 #562</H1>
<!-- received="Sun Jan  9 18:04:48 2000" -->
<!-- isoreceived="20000110010448" -->
<!-- sent="Mon, 10 Jan 2000 01:57:13 +0100" -->
<!-- isosent="20000110005713" -->
<!-- name="D.den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: &gt;H Re: transhuman-digest V1 #562" -->
<!-- id="38792E69.3198@geocities.com" -->
<!-- inreplyto="&gt;H Re: transhuman-digest V1 #562" -->
<STRONG>From:</STRONG> D.den Otter (<A HREF="mailto:neosapient@geocities.com?Subject=Re:%20&gt;H%20Re:%20transhuman-digest%20V1%20#562&In-Reply-To=&lt;38792E69.3198@geocities.com&gt;"><EM>neosapient@geocities.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Sun Jan 09 2000 - 17:57:13 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="0475.html">D.den Otter: "Re: SOC: Opposition to Transhumanism"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="0473.html">EvMick@aol.com: "Re: Fw: Audio Books"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="0501.html">Anders Sandberg: "Re: &gt;H Re: transhuman-digest V1 #562"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#474">[ date ]</A>
<A HREF="index.html#474">[ thread ]</A>
<A HREF="subject.html#474">[ subject ]</A>
<A HREF="author.html#474">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
----------
<BR>
<EM>&gt; From: Eliezer S. Yudkowsky &lt;<A HREF="mailto:sentience@pobox.com?Subject=Re:%20&gt;H%20Re:%20transhuman-digest%20V1%20#562&In-Reply-To=&lt;38792E69.3198@geocities.com&gt;">sentience@pobox.com</A>&gt;
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &quot;D.den Otter&quot; wrote:
</EM><BR>
<EM>&gt; &gt; 
</EM><BR>
<EM>&gt; &gt; How can you be so sure that this won't be the case? In the
</EM><BR>
<EM>&gt; &gt; absence of hard data either way, we must assume a 50% chance
</EM><BR>
<EM>&gt; &gt; of AIs causing our extinction.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; That's good enough for me!  What changed your mind?
</EM><BR>
<P>Oh, I haven't become more optimistic about the whole thing,
<BR>
if that's what you mean. Personally I think that the chance
<BR>
that the SI(s) will kill us is much greater than 50%, but for
<BR>
reasons of practicality it is better to stick to less 
<BR>
controversial (more conservative) estimates in a discussion. 
<BR>
A 50% chance of extinction is bad enough, mind you.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Oh golly, we've just been
</EM><BR>
<EM>&gt; &gt; reduced to a second-rate life form. We no longer control
</EM><BR>
<EM>&gt; &gt; our planet. We're at the mercy of hyperintelligent machines.
</EM><BR>
<EM>&gt; &gt; Yeah, that's something to be excited about...
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Sooner or later, we're gonna either toast the planet, or come up against
</EM><BR>
<EM>&gt; something smarter than we are.  You know that.  We've agreed on that. 
</EM><BR>
<EM>&gt; Your sole point of disagreement is that you believe that you'll be
</EM><BR>
<EM>&gt; better off if *you're* the first Power.  But Otter, that's silly.  
</EM><BR>
<P>Well, yes, of course it is. I'd love to be the first Power (who 
<BR>
wouldn't?), but it's hardly realistic. Hence the proposal for
<BR>
an upload project, comparable to your AI project. And yes,
<BR>
I know it's probably more difficult than your idea, but the
<BR>
reward is proportionally greater too; if you win you win for
<BR>
real. You are *free*, a true god. Not some cowering creature
<BR>
looking for SI handouts. There's a 50% chance that we'll
<BR>
*survive* if and when AIs turn into SIs, but that doesn't mean 
<BR>
that this life will be *good*, nor that the SIs &quot;mercy&quot; will be
<BR>
infinite. It could still kill or torture you anytime, or leave
<BR>
you forever stranded in some backwater simulation. The
<BR>
only truly positive scenario, unconditional uplifting, is
<BR>
just one of many possibilities within those 50%.
<BR>
<P><EM>&gt; If
</EM><BR>
<EM>&gt; transforming me into a Power might obliterate my tendency to care about
</EM><BR>
<EM>&gt; the welfare of others, it has an equal chance of obliterating my
</EM><BR>
<EM>&gt; tendency to care about myself.  
</EM><BR>
<P>Theoretically yes, though I still think that survival ranks well 
<BR>
above altruism; the latter has only evolved because it helps to 
<BR>
keep a creature alive. Altruism is the auxiliary of an auxiliary
<BR>
(survival), the supergoal being &quot;pleasure&quot; for example. If a SI 
<BR>
has goals, and needs to influence the world around it or at least
<BR>
experience mental states to achieve those goals, then it needs to 
<BR>
be alive, i.e. &quot;selfish&quot;. Altruism on the other hand is just 
<BR>
something that's useful when there are peers around, and which 
<BR>
becomes utterly obsolete if a SI gets so far ahead of the 
<BR>
competition that they no longer pose an immediate threat.
<BR>
<P><EM>&gt; If someone else, on becoming a Power,
</EM><BR>
<EM>&gt; might destroy you; then you yourself, on becoming a Power, might
</EM><BR>
<EM>&gt; overwrite yourself with some type of optimized being or mechanism.  You
</EM><BR>
<EM>&gt; probably wouldn't care enough to preserve any kind of informational or
</EM><BR>
<EM>&gt; even computational continuity.  
</EM><BR>
<P>Maybe, but I want to make that decision myself. I could stop
<BR>
caring about preserving the illusion of self, of course, but why
<BR>
should I care about anything else then? Why care about other
<BR>
people if your own life is meaningless? If one life is &quot;zero&quot;, than
<BR>
*all* lives are &quot;zero&quot;. Why do you want to save the world, your
<BR>
grandparents etc. if you don't care about personal identity?
<BR>
<P><EM>&gt; Both of these theories - unaltruism and
</EM><BR>
<EM>&gt; unselfishness - are equally plausible, and learning that either one was
</EM><BR>
<EM>&gt; the case would greatly increase the probability of the other.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; So, given that there's also a 50% chance that the Powers are nice guys,
</EM><BR>
<EM>&gt; or that no objective morality exists and Powers are freely programmable;
</EM><BR>
<EM>&gt; and given also that if the Powers *aren't* nice guys, then being the
</EM><BR>
<EM>&gt; Power-seed probably doesn't help; and given that your chance of winning
</EM><BR>
<EM>&gt; a competition to personally become the Power-seed is far more tenuous
</EM><BR>
<EM>&gt; than the chance of cooperatively writing an AI; and given that if we
</EM><BR>
<EM>&gt; *don't* create Powers, we're gonna get wiped out by a nanowar; and given
</EM><BR>
<EM>&gt; the fact that uploading is advanced drextech that comes after the
</EM><BR>
<EM>&gt; creation of nanoweapons, while AI can be run on IBM's Blue Gene; 
</EM><BR>
<P>It could be years, decades even before nanoweapons would actually 
<BR>
get used in a full-scale war. Viable space colonies could perhaps
<BR>
be developed before things get out of hand, neurohacking might
<BR>
beat both AI and nano if given proper attention, for example. AI
<BR>
isn't the only option; it's just a relatively &quot;easy&quot; way out. It 
<BR>
has a strong element of defeatism in it, IMO.
<BR>
<P><EM>&gt; and
</EM><BR>
<EM>&gt; given your admitted 50% chance that the Other Side of Dawn is a really
</EM><BR>
<EM>&gt; nice place to live, and that everyone can become Powers -
</EM><BR>
<P>A 50% chance that we'll *survive* (at least initially), that's 
<BR>
not the same as a 50% chance of paradise (that would be 25% at 
<BR>
best).
<BR>
&nbsp;
<BR>
<EM>&gt; In what sense is AI *not* something to be excited about?
</EM><BR>
<P>Oh, I just meant *positively* excited, of course. But I have
<BR>
to say that when it comes to dying, &quot;death by Singularity&quot; is
<BR>
a top choice.
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="0475.html">D.den Otter: "Re: SOC: Opposition to Transhumanism"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="0473.html">EvMick@aol.com: "Re: Fw: Audio Books"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="0501.html">Anders Sandberg: "Re: &gt;H Re: transhuman-digest V1 #562"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#474">[ date ]</A>
<A HREF="index.html#474">[ thread ]</A>
<A HREF="subject.html#474">[ subject ]</A>
<A HREF="author.html#474">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:02:11 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
