<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Informed consent and the exoself</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: Informed consent and the exoself">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Informed consent and the exoself</H1>
<!-- received="Mon Feb 21 20:12:25 2000" -->
<!-- isoreceived="20000222031225" -->
<!-- sent="Mon, 21 Feb 2000 21:12:06 -0600" -->
<!-- isosent="20000222031206" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Informed consent and the exoself" -->
<!-- id="38B1FE81.E3D42519@pobox.com" -->
<!-- inreplyto="Pine.GSO.4.10.10002211905490.15122-100000@morpheus.cis.yale.edu" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Informed%20consent%20and%20the%20exoself&In-Reply-To=&lt;38B1FE81.E3D42519@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon Feb 21 2000 - 20:12:06 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="3286.html">Eliezer S. Yudkowsky: "Re: Informed consent and the exoself"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3284.html">Damien Broderick: "what it's like to be uploaded"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3274.html">Dan Fabulich: "Re: Informed consent and the exoself"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3302.html">Dan Fabulich: "Re: Informed consent and the exoself"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="3302.html">Dan Fabulich: "Re: Informed consent and the exoself"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3285">[ date ]</A>
<A HREF="index.html#3285">[ thread ]</A>
<A HREF="subject.html#3285">[ subject ]</A>
<A HREF="author.html#3285">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Dan Fabulich wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Why not code in some contingent answers, more or less of the form:  &quot;Maybe
</EM><BR>
<EM>&gt; X is right,&quot; where X is one of our moral intuitions, but leave the total
</EM><BR>
<EM>&gt; probability that our moral intuitions are right far short of 100%.
</EM><BR>
<P>Actually, my current take calls for a comparative, rather than a
<BR>
quantitative, goal system.  So under that system, you'd put in the
<BR>
statement &quot;The humans want you to do X&quot;.  All else being equal, this
<BR>
statement is the only consideration; all else not being equal, it can be ignored.
<BR>
<P>The other half of the problem is phrasing the suggestions in such a way
<BR>
that they aren't crystalline; i.e., so they don't lead to the
<BR>
stereotypical &quot;logical but dumb&quot; conclusions.  A human goal system is
<BR>
composed of many different forces operating on approximately the same
<BR>
level, having evolutionary roots that stretch back for thousands of
<BR>
years; hence our resilience.  In a sense, what we *want* to say to the
<BR>
AI is completely explicable as the interaction of those forces; an AI
<BR>
that can fully understand those forces should be able to understand what
<BR>
we want.  That doesn't mean we can just ask it to do what we want,
<BR>
however; first and foremost, we want it not to be stupid.  So it gets complicated.
<BR>
<P><EM>&gt; Anyway, there's some reason to believe that there's just no way that the
</EM><BR>
<EM>&gt; computer will derive informed consent without a lot of knowledge and/or
</EM><BR>
<EM>&gt; experience about the world around it, experience which we've had coded
</EM><BR>
<EM>&gt; into our genes over the course of millions of years, and which we would be
</EM><BR>
<EM>&gt; sitting ducks without.  We're either going to have to wait millions of
</EM><BR>
<EM>&gt; years for the computer to figure out what we've had coded in already, (and
</EM><BR>
<EM>&gt; this time cannot be accelerated by faster computing power; these are facts
</EM><BR>
<EM>&gt; gotten from experience, not from deduction) or else we're going to have to
</EM><BR>
<EM>&gt; tell it about a few of our moral intuitions, and then let the AI decide
</EM><BR>
<EM>&gt; what to make of them.
</EM><BR>
<P>I don't think so.  Richness doesn't have to be extracted from the
<BR>
environment; it can as easily be extracted from mathematics or
<BR>
subjunctive simulations.
<BR>
<P>The general problem is that human beings are stupid, both passively and
<BR>
actively, and over the course of hundreds of thousands of years we've
<BR>
evolved personal and cultural heuristics for &quot;How not to be stupid&quot;. 
<BR>
The entire discipline of science is essentially a reaction to our
<BR>
tendency to believe statements for ideological reasons; if an AI doesn't
<BR>
have that tendency, will it evolve the rigorous methods of science?
<BR>
<P>My current take on the framework for a general solution is to ask the AI
<BR>
to design itself so that it's resilient under factual and cognitive
<BR>
errors; this resilience could be determined by evolutionary simulations
<BR>
or conscious design or whatever the AI decides works best.  The next
<BR>
question is how to represent non-crystalline suggestions such that they
<BR>
can be absorbed into the resulting framework.  Certainly the cognitive
<BR>
state and origins of the suggesting humans is a part of what is
<BR>
represented, if not necessarily of the initial representation itself.
<BR>
<P>Given a resilient, antibodied, actively smart representation, it might
<BR>
be possible to say:  &quot;Do what we want you to do, and don't do anything
<BR>
stupid,&quot; and have it work.  That statement can be fleshed out, i.e. by
<BR>
saying &quot;Pay more attention to the desires that are nonstupid according
<BR>
to your resilient implementation,&quot; but if the AI is actively smart it
<BR>
should be able to flesh out that statement for itself...
<BR>
<P>Anyway, it gets complicated.
<BR>
<PRE>
-- 
       <A HREF="mailto:sentience@pobox.com?Subject=Re:%20Informed%20consent%20and%20the%20exoself&In-Reply-To=&lt;38B1FE81.E3D42519@pobox.com&gt;">sentience@pobox.com</A>      Eliezer S. Yudkowsky
          <A HREF="http://pobox.com/~sentience/beyond.html">http://pobox.com/~sentience/beyond.html</A>
</PRE>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="3286.html">Eliezer S. Yudkowsky: "Re: Informed consent and the exoself"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3284.html">Damien Broderick: "what it's like to be uploaded"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3274.html">Dan Fabulich: "Re: Informed consent and the exoself"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3302.html">Dan Fabulich: "Re: Informed consent and the exoself"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="3302.html">Dan Fabulich: "Re: Informed consent and the exoself"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3285">[ date ]</A>
<A HREF="index.html#3285">[ thread ]</A>
<A HREF="subject.html#3285">[ subject ]</A>
<A HREF="author.html#3285">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:04:00 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
