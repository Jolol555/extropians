<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Luddites are everywhere!</TITLE>
<META NAME="Author" CONTENT="Zero Powers (zero_powers@hotmail.com)">
<META NAME="Subject" CONTENT="Re: Luddites are everywhere!">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Luddites are everywhere!</H1>
<!-- received="Wed Mar 15 02:38:50 2000" -->
<!-- isoreceived="20000315093850" -->
<!-- sent="Wed, 15 Mar 2000 01:38:16 PST" -->
<!-- isosent="20000315093816" -->
<!-- name="Zero Powers" -->
<!-- email="zero_powers@hotmail.com" -->
<!-- subject="Re: Luddites are everywhere!" -->
<!-- id="20000315093816.86272.qmail@hotmail.com" -->
<!-- inreplyto="Luddites are everywhere!" -->
<STRONG>From:</STRONG> Zero Powers (<A HREF="mailto:zero_powers@hotmail.com?Subject=Re:%20Luddites%20are%20everywhere!&In-Reply-To=&lt;20000315093816.86272.qmail@hotmail.com&gt;"><EM>zero_powers@hotmail.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Wed Mar 15 2000 - 02:38:16 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4954.html">Robert J. Bradbury: "Re: understanding neuroscience"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4952.html">Jim Fehlinger: "Bill Joy: enemy of the Singularity?"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="4833.html">John Clark: "Luddites are everywhere!"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4981.html">Zero Powers: "Re: Luddites are everywhere!"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4953">[ date ]</A>
<A HREF="index.html#4953">[ thread ]</A>
<A HREF="subject.html#4953">[ subject ]</A>
<A HREF="author.html#4953">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
<EM>&gt;From: &quot;Robert J. Bradbury&quot; &lt;<A HREF="mailto:bradbury@aeiveos.com?Subject=Re:%20Luddites%20are%20everywhere!&In-Reply-To=&lt;20000315093816.86272.qmail@hotmail.com&gt;">bradbury@aeiveos.com</A>&gt;
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;On Tue, 14 Mar 2000, Zero Powers wrote:
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt; &gt; I am glad to see this point of view being aired, if for no other reason 
</EM><BR>
<EM>&gt;than
</EM><BR>
<EM>&gt; &gt; that it will spur further debate.  I am as pro-way-out-tech as anybody 
</EM><BR>
<EM>&gt;on
</EM><BR>
<EM>&gt; &gt; this list.  But I do share Joy's concern that our pursuit of this tech 
</EM><BR>
<EM>&gt;is
</EM><BR>
<EM>&gt; &gt; not without *significant* dangers.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;Perhaps it is his way of getting more attention on the matter.  Joy
</EM><BR>
<EM>&gt;is certainly smart enough to write a piece that does not reflect his
</EM><BR>
<EM>&gt;true feelings.
</EM><BR>
<P>What makes you think he might take time from what is probably a very busy 
<BR>
schedule to write an essay about something he does not believe?  I also 
<BR>
heard an interview of him on the radio tonight and he certainly seemed 
<BR>
convinced of the concerns he expressed in the essay.
<BR>
<P><EM>&gt;The fact that there *are* dangers is very well known.
</EM><BR>
<P>Perhaps the dangers are well known by Senior Associates at the Foresight 
<BR>
Inst. and by the members of this list.  But I doubt that most of the other 
<BR>
readers of Wired have given much thought to these things.
<BR>
<P><EM>&gt;Those who are not Senior Associates at the Foresight Inst. (or who are
</EM><BR>
<EM>&gt;but haven't registered at the SAM site) do not know that there is a draft
</EM><BR>
<EM>&gt;copy of the &quot;Policy on Nanotechnology&quot; document.  One key point is:
</EM><BR>
<EM>&gt;- Replicators must not be capable of replication in a natural,
</EM><BR>
<EM>&gt;   uncontrolled environment.
</EM><BR>
<P>That's a good policy.  I like that policy.  But it will take a *lot* more 
<BR>
than a well stated policy to convince me that we ought to flip the GNR coin 
<BR>
at this point.
<BR>
<P><EM>&gt;That policy, if followed, removes Joy's &quot;unintentional&quot; accidents argument.
</EM><BR>
<P>You mean that policy if followed everywhere and always, without exception.  
<BR>
That's an awful big *if* you're tossing about.
<BR>
<P><EM>&gt;Yes, we can get into long discussions about how &quot;unenforceable&quot; it is,
</EM><BR>
<EM>&gt;but the point is the same one I made with &quot;almost everything&quot; machines.
</EM><BR>
<EM>&gt;The truth is that we have replicators replicating out in the uncontrolled
</EM><BR>
<EM>&gt;environment *now*.  If anything nanotech may make the real world *safer*.
</EM><BR>
<EM>&gt;Turn the argument on its head -- would you rather live in a world where
</EM><BR>
<EM>&gt;everything known and engineered for maximum safety (you can have it as
</EM><BR>
<EM>&gt;dangerous as you like in the virtual world) or would you rather live
</EM><BR>
<EM>&gt;in a world where the things that creep up on you in the night can and
</EM><BR>
<EM>&gt;do kill you?
</EM><BR>
<P>Sure a world where everything is engineered for maximum safety sounds good 
<BR>
to me.  It just seems to me that that is a lot easier said than done.
<BR>
<P><EM>&gt;His argument that we don't know how to produce reliable systems we have
</EM><BR>
<EM>&gt;discussed in previous threads re: trustability.  The current research
</EM><BR>
<EM>&gt;into secure transactions and reliabile nets *is* creating the body
</EM><BR>
<EM>&gt;of knowledge on how to engineer things that are fault tolerant and
</EM><BR>
<EM>&gt;don't cause excessive harm when they do break.  (Witness the recent
</EM><BR>
<EM>&gt;landing of the plane in San Francisco with one wheel up).  Do we
</EM><BR>
<EM>&gt;get it right all of the time?  No.
</EM><BR>
<P>Now you have hit the nail on the head!  The fact is we usually don't get it 
<BR>
right.  In fact we rarely if *ever* get it right the *first* time. Version 
<BR>
1.0 of any invention is usually very, very buggy.  The problem with GNR is 
<BR>
that unlike a computer or network failure, you don't simply get to reboot 
<BR>
and try again.  The first GNR mistake could very well be our last.
<BR>
<P><EM>&gt;But we seem to keep improving
</EM><BR>
<EM>&gt;our skills with time.  As Moravec points out we will have the computing
</EM><BR>
<EM>&gt;power to run simulations to see if there are potential problems before
</EM><BR>
<EM>&gt;we let things &quot;out&quot;.
</EM><BR>
<P>Even computer simulations can fail to disclose unforeseen problems. I'm sure 
<BR>
computer simulations were run prior to the Challenger disaster and prior to 
<BR>
the various Mars probe failures.  The fact is the only sure fire way to see 
<BR>
if we've got it right is to build the device in realspace, let it fly and 
<BR>
see if it works.
<BR>
<P><EM>&gt;The terrorism/mad-man letting loose nanotech horrors doesn't seem
</EM><BR>
<EM>&gt;to probable because the motivations for it mostly disappear when
</EM><BR>
<EM>&gt;human forms have all of the needs fulfilled.  You still have the
</EM><BR>
<EM>&gt;Sadaams and Abins to worry about but fortunately they are few
</EM><BR>
<EM>&gt;and far between and it will be much harder for them to recruit
</EM><BR>
<EM>&gt;a nano-terror cult in the world we envision.
</EM><BR>
<P>I wish I could be so optimistic.  There may be only one Saddaam and only one 
<BR>
Osama bin Laden.  But there are *millions* of people who hold similar 
<BR>
irrational and religious beliefs.  Giving them long life and unlimited 
<BR>
wealth may very well not assuage their belief that God has told them that 
<BR>
the &quot;Satanic&quot; new world order must be destroyed at all costs.
<BR>
<P><EM>&gt; &gt; I know there are people more
</EM><BR>
<EM>&gt; &gt; technologically and scientifically literate than me who feel that the
</EM><BR>
<EM>&gt; &gt; potential dangers are not worrisome enough to warrant stepping back long
</EM><BR>
<EM>&gt; &gt; enough to make sure that what *can* be done *should* be done.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;The point is we *are* doing those things.  The Foresight Inst. as
</EM><BR>
<EM>&gt;well as people on this list and many other places actively work
</EM><BR>
<EM>&gt;on these problems.  You have to keep in mind there *is* a cost
</EM><BR>
<EM>&gt;to slowing down.  I think the rate of death from hunger *alone*
</EM><BR>
<EM>&gt;is equivalent to a 747 loaded with people crashing into a mountain
</EM><BR>
<EM>&gt;*every* hour.  You just don't hear about it on the news every night.
</EM><BR>
<P>Hmmm.  Hundreds of people (on an over-crowded planet) dying every hour, 
<BR>
compared with the risk of a global gray goo problem.  Can you guess which 
<BR>
one I'd pick?
<BR>
<P><EM>&gt;The status quo has got to go.
</EM><BR>
<P>Surely it does.  But at the risk of jumping from the frying pan into the 
<BR>
fire?
<BR>
<P><EM>&gt; &gt; That alone is
</EM><BR>
<EM>&gt; &gt; enough to convince me that we cannot be assurred of smooth sailing as we 
</EM><BR>
<EM>&gt;set
</EM><BR>
<EM>&gt; &gt; out into these waters.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt;The *current* waters are filled with danger as well.  The only difference
</EM><BR>
<EM>&gt;is that you think you know about them and can avoid them.
</EM><BR>
<P>No, the difference is that the danger of the current waters is more of the 
<BR>
same: scattered famine, sickness and death.  The danger of the GNR waters is 
<BR>
*extinction.*
<BR>
<P><EM>&gt; &gt; Kurzweil seems to give us a 50% chance of avoiding our own extinction.
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt;Kurzweil is a pessimist (his self-image as an optimist not withstanding).
</EM><BR>
<EM>&gt;I'd put our chances at more like 80-90%.
</EM><BR>
<P>Is there any reason I should put more credence in your guess than 
<BR>
Kurzweil's?  He, after all, pessimist or no, has a fairly impressive record 
<BR>
of predicting the future.
<BR>
<P>-Zero
<BR>
<P>&quot;I like dreams of the future better than the history of the past&quot;
<BR>
--Thomas Jefferson
<BR>
<P>______________________________________________________
<BR>
Get Your Private, Free Email at <A HREF="http://www.hotmail.com">http://www.hotmail.com</A>
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4954.html">Robert J. Bradbury: "Re: understanding neuroscience"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4952.html">Jim Fehlinger: "Bill Joy: enemy of the Singularity?"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="4833.html">John Clark: "Luddites are everywhere!"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4981.html">Zero Powers: "Re: Luddites are everywhere!"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4953">[ date ]</A>
<A HREF="index.html#4953">[ thread ]</A>
<A HREF="subject.html#4953">[ subject ]</A>
<A HREF="author.html#4953">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:05:15 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
