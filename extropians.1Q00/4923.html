<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Luddites are everywhere!</TITLE>
<META NAME="Author" CONTENT="Robert J. Bradbury (bradbury@aeiveos.com)">
<META NAME="Subject" CONTENT="Re: Luddites are everywhere!">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Luddites are everywhere!</H1>
<!-- received="Tue Mar 14 19:22:54 2000" -->
<!-- isoreceived="20000315022254" -->
<!-- sent="Tue, 14 Mar 2000 18:22:53 -0800 (PST)" -->
<!-- isosent="20000315022253" -->
<!-- name="Robert J. Bradbury" -->
<!-- email="bradbury@aeiveos.com" -->
<!-- subject="Re: Luddites are everywhere!" -->
<!-- id="Pine.UW2.4.20.0003141748390.2859-100000@www.aeiveos.com" -->
<!-- inreplyto="20000314225250.47462.qmail@hotmail.com" -->
<STRONG>From:</STRONG> Robert J. Bradbury (<A HREF="mailto:bradbury@aeiveos.com?Subject=Re:%20Luddites%20are%20everywhere!&In-Reply-To=&lt;Pine.UW2.4.20.0003141748390.2859-100000@www.aeiveos.com&gt;"><EM>bradbury@aeiveos.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Tue Mar 14 2000 - 19:22:53 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4924.html">sayke: "[SEMANTICS?] atheisism/agnosticism/etc"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4922.html">Gina Miller: "Are you an Internet Addict?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4919.html">Zero Powers: "Re: Luddites are everywhere!"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4926.html">Damien Broderick: "AI done here cheap (was: Re: Luddites are everywhere!)"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4926.html">Damien Broderick: "AI done here cheap (was: Re: Luddites are everywhere!)"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4923">[ date ]</A>
<A HREF="index.html#4923">[ thread ]</A>
<A HREF="subject.html#4923">[ subject ]</A>
<A HREF="author.html#4923">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
On Tue, 14 Mar 2000, Zero Powers wrote:
<BR>
<P><EM>&gt; I am glad to see this point of view being aired, if for no other reason than 
</EM><BR>
<EM>&gt; that it will spur further debate.  I am as pro-way-out-tech as anybody on 
</EM><BR>
<EM>&gt; this list.  But I do share Joy's concern that our pursuit of this tech is 
</EM><BR>
<EM>&gt; not without *significant* dangers.
</EM><BR>
<P>Perhaps it is his way of getting more attention on the matter.  Joy
<BR>
is certainly smart enough to write a piece that does not reflect his
<BR>
true feelings.  The fact that there *are* dangers is very well known.
<BR>
<P>Those who are not Senior Associates at the Foresight Inst. (or who are
<BR>
but haven't registered at the SAM site) do not know that there is a draft
<BR>
copy of the &quot;Policy on Nanotechnology&quot; document.  One key point is:
<BR>
- Replicators must not be capable of replication in a natural,
<BR>
&nbsp;&nbsp;uncontrolled environment. 
<BR>
<P>That policy, if followed, removes Joy's &quot;unintentional&quot; accidents argument.
<BR>
Yes, we can get into long discussions about how &quot;unenforceable&quot; it is,
<BR>
but the point is the same one I made with &quot;almost everything&quot; machines.
<BR>
The truth is that we have replicators replicating out in the uncontrolled
<BR>
environment *now*.  If anything nanotech may make the real world *safer*.
<BR>
Turn the argument on its head -- would you rather live in a world where
<BR>
everything known and engineered for maximum safety (you can have it as
<BR>
dangerous as you like in the virtual world) or would you rather live
<BR>
in a world where the things that creep up on you in the night can and
<BR>
do kill you?
<BR>
<P>His argument that we don't know how to produce reliable systems we have
<BR>
discussed in previous threads re: trustability.  The current research
<BR>
into secure transactions and reliabile nets *is* creating the body
<BR>
of knowledge on how to engineer things that are fault tolerant and
<BR>
don't cause excessive harm when they do break.  (Witness the recent
<BR>
landing of the plane in San Francisco with one wheel up).  Do we
<BR>
get it right all of the time?  No.  But we seem to keep improving
<BR>
our skills with time.  As Moravec points out we will have the computing
<BR>
power to run simulations to see if there are potential problems before
<BR>
we let things &quot;out&quot;.
<BR>
<P>The terrorism/mad-man letting loose nanotech horrors doesn't seem
<BR>
to probable because the motivations for it mostly disappear when
<BR>
human forms have all of the needs fulfilled.  You still have the
<BR>
Sadaams and Abins to worry about but fortunately they are few
<BR>
and far between and it will be much harder for them to recruit
<BR>
a nano-terror cult in the world we envision.
<BR>
<P><EM>&gt; I know there are people more 
</EM><BR>
<EM>&gt; technologically and scientifically literate than me who feel that the 
</EM><BR>
<EM>&gt; potential dangers are not worrisome enough to warrant stepping back long 
</EM><BR>
<EM>&gt; enough to make sure that what *can* be done *should* be done.
</EM><BR>
<P>The point is we *are* doing those things.  The Foresight Inst. as
<BR>
well as people on this list and many other places actively work
<BR>
on these problems.  You have to keep in mind there *is* a cost
<BR>
to slowing down.  I think the rate of death from hunger *alone*
<BR>
is equivalent to a 747 loaded with people crashing into a mountain
<BR>
*every* hour.  You just don't hear about it on the news every night.
<BR>
<P>The status quo has got to go.
<BR>
<P><EM>&gt; That alone is 
</EM><BR>
<EM>&gt; enough to convince me that we cannot be assurred of smooth sailing as we set 
</EM><BR>
<EM>&gt; out into these waters.
</EM><BR>
<P>The *current* waters are filled with danger as well.  The only difference
<BR>
is that you think you know about them and can avoid them.
<BR>
<P><EM>&gt; But it certainly cannot hurt to debate and 
</EM><BR>
<EM>&gt; debate and debate these issues until we are blue in the face.
</EM><BR>
<P>We will do that.  I expect that there will be many conferences like that which
<BR>
defined biotech hazards and research protocols at Ansilomar.  We can have
<BR>
those because the discussion is *open*.   In contrast to the situation
<BR>
regarding the development of atomic weapons, that Bill uses for examples
<BR>
of why we shouldn't develop GNR (genetics/nanotech/robotics).
<BR>
<P><EM>&gt; 
</EM><BR>
<EM>&gt; Kurzweil seems to give us a 50% chance of avoiding our own extinction.  
</EM><BR>
<EM>&gt;
</EM><BR>
Kurzweil is a pessimist (his self-image as an optimist not withstanding).
<BR>
I'd put our chances at more like 80-90%.  I actually worry more about
<BR>
near earth asteroids than I worry about bio/nanotech.  I worry the *most*
<BR>
about self-evolving AI's with no moral code.  Kurzweil [&amp; Joy] are also
<BR>
way conservative on when we get human equivalent computing.  The hard
<BR>
part will be whether good AI takes 5 years or 20, but that's in Eliezer's,
<BR>
Doug's and a few others hands.
<BR>
<P><EM>&gt; Personally, I don't like those odds.  Heads you get immortality and 
</EM><BR>
<EM>&gt; unlimited wealth.  Tails you get global sterilization.  I need a little
</EM><BR>
<EM>&gt; more convincing before I vote to flip that coin.
</EM><BR>
<P>The article will have one interesting side effect is that it will probably
<BR>
function as a booster for the Mars Camp.  Of course the Mars Camp
<BR>
doesn't realize that if they flee to Mars because of the imminent
<BR>
development of nanotech Robots on Earth, the nanotech Roborts will
<BR>
probably be waiting for them when they arrive...
<BR>
<P>It seems Joy (and Kurzweil) clearly haven't thought things through
<BR>
completely.  The colonization of space is clearly silly in a universe
<BR>
that may be populated by Matrioshka Brains.
<BR>
<P>Robert
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4924.html">sayke: "[SEMANTICS?] atheisism/agnosticism/etc"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4922.html">Gina Miller: "Are you an Internet Addict?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4919.html">Zero Powers: "Re: Luddites are everywhere!"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4926.html">Damien Broderick: "AI done here cheap (was: Re: Luddites are everywhere!)"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4926.html">Damien Broderick: "AI done here cheap (was: Re: Luddites are everywhere!)"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4923">[ date ]</A>
<A HREF="index.html#4923">[ thread ]</A>
<A HREF="subject.html#4923">[ subject ]</A>
<A HREF="author.html#4923">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:05:13 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
