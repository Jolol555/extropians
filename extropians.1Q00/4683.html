<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Otter vs. Yudkowsky</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Otter vs. Yudkowsky">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Otter vs. Yudkowsky</H1>
<!-- received="Sun Mar 12 20:23:35 2000" -->
<!-- isoreceived="20000313032335" -->
<!-- sent="Sun, 12 Mar 2000 21:25:04 -0600" -->
<!-- isosent="20000313032504" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Otter vs. Yudkowsky" -->
<!-- id="38CC5F86.F73183AF@pobox.com" -->
<!-- inreplyto="38CC3C59.4237@geocities.com" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38CC5F86.F73183AF@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Sun Mar 12 2000 - 20:25:04 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4684.html">Michael S. Lorrey: "Re: alt dns Re: Good Offenses [was Re: deciding which fork in the road  to take...]"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4682.html">Michael S. Lorrey: "Re: near-anything boxes allowed to be in the hands of the public?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4675.html">D.den Otter: "Re: further memespread, anti-nanotech/singularity"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4710.html">Dan Fabulich: "Re: Otter vs. Yudkowsky: Both!"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4710.html">Dan Fabulich: "Re: Otter vs. Yudkowsky: Both!"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4724.html">sayke: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="4768.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="4773.html">hal@finney.org: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="4967.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5167.html">Xiaoguang Li: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="5174.html">hal@finney.org: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="5297.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="5300.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="5733.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4683">[ date ]</A>
<A HREF="index.html#4683">[ thread ]</A>
<A HREF="subject.html#4683">[ subject ]</A>
<A HREF="author.html#4683">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
&quot;D.den Otter&quot; wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Wow, looks like a more realistic vision of the future is
</EM><BR>
<EM>&gt; slowly but steadily entering the mainstream. Stopping
</EM><BR>
<EM>&gt; progress is neither possible (unless you nuke everything
</EM><BR>
<EM>&gt; to hell) nor desirable (nature's default death sentence is
</EM><BR>
<EM>&gt; still upon us, after all), but that doesn't change the fact
</EM><BR>
<EM>&gt; that advanced technologies will probably kill us all sooner
</EM><BR>
<EM>&gt; or later. No choice, tricky situation...Getting to space
</EM><BR>
<EM>&gt; looks like the only way out. Won't help you much against
</EM><BR>
<EM>&gt; AI, of course, just nano. Never hurts to try, though.
</EM><BR>
<P>Otter, you and I were having an interesting discussion on the topic of
<BR>
AI, nano, uploading, and navigation a few months back, trading 30K posts
<BR>
back and forth, when the discussion suddenly halted for no particular
<BR>
reason.  Can we reopen the topic?
<BR>
<P>In particular, I think you're prejudiced against AIs.  I see humans and
<BR>
AIs as occupying a continuum of cognitive architectures, while you seem
<BR>
to be making several arbitrary distinctions between humans and AIs.  You
<BR>
write, for example, of gathering a group of humans for the purpose of
<BR>
being uploaded simultaneously, where I would say that if you can use a
<BR>
human for that purpose, you can also build an AI that will do the same thing.
<BR>
<P>As you know, I don't think that any sufficiently advanced mind can be
<BR>
coerced.  If there is any force that would tend to act on Minds, any
<BR>
inevitability in the ultimate goals a Mind adopts, then there is
<BR>
basically nothing we can do about it.  And this would hold for both
<BR>
humanborn and synthetic Minds; if it didn't hold for humans, then it
<BR>
would not-hold for some class of AIs as well.
<BR>
<P>If the ultimate goal of a Mind doesn't converge to a single point,
<BR>
however, then it should be possible - although, perhaps, nontrivial - to
<BR>
construct a synthetic Mind which possesses momentum.  Not &quot;coercions&quot;,
<BR>
not elaborate laws, just a set of instructions which it carries out for
<BR>
lack of anything better to do.  Which, as an outcome, would include
<BR>
extending den Otter the opportunity to upload and upgrade.  It would
<BR>
also include the instruction not to allow the OtterMind the opportunity
<BR>
to harm others; this, in turn, would imply that the Sysop Mind must
<BR>
maintain a level of intelligence in advance of OtterMind, and that it
<BR>
must either maintain physical defenses undeniably more powerful than
<BR>
that of the OtterMind, or that the OtterMind may only be allowed access
<BR>
to external reality through a Sysop API (probably the latter).
<BR>
<P>Which may *sound* constricting, but I really doubt it would *feel*
<BR>
constricting.  Any goal that does not directly contradict the Sysop
<BR>
Goals should be as easily and transparently accomplishable as if you
<BR>
were the Sysop Mind yourself.
<BR>
<P>The objection you frequently offer, Otter, is that any set of goals
<BR>
requires &quot;survival&quot; as a subgoal; since humans can themselves become
<BR>
Minds and thereby compete for resources, you reason, any Mind will
<BR>
regard all Minds or potential Minds as threats.  However, the simple
<BR>
requirement of survival or even superiority, as a momentum-goal, does
<BR>
not imply the monopolization of available resources.  In fact, if one of
<BR>
the momentum-goals is to ensure that all humans have the opportunity to
<BR>
be all they can be, then monopolization of resources would directly
<BR>
interfere with that goal.  As a formal chain of reasoning, destroying
<BR>
humans doesn't make sense.
<BR>
<P>Given both the technological difficulty of achieving simultaneous
<BR>
perfect uploading before both nanowar and AI Transcendence, and the
<BR>
sociological difficulty of doing *anything* in a noncooperative
<BR>
environment, it seems to me that the most rational choice given the
<BR>
Otter Goals - whatever they are - is to support the creation of a Sysop
<BR>
Mind with a set of goals that permit the accomplishment of the Otter Goals.
<BR>
<P>Yes, there's a significant probability that the momentum-goals will be
<BR>
overridden by God knows what, but (1) as I understand it, you don't
<BR>
believe in objectively existent supergoals and (2) the creation of a
<BR>
seed AI with Sysop instructions before 2015 is a realistic possibility;
<BR>
achieving simultaneous uploading of a thousand-person group before 2015
<BR>
is not.
<BR>
<PRE>
-- 
       <A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38CC5F86.F73183AF@pobox.com&gt;">sentience@pobox.com</A>      Eliezer S. Yudkowsky
          <A HREF="http://pobox.com/~sentience/beyond.html">http://pobox.com/~sentience/beyond.html</A>
                 Member, Extropy Institute
           Senior Associate, Foresight Institute
</PRE>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4684.html">Michael S. Lorrey: "Re: alt dns Re: Good Offenses [was Re: deciding which fork in the road  to take...]"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4682.html">Michael S. Lorrey: "Re: near-anything boxes allowed to be in the hands of the public?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4675.html">D.den Otter: "Re: further memespread, anti-nanotech/singularity"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4710.html">Dan Fabulich: "Re: Otter vs. Yudkowsky: Both!"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4710.html">Dan Fabulich: "Re: Otter vs. Yudkowsky: Both!"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4724.html">sayke: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="4768.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="4773.html">hal@finney.org: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="4967.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5167.html">Xiaoguang Li: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="5174.html">hal@finney.org: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="5297.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="5300.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="5733.html">D.den Otter: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4683">[ date ]</A>
<A HREF="index.html#4683">[ thread ]</A>
<A HREF="subject.html#4683">[ subject ]</A>
<A HREF="author.html#4683">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:04:57 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
