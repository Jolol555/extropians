<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Otter vs. Yudkowsky</TITLE>
<META NAME="Author" CONTENT="D.den Otter (neosapient@geocities.com)">
<META NAME="Subject" CONTENT="Re: Otter vs. Yudkowsky">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Otter vs. Yudkowsky</H1>
<!-- received="Wed Mar 15 07:11:01 2000" -->
<!-- isoreceived="20000315141101" -->
<!-- sent="Wed, 15 Mar 2000 15:01:52 +0100" -->
<!-- isosent="20000315140152" -->
<!-- name="D.den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: Otter vs. Yudkowsky" -->
<!-- id="38CF97D0.3C07@geocities.com" -->
<!-- inreplyto="Otter vs. Yudkowsky" -->
<STRONG>From:</STRONG> D.den Otter (<A HREF="mailto:neosapient@geocities.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38CF97D0.3C07@geocities.com&gt;"><EM>neosapient@geocities.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Wed Mar 15 2000 - 07:01:52 MST
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4968.html">Brian D Williams: "[GUNS] Re: What are the reasons for killing?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4966.html">D.den Otter: "Re: Otter vs. Yudkowsky: Both!"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="4683.html">Eliezer S. Yudkowsky: "Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4984.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4984.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4967">[ date ]</A>
<A HREF="index.html#4967">[ thread ]</A>
<A HREF="subject.html#4967">[ subject ]</A>
<A HREF="author.html#4967">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
----------
<BR>
<EM>&gt; From: Eliezer S. Yudkowsky &lt;<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Otter%20vs.%20Yudkowsky&In-Reply-To=&lt;38CF97D0.3C07@geocities.com&gt;">sentience@pobox.com</A>&gt;
</EM><BR>
<P>[snip agreement]
<BR>
<EM>&gt; The points in dispute are these:
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; 1)  From a strictly selfish perspective, does the likely utility of
</EM><BR>
<EM>&gt; attempting to upload yourself outweigh the utility of designing a Sysop
</EM><BR>
<EM>&gt; Mind?  Sub-disputes include (2) whether it's practically possible to
</EM><BR>
<EM>&gt; develop perfect uploading before China initiates a nanowar or Eliezer
</EM><BR>
<EM>&gt; runs a seed AI; (3) whether the fact that humans can be trusted no more
</EM><BR>
<EM>&gt; than AIs will force your group to adopt a Sysop Mind approach in any
</EM><BR>
<EM>&gt; case; 
</EM><BR>
<P>Yes, this pretty much sums it up. 
<BR>
<P>(4) whether telling others that the Transtopians are going to
<BR>
<EM>&gt; upload and then erase the rest of humanity 
</EM><BR>
<P>This is optional. Also, this is not specifically a &quot;transtopian&quot; thing;
<BR>
if it's the most logical course of action, *all* ascending Minds
<BR>
would erase the rest of humanity. Wiping out humanity for
<BR>
its own sake is hardly interesting, and can be done more
<BR>
conveniently in simulations anyway, if so desired. 
<BR>
<P><EM>&gt; will generate opposition
</EM><BR>
<EM>&gt; making it impossible for you to gain access to uploading prerequisite
</EM><BR>
<EM>&gt; technologies.
</EM><BR>
<P>Opposition in the form of government bans should be no
<BR>
problem for a dedicated group. Besides, look who's
<BR>
talking; your pages are bound to scare the living crap
<BR>
out of many people, and not just the fanatical luddites
<BR>
(see KPJ's recent post regarding the Bill Joy poll at
<BR>
<A HREF="http://cgi.zdnet.com/zdpoll/question.html?pollid=17054&action=a">http://cgi.zdnet.com/zdpoll/question.html?pollid=17054&action=a</A>),
<BR>
for example). People are duly getting worried, and 
<BR>
statements like &quot;there's a significant chance that AIs 
<BR>
will indeed wipe us out, but hey, that's cool as long as 
<BR>
they find the meaning of life&quot; aren't likely to calm them
<BR>
down.
<BR>
&nbsp;
<BR>
<EM>&gt; I think that enough of the disputed points are dependent upon concrete
</EM><BR>
<EM>&gt; facts to establish an unambiguous rational answer in favor of seed AI.
</EM><BR>
<P>I think we'll need more details here. This may be the most
<BR>
important issue ever, so surely it deserves a more elab
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Well, it's certainly better than nothing, but the fact remains that
</EM><BR>
<EM>&gt; &gt; the Sysop mind could, at any time and for any reason, decide
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; If it doesn't happen in the first few hours, you're safe forever.
</EM><BR>
<P>&quot;Objective&quot; hours? That could be thousands if not millions
<BR>
of subjective years from the Sysop's pov. That's plenty of
<BR>
time to mutate beyond recognition.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; that it has better things to do than babysitting the OtterMind,
</EM><BR>
<EM>&gt; &gt; and terminate/adapt the latter.  Being completely at someone's
</EM><BR>
<EM>&gt; &gt; something's mercy is never a good idea.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; And here we come to the true crux of the problem.  You don't want to be
</EM><BR>
<EM>&gt; at someone else's mercy.  
</EM><BR>
<P>Damn right. This could be the only chance we'll ever get to
<BR>
become truly &quot;free&quot; (as far as mental structures allow), and
<BR>
it would be a shame to waste it.
<BR>
<P><EM>&gt; You don't want to entrust your fate to the
</EM><BR>
<EM>&gt; hidden variables.  You want to choose a course of action that puts you
</EM><BR>
<EM>&gt; in the driver's seat, even if it kills you.  You're prejudiced in favor
</EM><BR>
<EM>&gt; of plans that include what look like forceful actions against those
</EM><BR>
<EM>&gt; yucky possibilities, even if the actions are ineffective and have awful
</EM><BR>
<EM>&gt; side effects.  This is the same intuitive underpinning that underlies
</EM><BR>
<EM>&gt; Welfare, bombing Kosovo and the War on Drugs.
</EM><BR>
<P><EM>&gt; Screw personal independence and all such slogans; the fundamental
</EM><BR>
<EM>&gt; principle of Transhumanism is *rationality*.  If maintaining personal
</EM><BR>
<EM>&gt; control is dumb, then you shouldn't do it.
</EM><BR>
<P>Handing over control to AIs may initially offer an advantage, i.e.
<BR>
a greater chance of surviving the early phases of the Singularity,
<BR>
but may very well be a long-term terminal error. Uploading
<BR>
with the help of &quot;dumb&quot; computers on the other hand may
<BR>
increase your initial risks, but *if* you make it you've made
<BR>
it good. The key issue is: how big are these chances *really*?
<BR>
Ok, you say something like 30% vs 0,1%, but how exactly
<BR>
did you get these figures? Is there a particular passage in
<BR>
_Coding..._ or _Plan to.._ that deals with this issue? 
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Who monitors the Sysop?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I've considered the utility of including a &quot;programmer override&quot;, but my
</EM><BR>
<EM>&gt; current belief is that the social anxiety generated by planning to
</EM><BR>
<EM>&gt; include such an override has a negative utility that exceeds the danger
</EM><BR>
<EM>&gt; of not having an override.  We'll just have to get it right the first
</EM><BR>
<EM>&gt; time (meaning not flawlessness but flaw tolerance, of course).
</EM><BR>
<P>So the Sysop is God. Ok, let's hope Murphy's Law doesn't
<BR>
apply *too* much to coding AIs...
<BR>
<P>[regarding mind control in a human society]
<BR>
<EM>&gt; What makes the system unacceptable, if implemented by humans, is that
</EM><BR>
<EM>&gt; the humans have evolved to be corruptible and have an incredibly bad
</EM><BR>
<EM>&gt; track record at that sort of thing.  All the antigovernmental heuristics
</EM><BR>
<EM>&gt; of transhumanism have evolved from the simple fact that, historically,
</EM><BR>
<EM>&gt; government doesn't work.  However, an omniscient AI is no more likely to
</EM><BR>
<EM>&gt; become corrupt than a robot is likely to start lusting after human women.
</EM><BR>
<P>Actually, we just don't know what forces would be at work in 
<BR>
an AI that has reached the &quot;omniscient&quot; level. Maybe some
<BR>
form of terminal corruption is inevitable somewhere along
<BR>
the line, which would certainly explain the apparent lack of
<BR>
SI activity in the known universe. Bottom line: there are
<BR>
absolutely no guarantees. A dynamic, evolving system
<BR>
can't be trusted by defintion.
<BR>
<P><EM>&gt; &gt; And something else: you belief that a SI can do with
</EM><BR>
<EM>&gt; &gt; us as it pleases because of its massively superior
</EM><BR>
<EM>&gt; &gt; intelligence. Superior intelligence = superior morality,
</EM><BR>
<EM>&gt; &gt; correct?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; No.  I believe that, for some level of intelligence above X - where X is
</EM><BR>
<EM>&gt; known to be higher than the level attained by modern humans in modern
</EM><BR>
<EM>&gt; civilization - it becomes possible to see the objectively correct moral
</EM><BR>
<EM>&gt; decisions.  
</EM><BR>
<P>Maybe. Personally I think that SIs will only get a somewhat deeper 
<BR>
understanding of the relativity of it all. The idea that any
<BR>
intelligence,
<BR>
no matter how formidable, could say at some point that it &quot;knows
<BR>
everything&quot; seems utterly absurd. It could, for example, be part
<BR>
of an even smarter entity's simulation without ever finding out.
<BR>
<P><EM>&gt; It has nothing to do with who, or what, the SIs are.  Their
</EM><BR>
<EM>&gt; &quot;right&quot; is not a matter of social dominance due to superior
</EM><BR>
<EM>&gt; formidability, but a form of reasoning that both you or I would
</EM><BR>
<EM>&gt; inevitably agree with if we were only smart enough.
</EM><BR>
<P>Yes, *we* could &quot;inevitably&quot; agree that it makes perfect sense 
<BR>
to disassemble all lower life forms. Fat comfort to *them* that the 
<BR>
Almighty have decided so in their infinite wisdom. I wouldn't
<BR>
be much surprised if one of the &quot;eternal truths&quot; turns out to
<BR>
be &quot;might makes right&quot;.
<BR>
&nbsp;
<BR>
<EM>&gt; That human moral reasoning is observer-dependent follows from the
</EM><BR>
<EM>&gt; historical fact that the dominant unit of evolutionary selection was the
</EM><BR>
<EM>&gt; individual.  There is no reason to expect similar effects to arise in a
</EM><BR>
<EM>&gt; system that be programmed to conceptualize itself as a design component
</EM><BR>
<EM>&gt; as easily as an agent or an individual, and more likely would simply
</EM><BR>
<EM>&gt; have not have any moral &quot;self&quot; at all.  I mean, something resembling an
</EM><BR>
<EM>&gt; &quot;I&quot; will probably evolve whether we design it or not, but that doesn't
</EM><BR>
<EM>&gt; imply that the &quot;I&quot; gets tangled up in the goal system.  Why would it?
</EM><BR>
<P>I don't know, it's rather difficult to imagine an untangled &quot;I&quot;. This
<BR>
is basically an alien form of life...
<BR>
<P><EM>&gt; I've considered the possibility of a seed AI designed to pause itself
</EM><BR>
<EM>&gt; before it reached the point of being able to discover an objective
</EM><BR>
<EM>&gt; morality, upload humanity, give us a couple of thousand subjective
</EM><BR>
<EM>&gt; millennia of hiatus, and then continue.  This way, regardless of how the
</EM><BR>
<EM>&gt; ultimate answers turn out, everyone could have a reasonable amount of
</EM><BR>
<EM>&gt; fun.  I'm willing to plan to waste a few objective hours if that plan
</EM><BR>
<EM>&gt; relieves a few anxieties.
</EM><BR>
<P>Sounds good, but...
<BR>
<P><EM>&gt; The problem with this picture is that I don't think it's a plausible
</EM><BR>
<EM>&gt; &quot;suggestion&quot;.  The obvious historical genesis of the suggestion is your
</EM><BR>
<EM>&gt; fear that the Mind will discover objective meaning.  (You would regard
</EM><BR>
<EM>&gt; this as bad, 
</EM><BR>
<P>Only if it goes against my enlightened self-interest!
<BR>
<P><EM>&gt; I would regard this as good, we're fundamentally and
</EM><BR>
<EM>&gt; mortally opposed, and fortunately neither of us has any influence
</EM><BR>
<EM>&gt; whatsoever on how it turns out.)  But while the seed AI isn't at the
</EM><BR>
<EM>&gt; level where it can be *sure* that no objective meaning exists, 
</EM><BR>
<P>How can the AI be sure that it has reached this mystical
<BR>
plateau of true objective meaning? Because it &quot;just knows&quot;?
<BR>
<P><EM>&gt; it has to
</EM><BR>
<EM>&gt; take into account the possibility that it does.  The seed would tend to
</EM><BR>
<EM>&gt; reason:  &quot;Well, I'm not sure whether or not this is the right thing to
</EM><BR>
<EM>&gt; do, but if I just upgrade myself a bit farther, then I'll be sure.&quot;  
</EM><BR>
<P>Ad infinitum. A bit like chasing the horizon, IMO. 
<BR>
<P><EM>&gt; And
</EM><BR>
<EM>&gt; in fact, this *is* the correct chain of reasoning, and I'm not sure I or
</EM><BR>
<EM>&gt; anyone else could contradict it.
</EM><BR>
<P>It's logical to improve oneself indefinitely, but this is not the
<BR>
same as seeking THE truth. 
<BR>
&nbsp;
<BR>
<EM>&gt; The only way the Pause would be a valid suggestion is if there's such a
</EM><BR>
<EM>&gt; good reason for doing it that the seed itself would come up with the
</EM><BR>
<EM>&gt; suggestion independently.  
</EM><BR>
<P>Again, it's all a matter of perspective; what's good from the AI's
<BR>
pov doesn't have to be good from *our* pov. There may be a
<BR>
fundamental conflict between the two without one being
<BR>
more (ojectively) &quot;right&quot; than the other. Compare it to the
<BR>
situation of, for example, a snake and a rodent. The snake
<BR>
has to feed in order to survive, while the rodent obviously
<BR>
needs to avoid capture to achieve the same goal. Has one
<BR>
more &quot;right&quot; to live than the other? No, they are both &quot;right&quot;
<BR>
from their own pov. It would probably be no different in a
<BR>
human-AI conflict of interest. 
<BR>
<P><EM>&gt; The chain of reasoning you're proposing is &quot;destroying humans
</EM><BR>
<EM>&gt; because they pose a potential threat to the goal of protecting humans&quot;. 
</EM><BR>
<EM>&gt; I mean, &quot;destroying humans because they pose a potential threat to the
</EM><BR>
<EM>&gt; goal of manufacturing shoes&quot; might be a &quot;valid&quot; chain of logic, but not
</EM><BR>
<EM>&gt; destroying humans to protect them.
</EM><BR>
<P>No, I'm proposing that the AI, Sysop or otherwise, could dump
<BR>
the initial set of instructions (protect humans etc.) altogether
<BR>
for some known (glitch) or yet-unknown (alien mental dynamics)
<BR>
reason, in which case wiping out humanity would no longer
<BR>
be fundamentally illogical, but perhaps even a *necessary*
<BR>
measure (in support of some other, unknown, goal). As long
<BR>
as the AI sticks to its original goals there shouldn't be a
<BR>
problem. Presumably. I repeat: you can't trust an (ultra-
<BR>
rapidly) evolving system.
<BR>
<P><EM>&gt; I think that we have enough concrete knowledge of the social situation,
</EM><BR>
<EM>&gt; and of the pace of technological development, to say that a Sysop
</EM><BR>
<EM>&gt; arrangement will almost certainly become necessary.  
</EM><BR>
<P>Show me the evidence. If it adds up, I'm game.
<BR>
<P><EM>&gt; In which case,
</EM><BR>
<EM>&gt; delaying Sysop deployment involves many definite risks.  
</EM><BR>
[snip risks, on which we agree]
<BR>
<P><EM>&gt; You and a thousand other Mind-wannabes wish to
</EM><BR>
<EM>&gt; ensure your safety and survival.  One course of action is to upload,
</EM><BR>
<EM>&gt; grow on independent hardware, and then fight it out in space.  If
</EM><BR>
<EM>&gt; defense turns out to have an absolute, laws-of-physics advantage over
</EM><BR>
<EM>&gt; offense, then you'll all be safe.  I think this is extraordinarily
</EM><BR>
<EM>&gt; unlikely to be the case, given the historical trend. If offense has an
</EM><BR>
<EM>&gt; advantage over defense, you'll all fight it out until only one Mind
</EM><BR>
<EM>&gt; remains with a monopoly on available resources.  
</EM><BR>
<P>True, but who knows what the ascending Minds will decide
<BR>
in their &quot;infinite&quot; wisdom? Perhaps they'll come up with some
<BR>
really clever solution. Perhaps they'll even consider the needs
<BR>
of the rest of humanity. And yes, maybe the first thing they'll
<BR>
do is make some really big guns and try to blast eachother.
<BR>
Only one way to find out...
<BR>
<P><EM>&gt; However, is the utility
</EM><BR>
<EM>&gt; of having the whole Solar System to yourself really a thousand times the
</EM><BR>
<EM>&gt; utility, the &quot;fun&quot;, of having a thousandth of the available resources? 
</EM><BR>
<EM>&gt; No.  You cannot have a thousand times as much fun with a thousand times
</EM><BR>
<EM>&gt; as much mass.
</EM><BR>
<P>I think you'd have to be a SI to know that for sure. But, regardless of
<BR>
this, you don't need others to have &quot;fun&quot; once you can manipulate
<BR>
your cognitive/emotional structure, and have the option of creating
<BR>
any number of simulation worlds. A true SI is by definition fully
<BR>
self-sufficient, or at least has the option of becoming fully self-
<BR>
sufficient at any given time by tweaking its mental structure.
<BR>
Whether or not it would actually need all the resources of the
<BR>
solar system/galaxy/universe is besides the point.
<BR>
&nbsp;
<BR>
<EM>&gt; You need a peace treaty.  You need a system, a process, which ensures
</EM><BR>
<EM>&gt; your safety.  
</EM><BR>
<P>MAD. Not a very stable system, perhaps, but neither are 
<BR>
superintelligent, evolving Sysops.
<BR>
<P><EM>&gt; The
</EM><BR>
<EM>&gt; clever thing to do would be to create a Sysop which ensures that the
</EM><BR>
<EM>&gt; thousand uploadees do not harm each other, which divides resources
</EM><BR>
<EM>&gt; equally and executes other commonsense rules.  Offense may win over
</EM><BR>
<EM>&gt; defense in physical reality, but not in software.  But now you're just
</EM><BR>
<EM>&gt; converging straight back to the same method I proposed...
</EM><BR>
<P>In a way yes, for the time being anyway. The main issue now
<BR>
is what level of &quot;intelligence&quot; the Sysop should have.
<BR>
&nbsp;
<BR>
<EM>&gt; The other half of the &quot;low utility&quot; part is philosophical; if there are
</EM><BR>
<EM>&gt; objective goals, you'll converge to them too, thus accomplishing exactly
</EM><BR>
<EM>&gt; the same thing as if some other Mind converged to those goals.  Whether
</EM><BR>
<EM>&gt; or not the Mind happens to be &quot;you&quot; is an arbitrary prejudice; if the
</EM><BR>
<EM>&gt; Otterborn Mind is bit-by-bit indistinguishable from an Eliezerborn or
</EM><BR>
<EM>&gt; AIborn Mind, but you take an action based on the distinction which
</EM><BR>
<EM>&gt; decreases your over-all-branches probability of genuine personal
</EM><BR>
<EM>&gt; survival, it's a stupid prejudice.
</EM><BR>
<P>To me genuine personal survival is not just the destination,
<BR>
(if there even is such a thing) but also the journey. Call it an
<BR>
irrational prejudice, but hey, I happen to like the illusion of
<BR>
continuity. A solution is only acceptable if it respects this
<BR>
pov. To ask of me to just forget about continuity is like asking
<BR>
you to just forget about the Singularity.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Well, that's your educated (and perhaps a wee bit biased)
</EM><BR>
<EM>&gt; &gt; guess, anyway. We'll see.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Perhaps.  I do try to be very careful about that sort of thing, though.
</EM><BR>
<P>I sure hope so.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; P.s: do you watch _Angel_ too?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Of course.
</EM><BR>
<P>Ah yes, same here. Nice altruistic chap, isn't he?
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4968.html">Brian D Williams: "[GUNS] Re: What are the reasons for killing?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4966.html">D.den Otter: "Re: Otter vs. Yudkowsky: Both!"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="4683.html">Eliezer S. Yudkowsky: "Otter vs. Yudkowsky"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4984.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4984.html">Eliezer S. Yudkowsky: "Re: Otter vs. Yudkowsky"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4967">[ date ]</A>
<A HREF="index.html#4967">[ thread ]</A>
<A HREF="subject.html#4967">[ subject ]</A>
<A HREF="author.html#4967">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:05:16 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
