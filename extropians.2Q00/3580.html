<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: SITE: Coding a Transhuman AI 2.0a</TITLE>
<META NAME="Author" CONTENT="Dan Fabulich (daniel.fabulich@yale.edu)">
<META NAME="Subject" CONTENT="Re: SITE: Coding a Transhuman AI 2.0a">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: SITE: Coding a Transhuman AI 2.0a</H1>
<!-- received="Mon May 22 22:01:39 2000" -->
<!-- isoreceived="20000523040139" -->
<!-- sent="Tue, 23 May 2000 00:02:39 -0400 (EDT)" -->
<!-- isosent="20000523040239" -->
<!-- name="Dan Fabulich" -->
<!-- email="daniel.fabulich@yale.edu" -->
<!-- subject="Re: SITE: Coding a Transhuman AI 2.0a" -->
<!-- id="Pine.GSO.4.10.10005222221250.15546-100000@morpheus.cis.yale.edu" -->
<!-- inreplyto="00052220424001.00545@localhost.localdomain" -->
<STRONG>From:</STRONG> Dan Fabulich (<A HREF="mailto:daniel.fabulich@yale.edu?Subject=Re:%20SITE:%20Coding%20a%20Transhuman%20AI%202.0a&In-Reply-To=&lt;Pine.GSO.4.10.10005222221250.15546-100000@morpheus.cis.yale.edu&gt;"><EM>daniel.fabulich@yale.edu</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon May 22 2000 - 22:02:39 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="3581.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3579.html">Eugene Leitl: "Re: Humanoid Robots on the Mass Market"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3568.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3734.html">mjg223: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="3734.html">mjg223: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3580">[ date ]</A>
<A HREF="index.html#3580">[ thread ]</A>
<A HREF="subject.html#3580">[ subject ]</A>
<A HREF="author.html#3580">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Famous model Matt Gingell wrote:
<BR>
<P><EM>&gt; You seem to think a model is right because it's useful. That's
</EM><BR>
<EM>&gt; backwards: A model is useful because it's right. Concepts reflect
</EM><BR>
<EM>&gt; regularities in the world - their accuracy is a function of their
</EM><BR>
<EM>&gt; correlation to fact. That our aims are well served by a accurate 
</EM><BR>
<EM>&gt; conception of the world is true but beside the essential point.
</EM><BR>
<P>No.  Causation is not at work in my picture.  Rather, I come to know
<BR>
that a model is right once I know that the model is useful.  Actually,
<BR>
I think I'm spinning tautological wheels when I talk like that; that
<BR>
we're wasting our time to ask &quot;we know it's useful... but are we sure
<BR>
it's RIGHT?&quot;  &quot;We know it's right... but are we sure it's useful?&quot;  I
<BR>
know them both at once, in the same way that I know that A and ~~A
<BR>
simultaneously, though I may derive the one from the other
<BR>
immediately, if I must.
<BR>
<P>Euclid, for example, isn't &quot;right&quot; about anything at all in the
<BR>
strictest sense, except imaginary Euclid space.  The relevant property
<BR>
which Euclid's axioms DO have, along with all of the rest of the
<BR>
beliefs we hold, is the property of being &quot;close enough.&quot;  The very
<BR>
term &quot;model&quot; implies this willingness to accept deviations, so long as
<BR>
they are kept within acceptable limits.
<BR>
<P>But the notion of &quot;close enough,&quot; which is what we normally mean when
<BR>
we say &quot;right,&quot; is intimately tied up in the question of what you're
<BR>
using it for.  Peano's arithmetic is a good model for bricks, but a
<BR>
bad model for rabbits.  (&quot;1 + 1 = ... hold STILL won't you!?&quot;)
<BR>
<P><EM>&gt; Certainly there's an issue of perspective - if I live near a black
</EM><BR>
<EM>&gt; hole or move around near the speed of light, my model of reality would
</EM><BR>
<EM>&gt; be different. I would never claim otherwise - I'd only expect a
</EM><BR>
<EM>&gt; machine to develop concepts similar to my own if it's experience were
</EM><BR>
<EM>&gt; also similar. If I'm a picometer tall and you're a Jupiter brain,
</EM><BR>
<EM>&gt; neither of our world views is wrong - we are just each the other's
</EM><BR>
<EM>&gt; special case. The same holds for hyperbolic vs classical geometries -
</EM><BR>
<EM>&gt; neither is arbitrary: they're just describing different things. (Or rather
</EM><BR>
<EM>&gt; the first is a generalization of the second.)
</EM><BR>
<P>What is Euclid describing other than Euclid space?  Or is that it?
<BR>
Can't we read ANY belief as &quot;right, about whatever it is it's talking
<BR>
about&quot;?
<BR>
<P><EM>&gt; Goals do effect attention (While I think they're essential arbitrary
</EM><BR>
<EM>&gt; and uninteresting, I admit we have them.) Goals effect what you choose
</EM><BR>
<EM>&gt; to spend your resources contemplating - there being more things in
</EM><BR>
<EM>&gt; heaven and earth than dreamable at our speed C. A learning machine
</EM><BR>
<EM>&gt; without a goal is like a Zan master, sitting still, hallucinating,
</EM><BR>
<EM>&gt; sucking up fact and building purposeless towers of abstraction. The
</EM><BR>
<EM>&gt; mind is a means and not an end. Our drive to perpetuate ourselves and
</EM><BR>
<EM>&gt; our species is an artifact of our evolutionary history, our will to
</EM><BR>
<EM>&gt; survive is vestigial and as random as an appendix. Intelligence is an
</EM><BR>
<EM>&gt; engine perched on an animal - the forebrain being a survival subsystem
</EM><BR>
<EM>&gt; for an idiot limbic blob. Plug in some other root set of desires and
</EM><BR>
<EM>&gt; it'll as usefully tell you how to castrate yourself as how to spread
</EM><BR>
<EM>&gt; your genes. It'll identify cliffs I can jump off as faithfully as it
</EM><BR>
<EM>&gt; does wolves to run away from.
</EM><BR>
<P>... but despite our differing motivations, we will, at least, agree on
<BR>
the Facts, right?  What are these except the beliefs which we cannot
<BR>
imagine ourselves rejecting?  How could we tell the difference between
<BR>
the two?  Why would we care about such a difference?
<BR>
<P>I'm not pulling this &quot;radical interpreter&quot; stuff out of a hat, as it
<BR>
were.  Have you read Quine on this question?
<BR>
<P><EM>&gt; If motivation is made up but reality isn't, then it seems better to
</EM><BR>
<EM>&gt; describe the mind as something that parses the real world than a hack
</EM><BR>
<EM>&gt; that keeps you alive. 
</EM><BR>
<P>I'm rejecting the notion that a useful distinction can be made between
<BR>
realism and anti-realism.  I'm a pragmatist about that question: it
<BR>
just doesn't matter.  The whole distinction between &quot;made up&quot; and &quot;not
<BR>
made up&quot; in this context is a useless artifact: there are no answers
<BR>
there, and no need for answers.
<BR>
<P>Will you wear a different tie based on whether you have free will or
<BR>
not?  Will you build a bridge differently on the basis of whether your
<BR>
worldview is right for your own purposes or objectively right?  Will
<BR>
you behave any differently at all if you decide that P is a
<BR>
proposition you believe unflinchingly or if you think that P is a
<BR>
Fact?
<BR>
<P>You won't even build an AI differently, I argue.  This question is
<BR>
totally irrelevant as to whether there is a simple generic
<BR>
truth-finding algoritm that we're running, or whether there is a nasty
<BR>
complicated mess leading us to our conclusions.  We are willing to
<BR>
agree that anything that follows our algorithm (or one like it) will
<BR>
reach our conclusions.  We're even willing to agree that our algorithm
<BR>
is mostly right.  The only added claim, a useless one, is that we got
<BR>
to have this algorithm, and not another, because it's right.  I don't
<BR>
see what you get out of saying this.
<BR>
<P><EM>&gt; I wouldn't claim that the semantics, the ideas, I'm writing would be
</EM><BR>
<EM>&gt; understandable, even in principle, by anyone but an English
</EM><BR>
<EM>&gt; speaker. Yet there is still information content - it isn't random
</EM><BR>
<EM>&gt; noise (Even if it occasionally sounds a bit like it). The structure -
</EM><BR>
<EM>&gt; that is the characters, words, simple syntax, etc - are
</EM><BR>
<EM>&gt; extractable. Whether anybody would bother to investigate it is one
</EM><BR>
<EM>&gt; question, but that the structure is real and determinable, and is
</EM><BR>
<EM>&gt; independent of goals or survivability or what not, is unambiguously
</EM><BR>
<EM>&gt; true.
</EM><BR>
<P>How is this different from drawing a line in the epistemological sand
<BR>
and saying &quot;No uncertainty past this point!&quot;  I can always raise
<BR>
useless questions like &quot;how do you know that's a structure, rather
<BR>
than some arbitrary set?&quot;  Or is that a structure simply because you
<BR>
CALL it a structure?
<BR>
<P><EM>&gt; I've done a little bit of work on computer models of language
</EM><BR>
<EM>&gt; acquisition - the problem for a child is turning examples of speech
</EM><BR>
<EM>&gt; into general rules, inferring grammars from instances. It's a bit like
</EM><BR>
<EM>&gt; trying to turn object code back into source, figuring out structures
</EM><BR>
<EM>&gt; like for-loops from an untagged stream of machine instructions. Not
</EM><BR>
<EM>&gt; entirely unlike trying to unscramble an egg... That we are able to do
</EM><BR>
<EM>&gt; it at all, even to the controversial extent language is actually
</EM><BR>
<EM>&gt; really learned, amazes me. 
</EM><BR>
<P>Again, I highly advocate some Quine, who was, ah, also interested in
<BR>
this question (though he approached it from the perspective of a
<BR>
linguist out in the field attempting to understand the language of
<BR>
natives).
<BR>
<P><EM>&gt; Out of curiosity, how would you explain Kasparov's ability to play a
</EM><BR>
<EM>&gt; decent game of chess against a computer analyzing 200 million
</EM><BR>
<EM>&gt; positions a second? Certainly not a regular occurrence on the plains of
</EM><BR>
<EM>&gt; Africa, what more general facility does it demonstrate?
</EM><BR>
<P>Eh?  It can't just be his &quot;chess-playing&quot; facility?  I'm not aware of
<BR>
anything *else* at which Kasparov is the best in the world, suggesting
<BR>
that it is his chess-playing facility, and, apparently, nothing else,
<BR>
that did the work.  :)
<BR>
<P>Were I to assume that something more general was, in fact, at work,
<BR>
I'd have to guess that his capacities to plan ahead, imagine in a
<BR>
structured manner, and empathize with his opponent were all at work.
<BR>
Shot in the dark on my part...
<BR>
<P><EM>&gt; This is a very anthropomorphic view - I'm looking for a definition
</EM><BR>
<EM>&gt; that transcends humans and evolution, the essential properties shared
</EM><BR>
<EM>&gt; by all possible intelligences. You seem to be saying there isn't such
</EM><BR>
<EM>&gt; a thing - or it's the null set.
</EM><BR>
<P>No *interesting* properties, other than stuff like &quot;X can pass the
<BR>
Turing test,&quot; &quot;we'd probably call X intelligent,&quot; etc.
<BR>
<P><EM>&gt; You can go ahead and start coding, bang out behaviors for all the
</EM><BR>
<EM>&gt; situations you want - write vision systems, theorem provers, cunningly
</EM><BR>
<EM>&gt; indexable databases - but without an understanding of the principles
</EM><BR>
<EM>&gt; at work all you'll end up with is a undebuggable heap of brain damaged
</EM><BR>
<EM>&gt; cruft.
</EM><BR>
<P>If the code is written in a way that a &quot;programmer&quot; subsystem could
<BR>
understand it, or, as Eliezer calls it, a &quot;codic cortex&quot; analogous to
<BR>
the visual cortex, then each part may attempt to code itself better.
<BR>
Each domdule can participate in improving the capacity of the rest of
<BR>
the domdules.  That's how the system will improve, and improve itself
<BR>
better and faster than any human could have done.  But you need a rich
<BR>
system before this can take off.  That's where the hand-coding comes in.
<BR>
<P><EM>&gt; The answer to that question is 42. I could give you a reasonable,
</EM><BR>
<EM>&gt; logical argument that logic and reason are a good way at looking at
</EM><BR>
<EM>&gt; the world, but that would be circular. (Though if we're not assuming
</EM><BR>
<EM>&gt; logic, maybe there's nothing wrong with a circular argument...)
</EM><BR>
<P>Try this one.  I've got some beliefs which must be interpreted by
<BR>
others in order to be understood.  Suppose people in the future
<BR>
figure out the best model of the laws of physics we'll ever figure
<BR>
out.  Suppose they look back at Aristotle's physics.  They'll find
<BR>
that Aristotle was right about some things, even right about MOST
<BR>
things, but that his theory could have been substantially improved.
<BR>
They'd look back at Newton, at Einstein, at Bohr and Heisenberg, and
<BR>
find that they were right, mostly, but that their theory could have
<BR>
improved.  Similarly, if Aristotle was hard-headed but charitable, he
<BR>
would look FORWARD at the history of science, and say much the same
<BR>
things about the developments to come: mostly right, could be better.
<BR>
<P>In general, unless I employ the principles which Quine and friends
<BR>
have laid out: princples of charity, of humanity, etc. I can't even
<BR>
understand a bit of speech as *language* at all, say nothing of true
<BR>
or false language.  I have to interpret a person as having a body of
<BR>
mostly true beliefs before I can say that I understand the person at
<BR>
all, before I can begin to pick out one belief as right or wrong.
<BR>
<P>But Quine's radical interpretation begins at home.  Our own beliefs,
<BR>
by our best lights, are mostly right, though some of them, presumably,
<BR>
are wrong.
<BR>
<P>So whatever way you look at it, if I'm making sense to begin with, I'm
<BR>
mostly right.
<BR>
<P>-Dan
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-unless you love someone-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;-nothing else makes any sense-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e.e. cummings
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="3581.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3579.html">Eugene Leitl: "Re: Humanoid Robots on the Mass Market"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3568.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3734.html">mjg223: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="3734.html">mjg223: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3580">[ date ]</A>
<A HREF="index.html#3580">[ thread ]</A>
<A HREF="subject.html#3580">[ subject ]</A>
<A HREF="author.html#3580">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:11:32 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
