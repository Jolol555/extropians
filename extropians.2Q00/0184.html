<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Hofstadter Symposium [was Re: it was all a gag]</TITLE>
<META NAME="Author" CONTENT="Mike Linksvayer (ml@justintime.com)">
<META NAME="Subject" CONTENT="Re: Hofstadter Symposium [was Re: it was all a gag]">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Hofstadter Symposium [was Re: it was all a gag]</H1>
<!-- received="Tue Apr  4 00:16:45 2000" -->
<!-- isoreceived="20000404061645" -->
<!-- sent="Mon, 03 Apr 2000 23:16:47 -0700" -->
<!-- isosent="20000404061647" -->
<!-- name="Mike Linksvayer" -->
<!-- email="ml@justintime.com" -->
<!-- subject="Re: Hofstadter Symposium [was Re: it was all a gag]" -->
<!-- id="38E988CF.DFE88DF6@justintime.com" -->
<!-- inreplyto="38E78209.B2CB5AA8@Innovation-On-Demand.com" -->
<STRONG>From:</STRONG> Mike Linksvayer (<A HREF="mailto:ml@justintime.com?Subject=Re:%20Hofstadter%20Symposium%20[was%20Re:%20it%20was%20all%20a%20gag]&In-Reply-To=&lt;38E988CF.DFE88DF6@justintime.com&gt;"><EM>ml@justintime.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Tue Apr 04 2000 - 00:16:47 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="0185.html">Damien Broderick: "neurons from stem cells"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="0183.html">T0Morrow@aol.com: "Ninth Amendment (was Re: Spy planes was: Transhuman fascists?)"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="0053.html">Ken Clements: "Hofstadter Symposium [was Re: it was all a gag]"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="0211.html">Billy Brown: "RE: Hofstadter Symposium [was Re: it was all a gag]"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="0211.html">Billy Brown: "RE: Hofstadter Symposium [was Re: it was all a gag]"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="0222.html">Spike Jones: "Re: Hofstadter Symposium [was Re: it was all a gag]"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="0282.html">Mike Linksvayer: "Re: Hofstadter Symposium [was Re: it was all a gag]"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#184">[ date ]</A>
<A HREF="index.html#184">[ thread ]</A>
<A HREF="subject.html#184">[ subject ]</A>
<A HREF="author.html#184">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Ken Clements wrote:
<BR>
<EM>&gt; Hofstadter admitted that he had stacked the panel by not asking anyone
</EM><BR>
<EM>&gt; from the anti-technology movement (Bill made up that whole side).
</EM><BR>
<P>Hofstadter didn't invite anyone who believes that intelligence requires
<BR>
a biological brain, which is quite different from believing that
<BR>
technology is bad.  Joy seems to believe some technology is bad, but he
<BR>
doesn't seem to fall into the &quot;intelligence requires biology&quot; camp.
<BR>
(Offtopic aside:  Searle sounds like a very reasonable classical
<BR>
liberal in a recent interview with Reason magazine
<BR>
&lt;<A HREF="http://www.reasonmag.com/0002/fe.ef.reality.html">http://www.reasonmag.com/0002/fe.ef.reality.html</A>&gt;.  Just more proof,
<BR>
not that any was needed, that even reasonable people often take dumb
<BR>
arguments seriously.)
<BR>
<P>There were really two debates going on (though the atmosphere wasn't
<BR>
contentious at all):  rapid vs. slow development/evolution of
<BR>
human-level or greater machine &quot;intelligence&quot; (in quotes because what
<BR>
this means is nebulous and wasn't discussed) (primarily Kurzweil and
<BR>
Moravec vs. Holland and Koza respectively) and &quot;we must relinquish
<BR>
dangerous technology now or face catastrophy&quot; (Joy, with support from
<BR>
Koza, vs. Merkle and Moravec, with support from Kurzweil).
<BR>
<P>Kurzweil and Moravec's initial talks were quite boring, though their
<BR>
contributions to the discussion and Q&amp;A periods were the most
<BR>
insightful of the group.  After droning about exponential and double
<BR>
exponential increases in computational speed, Kurzweil did sneak in one
<BR>
gem:  he indicated that Moore's law, or something like it, also applies
<BR>
to software, of course very much contrary to most people's intuitions.
<BR>
I was very eager to hear a rationale for this claim.  Unfortunately
<BR>
when Holland asked about it at one point, Kurzweil only mentioned
<BR>
better development tools.
<BR>
<P>Joy seemed quite proud (in a very serious way) that the media is paying
<BR>
attention to him and that he is well read (or at least can scour books
<BR>
for emotional quotes supporting his argument, or at least pay someone
<BR>
to do so).  His argument basically boiled down to this:  supervirulent
<BR>
pathogens will be easily engineered and/or produced in crazy and/or
<BR>
sick people's basements, and if only a few of the millions of
<BR>
certifiably crazy, evil people in the world do this, we're all doomed.
<BR>
We must not allow the democratization of KMDs (Knowledge of Mass
<BR>
Destruction?).  Oh yeah, and remember how bad the plague was in Greek
<BR>
times or the middle ages?  Why, they catapulted plague-infected bodies
<BR>
over city walls, and people died horrible deaths and doctors couldn't
<BR>
help at all.  Clearly we have not evolved to the point where people can
<BR>
be trusted with knowledge of biology sufficient to engineer pathogens.
<BR>
And oh yeah, there are a bunch of famous people and  books that agree
<BR>
with Joy, and he can quote them all (I think Einstein was probably
<BR>
most quoted).
<BR>
<P>Joy's solution is &quot;relinquishment&quot;, though he didn't really give any
<BR>
details of what this would involve, though he seems to think that arms
<BR>
control treaties and subsequent verification protocols point in the
<BR>
right direction.  He also mentioned, once, strict corporate liability
<BR>
as a deterrent to corporations developing dangerous technologies.  I
<BR>
got a tiny chuckle out of that, as strict liability is one of those
<BR>
libertarian catchall answers.
<BR>
<P>I believe Joy said that he thinks there is a 30-50% chance of human
<BR>
extinction (presumably with no posthuman successor), not including all
<BR>
the other horrible outcomes that are likely.  I didn't get the
<BR>
impression from the other panelists (I should have asked that
<BR>
question), not to mention reading this mailing list, that human
<BR>
extinction isn't a real possibility.  I'd say that many of his concerns
<BR>
are valid, though his scaremonger/authoritarian approach seems
<BR>
contrived to create fame for himself.
<BR>
<P>If Joy was &quot;wrong&quot; and annoying, Merkle was &quot;right&quot; and extremely
<BR>
annoying.  I felt that Merkle came across as a (highly intelligent)
<BR>
pompous ass with a really bad sense of humor.  He didn't even attempt
<BR>
to address Joy's points, not counting wisecracks (&quot;Would those
<BR>
nanomachines be using the broadcast architecture, or some other
<BR>
architecture?&quot;  Ok, you had to be there.  I cringed.)  I got the same
<BR>
impression of Merkle when I saw him on stage with Michio Kaku at a
<BR>
&quot;Next 20 Years&quot; event.  My tentative evaluation:  brilliant researcher,
<BR>
rotten public spokesperson.
<BR>
<P>I hadn't heard of the broadcast architecture before (I don't attempt to
<BR>
keep current with nanotech research, though hardly anyone in the
<BR>
audience raised their hands when Merkle asked if anyone had heard of
<BR>
it, and I suspect many of them were imaginging some networking or
<BR>
distributed computing architecture, as I was when I considered
<BR>
half-raising my hand).  The idea seems to be that nanobots would
<BR>
somehow be broadcast instructions, eliminating the need for them to act
<BR>
completely independently (an analogy with DNA was made -- these
<BR>
broadcast architecture nanobots wouldn't carry around a full complement
<BR>
of DNA) and making them much cheaper and more controllable.  The last
<BR>
point was held forth as a promising means of preventing a runaway
<BR>
self-replicator catastrophe.
<BR>
<P>My intuition (and that's all I have on this point) doesn't
<BR>
find this one-sentence version of the broadcast architecture very
<BR>
compelling in terms of cost or danger.  Embedding instructions in a
<BR>
nanobot seems really cheap, considering the capacity of nanotech
<BR>
storage.  Would an embedded communications device be cheaper?  Well, it
<BR>
may be in one sense at least:  it would be much easier to program
<BR>
nanobots to do some very limited function and await instructions than
<BR>
it would be to program nanobots to do generalized tasks and to handle
<BR>
general contingencies.  But then it would be even simpler (not to
<BR>
mention safer) to program nanobots to do one task, then &quot;die&quot; after
<BR>
doing that task a desired number of times.  On controllability, it
<BR>
seems that if nanobots can be broadcast instructions, then they, having
<BR>
security bugs, can be broadcast bad instructions.
<BR>
<P>John Holland's comments were all very brief and generally well spoken.
<BR>
He was highly skeptical of Kurzweil and Moravec's predictions.  Holland
<BR>
said that we have a very slight understanding of intelligence, and
<BR>
without much better theory we won't get very far.  He drew an analogy
<BR>
between machine intelligence and fusion power -- he believes that we
<BR>
haven't gotten very far in five decades with the latter because we
<BR>
don't have sufficiently good theory, despite spending billions trying
<BR>
to make it work, and despite fusion power potentially being a really
<BR>
good thing.
<BR>
<P>Throughout the afternoon there were several comments that alluded to
<BR>
the need for better theory, or at least different approaches, in order
<BR>
to make breakthroughs.  Or, as Jeff Davis' Ray Charles signature quote
<BR>
says &quot;Everything's hard till you know how to do it.&quot;  Kurzweil and
<BR>
Moravec were asked whether if 100 years in the future we knew how to
<BR>
create machine intelligence, we couldn't run such an intelligence on
<BR>
today's computers (this followed someone mentioning a tinkertoy
<BR>
computer (but it doesn't run Linux!)).  Both seemed to indicate that
<BR>
today's computers simply don't have the storage or horsepower needed.
<BR>
I can understand storage, but given an intelligent program and
<BR>
glacially slow hardware, why can't it just be really slow?
<BR>
<P>Another comment in this vein from the audience mentioned that someone
<BR>
(at Sandia?) had created a robot that could walk with only twelve
<BR>
transistors, involving an analog feedback system, wheras it has been
<BR>
extremely hard to get many-MIPS digital-brained computers to walk.
<BR>
Moravec seemed to say that because analog requires some bulk
<BR>
technology, digital nanocomputers would probably be more cost effective
<BR>
even if they must be really complex.  Well, yeah, but we don't have
<BR>
nanocomputers yet.  There's lots of cool stuff remaining to be done
<BR>
with old technology, and I bet it will sometimes be much more cost
<BR>
effective from a development perspective.
<BR>
<P>Kevin Kelly's answer to the symposium's title &quot;Will spiritual robots
<BR>
replace humanity by 2100&quot; was &quot;NO WAY&quot;.  His argument, to the extent I
<BR>
caught it (I kind of zoned out for awhile do to extreme thirst) was
<BR>
that machine intelligence will fill lots of specialized niches, some of
<BR>
them niches previously filled by humans, but no machine will completely
<BR>
replace humans.  He used as a calculator as a primitive example -- it's
<BR>
much better than any human at arithmatic, but not good for much else.
<BR>
I'm not making the point as eloquently as he did.  Perhaps it was the
<BR>
graph with lots of little dots on it, all representing little niches
<BR>
for intelligent entities.  At best, he seemed to say, intelligent
<BR>
machines will free humans from having to work.
<BR>
<P>I also remember Kelly being the first to mention that communicating
<BR>
with intelligent machines of our creation could be a very spiritual
<BR>
thing, much like communicating with &quot;ET&quot; would be.  Kurzweil made a
<BR>
similar point several times.
<BR>
<P>Frank Drake came off as a mildly boring, mildly crackpot case.  We'll
<BR>
judge the aliens intelligence by the size of their radio telescopes,
<BR>
har, har, har.
<BR>
<P>John Koza said that in numerous attempts to have a genetic program
<BR>
learn to model some tiny aspect of human intelligence or perception,
<BR>
perhaps equivalent to one second of brain activity (I know this doesn't
<BR>
really make sense, I'm fuzzy on the details and I don't recall any of
<BR>
the specific cases) that he found he required 10^15 operations
<BR>
(requiring months on standard PCs).  So, a &quot;brain second&quot; is 10^15
<BR>
operations, and this huge number obviously poses a huge barrier to
<BR>
machine intelligence.  Or something like that.  I'll have to watch the
<BR>
webcast when it is available, seemed like an interesting point.
<BR>
<P>Even while listening, I was confused concerning Koza's argument
<BR>
vis-a-vis the hardness of machine intelligence.  It seems (as Kurzweil
<BR>
later pointed out concerning his speech recognition software) that once
<BR>
a genetic program &quot;learns&quot; a desired behavior, it can be copied
<BR>
infinitely, so the operations required to get to a certain level of
<BR>
functioning are mostly irrelevant.
<BR>
<P>There was lots of good stuff in the discussion and Q&amp;A sessions, but
<BR>
it's mostly a blur to me.  I'll mention three things I remember:
<BR>
<P>Kurzweil said that he was using genetic programming to simulate stock
<BR>
traders (presumably using historical data?)  Successful trader programs
<BR>
get to recombinate with other successful trader programs.  He didn't
<BR>
mention whether they were making real trades and if so, how
<BR>
successfully.  I'm sure lots of people are doing similar research,
<BR>
given the potential payoffs.
<BR>
<P>A few people mentioned consciousness being a pattern that presumably
<BR>
could be mapped to any substrate.  An example, given by either Kurzweil
<BR>
or Moravec, was that of a pattern in a river -- the water molecules
<BR>
constantly change, but the pattern may remain for long periods of
<BR>
time.  Moravec went even further, saying that perhaps conciousness is
<BR>
an interpretation of a pattern, so if you know what you're looking for,
<BR>
you could perhaps find conscious patterns, say in rocks, to pick a
<BR>
cliche.  Sure, this is run of the mill daydreaming for extropians, but
<BR>
somehow it's pleasant to hear it in public.
<BR>
<P>In response to an audience question about spirituality, Joy said that
<BR>
he had read a book (of course!) by E.O. Wilson in which Wilson had
<BR>
hinted at explaining all beliefs, including spiritual beliefs, in
<BR>
physical terms.  Joy said, roughly paraphrased, &quot;the game's changed,
<BR>
they [religious people] just haven't been told yet.&quot;  See, he has some
<BR>
sense!  Yeah, he wrote vi too.
<BR>
<P>After the event let out, I wondered around a bit and laid down under
<BR>
the pleasant sun in the deserted engineering quad.  The cirrus clouds
<BR>
above were beautiful and the temperature perfect.  The experience was
<BR>
giddy.  I rededicated myself to experiencing the wonder of life, even
<BR>
as a mere human, and eagerly look forward to attaining ever giddier
<BR>
heights, perhaps with some technological assistance in the future.
<BR>
<P>Later I wondered around Palo Alto while waiting for the next Caltrain.
<BR>
I hadn't been there in a few years.  On a saturday night, it's like
<BR>
fairyland.  Healthy and obviously wealthy people literally spilling out
<BR>
of every immaculate restaurant.  Someone went out of their way to pick
<BR>
up a pen I dropped in the bustle.  Even the sole homeless man seemed to
<BR>
be doing pretty well.  Reminded me of Santa Barbara, except that
<BR>
Stanford is where the ocean would be, and the workers aren't mostly
<BR>
Mexican.  Amazing what extraordinary wealth can do.  Don't imagine too
<BR>
many happy faces there today (NASDAQ selloff).
<BR>
<P>Mike
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="0185.html">Damien Broderick: "neurons from stem cells"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="0183.html">T0Morrow@aol.com: "Ninth Amendment (was Re: Spy planes was: Transhuman fascists?)"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="0053.html">Ken Clements: "Hofstadter Symposium [was Re: it was all a gag]"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="0211.html">Billy Brown: "RE: Hofstadter Symposium [was Re: it was all a gag]"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="0211.html">Billy Brown: "RE: Hofstadter Symposium [was Re: it was all a gag]"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="0222.html">Spike Jones: "Re: Hofstadter Symposium [was Re: it was all a gag]"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="0282.html">Mike Linksvayer: "Re: Hofstadter Symposium [was Re: it was all a gag]"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#184">[ date ]</A>
<A HREF="index.html#184">[ thread ]</A>
<A HREF="subject.html#184">[ subject ]</A>
<A HREF="author.html#184">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:09:02 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
