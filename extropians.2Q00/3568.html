<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: SITE: Coding a Transhuman AI 2.0a</TITLE>
<META NAME="Author" CONTENT="Matt Gingell (mjg223@is7.nyu.edu)">
<META NAME="Subject" CONTENT="Re: SITE: Coding a Transhuman AI 2.0a">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: SITE: Coding a Transhuman AI 2.0a</H1>
<!-- received="Mon May 22 18:40:14 2000" -->
<!-- isoreceived="20000523004014" -->
<!-- sent="Mon, 22 May 2000 20:39:05 -0400" -->
<!-- isosent="20000523003905" -->
<!-- name="Matt Gingell" -->
<!-- email="mjg223@is7.nyu.edu" -->
<!-- subject="Re: SITE: Coding a Transhuman AI 2.0a" -->
<!-- id="00052220424001.00545@localhost.localdomain" -->
<!-- inreplyto="Pine.GSO.4.10.10005220228490.796-100000@morpheus.cis.yale.edu" -->
<STRONG>From:</STRONG> Matt Gingell (<A HREF="mailto:mjg223@is7.nyu.edu?Subject=Re:%20SITE:%20Coding%20a%20Transhuman%20AI%202.0a&In-Reply-To=&lt;00052220424001.00545@localhost.localdomain&gt;"><EM>mjg223@is7.nyu.edu</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon May 22 2000 - 18:39:05 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="3569.html">Eugene Leitl: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3567.html">Matt Gingell: "Re: law enforcement for profit"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3509.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3580.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="3580.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3568">[ date ]</A>
<A HREF="index.html#3568">[ thread ]</A>
<A HREF="subject.html#3568">[ subject ]</A>
<A HREF="author.html#3568">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Dan, who at a genetic level is little different from Stalin, wrote:
<BR>
<P><EM>&gt; Matt Gingell, stinking Nazi [;)] wrote:
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &gt; Well, take something like geometry: is it true that the interior
</EM><BR>
<EM>&gt; &gt; angles of a triangle always add up to 180, or is that just an
</EM><BR>
<EM>&gt; &gt; arbitrary decision we made because it happens to suit our particular
</EM><BR>
<EM>&gt; &gt; purposes?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Arbitrary decisions!  In non-Euclidean geometries, the angles of a
</EM><BR>
<EM>&gt; triangle may NOT add up to 180.  And HOW do we decide which geometry
</EM><BR>
<EM>&gt; to use in a given situation?  Those darn *goals* again!  If only we
</EM><BR>
<EM>&gt; could rid them from our thought; then we'd see the Forms at last.
</EM><BR>
<P>You seem to think a model is right because it's useful. That's
<BR>
backwards: A model is useful because it's right. Concepts reflect
<BR>
regularities in the world - their accuracy is a function of their
<BR>
correlation to fact. That our aims are well served by a accurate 
<BR>
conception of the world is true but beside the essential point.
<BR>
<P>Certainly there's an issue of perspective - if I live near a black
<BR>
hole or move around near the speed of light, my model of reality would
<BR>
be different. I would never claim otherwise - I'd only expect a
<BR>
machine to develop concepts similar to my own if it's experience were
<BR>
also similar. If I'm a picometer tall and you're a Jupiter brain,
<BR>
neither of our world views is wrong - we are just each the other's
<BR>
special case. The same holds for hyperbolic vs classical geometries -
<BR>
neither is arbitrary: they're just describing different things. (Or rather
<BR>
the first is a generalization of the second.)
<BR>
<P>Goals do effect attention (While I think they're essential arbitrary
<BR>
and uninteresting, I admit we have them.) Goals effect what you choose
<BR>
to spend your resources contemplating - there being more things in
<BR>
heaven and earth than dreamable at our speed C. A learning machine
<BR>
without a goal is like a Zan master, sitting still, hallucinating,
<BR>
sucking up fact and building purposeless towers of abstraction. The
<BR>
mind is a means and not an end. Our drive to perpetuate ourselves and
<BR>
our species is an artifact of our evolutionary history, our will to
<BR>
survive is vestigial and as random as an appendix. Intelligence is an
<BR>
engine perched on an animal - the forebrain being a survival subsystem
<BR>
for an idiot limbic blob. Plug in some other root set of desires and
<BR>
it'll as usefully tell you how to castrate yourself as how to spread
<BR>
your genes. It'll identify cliffs I can jump off as faithfully as it
<BR>
does wolves to run away from.
<BR>
<P>If motivation is made up but reality isn't, then it seems better to
<BR>
describe the mind as something that parses the real world than a hack
<BR>
that keeps you alive. 
<BR>
<P><EM>&gt; &gt; Consider this message: you're looking at a bunch of phosphors lit up
</EM><BR>
<EM>&gt; &gt; on a screen, does it have any information content outside our
</EM><BR>
<EM>&gt; &gt; idiosyncratic conventions?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Language is the *quintessential* example of a set of idiosyncratic
</EM><BR>
<EM>&gt; conventions.  If this weren't true, then our language would have to
</EM><BR>
<EM>&gt; be just as it is, necessarily.  I don't think you'd want to make that
</EM><BR>
<EM>&gt; strong a claim.
</EM><BR>
<P>I wouldn't claim that the semantics, the ideas, I'm writing would be
<BR>
understandable, even in principle, by anyone but an English
<BR>
speaker. Yet there is still information content - it isn't random
<BR>
noise (Even if it occasionally sounds a bit like it). The structure -
<BR>
that is the characters, words, simple syntax, etc - are
<BR>
extractable. Whether anybody would bother to investigate it is one
<BR>
question, but that the structure is real and determinable, and is
<BR>
independent of goals or survivability or what not, is unambiguously
<BR>
true.
<BR>
<P>I've done a little bit of work on computer models of language
<BR>
acquisition - the problem for a child is turning examples of speech
<BR>
into general rules, inferring grammars from instances. It's a bit like
<BR>
trying to turn object code back into source, figuring out structures
<BR>
like for-loops from an untagged stream of machine instructions. Not
<BR>
entirely unlike trying to unscramble an egg... That we are able to do
<BR>
it at all, even to the controversial extent language is actually
<BR>
really learned, amazes me. 
<BR>
<P>Out of curiosity, how would you explain Kasparov's ability to play a
<BR>
decent game of chess against a computer analyzing 200 million
<BR>
positions a second? Certainly not a regular occurrence on the plains of
<BR>
Africa, what more general facility does it demonstrate?
<BR>
<P><EM>&gt; ...
</EM><BR>
<P><EM>&gt; &gt; If we are to understand what intelligence is, we must construct a
</EM><BR>
<EM>&gt; &gt; definition which is not particular to a design process, a set of
</EM><BR>
<EM>&gt; &gt; goals, or a set of sensors and limbs. Implicit in this statement is
</EM><BR>
<EM>&gt; &gt; the notion that the word 'intelligent' actually means something, that
</EM><BR>
<EM>&gt; &gt; there's a meaningful line somewhere between intelligence and clever
</EM><BR>
<EM>&gt; &gt; algorithms which fake universality by virtue of shear vastness.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I reject the notion that arguing against you would require me to
</EM><BR>
<EM>&gt; conclude that the word &quot;intelligent&quot; is meaningless.  On the contrary,
</EM><BR>
<EM>&gt; I argue that the word &quot;intelligent&quot; does have meaning *to us*, in the
</EM><BR>
<EM>&gt; language *we* speak, *today* at the beginning of the 21st century.
</EM><BR>
<EM>&gt; Your assertion requires me to believe that this word somehow has
</EM><BR>
<EM>&gt; meaning beyond our language, beyond us.  It requires &quot;intelligence&quot; to
</EM><BR>
<EM>&gt; be something transcendent, rather than simply meaningful.
</EM><BR>
<P>This is a very anthropomorphic view - I'm looking for a definition
<BR>
that transcends humans and evolution, the essential properties shared
<BR>
by all possible intelligences. You seem to be saying there isn't such
<BR>
a thing - or it's the null set.
<BR>
<P><EM>&gt; It is in coming to terms with the fact that intelligence is not
</EM><BR>
<EM>&gt; transcendent that AI will finally get off the ground.  We'll finally
</EM><BR>
<EM>&gt; start coding in the stuff that we'd hoped the AI would find simply by
</EM><BR>
<EM>&gt; virtue of it being true.  (&quot;...and, since it's preloaded with the
</EM><BR>
<EM>&gt; general truth finding algorithm, it'll SURELY find it eventually,
</EM><BR>
<EM>&gt; given enough computing power, time, and most of all GRANT MONEY...&quot;).
</EM><BR>
<P>You can go ahead and start coding, bang out behaviors for all the
<BR>
situations you want - write vision systems, theorem provers, cunningly
<BR>
indexable databases - but without an understanding of the principles
<BR>
at work all you'll end up with is a undebuggable heap of brain damaged
<BR>
cruft.
<BR>
<P>(There should probably be an IMHO in there somewhere... Pinch me, I'm
<BR>
pontificating.)
<BR>
<P><EM>&gt; ...
</EM><BR>
<P><EM>&gt; &gt; &gt; Epistemologically speaking, how would we know if we had stumbled upon
</EM><BR>
<EM>&gt; &gt; &gt; the general algorithm, or whether we were just pursuing our own
</EM><BR>
<EM>&gt; &gt; &gt; purposes again?  For that matter, why would we care?  Why not call our
</EM><BR>
<EM>&gt; &gt; &gt; own beliefs Right out of elegance and get on with Coding a Transhuman
</EM><BR>
<EM>&gt; &gt; &gt; AI?
</EM><BR>
<EM>&gt; &gt; 
</EM><BR>
<EM>&gt; &gt; We couldn't know, but if we got good results then we'd be pretty sure
</EM><BR>
<EM>&gt; &gt; we were at least close. Whether you care depends on your motivation:
</EM><BR>
<EM>&gt; &gt; I'm interested in intelligence because I like general solutions to
</EM><BR>
<EM>&gt; &gt; problems more than I like special case ones, and you don't get a more
</EM><BR>
<EM>&gt; &gt; general solution than AI. I futz around with this stuff out of
</EM><BR>
<EM>&gt; &gt; intellectual curiosity, if the mind turns out to be necessarily ugly
</EM><BR>
<EM>&gt; &gt; I'll go do something else. I don't really care about saving the world 
</EM><BR>
<EM>&gt; &gt; from grey goo or the future of the human race or whatever.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; That's a little cavalier, considering that you're one of us, isn't it?
</EM><BR>
<EM>&gt; ;) Sure, sure, let the rest of us do the HARD work... ;)
</EM><BR>
<P>Well, we acknowledge that trenches have to be dug, but I don't suppose
<BR>
either of us would with doing it for a living very rewarding. Better
<BR>
dead than ugly, I always say. 
<BR>
&nbsp;
<BR>
<EM>&gt; Anyway.  I think you're forgetting that this is the general truth
</EM><BR>
<EM>&gt; finding algorithm which WE supposedly use in finding truth.  So how
</EM><BR>
<EM>&gt; would we know if we'd found it?  We'd &quot;check&quot; to see if our results
</EM><BR>
<EM>&gt; were good, and if so, we're close, you say.  But how would we &quot;check&quot;
</EM><BR>
<EM>&gt; on this?  Well, we'd run our truth finding algorithm again, of course,
</EM><BR>
<EM>&gt; since that's all we've got in the search for truth.  According to it,
</EM><BR>
<EM>&gt; our results are &quot;good.&quot;  Have we found the general truth finding
</EM><BR>
<EM>&gt; algorithm?  Well, the general truth finding algorithm seems to say so!
</EM><BR>
<P>The answer to that question is 42. I could give you a reasonable,
<BR>
logical argument that logic and reason are a good way at looking at
<BR>
the world, but that would be circular. (Though if we're not assuming
<BR>
logic, maybe there's nothing wrong with a circular argument...)
<BR>
<P>There's a quote that comes to mind, Jack Handey or someone, along the
<BR>
lines:
<BR>
<P>&quot;I used to think my brain was the most important part of my body. But
<BR>
then I was like, hey, consider the source.&quot;
<BR>
<P>-matt 
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="3569.html">Eugene Leitl: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3567.html">Matt Gingell: "Re: law enforcement for profit"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3509.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3580.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="3580.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3568">[ date ]</A>
<A HREF="index.html#3568">[ thread ]</A>
<A HREF="subject.html#3568">[ subject ]</A>
<A HREF="author.html#3568">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:11:31 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
