<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: SITE: Coding a Transhuman AI 2.0a</TITLE>
<META NAME="Author" CONTENT="Dan Fabulich (daniel.fabulich@yale.edu)">
<META NAME="Subject" CONTENT="Re: SITE: Coding a Transhuman AI 2.0a">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: SITE: Coding a Transhuman AI 2.0a</H1>
<!-- received="Mon May 22 01:10:05 2000" -->
<!-- isoreceived="20000522071005" -->
<!-- sent="Mon, 22 May 2000 03:10:54 -0400 (EDT)" -->
<!-- isosent="20000522071054" -->
<!-- name="Dan Fabulich" -->
<!-- email="daniel.fabulich@yale.edu" -->
<!-- subject="Re: SITE: Coding a Transhuman AI 2.0a" -->
<!-- id="Pine.GSO.4.10.10005220228490.796-100000@morpheus.cis.yale.edu" -->
<!-- inreplyto="00052112512200.00491@localhost.localdomain" -->
<STRONG>From:</STRONG> Dan Fabulich (<A HREF="mailto:daniel.fabulich@yale.edu?Subject=Re:%20SITE:%20Coding%20a%20Transhuman%20AI%202.0a&In-Reply-To=&lt;Pine.GSO.4.10.10005220228490.796-100000@morpheus.cis.yale.edu&gt;"><EM>daniel.fabulich@yale.edu</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon May 22 2000 - 01:10:54 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="3510.html">KPJ: "Re: law enforcement for profit"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3508.html">Spike Jones: "Re: a new thread"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3465.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3568.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="3568.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3509">[ date ]</A>
<A HREF="index.html#3509">[ thread ]</A>
<A HREF="subject.html#3509">[ subject ]</A>
<A HREF="author.html#3509">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Matt Gingell, stinking Nazi [;)] wrote:
<BR>
<P><EM>&gt; Well, take something like geometry: is it true that the interior
</EM><BR>
<EM>&gt; angles of a triangle always add up to 180, or is that just an
</EM><BR>
<EM>&gt; arbitrary decision we made because it happens to suit our particular
</EM><BR>
<EM>&gt; purposes?
</EM><BR>
<P>Arbitrary decisions!  In non-Euclidean geometries, the angles of a
<BR>
triangle may NOT add up to 180.  And HOW do we decide which geometry
<BR>
to use in a given situation?  Those darn *goals* again!  If only we
<BR>
could rid them from our thought; then we'd see the Forms at last.
<BR>
<P><EM>&gt; Consider this message: you're looking at a bunch of phosphors lit up
</EM><BR>
<EM>&gt; on a screen, does it have any information content outside our
</EM><BR>
<EM>&gt; idiosyncratic conventions?
</EM><BR>
<P>Language is the *quintessential* example of a set of idiosyncratic
<BR>
conventions.  If this weren't true, then our language would have to
<BR>
be just as it is, necessarily.  I don't think you'd want to make that
<BR>
strong a claim.
<BR>
<P><EM>&gt; Independent of our desire to communicate, is any way of perceiving
</EM><BR>
<EM>&gt; it as good as any other? I would say it has structure: that it is
</EM><BR>
<EM>&gt; built from symbols drawn from a finite alphabet and that this is
</EM><BR>
<EM>&gt; true regardless of the perceivers goal.
</EM><BR>
<P><EM>&gt; This is where the criterion of minimum description length comes in: if
</EM><BR>
<EM>&gt; I generalize a little bit and allow pixels some fuzziness, then I can
</EM><BR>
<EM>&gt; rerepresent this message at 7 bits per symbol - which is a much
</EM><BR>
<EM>&gt; smaller encoding than a bitmap. This is a nice evaluation function
</EM><BR>
<EM>&gt; for a hypothesis because it doesn't require feedback with the outside
</EM><BR>
<EM>&gt; world. With a big enough sample I can get space savings classifying
</EM><BR>
<EM>&gt; common strings into words, and then lists into structured instances of
</EM><BR>
<EM>&gt; a grammar.
</EM><BR>
<P>Sure.  It's got a structure.  Anybody who's like us in the relevant
<BR>
way would notice that.  But you elide too much if you fail to take a
<BR>
close look at the ways in which we'd have to be similar.  You think
<BR>
that any old &quot;radical interpreter&quot; will do.  I don't see any
<BR>
motivation for believing this.
<BR>
<P>(Unless, as I suggest later, you simply refrain calling something
<BR>
&quot;intelligent&quot; unless it can find the results you want, in which case,
<BR>
you've got a hollow victory on your hands.)
<BR>
<P><EM>&gt; If we are to understand what intelligence is, we must construct a
</EM><BR>
<EM>&gt; definition which is not particular to a design process, a set of
</EM><BR>
<EM>&gt; goals, or a set of sensors and limbs. Implicit in this statement is
</EM><BR>
<EM>&gt; the notion that the word 'intelligent' actually means something, that
</EM><BR>
<EM>&gt; there's a meaningful line somewhere between intelligence and clever
</EM><BR>
<EM>&gt; algorithms which fake universality by virtue of shear vastness.
</EM><BR>
<P>I reject the notion that arguing against you would require me to
<BR>
conclude that the word &quot;intelligent&quot; is meaningless.  On the contrary,
<BR>
I argue that the word &quot;intelligent&quot; does have meaning *to us*, in the
<BR>
language *we* speak, *today* at the beginning of the 21st century.
<BR>
Your assertion requires me to believe that this word somehow has
<BR>
meaning beyond our language, beyond us.  It requires &quot;intelligence&quot; to
<BR>
be something transcendent, rather than simply meaningful.
<BR>
<P>It is in coming to terms with the fact that intelligence is not
<BR>
transcendent that AI will finally get off the ground.  We'll finally
<BR>
start coding in the stuff that we'd hoped the AI would find simply by
<BR>
virtue of it being true.  (&quot;...and, since it's preloaded with the
<BR>
general truth finding algorithm, it'll SURELY find it eventually,
<BR>
given enough computing power, time, and most of all GRANT MONEY...&quot;).
<BR>
<P><EM>&gt; Minds are imperfect and heuristic, they only approximate a truth which
</EM><BR>
<EM>&gt; is, as you point out, uncomputable. A machine might out do us, as
</EM><BR>
<EM>&gt; Newton was out done by Einstein, by finding a better model than
</EM><BR>
<EM>&gt; ours. But any intelligent machine would have a concept of, say,
</EM><BR>
<EM>&gt; integer at least as an special case of something (perhaps vastly)
</EM><BR>
<EM>&gt; broader.
</EM><BR>
<P>Actually, I think I'd largely agree with you in saying that any
<BR>
&quot;intelligent&quot; machine would have concepts like that, but not for the
<BR>
reasons you state.  Rather, this is true just because *we'd* rule out
<BR>
the possibility that something is intelligent if it wasn't
<BR>
sufficiently like our image of an intelligent being, integers and all.
<BR>
This isn't telling me anything interesting about a &quot;natural kind&quot; (tm)
<BR>
which we call &quot;intelligent&quot;, but about the sort of things which we'd
<BR>
be likely to call intelligent.
<BR>
<P>In the same respect, any human-equivalent intelligent machine would be
<BR>
able to pass the Turing Test.  And of course, you simply *must* have a
<BR>
concept of integers (or something close enough) to pass the Turing
<BR>
Test.  So any human-equivalent intelligent machine must have something
<BR>
like a concept of integers.  Doesn't have the same *oomph*, I think.
<BR>
<P><EM>&gt; Ockham's razor would be one of the core principles of the general
</EM><BR>
<EM>&gt; purpose learning system I'm interested in - hand coded rather than
</EM><BR>
<EM>&gt; acquired, though not necessarily explicitly. Something is wired in,
</EM><BR>
<EM>&gt; obviously I don't think you can just take a blank Turning machine tape
</EM><BR>
<EM>&gt; and expect it to do something useful.
</EM><BR>
<P>No, you just think that if you take a Turing machine and plug in the 
<BR>
&quot;general answer finder&quot; tape along with raw sense data, you get all
<BR>
the same beliefs about objects we do.  But the &quot;general answer finder&quot;
<BR>
isn't a simple learning algorithm.  It's just the way we go about
<BR>
solving problems: hacks, kludges and all.  It's all got to go in by
<BR>
hand; the simple learning algorithm was &quot;try it and see if you breed.&quot;
<BR>
We just don't have that kind of time on our hands.
<BR>
<P><EM>&gt; &gt; Epistemologically speaking, how would we know if we had stumbled upon
</EM><BR>
<EM>&gt; &gt; the general algorithm, or whether we were just pursuing our own
</EM><BR>
<EM>&gt; &gt; purposes again?  For that matter, why would we care?  Why not call our
</EM><BR>
<EM>&gt; &gt; own beliefs Right out of elegance and get on with Coding a Transhuman
</EM><BR>
<EM>&gt; &gt; AI?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; We couldn't know, but if we got good results then we'd be pretty sure
</EM><BR>
<EM>&gt; we were at least close. Whether you care depends on your motivation:
</EM><BR>
<EM>&gt; I'm interested in intelligence because I like general solutions to
</EM><BR>
<EM>&gt; problems more than I like special case ones, and you don't get a more
</EM><BR>
<EM>&gt; general solution than AI. I futz around with this stuff out of
</EM><BR>
<EM>&gt; intellectual curiosity, if the mind turns out to be necessarily ugly
</EM><BR>
<EM>&gt; I'll go do something else. I don't really care about saving the world 
</EM><BR>
<EM>&gt; from grey goo or the future of the human race or whatever.
</EM><BR>
<P>That's a little cavalier, considering that you're one of us, isn't it?
<BR>
;) Sure, sure, let the rest of us do the HARD work... ;)
<BR>
<P>Anyway.  I think you're forgetting that this is the general truth
<BR>
finding algorithm which WE supposedly use in finding truth.  So how
<BR>
would we know if we'd found it?  We'd &quot;check&quot; to see if our results
<BR>
were good, and if so, we're close, you say.  But how would we &quot;check&quot;
<BR>
on this?  Well, we'd run our truth finding algorithm again, of course,
<BR>
since that's all we've got in the search for truth.  According to it,
<BR>
our results are &quot;good.&quot;  Have we found the general truth finding
<BR>
algorithm?  Well, the general truth finding algorithm seems to say so!
<BR>
<P>Our truth finding algorithm is all we've got.  It's not general or
<BR>
universal, but it is mostly right.  That's good enough for me... for
<BR>
my purposes.
<BR>
<P>-Dan
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-unless you love someone-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;-nothing else makes any sense-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e.e. cummings
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="3510.html">KPJ: "Re: law enforcement for profit"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3508.html">Spike Jones: "Re: a new thread"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3465.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3568.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="3568.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3509">[ date ]</A>
<A HREF="index.html#3509">[ thread ]</A>
<A HREF="subject.html#3509">[ subject ]</A>
<A HREF="author.html#3509">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:11:29 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
