<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: A Spring-Powered Theory of Consciousness (6 of 6)</TITLE>
<META NAME="Author" CONTENT="Jim Fehlinger (fehlinger@home.com)">
<META NAME="Subject" CONTENT="A Spring-Powered Theory of Consciousness (6 of 6)">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>A Spring-Powered Theory of Consciousness (6 of 6)</H1>
<!-- received="Mon Jun 19 22:19:55 2000" -->
<!-- isoreceived="20000620041955" -->
<!-- sent="Tue, 20 Jun 2000 00:00:33 -0400" -->
<!-- isosent="20000620040033" -->
<!-- name="Jim Fehlinger" -->
<!-- email="fehlinger@home.com" -->
<!-- subject="A Spring-Powered Theory of Consciousness (6 of 6)" -->
<!-- id="394EEC61.401BEBC7@home.com" -->
<STRONG>From:</STRONG> Jim Fehlinger (<A HREF="mailto:fehlinger@home.com?Subject=Re:%20A%20Spring-Powered%20Theory%20of%20Consciousness%20(6%20of%206)&In-Reply-To=&lt;394EEC61.401BEBC7@home.com&gt;"><EM>fehlinger@home.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon Jun 19 2000 - 22:00:33 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5577.html">John  M Grigg: "Re: lol"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5575.html">Jim Fehlinger: "A Spring-Powered Theory of Consciousness (2 of 6)"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5586.html">Eliezer S. Yudkowsky: "Re: A Spring-Powered Theory of Consciousness (6 of 6)"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5586.html">Eliezer S. Yudkowsky: "Re: A Spring-Powered Theory of Consciousness (6 of 6)"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5576">[ date ]</A>
<A HREF="index.html#5576">[ thread ]</A>
<A HREF="subject.html#5576">[ subject ]</A>
<A HREF="author.html#5576">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Returning to CaTAI: &quot;AI has an embarassing tendency to predict
<BR>
success where none materializes,... and to assert that some
<BR>
simpleminded pattern of suggestively-named LISP tokens completely
<BR>
explains some incredibly high-level thought process...  There are
<BR>
several ways to avoid making this class of mistake...  One is an
<BR>
intuition of causal analysis... that says 'This cause does not
<BR>
have sufficient complexity to explain this effect.'  One is to be
<BR>
instinctively wary of trying to implement cognition directly on
<BR>
the token level&quot;.  &quot;Any form of cognition which can be
<BR>
mathematically formalized, or which has a provably correct
<BR>
implementation, is too simple to contribute materially to
<BR>
intelligence&quot;.  &quot;Clearly [there] is vastly more mental material,
<BR>
more cognitive &quot;stuff&quot;, than classical-AI propositional logic
<BR>
involves&quot;.  And, &quot;special-purpose low-level code that directly
<BR>
implements a high-level case is usually a Bad Thing&quot;.  On the
<BR>
other hand, the title of Eliezer's article has an awfully
<BR>
top-down ring due to the word &quot;coding&quot;, and his notion of a
<BR>
&quot;codic cortex&quot; or &quot;codic sensory modality&quot; seems to swamp the
<BR>
usual notion of a sensory modality.  I wonder how, using
<BR>
Edelman's schema, one would select &quot;values&quot; for a codic sensory
<BR>
modality without instantly facing all the difficulties of
<BR>
traditional AI, and in an even more concentrated form than usual
<BR>
(we don't want a robot that can just sweep the floor, we want a
<BR>
robot than can write a computer program for a newer robot that
<BR>
can program better than it does!).  It seems like a bit of a leap
<BR>
from &quot;Blue is bad, red is good&quot; to &quot;COBOL is bad, Java is good&quot;
<BR>
;-&gt;.
<BR>
<P>Edelman has emphasized the intimate dependence of the
<BR>
selectionist shaping of the brain on the richly-varied
<BR>
repertoires made possible by its morphology and biochemistry, and
<BR>
has warned that these cannot be easily brushed aside by those
<BR>
looking to build artificial brain-like devices.  The economies of
<BR>
mapping of function to form achieved by evolution taking
<BR>
advantage of the fine-grained variation available to biological
<BR>
systems may have to be sacrificed when simulating such a
<BR>
selectionist system on a computational substrate with a simpler
<BR>
and more regular structure, but the feasibility of this project
<BR>
will, once again, depend on the empirical determination of the
<BR>
computational penalty for doing so, and on the economics of the
<BR>
available hardware.  In CaTAI, Yudkowsky writes: &quot;The
<BR>
nanotechnology described in Nanosystems, which is basically the
<BR>
nanotechnological equivalent of a vacuum tube - acoustic
<BR>
computing, diamondoid rod logics (4) - describes a one-kilogram
<BR>
computer, running on 100 kW of power, which performs 10^21
<BR>
ops/sec using 10^12 CPUs running at 10^9 ops/sec.  The human
<BR>
brain is composed of approximately 100 billion neurons and 100
<BR>
trillion synapses, firing 200 times per second, for approximately
<BR>
10^17 ops/sec total&quot;.  If hardware such as this becomes
<BR>
available, it might be economically feasible to throw away a
<BR>
factor of 10,000 in computational capacity just to simulate the
<BR>
morphological granularity of the brain on a simpler substrate --
<BR>
what might be called the selectional penalty.
<BR>
<P>In the meanwhile, and depending on the relative rate of
<BR>
advancement of biotechnology compared to that of other fields,
<BR>
it's conceivable that the first conscious artifacts may, in fact,
<BR>
be based on biological tissue, perhaps genetically engineered to
<BR>
suit the purpose.  Having proto-neurons that can both
<BR>
self-replicate and follow gradients of Cell Adhesion Molecules
<BR>
and Substrate Adhesion Molecules to wire themselves together, as
<BR>
a living nervous system does during embryogenesis, solves the
<BR>
sort of manufacturing difficulties alluded to by Gordon Moore in
<BR>
the June 19, 2000 special issue of _Time_ magazine entitled &quot;The
<BR>
Future of Technology&quot;: &quot;I'm a bit of a skeptic on molecular
<BR>
chips.  Maybe I'm getting old.  It's hard for me to see how those
<BR>
billions of transistors can be interconnected at that level&quot;
<BR>
(p. 99).  Squishy, biologically-based AIs seem to be getting more
<BR>
common in contemporary science fiction, as, for example, with the
<BR>
bacterial AI &quot;Roddy&quot; in Greg Bear's _Slant_ (Tor Books, 1997, see
<BR>
p. 476), or with the current televison series _Star Trek:
<BR>
Voyager_'s reference to &quot;bio-neural gel packs&quot; in a couple of
<BR>
episodes (see _The Star Trek Encylopedia_ [expanded edition], by
<BR>
Denise and Michael Okuda, Simon &amp; Schuster, 1999, p. 45).  People
<BR>
may be more squeamish about using squishy AI -- imagine a smart
<BR>
car with a synthetic brain in a tank perfused by nutrients and
<BR>
oxygen.  Imagine leaving that car parked at the mall on a hot
<BR>
summer day and coming back to find that the the power went off
<BR>
(due to a faulty fuel cell, perhaps), leading to the failure of
<BR>
the squishy AI's artificial heart/lung/dialysis machine and its
<BR>
air-conditioner.  He's dead, Jim.  Smells like it, too.  What
<BR>
would you tell the kids?  People on this list do not fantasize
<BR>
about being &quot;uploaded&quot; into giant squishy, gurgling, pulsating
<BR>
biological brains; diamondoid processors are a much more
<BR>
appealing idea.  We want to ditch these squishy bodies, not be
<BR>
transferred into even ickier ones.
<BR>
<P>I chose the title of this article partly in hopes that its sheer
<BR>
lameness would attract attention, but I also intended it as a
<BR>
humorous allusion to an illustration on p. 172 of UoC
<BR>
(Fig. 13.3): a visual metaphor for the authors' dynamic core
<BR>
hypothesis.  In this illustration (which I like a lot), the
<BR>
dynamic core of consciousness is represented by a tangle of
<BR>
tightly-wound springs under tension, which represents the
<BR>
capacity of the reentrantly-connected neuronal groups of the core
<BR>
to rapidly propagate any perturbation of one group to the entire
<BR>
core.  The functionally-insulated parallel loops of the cortical
<BR>
appendages and organs of succession (connected to the core by
<BR>
what are called &quot;ports in&quot; and &quot;ports out&quot; in UoC [pp. 178-186])
<BR>
are represented in the same picture by loose springs which can
<BR>
propagate travelling waves in one direction only.  I suppose the
<BR>
title could also be interpreted as alluding to Edelman's
<BR>
contention that only artifacts undergoing selection constrained
<BR>
by value, characterized by stochastic variation interacting with
<BR>
a stochastic world, can have a mainspring for self-organizing
<BR>
behavior that would make them anything more than external
<BR>
appendages of human brains.  Edelman and Tononi give another
<BR>
visual metaphor for the dynamic core hypothesis in UoC on p. 145
<BR>
(Fig. 12.1): an astronomical photo of M83, a spiral galaxy in
<BR>
Hydra, with the caption &quot;No visual metaphor can capture the
<BR>
properties of the dynamic core, and a galaxy with complicated,
<BR>
fuzzy borders may be as good or as bad as any other&quot;.  Child of
<BR>
60's television that I am, I was presented by memory with a
<BR>
different visual metaphor -- the vacuum-cleaner monster from an
<BR>
episode (&quot;It Crawled Out of the Woodwork&quot;) of the original _The
<BR>
Outer Limits_ television SF anthology series (reviewed at
<BR>
Amazon.com:
<BR>
<A HREF="http://www.video-department.com/video/0/28/7214.html">http://www.video-department.com/video/0/28/7214.html</A>).
<BR>
<P>Cheers.
<BR>
<P>Jim F.
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5577.html">John  M Grigg: "Re: lol"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5575.html">Jim Fehlinger: "A Spring-Powered Theory of Consciousness (2 of 6)"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5586.html">Eliezer S. Yudkowsky: "Re: A Spring-Powered Theory of Consciousness (6 of 6)"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5586.html">Eliezer S. Yudkowsky: "Re: A Spring-Powered Theory of Consciousness (6 of 6)"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5576">[ date ]</A>
<A HREF="index.html#5576">[ thread ]</A>
<A HREF="subject.html#5576">[ subject ]</A>
<A HREF="author.html#5576">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:13:51 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
