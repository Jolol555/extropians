<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: A Spring-Powered Theory of Consciousness (5 of 6)</TITLE>
<META NAME="Author" CONTENT="Jim Fehlinger (fehlinger@home.com)">
<META NAME="Subject" CONTENT="A Spring-Powered Theory of Consciousness (5 of 6)">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>A Spring-Powered Theory of Consciousness (5 of 6)</H1>
<!-- received="Mon Jun 19 22:18:42 2000" -->
<!-- isoreceived="20000620041842" -->
<!-- sent="Mon, 19 Jun 2000 23:59:16 -0400" -->
<!-- isosent="20000620035916" -->
<!-- name="Jim Fehlinger" -->
<!-- email="fehlinger@home.com" -->
<!-- subject="A Spring-Powered Theory of Consciousness (5 of 6)" -->
<!-- id="394EEC14.48FFEEA3@home.com" -->
<STRONG>From:</STRONG> Jim Fehlinger (<A HREF="mailto:fehlinger@home.com?Subject=Re:%20A%20Spring-Powered%20Theory%20of%20Consciousness%20(5%20of%206)&In-Reply-To=&lt;394EEC14.48FFEEA3@home.com&gt;"><EM>fehlinger@home.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon Jun 19 2000 - 21:59:16 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5580.html">Jim Fehlinger: "A Spring-Powered Theory of Consciousness (3 of 6)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5578.html">Jim Fehlinger: "A Spring-Powered Theory of Consciousness (4 of 6)"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5579">[ date ]</A>
<A HREF="index.html#5579">[ thread ]</A>
<A HREF="subject.html#5579">[ subject ]</A>
<A HREF="author.html#5579">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
After writing the above two paragraphs, and re-reading the
<BR>
relevant passages in the book, I realized that although I seemed
<BR>
to understand these comments when I first read them, I am no
<BR>
longer sure that I grasp the point the authors were trying to
<BR>
make here.  Choosing one out of &quot;billions&quot; of alternatives in
<BR>
fractions of a second can be accomplished with as few as 30-odd
<BR>
bits per second with a preassigned code (which is clearly **not**
<BR>
what the authors mean), but Edelman and Tononi do not specify
<BR>
precisely the quantity that must be &quot;better than any present-day
<BR>
engineer could dream of&quot;.  There is a reference in the end notes
<BR>
to the book _The User Illusion: Cutting Consciousness Down To
<BR>
Size_ by Tor Norretranders (Viking, New York, 1998); Chapter 6 of
<BR>
this book is called &quot;The Bandwidth of Consciousness&quot;, and there
<BR>
is a diagram on p. 145, taken from the 1971 work of Karl
<BR>
Kupfmuller, giving the information flow from the senses through
<BR>
the brain and consciousness to the motor apparatus.  This diagram
<BR>
gives 11 million bits/sec coming in through the senses, 10
<BR>
billion bits/sec as the throughput of the brain (conservatively
<BR>
based on an estimate of 10 billion neurons each processing 1
<BR>
bit/sec), and 20 bits/sec as the throughput of consciousness.
<BR>
Edelman and Tononi seem to be saying that the bandwidth of
<BR>
consciousness should be identified with the figure Kupfmuller
<BR>
gives as the bandwidth of the brain (or at least with that
<BR>
portion of it participating in the dynamic core), not limited to
<BR>
the bit rate associated with the &quot;chunking&quot; capacity.
<BR>
<P>Development and early experience may be thought of as adding
<BR>
dimensions to the dynamic core.  &quot;[I]t is likely that among the
<BR>
earliest conscious dimensions and discriminations are those
<BR>
concerned with the body itself -- mediated through structures in
<BR>
the brain stem that map the state of the body and its relation to
<BR>
both the inside and outside environment...  Equally early and
<BR>
central are the dimensions provided by value systems indicating
<BR>
salience for the entire organism...  [T]his early, bodily based
<BR>
consciousness may provide the initial dominant axes of the
<BR>
N-dimensionsal neural reference space...  [A]s increasing
<BR>
numbers...  of signals [from the world ('nonself')] are
<BR>
assimilated, they would be discriminated according to modality
<BR>
and category in reference to these initial dimensions that
<BR>
constitute the protoself...  With the accession of new dimensions
<BR>
related to language and their integration in the dynamic
<BR>
core... higher-order consciousness appears in humans...  A
<BR>
discriminable and nameable self, developed through social
<BR>
interactions, can now be connected to the simultaneous experience
<BR>
of the scenes of primary consciousness and conceptually-based
<BR>
imagery in which experiences of all kinds are linked...  Thus,
<BR>
one can see development and experience as a progressive increase
<BR>
in the complexity of the dynamic core, both in terms of the
<BR>
number of available dimensions and the number of points in the
<BR>
corresponding N-dimensional space that can be differentiated&quot;
<BR>
(UoC pp. 174-175).
<BR>
<P>As I admitted at the beginning of this article, I do not have the
<BR>
expertise to offer serious scientific criticism of Edelman's
<BR>
books, but it appears that they have not been well-received among
<BR>
some scholars in neuroscience, cognitive science, and philosophy.
<BR>
John Horgan serves up some of this negative reaction as gossip in
<BR>
_The End of Science_ (p. 171): &quot;Francis Crick spoke for many of
<BR>
his fellow neuroscientists when he accused Edelman of hiding
<BR>
'presentable' but not terribly original ideas behind a 'smoke
<BR>
screen of jargon.'  Edelman's Darwinian terminology, Crick added,
<BR>
has less to do with any real analogies to Darwinian evolution
<BR>
than with rhetorical grandiosity...  The philosopher Daniel
<BR>
Dennett of Tufts University remained unimpressed after visiting
<BR>
Edelman's laboratory.  In a review of Edelman's _Bright Air,
<BR>
Brilliant Fire_, Dennett argued that Edelman had merely presented
<BR>
rather crude versions of old ideas.  Edelman's denials
<BR>
notwithstanding, his model **was** a neural network, and reentry
<BR>
**was** feedback, according to Dennett.  Edelman also
<BR>
'misunderstands the philosophical issues he addresses at an
<BR>
elementary level,' Dennett asserted.  Edelman may profess scorn
<BR>
for those who think that the brain is a computer, but his use of
<BR>
a robot to 'prove' his theory shows that he holds the same
<BR>
belief, Dennett explained&quot;.
<BR>
<P>Edelman himself states in _Bright Air, Brilliant Fire_ that &quot;The
<BR>
two concepts of the theory that have come under the most intense
<BR>
attack are those of neuronal groups and of selection itself.
<BR>
Horace Barlow and, separately, Francis Crick have attacked the
<BR>
notion of the existence of groups...  Crick's claim is that
<BR>
neuronal groups have little evidence to support them.  He also
<BR>
asserts that neuronal group selection is not necessary to support
<BR>
ideas of global mapping.  Finally, he claims that he has not
<BR>
found it possible to make a worthwhile comparison between the
<BR>
theory of natural selection and what happens in the developing
<BR>
brain&quot; (BABF pp. 94-95).  Edelman goes on to answer these
<BR>
criticisms on pp. 95 - 98.
<BR>
<P>This layman finds quite clear and persuasive Edelman's
<BR>
description of how sensory maps form reentrantly-connected
<BR>
classification couples which facilitate perceptual categorization
<BR>
through correlated activity -- although Crick points out (in _The
<BR>
Astonishing Hypothesis_, Simon &amp; Schuster, London, 1994) that
<BR>
Edelman did not originate this idea: &quot;In 1974, the psychologist
<BR>
Peter Milner published a very perceptive paper&quot; in which he
<BR>
&quot;proposed the idea of correlated firing to solve the binding
<BR>
problem&quot;.  In the same 1974 paper, Crick says, Milner anticipated
<BR>
Edelman's discussion of recursive synthesis: &quot;[Milner] argued
<BR>
that... early cortical areas would have to be involved in visual
<BR>
awareness as well as the higher cortical areas.  He suggested
<BR>
that this could be implemented by a mechanism involving the
<BR>
numerous backprojections from neurons higher in the visual
<BR>
hierarchy to those lower down&quot; (_The Astonishing Hypothesis_,
<BR>
p. 232 [and footnote]).  This is in a context in which Crick
<BR>
warns against premature reliance on reentry as an explanatory
<BR>
tool: &quot;Another possible tack is to ask whether awareness
<BR>
involves, in some sense, the brain talking to itself.  In neural
<BR>
terms this might imply that reentrant pathways -- that is, one
<BR>
that, after one or more steps, arrives back at the starting
<BR>
point, are essential, as Gerald Edelman has suggested.  The
<BR>
problem here is that it is difficult to find a pathway that is
<BR>
not reentrant...  [W]e must use the reentry criterion with care&quot;
<BR>
(Ibid., p. 234).
<BR>
<P>Daniel Dennett sounds a similar warning about Edelman's use of
<BR>
reentry as an explanatory device (in _Consciousness Explained_,
<BR>
p. 268): &quot;It is not just philosophers' theories that need to be
<BR>
made honest by modeling at this level [the level of AI models
<BR>
such as John Anderson's (1983) ACT* or Rosenbloom, Laird, and
<BR>
Newell's (1987) Soar]; neuroscientists' theories are in the same
<BR>
boat.  For instance, Gerald Edelman's (1989) elaborate theory of
<BR>
're-entrant' circuits in the brain makes many claims about how
<BR>
such re-entrants can accomplish the discriminations, build the
<BR>
memory structures, co-ordinate the sequential steps of problem
<BR>
solving, and in general execute the activities of a human mind,
<BR>
but in spite of a wealth of neuroanatomical detail, and
<BR>
enthusiastic and often plausible assertions from Edelman, we
<BR>
won't know what his re-entrants can do -- we won't know that
<BR>
re-entrants are the right way to conceive of the functional
<BR>
neuroanatomy -- until they are fashioned into a whole cognitive
<BR>
architecture at the grain-level of ACT* or Soar and put through
<BR>
their paces&quot;.  A footnote continues: &quot;Edelman (1989) is one
<BR>
theorist who has tried to put it all together, from the details
<BR>
of neuroanatomy to cognitive psychology to computational models
<BR>
to the most abstruse philosophical controversies.  The result is
<BR>
an instructive failure.  It shows in great detail how many
<BR>
different sorts of question must be answered, before we can claim
<BR>
to have secured a complete theory of consciousness, but it also
<BR>
shows that no one theorist can appreciate all the subtleties of
<BR>
the problems addressed by the different fields.  Edelman has
<BR>
misconstrued, and then simply dismissed, the work of many of his
<BR>
potential allies, so he has isolated his theory from the sort of
<BR>
sympathetic and informed attention it needs if it is to be saved
<BR>
from its errors and shortcomings&quot;.
<BR>
<P>Ascending further in Edelman's infrastructural hierarchy, I found
<BR>
the discussions of global mappings, which wed motor activity to
<BR>
sensory classification n-tuples as an inseparable component of
<BR>
perceptual categorization, and which involve the operations of
<BR>
&quot;cortical appendages&quot; such as the cerebellum and basal ganglia,
<BR>
somewhat vaguer and harder to grasp.  They are also highly
<BR>
speculative, as Edelman admits in the Preface to _The Remembered
<BR>
Present_: &quot;The description of cortical appendages...  contains
<BR>
specific models of cerebellar, hippocampal, and basal ganglion
<BR>
function.  In formulating them, I have risked being accused of
<BR>
indulging in speculative neurology.  I have nonetheless attempted
<BR>
to keep the functional aspects of these models well within the
<BR>
known properties of these brain structures.  The models, risky
<BR>
though they may be, are intended to reinforce the view that the
<BR>
cortical appendages are temporal organs necessary for the
<BR>
eventual emergence of rich conscious activity&quot; (RP pp. xix-xx).
<BR>
Finally, it seems to me that there is an element of the deus ex
<BR>
machina in the notion of conceptual categorization, in which the
<BR>
brain is supposedly linking correlated maps of its own activity.
<BR>
Still, Edelman manages to weave all this into a plausible story
<BR>
leading straight up the mountainside to the summit of primary
<BR>
consciousness itself (and even further, to what he calls
<BR>
higher-order consciousness), and since his avowed purpose is to
<BR>
demonstrate the feasibility, at this point in history, of
<BR>
constructing a first draft of such an ambitious but
<BR>
scientifically plausible story, this reader was able to tolerate
<BR>
the threadbare patches.  &quot;Above all, the present essay is
<BR>
intended to provoke thought by scientists about a subject
<BR>
considered in most scientific circles to be beyond scientific
<BR>
reach.  To show the feasibility of constructing a brain-based
<BR>
theory of consciousness is perhaps all that can be reasonably
<BR>
expected at this stage of knowledge.  On that basis, others may
<BR>
build more firmly when the necessary facts and methods become
<BR>
available&quot; (RP p. xx).
<BR>
<P>As suggested by some of Dennett's remarks quoted above, Edelman
<BR>
seems perfectly poised, straddling as he does the fields of
<BR>
neurobiology, psychology, philosophy, and computer science, to
<BR>
maximally irritate his colleagues with his claims to have gone
<BR>
farther than any prior theorist in building the scaffolding for a
<BR>
complete theory of consciousness (and with the sort of immodesty
<BR>
illustrated by the gossipy John Horgan [_The End of Science_,
<BR>
p. 167], who quotes Edelman making remarks such as &quot;I'm
<BR>
**astonished** that people don't sit and put these things
<BR>
together&quot;).  It seems likely that only an eminence grise with a
<BR>
big ego and a Nobel already under his belt would be in a position
<BR>
to risk career and reputation by embarking on work spanning
<BR>
fields in some of which he would be considered an arrogant and
<BR>
blundering outsider.  Another possible factor in negative
<BR>
reactions to a selectionist theory of the brain such as Edelman's
<BR>
(though not necessarily on the part of those scholars I have
<BR>
specifically mentioned) may have to do with the antipathy and
<BR>
anger that any invocation of the name of Darwin still stirs up,
<BR>
almost a century and a half after the publication of _The Origin
<BR>
of Species_, in some intellectual circles.  For example, the
<BR>
encroachment of Darwinian theory into the social sciences of
<BR>
psychology and sociology (the newer Darwinian annexes of these
<BR>
fields were once known as &quot;biosocial approaches&quot; or
<BR>
&quot;sociobiology&quot;, but I see that the term &quot;evolutionary psychology&quot;
<BR>
is more common today), the implications of which were at first
<BR>
largely ignored by mainstream social scientists, began to provoke
<BR>
a strong intellectual counterreaction with the publication in
<BR>
1975 of Edward O.  Wilson's _Sociobiology: The New Synthesis_.
<BR>
The complex politics of this counterreaction (spearheaded,
<BR>
ironically enough, by some of Wilson's fellow biologists, but the
<BR>
alarm thus sounded certainly rallied the defenses of fields in
<BR>
the humanities which were &quot;under attack&quot;) are recounted in
<BR>
intricate detail in the recently-published _Defenders of the
<BR>
Truth: The Battle For Science in the Sociobiology Debate and
<BR>
Beyond_ by Ullica Segerstrale (Oxford University Press, 2000).
<BR>
<P>Of likeliest interest to those on this list are the discussions
<BR>
in Edelman's books concerning constraints on the design of
<BR>
conscious artifacts and the role of digital computers both in
<BR>
theories about the brain and in the future construction of
<BR>
brain-like devices.  When electronic digital computers came into
<BR>
existence after World War II, they were like no kind of machine
<BR>
that had ever been seen before.  The spectacle of a huge array of
<BR>
electronic components connected to a teletypewriter, accepting
<BR>
and parsing commands typed into the keyboard and responding by
<BR>
typing back human-readable strings of symbols might easily
<BR>
suggest, to a thoughtful person, the sort of experiment which,
<BR>
when described by Alan Turing, became a basis for speculations
<BR>
about whether such symbol manipulation could be developed into
<BR>
the basis for an intelligent conversation between a human and an
<BR>
electronic device connected to a typewriter.  Douglas
<BR>
R. Hofstadter (in _Le Ton beau de Marot_, Basic Books, New York,
<BR>
1997, p. 88) has described the impact that words issuing from a
<BR>
machine can have on a naive person as the &quot;Eliza effect&quot;: the
<BR>
idea that &quot;lay people have a strong tendency -- indeed, a great
<BR>
willingness -- to attribute to words produced by a computer just
<BR>
as much meaning as if they had come from a human being.  The
<BR>
reason for this weakness is clear enough: Most people experience
<BR>
language only with other humans, and in that case, there is no
<BR>
reason to doubt the depth of its rootedness in experience.
<BR>
Although we all can chit-chat fairly smoothly while running on a
<BR>
mere half a cylinder, and often do at cocktail parties, the
<BR>
syntactically correct use of words absolutely drained of every
<BR>
last drop of meaning is something truly alien to us&quot;.  Given that
<BR>
in the early days of digital computers almost everyone was more
<BR>
or less naive in this regard, even those most intimate with their
<BR>
construction and operation, the fact that the newely-invented
<BR>
digital computers almost immediately suggested themselves as a
<BR>
basis for &quot;artificial intelligence&quot; is hardly surprising.
<BR>
<P>To the mathematicians such as John von Neumann who worked out the
<BR>
instruction sets and devised the earliest programming languages
<BR>
for these machines, there must have been an added appeal.  Up to
<BR>
that time, any large-scale computational effort, such as the
<BR>
compilation of tables of functions (the need for which funded
<BR>
Charles Babbage's dreams of a mechanical digital computer in the
<BR>
19th century), or the calculations needed by the Manhattan
<BR>
Project, which had physicist Richard Feynman coordinating and
<BR>
cross-checking teams of pink-collar workers operating adding
<BR>
machines, or even the detailed exposition and checking of an
<BR>
intricate proof in mathematical logic, such as those contained in
<BR>
Russell and Whitehead's _Principia Mathematica_, was a laborious,
<BR>
numbingly tedious, and error-prone undertaking.  The heights of
<BR>
mathematical and logical reasoning were also restricted to an
<BR>
elite who, like those initiated into the secrets of literacy
<BR>
itself in an earlier age, were considered to be possessed of
<BR>
intellectual powers above the common herd of humanity, fluent in
<BR>
a realm of discourse constituting one of the pinnacles of
<BR>
achievement of the human mind.  Of course, the comprehension and
<BR>
invention of mathematical knowledge isn't the same as the labor
<BR>
involved in making use of it, but given that those capable of the
<BR>
former also often had to be the ones to perform, or at least
<BR>
organize and supervise, the latter, the anticipation that a
<BR>
digital computer would be able to perform such tasks, so
<BR>
difficult for human beings, with astonishing speed and freedom
<BR>
from errors, must have caused the breath to catch in the throat
<BR>
of many slide-rule-toting workers in physics and the more
<BR>
intensely mathematical sciences.  The computational prowess of
<BR>
digital computers also probably engendered (as the Eliza effect
<BR>
did with ordinary language) dreams of being able to develop these
<BR>
machines, evidently already capable of astounding feats of
<BR>
mathematical computation, into devices capable of mathematical
<BR>
reasoning and invention.
<BR>
<P>There was a counter-trend in the nascent field of robotics during
<BR>
the 1950's and 1960's, led in its final days by Frank Rosenblatt,
<BR>
composed of researchers who were more interested in exploring the
<BR>
capabilities of networks of analog electronics than in the new
<BR>
digital symbol manipulators.  This led to a battle between the
<BR>
analog and digital approaches to AI for students and funding,
<BR>
made all the more urgent by the fact that the new digital toys
<BR>
were **expensive**, which culminated at the end of the 1960s with
<BR>
the triumph of the digital, symbol-based pursuit of AI and the
<BR>
abandonment of &quot;connectionist&quot; approaches to AI for the next
<BR>
decade (see, for example, _The Brain Makers_, H. P. Newquist,
<BR>
Prentice Hall, 1994, pp. 71-75).  However, already by this time
<BR>
at least one AI researcher, from the heart of the symbolic AI
<BR>
faction at MIT, had come to be dismayed by how easily people
<BR>
could be led to mistake parlor tricks of syntatic manipulation
<BR>
for something more genuinely intelligent (see Hofstadter, _Le Ton
<BR>
beau de Marot_, pp. 87-89).  This dismay led Joseph Weizenbaum to
<BR>
create his famous demonstration program Eliza, and later to write
<BR>
a book about it and his disillusionment with the field of AI, as
<BR>
it was then practiced (_Computer Power and Human Reason_,
<BR>
W. H. Freeman, San Francisco, 1976).
<BR>
<P>It seems to me, though, that suspicion of hyperbole in discourse
<BR>
about the prospects for intelligent digital computers was quite
<BR>
widespread throughout the 1960s, even in popular culture.  One
<BR>
need only recall the repeated instances, in the original _Star
<BR>
Trek_ television series, of Captain Kirk being able to use simple
<BR>
logical paradoxes to disable an otherwise invincible intelligent
<BR>
computer or android by trapping its CPU in a loop, or to talk the
<BR>
machine into turning itself off or blowing itself up, as a
<BR>
demonstration that a computer is brittle, or lacks depth, or that
<BR>
its seeming intelligence is &quot;fake&quot; in some way that a human's is
<BR>
not. (All while Mr. Spock, the show's master logician, looked on
<BR>
in awe.  Well, it was also implausible that Kirk could beat Spock
<BR>
at chess -- no doubt Kirk's intellectual superiority was
<BR>
stipulated in Shatner's contract -- that's what it means to be
<BR>
the leading man!).  It's easy to dismiss this negative attitude
<BR>
as fear of losing one's job to automation, as fear of
<BR>
relinquishing the uniqueness of humanity, or as resistance to
<BR>
scientific progress in the tradition of clinging to
<BR>
heliocentrism, spontaneous generation of microbes, vitalism, or
<BR>
special creation of species, but the continued insistence by
<BR>
philosophers such as John Searle that semantics cannot be
<BR>
generated from syntax alone suggests some deeper point that's
<BR>
struggling to be made.  Whatever it is, it's hard to articulate
<BR>
-- Searle's own &quot;Chinese Room&quot; illustration simply begs the
<BR>
question of whether the implementation of an algorithm could
<BR>
generate understanding of a human language.  Imagining an
<BR>
algorithm for understanding Chinese being carried out by an
<BR>
English-speaking human with no personal knowledge of Chinese who
<BR>
interprets a set of rules written in English is a clever literary
<BR>
device, but it makes the idea of machine intelligence no less
<BR>
plausible (as a thought experiment, not a practical endeavor)
<BR>
than the notion that such an algorithm could be implemented on a
<BR>
digital computer.
<BR>
<P>The belief that the human capacity for speech is a divine gift
<BR>
may still be widespread, but among scientists it is generally
<BR>
accepted that language is a capacity that evolved along with
<BR>
human beings, and as such it exists because of its practical
<BR>
value in human life.  Its utterance is a useful behavior
<BR>
exhibited by embodied beings in intimate interaction with the
<BR>
world and with their neighbors, and it is apprehended by fellow
<BR>
beings with a history of equally rich and extensive contact with
<BR>
the world.  The details of the matching among speech, speakers,
<BR>
and the world blur and shift with variations in time, place, and
<BR>
context.  The notion that a body of such utterances or their
<BR>
symbolic representations, together with a set of rules for
<BR>
transforming them into each other and into new utterances,
<BR>
independently encodes a set of &quot;facts&quot; or &quot;truths&quot; about the
<BR>
world when divorced from the community of speakers, mired in the
<BR>
contingent messiness of the world, who originally anchored those
<BR>
utterances to reality, is an idea that doesn't seem to have much
<BR>
credence among philosophers these days (at least as far as this
<BR>
layman can tell).  It is somewhat analogous to the hopes of
<BR>
turn-of-the-century mathematical logicians that the truth of
<BR>
mathematics could be put on a firm foundation by deducing all of
<BR>
mathematics from a few logical principles also safely out of
<BR>
reach of the fuzzy contingencies of the world.  Now that digital
<BR>
computers are cheap and widespread, no longer rare objects of
<BR>
veneration, and now that the Eliza effect has had the opportunity
<BR>
to wear thin, it is probably also the case that most computer
<BR>
programmers, at least, would no longer ascribe intelligence to a
<BR>
disembodied digital computer performing logical operations on a
<BR>
body of symbolically-encoded facts about the world (such as
<BR>
Douglas Lenat's Cyc).  Such a system may have its uses, as a
<BR>
front end for a database perhaps, but only a marketing department
<BR>
would call it artifical intelligence.
<BR>
<P>Edelman is the first author I have encountered who seems (on
<BR>
initial reading, at least) to be clearly and sensibly
<BR>
articulating the unease that Searle and others apparently feel
<BR>
about the notion of intelligent computers.  Edelman depicts the
<BR>
contrast between brains and digital computers in a way that, for
<BR>
this reader at least, does not cause the impatience with which I
<BR>
respond to Searle's Chinese Room argument.  To Edelman, the
<BR>
crucial distinction is between the conscious brain as a
<BR>
selectional system constrained by value, from which
<BR>
context-dependent categories with fuzzy boundaries spontaneously
<BR>
emerge, and the digital computer as an instructional system in
<BR>
which categories with predefined boundaries are explicitly
<BR>
programmed.  Edelman points to the intersection of two streams of
<BR>
unpredictable variation (the variation in the repertoires of the
<BR>
brain, and the variation in the signals from the world) as the
<BR>
seed from which categorization of the unlabelled world and
<BR>
consciousness (and later, intelligence) can emerge -- the kernel
<BR>
of the &quot;soul&quot;, if you will.  Edelman also treats the body, with
<BR>
its linked sensory and motor activity, as an inseparable
<BR>
component of the perceptual categorization underlying
<BR>
consciousness.  Edelman claims affinity (in BABF, p. 229) between
<BR>
his views on these issues and those of a number of scholars (a
<BR>
minority, says Edelman, which he calls the Realists Club) in the
<BR>
fields of cognitive psychology, linguistics, philosophy, and
<BR>
neuroscience; including John Searle, Hilary Putnam, Ruth Garret
<BR>
Millikan, George Lakoff, Ronald Langacker, Alan Gould, Benny
<BR>
Shanon, Claes von Hofsten, and Jerome Bruner (I do not know if
<BR>
the scholars thus named would acknowledge this claimed affinity).
<BR>
<P>On further reflection, the distinction Edelman is attempting to
<BR>
draw between selectionism and instructionism, at least as applied
<BR>
to the simulation of selectionist system on a computer, seems
<BR>
less clear-cut.  In the case of a living organism, in which the
<BR>
body, nervous system, and sensory and motor capabilities are are
<BR>
all products of natural selection in an evolutionary sequence
<BR>
going all the way back to the origin of life on Earth, the
<BR>
distinction between the living brain and any sort of computer
<BR>
remains clear.  However, when human designers intervene to choose
<BR>
the initial characteristics for a simulated selectional system,
<BR>
the arbitrariness of the choice of these initial conditions --
<BR>
especially the details of the values according to which the
<BR>
adjustment of synaptic weights will occur, but also choices about
<BR>
how large and fine-grained the repertoires will be, as well as
<BR>
the details of the body and its sensory and motor capabilities,
<BR>
blurs somewhat the distinction that Edelman is trying to draw.
<BR>
At what point, for example, does the definition of a value
<BR>
criterion become too high-level a specification, crossing some
<BR>
boundary to become direct programming of a category and hence,
<BR>
instructionism?  It seems to me that this is an empirical
<BR>
question, to be answered by the actual construction of such
<BR>
simulations and observation of the results.
<BR>
<P>Desigining and manufacturing a robot that bootstraps itself up
<BR>
from a very low-level set of value criteria to a complex
<BR>
behavioral repertoire of great generality will always be more
<BR>
expensive than seeking a shortcut to a high-level specification
<BR>
of behavior that gets a more limited-scope job done, even at the
<BR>
cost of curtailing the artifact's ultimate ability to deal with
<BR>
novelty.  At least, this is likely to be the case until the
<BR>
barely-imaginable day when the technology of consciousness
<BR>
becomes well established, and the hardware required to implement
<BR>
such complexity is cheap enough not to be a significant economic
<BR>
burden.  Until then, no venture capitalist is likely to accept a
<BR>
proposed business plan simply because the product envisioned
<BR>
contains the seed of a &quot;soul&quot;.  John Horgan illustrates this &quot;so
<BR>
what?&quot;  attitude in _The End of Science_ (pp. 168-170) in his
<BR>
reaction to Edelman's latest recognition automaton, Darwin IV:
<BR>
&quot;Edelman and his coworkers had built four robots, each named
<BR>
Darwin, each more sophisticated than the last.  Indeed, Darwin 4,
<BR>
Edelman assured me, was not a robot at all, but a 'real
<BR>
creature'.  It was 'the first nonliving thing that learns,
<BR>
okay?'...  What is its end goal? I asked. 'It **has** no end
<BR>
goals,' Edelman reminded me with a frown. 'We have given it
<BR>
**values**.  Blue is bad, red is good.'  Values are general and
<BR>
thus better suited to helping us cope with the polymorphous world
<BR>
than are goals, which are much more specific...  I asked how this
<BR>
robot differed from all the others built by scientists over the
<BR>
past few decades, many of which were capable of feats at least as
<BR>
impressive as those achieved by Darwin 4.  The difference,
<BR>
Edelman replied, his jaw setting, was that Darwin 4 possessed
<BR>
values or instincts, whereas other robots needed specific
<BR>
instruction to accomplish any task.  But don't all neural
<BR>
networks, I asked, eschew specific instructions for general
<BR>
learning programs?  Edelman frowned.  &quot;But [with] all of those,
<BR>
you have to exclusively define the input and output...
<BR>
Edelman... noted that most artificial-intelligence designers
<BR>
tried to program knowledge in from the top down with explicit
<BR>
instructions for every situation, instead of having knowledge
<BR>
arise naturally from values.  Take a dog, he said.  Hunting dogs
<BR>
acquire their knowledge from a few basic instincts.  'That is
<BR>
more efficacious than any bunch of Harvard boys writing a program
<BR>
for swamps!' Edelman guffawed...  But Darwin 4 is still a
<BR>
computer, a robot, with a limited repertoire of responses to the
<BR>
world, I persisted; Edelman was using language metaphorically
<BR>
when he called it a 'creature' with a 'brain'...  If a computer,
<BR>
[Edelman] said, is defined as something driven by algorithms, or
<BR>
effective procedures, then Darwin 4 is not a computer.  True,
<BR>
computer scientists might program robots to do what Darwin 4
<BR>
does.  But they would just be faking biological behavior, whereas
<BR>
Darwin 4's behavior is authentically biolgical.  If some random
<BR>
electronic glitch scrambles a line of code in his creature,
<BR>
Edelman informed me, 'it'll just correct like a wounded organism
<BR>
and it'll go around again.  I do that for the other one and it'll
<BR>
drop dead in its tracks.'  Rather than pointing out that all
<BR>
neural networks and many conventional computer programs have this
<BR>
capability, I asked Edelman about the complaints of some
<BR>
scientists that they simply did not understand his theories...&quot;
<BR>
<P>I notice that Edelman's observations on the subject of conscious
<BR>
artifacts correspond to some of the reflections of this list's
<BR>
own Eliezer S. Yudkowsky, in his Web document _Coding a
<BR>
Transhuman AI 2.0a_ (<A HREF="http://www.singinst.org/CaTAI.html">http://www.singinst.org/CaTAI.html</A>).  Some
<BR>
excerpts from Yudkowsky (I trust I am not exhibiting them too
<BR>
seriously out of context): &quot;It's actually rather surprising that
<BR>
the vast body of knowledge about human neuroscience and cognition
<BR>
has not yet been reflected in proposed designs for AIs.  It makes
<BR>
you wonder if there's some kind of rule that says that AI
<BR>
researchers don't study cognitive science.  This wouldn't make
<BR>
any sense, and is almost certainly false, but you do get that
<BR>
impression&quot;.  Edelman (BABF pp. 13-14) says that the cognitive
<BR>
scientists don't pay enough attention to neuroscience, either:
<BR>
&quot;[T]he cognitivist enterprise rests on a set of unexamined
<BR>
assumptions.  One of its most curious deficiencies is that it
<BR>
makes only marginal reference to the biological foundations that
<BR>
underlie the mechanisms it purports to explain.  The result is a
<BR>
scientific deviation as great as that of the behaviorism it has
<BR>
attempted to supplant.  The critical errors underlying this
<BR>
deviation are as unperceived by most cognitive scientists as
<BR>
relativity was before Einstein and heliocentrism was before
<BR>
Copernicus&quot; (BABF p. 14).
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5580.html">Jim Fehlinger: "A Spring-Powered Theory of Consciousness (3 of 6)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5578.html">Jim Fehlinger: "A Spring-Powered Theory of Consciousness (4 of 6)"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5579">[ date ]</A>
<A HREF="index.html#5579">[ thread ]</A>
<A HREF="subject.html#5579">[ subject ]</A>
<A HREF="author.html#5579">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:13:51 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
