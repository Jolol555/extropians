<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: A Spring-Powered Theory of Consciousness (3 of 6)</TITLE>
<META NAME="Author" CONTENT="Jim Fehlinger (fehlinger@home.com)">
<META NAME="Subject" CONTENT="A Spring-Powered Theory of Consciousness (3 of 6)">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>A Spring-Powered Theory of Consciousness (3 of 6)</H1>
<!-- received="Mon Jun 19 22:15:59 2000" -->
<!-- isoreceived="20000620041559" -->
<!-- sent="Mon, 19 Jun 2000 23:56:35 -0400" -->
<!-- isosent="20000620035635" -->
<!-- name="Jim Fehlinger" -->
<!-- email="fehlinger@home.com" -->
<!-- subject="A Spring-Powered Theory of Consciousness (3 of 6)" -->
<!-- id="394EEB73.EA096BD2@home.com" -->
<STRONG>From:</STRONG> Jim Fehlinger (<A HREF="mailto:fehlinger@home.com?Subject=Re:%20A%20Spring-Powered%20Theory%20of%20Consciousness%20(3%20of%206)&In-Reply-To=&lt;394EEB73.EA096BD2@home.com&gt;"><EM>fehlinger@home.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon Jun 19 2000 - 21:56:35 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5581.html">John  M Grigg: "crime in big cities and Europe"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5579.html">Jim Fehlinger: "A Spring-Powered Theory of Consciousness (5 of 6)"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5580">[ date ]</A>
<A HREF="index.html#5580">[ thread ]</A>
<A HREF="subject.html#5580">[ subject ]</A>
<A HREF="author.html#5580">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
The high-level components in Edelman's model of the
<BR>
infrastructure of primary consciousness (UoC pp. 107-110; RP
<BR>
pp. 95-98, Chap. 9; BABF pp. 117-123) are, first of all, the
<BR>
process of perceptual categorization of combined sensory and
<BR>
motor activity described above, which Edleman calls &quot;nonself&quot;,
<BR>
symbolized as C(W): &quot;C(W) is the neural basis for perceptual
<BR>
categorization of W, the exteroceptive input -- peripheral,
<BR>
voluntary motor, proprioceptive, and polymodal sensory signals --
<BR>
and is mediated by the thalamus and cortical areas&quot; (RP p. 156
<BR>
[Fig. 9.1]).  Secondly, the correlation of the activity of the
<BR>
evolutionarily older, hedonic systems of the brain, which Edelman
<BR>
calls &quot;self&quot;, and which is not carried out by the cortex but by
<BR>
the hippocampus, amygdala, and septum: &quot;C(I) is the neural basis
<BR>
for categorization of I, the interoceptive input -- autonomic,
<BR>
hypothalamic, endocrine.  It is evolutionarily earlier, driven by
<BR>
inner events, mediated by limbic and brain-stem circuits coupled
<BR>
to biochemical circuits, and it shows slow phasic activity&quot; (RP
<BR>
p. 156).  Third, these two types of categorizations both feed the
<BR>
delay loop through the hippocampus, the convolved and sequenced
<BR>
output of which returns to the cortex to be stored as conceptual
<BR>
categorizations: &quot;C(W)*C(I) represents the neural basis of
<BR>
interaction and comparison of two categorical systems that
<BR>
occurs, for example, at the hippocampus, septum, and cingulate
<BR>
gyri.  C[C(W)*C(I)] is the neural basis of conceptual
<BR>
recategorization of this comparison, which takes place in the
<BR>
cingulate gyri, temporal lobes, and parietal and frontal cortex&quot;
<BR>
(RP p. 156).  These conceptual categorizations, based on both
<BR>
value and perception, are stored by the cortex in what Edelman
<BR>
calls the &quot;value-category memory&quot;: &quot;Unlike the system of
<BR>
perceptual categorization, this conceptual memory system is able
<BR>
to **categorize responses in the different brain systems** that
<BR>
carry out perceptual categorization and it does this according to
<BR>
the demands of limbic-brain stem value systems.  This
<BR>
'value-category' memory allows conceptual responses to occur in
<BR>
terms of the **mutual** interactions of the thalamocortical and
<BR>
limbic-brain stem systems&quot; (BABF p. 119).
<BR>
<P>The final element in the primary consciousness model is what
<BR>
Edelman terms the &quot;key reentrant loop&quot; (RP p. 162) which connects
<BR>
cortical areas associated with value-category memory to those
<BR>
areas carrying out current perceptual categorization: &quot;The key
<BR>
circuits underlying primary consciousness contain particular
<BR>
reentrant pathways connecting this self-nonself memory system to
<BR>
primary and secondary repertoires carrying out peceptual
<BR>
categorization in all modalities -- smell and taste, sight,
<BR>
hearing, touch, proprioception...  [In other words], the various
<BR>
memory repertoires dedicated to storage of the categorization of
<BR>
**past** matches of value to perceptual category are reentrantly
<BR>
connected to mapped classification couples dealing with
<BR>
**current** sensory input and motor response.  By such means, the
<BR>
past correlations of category with value are now themselves
<BR>
interactive in real time with current perceptual categorizations
<BR>
**before** they are altered by the value-dependent portions of
<BR>
the nervous system.  A kind of bootstrapping occurs in which
<BR>
current value-free perceptual categorization interacts with
<BR>
value-dominated memory before further contributing to alteration
<BR>
of that memory (RP p. 97).  &quot;[T]his 'bootstrapping process' takes
<BR>
place in all sensory modalities in parallel and simultaneously,
<BR>
thus allowing for the construction of a complex scene.  The
<BR>
coherence of this scene is coordinated by the conceptual
<BR>
value-category memory even if the individual perceptual
<BR>
categorization events that contribute to it are causally
<BR>
independent&quot; (BABF p. 119).
<BR>
<P>Once established by the concurrent developmental selection of
<BR>
brain and body morphologies and experiential selection of the
<BR>
population of potential activation patterns of the secondary
<BR>
repertoire, primary consciousness is a self-perpetuating process.
<BR>
A somewhat misleadingly sequential description of what is
<BR>
actually interactive and overlapping activity would be that the
<BR>
process of primary consciousness continually answers the
<BR>
following implicit questions: while (conscious) {1. What's out
<BR>
there now?  2. How do I feel about it?  3. What shall I do about
<BR>
it?} These questions are jointly answered by the activation of a
<BR>
particular set of global mappings and the conceptual categories
<BR>
(including value-category concepts) that link them, out of all
<BR>
the possibilities in the secondary repertoire, within a couple of
<BR>
hundred milliseconds after the initial sampling of sensory data
<BR>
(UoC pp. 27, 150-151).  The behavior constituting the answer to
<BR>
question 3 has consequences which may be beneficial (pleasant) or
<BR>
harmful (painful) to the organism (as determined by the values
<BR>
resulting from the evolutionary history of the organism's
<BR>
ancestors).  These consequences will modify future answers to
<BR>
questions 2 and 3, in future circumstances in which there is a
<BR>
similar answer to question 1.  &quot;In an animal with primary
<BR>
consciousness...  [t]he reward or threat of a scene consisting of
<BR>
both causally connected and causally unrelated events is assessed
<BR>
in terms of past experience of that individual animal, and that
<BR>
scene drives behavior...  The number of brain areas contributing
<BR>
to the...  global mappings that are simultaneously engaged is
<BR>
large, fluctuating, and subject to various linkages.  Still, what
<BR>
appears to be a large number of circuits and cells that are
<BR>
actually engaged at any one time is only a small fraction of the
<BR>
number of combinations that are possible in the repertoire of a
<BR>
selectional brain&quot; (UoC p. 205).  Consciousness continues as long
<BR>
as the neuronal groups in the thalamocortical system are capable
<BR>
of assuming the functionally integrated yet highly differentiated
<BR>
states characteristic of global mappings.
<BR>
<P>When this differentiated activity is lost, as during an epileptic
<BR>
seizure or slow-wave sleep, unconsciousness results: &quot;During an
<BR>
epileptic seizure, within a short time the discharge of neurons
<BR>
in much of the cerebral cortex and thalamus becomes
<BR>
hypersynchronous; that is, most neurons discharge at high rates
<BR>
and almost simultaneously...  The loss of consciousness during a
<BR>
seizure is therefore associated with a dramatic reduction in the
<BR>
complexity of the diverse repertoire of neural states that are
<BR>
normally available&quot; (UoC pp. 70-71).  &quot;The stereotyped
<BR>
'burst-pause' mode of activity during [slow-wave] sleep affects a
<BR>
large number of neurons dispersed over the entire brain.
<BR>
Furthermore, the slow, oscillatory firing of such distributed
<BR>
populations of neurons is highly synchronized globally, in sharp
<BR>
contrast with waking, when groups of neurons dynamically assemble
<BR>
and reassemble into continuously changing patterns of firing...
<BR>
While the patterns of neural firing are remarkably diverse and
<BR>
the repertoire of available neural states is large during waking,
<BR>
the repertoire of available neural states is greatly reduced
<BR>
during slow-wave sleep.  Corresponding to this dramatic reduction
<BR>
in the number of differentiated neural states, consciousness is
<BR>
diminished or lost, just as it is in generalized epileptic
<BR>
discharges.  Thus, it appears that consciousness requires not
<BR>
just neural activity, but neural activity that changes
<BR>
continually and is thus spatially and temporally differentiated.
<BR>
If most groups of neurons in the cortex discharge synchronously,
<BR>
functional discriminations among them are obliterated, brain
<BR>
states become extremely homogeneous, and with the shrinking
<BR>
repertoire of brain states that are available for selection,
<BR>
consciousness itself is lost&quot; (UoC pp. 72-74).
<BR>
<P>Because consciousness is the result of widely distributed neural
<BR>
activity throughout the cortex, local damage to the cortex may
<BR>
result in focused impairments in performance, but seldom in
<BR>
unconsciousness: &quot;Despite occasional claims to the contrary, it
<BR>
has never been conclusively shown that a lesion of a restricted
<BR>
portion of the cerebral cortex leads to unconsciousness...  The
<BR>
only localized brain lesions that result in loss of consciousness
<BR>
typically affect the so-called reticular activating system.  This
<BR>
highly heterogeneous system, which is located in the
<BR>
evolutionarily older part of the brain -- upper portions of the
<BR>
brainstem (upper pons and mesencephalon) and extends into the
<BR>
posterior hypothalamus, the so-called thalamic intralaminar and
<BR>
reticular nuclei, and the basal forebrain -- projects diffusely
<BR>
to the thalamus and cortex.  It is thought to 'energize' or
<BR>
'activate' the thalamocortical system and facilitate interactions
<BR>
among distant cortical regions...  During wakefulness, when this
<BR>
system is active, thalamocortical neurons are depolarized, fire
<BR>
in a tonic or continuous way, and respond well to incoming
<BR>
stimuli.  During dreamless sleep, this system becomes less active
<BR>
or inactive; thalamocortical neurons become hyperpolarized, fire
<BR>
in repetitive bursts and pauses, and are hardly responsive to
<BR>
incoming stimuli.  Moreover, if this system is lesioned, all
<BR>
consciousness is lost, and a person enters a state of coma&quot; (UoC
<BR>
p. 54).
<BR>
<P>Edelman goes on to consider the bases of language and
<BR>
higher-order consciousness in human beings (UoC Chap. 15; RP
<BR>
Chap. 10, 11; BABF Chap. 12), but I will stop here with the
<BR>
completion of his model of primary consciousness.  Primary and
<BR>
higher-order consciousness are given schematic diagrams in RP
<BR>
p. 96 (Fig 5.1 A and B); BABF pp. 120 (Fig. 11-1), 132
<BR>
(Fig. 12-4); the same diagrams are repeated in UoC pp. 108 (Fig
<BR>
9.1), 194 (Fig. 15.1) but with the captions reversed!  Most
<BR>
roboticists would be quite happy, and would consider it quite
<BR>
useful, to be able to construct artifacts exhibiting what Edelman
<BR>
calls primary consciousness.  Edelman believes that, in the
<BR>
biological realm, this capability is quite old: &quot;Which animals
<BR>
have consciousness?  ...  Going backward from the human referent,
<BR>
we may be reasonably sure...  that chimpanzees have it.  In all
<BR>
likelihood, most mammals and some birds may have it...  If the
<BR>
brain systems required by the present model represent the
<BR>
**only** evolutionary path to primary consciousness, we can be
<BR>
fairly sure that animals without a cortex or its equivalent lack
<BR>
it.  An amusing speculation is that cold-blooded animals with
<BR>
primitive cortices would face severe restrictions on primary
<BR>
consciousness because their value systems and value-category
<BR>
memory lack a stable enough biochemical milieu in which to make
<BR>
appropriate linkages to a system that could sustain such
<BR>
consciousness.  So snakes are in (dubiously, depending on the
<BR>
temperature), but lobsters are out.  If further study bears out
<BR>
this surmise, consciousness is about 300 million years old&quot; (BABF
<BR>
pp. 122-123).
<BR>
<P>Edelman and his colleagues have constructed a number of computer
<BR>
simulations which are based on the principles of the &quot;Theory of
<BR>
Neuronal Group Selection&quot; (TNGS), and which function quite
<BR>
differently, Edelman says, from the computer programs, robotic
<BR>
devices, and neural networks produced heretofore by roboticists
<BR>
and the artificial intelligence community.  Edelman claims that
<BR>
his own artifacts exhibit behavior resulting from the spontaneous
<BR>
emergence within the simulation of perceptual categorization,
<BR>
through selection of synaptic weights and network firing patterns
<BR>
constrained by value: &quot;I have called the study of such devices
<BR>
**noetics** from the Greek **noein**, to perceive.  Unlike
<BR>
cybernetic devices that are able to adapt within fixed design
<BR>
constraints, and unlike robotic devices that use cybernetic
<BR>
principles under programmed control, noetic devices act on their
<BR>
environment by selectional means and by categorization on value&quot;
<BR>
(BABF p. 192).  In such artifacts, &quot;[c]ertain specialized
<BR>
networks... reflect the relative adaptive value to the automaton
<BR>
of the effects of its various motor actions and sensory
<BR>
experience...  Selective amplification of synapses [is] made to
<BR>
depend on adaptive value as registered by such internal
<BR>
structures...  External or explicit **categorical** criteria for
<BR>
amplification (such as those a programmer might provide) are
<BR>
**not** permitted, however&quot; (RP pp. 59-62).
<BR>
<P>Among those &quot;noetic&quot; devices described in the books are models of
<BR>
the visual cortex (RP pp. 72-90; UoC pp. 114-120) and a series of
<BR>
recognition automata named &quot;Darwin&quot; (Darwin II: ND Chap. 10;
<BR>
Darwin III: RP pp. 57-63, BABF pp. 91-94; Darwin IV: UoC
<BR>
pp. 90-91).  Darwin IV seems to be the same device described as
<BR>
&quot;under construction&quot; in BABF and there called NOMAD: &quot;Neurally
<BR>
Organized Multiply Adaptive Device&quot; (BABF pp. 192-193).  These
<BR>
models were built to answer the question &quot;Can a prewired network
<BR>
or congeries of networks based on selective principles and
<BR>
reentry respond stably and adaptively to structural inputs to
<BR>
yield pattern recognition, categorization, and association
<BR>
without prior instructions, explicit semantic rules, or forced
<BR>
learning?&quot; despite the fact that &quot;while the three major premises
<BR>
of the theory of neuronal group selection (developmental
<BR>
selection leading to a primary repertoire, synaptic selection to
<BR>
yield a secondary repertoire, and reentry) can all be stated
<BR>
reasonably simply, their actual operation in interacting
<BR>
nonlinear networks is highly complex&quot; (ND p. 271).  &quot;Unlike the
<BR>
standard artificial intelligence paradigm, selective recognition
<BR>
automata avoid preestablished categories and programmed
<BR>
instructions for behavior...  Although programming is used to
<BR>
instruct a computer how to simulate the neuronal units in a
<BR>
recognition automaton, **the actual function of these units is
<BR>
not itself programmed**&quot; (RP p. 58).
<BR>
<P>Darwin III, for example, &quot;has a four-jointed arm with touch
<BR>
receptors on the part of its arm distal to the last joint,
<BR>
kinesthetic neurons in its joints, and a movable eye...  Although
<BR>
it sits still, it can move its eye and arm in any pattern
<BR>
possible within the bounds imposed by its mechanical arrangement.
<BR>
Objects in a world of randomly chosen shapes move at random past
<BR>
its field of vision and occasionally within reach of its arm and
<BR>
touch&quot; (BABF p. 191).  &quot;After a suitable period of experience
<BR>
with various moving stimuli, the eye of Darwin III in fact begins
<BR>
to make the appropriate saccades and fine tracking movements with
<BR>
no further specification of its task other than that implicit in
<BR>
the value scheme&quot; (RP. p. 62).  &quot;Values are arbitrary: in a given
<BR>
example of Darwin III, they have a specific structure and
<BR>
correspond to various kinds of evolutionarily determined
<BR>
characteristics that contribute to phenotypic fitness.  Such
<BR>
low-level values are, for example, 'Seeing is better than not
<BR>
seeing' -- translated as 'Increase the probability that, when the
<BR>
retina and its visual networks are stimulated, those synapses
<BR>
that were active in the recent past (and thus potentially
<BR>
involved in the behavior that brought about any increased
<BR>
stimulation) will change in strength&quot; (RP p. 59).  &quot;In Darwin
<BR>
III, pairs of higher-order networks form classification
<BR>
couples...  [O]ne network... responds to local visual features...
<BR>
The other responds to kinesthetic features as the last joint of
<BR>
the arm traces contours by light touch (since the low-level value
<BR>
for touch is 'More pressure is better than less pressure' this
<BR>
joint will tend to seek edges).  The visual and kinesthetic
<BR>
repertoires are reentrantly connected.  This reentry allows
<BR>
correlation of the responses of these different repertoires that
<BR>
have disjunctively sampled visual and kinesthetic signals, and it
<BR>
yields a primitive form of categorization&quot; (RP p. 62).  &quot;No two
<BR>
versions of Darwin III so constituted show identical behavior
<BR>
but, provided their low-level values are similar, their behavior
<BR>
tends to converge in terms of particular kinds of categorization
<BR>
upon a given value.  Most strikingly, however, if the
<BR>
**lower-level values** (expressed as biases acting upon synaptic
<BR>
changes) are removed from the simulation, these automata show no
<BR>
convergent behavior&quot; (RP p. 63).
<BR>
<P>The preservation of &quot;realism&quot; in these models by strict adherence
<BR>
to the TNGS and avoidance of &quot;shortcuts&quot; to categorization and
<BR>
behavior exacts a high price in complexity: &quot;There is a dilemma
<BR>
in modeling the degree of complexity underlying the function of
<BR>
higher neural networks.  On the one hand, any representation in a
<BR>
machine must be very limited as compared with real neural
<BR>
networks.  On the other hand, the internal design of even a
<BR>
highly simplified and minimal model of a classification
<BR>
couple... must be highly complex as compared with computer logic.
<BR>
This is so because of the minimal size requirements on
<BR>
repertoires, the parallelism of classification couples, the
<BR>
nonlinearlity of network behavior, and the deliberate avoidance
<BR>
of semantic or instructional components in the design of the
<BR>
machine&quot; (ND p. 272).  &quot;While Darwin... is not a model of an
<BR>
actual nervous system, it does set out to approach one of the
<BR>
problems that evolution had to solve in forming complex nervous
<BR>
systems -- the need to form categories in a bottom-up manner from
<BR>
structures in the environment.  Five key features of the model
<BR>
make this possible: (1) Darwin... incorporates selective networks
<BR>
whose initial specificities enable them to respond without
<BR>
instruction to unfamiliar stimuli; (2) degeneracy provides
<BR>
multiple possibilities of response to any stimulus, at the same
<BR>
time providing functional redundancy against component failure;
<BR>
(3) the output of Darwin... is a **pattern** of response, making
<BR>
use of the simultaneous responses of multiple degenerate groups
<BR>
to avoid the need for very high specificity and the combinatorial
<BR>
disaster that this would imply; (4) reentry within individual
<BR>
networks vitiates the limitations described by Minsky and Papert
<BR>
(1969) for a class of perceptual automata (perceptrons) lacking
<BR>
such connections; and (5) reentry between communicating networks
<BR>
with different functions gives rise to new functions, such as
<BR>
association, that each network alone could not carry out.
<BR>
Neither the representative transformations nor the limited
<BR>
generalizations performed by Darwin... require the direct
<BR>
intervention of a classifying observer, either through
<BR>
instruction or through forced learning&quot; (ND pp. 288-289).
<BR>
<P>In the design of such bottom-up categorization devices, one
<BR>
element that cannot be avoided is the necessity to choose a set
<BR>
of initial value constraints: &quot;Value is a sign of **nested**
<BR>
selective systems -- a result of **natural selection** that
<BR>
yields alterations of the phenotype that can then serve as
<BR>
constraints on the **somatic selection** occurring in an
<BR>
individual's nervous system.  Unlike evolution, somatic selection
<BR>
can deal with contingencies of immediate environments that are
<BR>
rich and unpredictable -- even ones that have never occurred
<BR>
before -- by enabling an individual animal to categorize critical
<BR>
features of these environments during short periods.  But we
<BR>
again emphasize that neuronal group selection can consistently
<BR>
accomplish this categorization only under the constraint of
<BR>
inherited values determined by evolution.  The nest of systems is
<BR>
a beautiful one, guaranteeing survival for each species in terms
<BR>
of what may be called necessary prejudice -- one required for
<BR>
survival under behavioral control by a selectional brain&quot; (UoC
<BR>
p. 92).
<BR>
<P>At many points in these books, Edelman stresses his belief that
<BR>
the analogy which has repeatedly been drawn during the past fifty
<BR>
years between digital computers and the human brain is a false
<BR>
one (BABF p. 218), stemming largely from &quot;confusions concerning
<BR>
what can be assumed about how the brain works without bothering
<BR>
to study how it is physically put together&quot; (BABF p. 227).  The
<BR>
lavish, almost profligate, morphology exhibited by the multiple
<BR>
levels of degeneracy in the brain is in stark contrast to the
<BR>
parsimony and specificity of present-day human-made artifacts,
<BR>
composed of parts of which the variability is deliberately
<BR>
minimized, and whose components are chosen from a relatively
<BR>
limited number of categories of almost identical units.
<BR>
Statistical variability among (say) electronic components occurs,
<BR>
but it's usually merely a nuisance that must be accommodated,
<BR>
rather than an opportunity that can be exploited as a fundamental
<BR>
organizational principle, as Edelman claims for the brain.  In
<BR>
human-built computers, &quot;the small deviations in physical
<BR>
parameters that do occur (noise levels, for example) are ignored
<BR>
by agreement and design&quot; (BABF p. 225).  &quot;The analogy between the
<BR>
mind and a computer fails for many reasons.  The brain is
<BR>
constructed by principles that ensure diversity and degeneracy.
<BR>
Unlike a computer, it has no replicative memory.  It is
<BR>
historical and value driven.  It forms categories by internal
<BR>
criteria and by constraints acting at many scales, not by means
<BR>
of a syntactically constructed program.  The world with which the
<BR>
brain interacts is not unequivocally made up of classical
<BR>
categories&quot; (BABF p. 152).
<BR>
<P>This contrast between the role of stochastic variation in the
<BR>
brain and the absence of such a role in electronic devices such
<BR>
as computers is one of the distinctions between what Edelman
<BR>
calls &quot;instructionism&quot; in his own terminology (RP p. 30), but has
<BR>
also been called &quot;functionalism&quot; or &quot;machine functionalism&quot; (RP
<BR>
p. 30; BABF p. 220); and &quot;selectionism&quot; (UoC p. 16; RP
<BR>
pp. 30-33).  Up to the present, all human artifacts and machines
<BR>
(including computers and computer programs) have been based on
<BR>
functionalist or instructionist design principles.  In these
<BR>
devices, the parts and their interactions are precisely specified
<BR>
by a designer, and precisely matched to expected inputs and
<BR>
outputs.  This is a construction approach based on cost
<BR>
consciousness, parsimonious allocation of materials, and limited
<BR>
levels of manageable complexity in design and manufacture.  The
<BR>
workings of such artifacts are &quot;held to be describable in a
<BR>
fashion similar to that used for algorithms&quot;.
<BR>
<P>By analogy to the hardware-independence of computer programs,
<BR>
functionalist models of neural &quot;algorithms&quot; underlying cognition
<BR>
and behavior have attempted to separate these functions from
<BR>
their physical instantiation in the brain: &quot;In the functionalist
<BR>
view, what is ultimately important for understanding psychology
<BR>
are the algorithms, not the hardware on which they are
<BR>
executed... Furthermore, the tissue organization and composition
<BR>
of the brain shouldn't concern us as long as the algorithm 'runs'
<BR>
or comes to a successful halt.&quot; (BABF p. 220).  In Edelman's
<BR>
view, the capabilities of the human brain are much more
<BR>
intimately dependent on its morphology than the functionalist
<BR>
view admits, and any attempt to minimize the contribution of the
<BR>
brain's biological substrate by assuming functional equivalence
<BR>
with the sort of impoverished and rigid substrates characteristic
<BR>
of modern-day computers is bound to be misleading.
<BR>
<P>On the other hand, &quot;selectionism&quot;, according to Edelman, is
<BR>
quintessentially characteristic of biological systems (such as
<BR>
the brain), whose fine-grained structure (not yet achievable by
<BR>
human manufacturing processes, but imagined in speculations about
<BR>
molecular electronics, nanotechnology, and the like) permits
<BR>
luxuriantly large populations of statistically-varying components
<BR>
to vie in Darwinian competition based on their ability to
<BR>
colonize available functional niches created by the growth of a
<BR>
living organism and its ongoing interaction with the external
<BR>
world.  The fine-grained variation in functional repertoires
<BR>
matches the fine-grained variation in the world itself: &quot;the
<BR>
nature of the physical world itself imposes commonalities as well
<BR>
as some very stringent requirements on any representation of that
<BR>
world by conscious beings... [W]hatever the mental representation
<BR>
of the world is at any one time, there are almost always very
<BR>
large numbers of additional signals linked to any chunk of the
<BR>
world...  [S]uch properties are inconsistent with a fundamental
<BR>
**symbolic** representation of the world considered as an
<BR>
**initial** neural transform.  This is so because a symbolic
<BR>
representation is **discontinuous** with respect to small changes
<BR>
in the world...&quot; (RP p. 33).
<BR>
<P>Edelman's selectionist scenarios are highly dynamic, both in
<BR>
terms of events within the brain and in terms of the interaction
<BR>
of the organism with its environment: &quot;In the creation of a
<BR>
neural construct, motion plays a pivotal role in selectional
<BR>
events both in primary and in secondary repertoire development.
<BR>
The morphogenetic conditions for establishing primary repertoires
<BR>
(modulation and regulation of cell motion and process extension
<BR>
under regulatory constraint to give constancy and variation in
<BR>
neural circuits) have a counterpart in the requirement for
<BR>
organismic motion during early perceptual categorization and
<BR>
learning.&quot;  (ND p. 320).  &quot;Selective systems... involve **two
<BR>
different domains of stochastic variation** (world and neural
<BR>
repertoires).  The domains map onto each other in an individual
<BR>
**historical** manner...  Neural systems capable of this mapping
<BR>
can deal with novelty and generalize upon the results of
<BR>
categorization.  Because they do not depend upon specific
<BR>
programming, they are self-organizing and do not invoke
<BR>
homunculi.  Unlike functionalist systems, they can take account
<BR>
of an open-ended environment&quot; (RP p. 31).
<BR>
<P>A human-designed computer or computer program operates upon input
<BR>
which has been coded by, or has had a priori meaning assigned by,
<BR>
human beings: &quot;For ordinary computers, we have little difficulty
<BR>
accepting the functionalist position because the only meaning of
<BR>
the symbols on the tape and the states in the processor is **the
<BR>
meaning assigned to them by a human programmer**.  There is no
<BR>
ambiguity in the interpretation of physical states as symbols
<BR>
because the symbols are represented digitally according to rules
<BR>
in a syntax.  The system is **designed** to jump quickly between
<BR>
defined states and to avoid transition regions between them...&quot;
<BR>
(BABF p. 225).  It functions according to a set of deterministic
<BR>
algorithms (&quot;effective procedures&quot; [UoC p. 214]) and produces
<BR>
outputs whose significance must, once again, be interpreted by
<BR>
human beings.
<BR>
<P>A similar &quot;instructionist&quot; theory of the brain, based on logical
<BR>
manipulation of coded inputs and outputs, cannot escape the
<BR>
embarrassing necessity to posit a &quot;homunculus&quot; to assign and
<BR>
interpret the input and output codes (BABF pp. 79, 80 [Fig. 8-2],
<BR>
8).  In contrast, a &quot;selectionist&quot; theory of the brain based on
<BR>
competition among a degenerate set of &quot;effective structures [UoC
<BR>
p. 214]&quot;, can escape this awkwardness, with perceptual categories
<BR>
of evolutionary significance to the organism spontaneously
<BR>
emerging from the ongoing loop of sensory sampling continuously
<BR>
modified by movement that is characteristic of an embodied brain
<BR>
(UoC pp. 81, 214; ND pp. 20, 37; RP p. 532).  It's clear that
<BR>
Edelman, in formulating the TNGS (UoC Chap. 7; see also ND
<BR>
Chap. 3; RP Chap. 3, p. 242; BABF Chap. 9) has generalized to the
<BR>
nervous system the insights he gained from his earlier work in
<BR>
immunology, which also relies on fortuitous matching by a
<BR>
biological recognition system (BABF Chap. 8) between a novel
<BR>
antigen and one of a large repertoire of variant
<BR>
proto-antibodies, with the resulting selection being
<BR>
differentially amplified to produce the organism's immune
<BR>
response (BABF p. 76 [Fig. 8-2]).
<BR>
<P>Despite his dismissive attitude toward traditional &quot;top-down&quot;,
<BR>
symbolic approaches to artificial intelligence, and to the sorts
<BR>
of neural-network models in which specific roles are assigned to
<BR>
input and output neurons by the network designer, Edelman does
<BR>
not deny the possibility that conscious artifacts can be
<BR>
constructed (BABF Chap. 19): &quot;I have said that the brain is not a
<BR>
computer and that the world is not so unequivocally specified
<BR>
that it could act as a set of instructions.  Yet computers can be
<BR>
used to **simulate** parts of brains and even to help build
<BR>
perception machines based on selection rather than instruction...
<BR>
A system undergoing selection has two parts: the animal or organ,
<BR>
and the environment or world...  No instructions come from events
<BR>
of the world to the system on which selection occurs, [and]
<BR>
events occurring in an environment or world are unpredictable...
<BR>
[W]e simulate events and their effects... as follows: 1. Simulate
<BR>
the organ or animal... making provision for the fact that, as a
<BR>
selective system, it contains a generator of diversity --
<BR>
mutation, alterations in neural wiring, or synaptic changes that
<BR>
are unpredictable.  2. Independently simulate a world or
<BR>
environment constrained by known physical principles, but allow
<BR>
for the occurrence of unpredictable events.  3. Let the simulated
<BR>
organ or animal interact with the simulated world or the real
<BR>
world without prior information transfer, so that selection can
<BR>
take place.  4. See what happens...  Variational conditions are
<BR>
placed in the simulation by a technique called a pseudo-random
<BR>
number generator...  [I]f we wanted to capture randomness
<BR>
absolutely, we could hook up a radioactive source emitting alpha
<BR>
particles, for example, to a counter that would **then** be
<BR>
hooked up to the computer&quot; (BABF p. 190).
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5581.html">John  M Grigg: "crime in big cities and Europe"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5579.html">Jim Fehlinger: "A Spring-Powered Theory of Consciousness (5 of 6)"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5580">[ date ]</A>
<A HREF="index.html#5580">[ thread ]</A>
<A HREF="subject.html#5580">[ subject ]</A>
<A HREF="author.html#5580">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:13:51 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
