<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Nanotech Restrictions (was: RE: Transparency De</TITLE>
<META NAME="Author" CONTENT="Adrian Tymes (wingcat@pacbell.net)">
<META NAME="Subject" CONTENT="Re: Nanotech Restrictions (was: RE: Transparency Debate)">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Nanotech Restrictions (was: RE: Transparency Debate)</H1>
<!-- received="Sat Apr  8 14:05:04 2000" -->
<!-- isoreceived="20000408200504" -->
<!-- sent="Sat, 08 Apr 2000 12:38:33 -0700" -->
<!-- isosent="20000408193833" -->
<!-- name="Adrian Tymes" -->
<!-- email="wingcat@pacbell.net" -->
<!-- subject="Re: Nanotech Restrictions (was: RE: Transparency Debate)" -->
<!-- id="38EF8AB9.55E9F996@pacbell.net" -->
<!-- inreplyto="20000408060744.37627.qmail@hotmail.com" -->
<STRONG>From:</STRONG> Adrian Tymes (<A HREF="mailto:wingcat@pacbell.net?Subject=Re:%20Nanotech%20Restrictions%20(was:%20RE:%20Transparency%20Debate)&In-Reply-To=&lt;38EF8AB9.55E9F996@pacbell.net&gt;"><EM>wingcat@pacbell.net</EM></A>)<BR>
<STRONG>Date:</STRONG> Sat Apr 08 2000 - 13:38:33 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="0418.html">Dan McGuirk: "RE: The purposes of the Extropian list"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="0416.html">Adrian Tymes: "Re: e-publishing fiction"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="0399.html">phil osborn: "Re: Nanotech Restrictions (was: RE: Transparency Debate)"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="0423.html">Harvey Newstrom: "Re: Nanotech Restrictions (was: RE: Transparency Debate)"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="0423.html">Harvey Newstrom: "Re: Nanotech Restrictions (was: RE: Transparency Debate)"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#417">[ date ]</A>
<A HREF="index.html#417">[ thread ]</A>
<A HREF="subject.html#417">[ subject ]</A>
<A HREF="author.html#417">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
phil osborn wrote:
<BR>
<EM>&gt; From: Adrian Tymes &lt;<A HREF="mailto:wingcat@pacbell.net?Subject=Re:%20Nanotech%20Restrictions%20(was:%20RE:%20Transparency%20Debate)&In-Reply-To=&lt;38EF8AB9.55E9F996@pacbell.net&gt;">wingcat@pacbell.net</A>&gt;
</EM><BR>
<EM>&gt; &gt;The pursuit of knowledge and true power makes one less interested in the
</EM><BR>
<EM>&gt; &gt;pursuit of power over people?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; This line of argument is probably one of those more likely to yield real
</EM><BR>
<EM>&gt; results.  The same problem is faced in bringing up baby SI's.  Either there
</EM><BR>
<EM>&gt; is or there isn't rational grounds for something that we would recognize as
</EM><BR>
<EM>&gt; morality, as in ethics - such as, the non-aggression principle.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; The other part, without getting into any of the details, is that the
</EM><BR>
<EM>&gt; rationality of being ethical may in fact be contextual.  Eg., lieing in a
</EM><BR>
<EM>&gt; society in which aggression is rare and one is not being threatened for
</EM><BR>
<EM>&gt; telling the truth may be unethical and immoral, whereas lieing as a Jew in
</EM><BR>
<EM>&gt; NAZI Germany may be morally correct.  In fact, in societies such as that, a
</EM><BR>
<EM>&gt; rational ethics may be so impossible to derive on a moment by moment basis,
</EM><BR>
<EM>&gt; that the processing costs outweigh any possible benefits, and crude, first
</EM><BR>
<EM>&gt; approximations and main chances rule.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; If these two general perspectives are accurate, then the questions become
</EM><BR>
<EM>&gt; related to what would be - in ideal circumstances - a rational ethics, and
</EM><BR>
<EM>&gt; what kind of society can support it.  I believe that these are two of the
</EM><BR>
<EM>&gt; main, if not most important questions, and, having partially resolved both,
</EM><BR>
<EM>&gt; I am working on systems to bring such a society about, i.e., social
</EM><BR>
<EM>&gt; infrastructure, such as an explicit universal social contract.
</EM><BR>
<P>I've been working on a system that seems similar - perhaps you could
<BR>
tell me if it is?
<BR>
<P>I call it &quot;enlightened greed&quot;, and it is driven by the principle that
<BR>
the correct action is that which will benefit me most in the long term.
<BR>
For example, longevity research is a good thing in part because it will
<BR>
result in practices and objects that will allow me to live longer, but
<BR>
the easiest way to get it is by enouraging this research for application
<BR>
to the general public: when the benefits are so applied, then I, as a
<BR>
member of the public, can take advantage of it.  Therefore encouraging
<BR>
longevity research so that anyone who wants to can live longer is the
<BR>
correct approach for me to take.
<BR>
<P>The important thing is that nowhere in the decision is there
<BR>
consideration of what would be good or bad for anybody else, save how
<BR>
that may eventually affect myself.  And yet the more I tinker with it,
<BR>
the more it seems to come up with ends that, under more common ethical
<BR>
measures, are labeled &quot;noble&quot; or &quot;altruistic&quot; - which are qualities not
<BR>
usually associated with greed.
<BR>
<P><EM>&gt; In a contractual society, in which - given my other assumptions -
</EM><BR>
<EM>&gt; presumeably all rational people could be convinced of the advantage of being
</EM><BR>
<EM>&gt; ethical, a lot of the really bad scenarios that would otherwise require
</EM><BR>
<EM>&gt; draconian measures, such as a police state to stop nanoterrorists, would be
</EM><BR>
<EM>&gt; limited to a probably small minority of very irrational people, who would
</EM><BR>
<EM>&gt; likely be identified fairly early as creating high risks for anyone who
</EM><BR>
<EM>&gt; dealt with them long term.  Thus, those people would be watched and would
</EM><BR>
<EM>&gt; probably pay high insurance premiums and find themselves unwelcome in many
</EM><BR>
<EM>&gt; social settings.
</EM><BR>
<P>But, how do you make sure that the criteria for &quot;watch&quot; and &quot;pay high
<BR>
insurance premiums&quot; only targets those who would have a negative effect
<BR>
on society?  Nanoterrorists are a subset of people who know how to use
<BR>
nanotech, but if it is far easier/cheaper to identify nanotech users
<BR>
than just malicious nanotech users, how do you avoid alienating those
<BR>
who would use nanotech to benefit society?  (Note that this alienation
<BR>
may, in some cases, turn those who would otherwise work for everyone's
<BR>
benefit into revenge-driven terrorists.)  This might work if you could
<BR>
get around that problem.
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="0418.html">Dan McGuirk: "RE: The purposes of the Extropian list"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="0416.html">Adrian Tymes: "Re: e-publishing fiction"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="0399.html">phil osborn: "Re: Nanotech Restrictions (was: RE: Transparency Debate)"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="0423.html">Harvey Newstrom: "Re: Nanotech Restrictions (was: RE: Transparency Debate)"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="0423.html">Harvey Newstrom: "Re: Nanotech Restrictions (was: RE: Transparency Debate)"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#417">[ date ]</A>
<A HREF="index.html#417">[ thread ]</A>
<A HREF="subject.html#417">[ subject ]</A>
<A HREF="author.html#417">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:09:09 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
