<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: SITE: Coding a Transhuman AI 2.0a</TITLE>
<META NAME="Author" CONTENT="Dan Fabulich (daniel.fabulich@yale.edu)">
<META NAME="Subject" CONTENT="Re: SITE: Coding a Transhuman AI 2.0a">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: SITE: Coding a Transhuman AI 2.0a</H1>
<!-- received="Sat May 20 02:53:05 2000" -->
<!-- isoreceived="20000520085305" -->
<!-- sent="Sat, 20 May 2000 04:53:42 -0400 (EDT)" -->
<!-- isosent="20000520085342" -->
<!-- name="Dan Fabulich" -->
<!-- email="daniel.fabulich@yale.edu" -->
<!-- subject="Re: SITE: Coding a Transhuman AI 2.0a" -->
<!-- id="Pine.GSO.4.10.10005200410080.15307-100000@morpheus.cis.yale.edu" -->
<!-- inreplyto="00052001001900.00501@localhost.localdomain" -->
<STRONG>From:</STRONG> Dan Fabulich (<A HREF="mailto:daniel.fabulich@yale.edu?Subject=Re:%20SITE:%20Coding%20a%20Transhuman%20AI%202.0a&In-Reply-To=&lt;Pine.GSO.4.10.10005200410080.15307-100000@morpheus.cis.yale.edu&gt;"><EM>daniel.fabulich@yale.edu</EM></A>)<BR>
<STRONG>Date:</STRONG> Sat May 20 2000 - 02:53:42 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="3413.html">Eugene Leitl: "Re: A new level of sophistication in cyber security"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3411.html">Forrest Bishop: "any minute now"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3407.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3439.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="3439.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3412">[ date ]</A>
<A HREF="index.html#3412">[ thread ]</A>
<A HREF="subject.html#3412">[ subject ]</A>
<A HREF="author.html#3412">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Matt Gingell wondered:
<BR>
<P><EM>&gt; Sure - I'm completely sympathetic to this point of view. Would you
</EM><BR>
<EM>&gt; agree though that we shouldn't aim to hand code a machine that does
</EM><BR>
<EM>&gt; anything a baby can't?
</EM><BR>
<P>No, I disagree.  As a general point, we're trying to code a transhuman
<BR>
AI.  This thing SHOULD be more powerful than we are.  This, to a great
<BR>
extent, is The Whole Point.
<BR>
<P>But glib answers aside, Eliezer lists a number of things babies can't
<BR>
do that we'd clearly want a Seed AI to be able to do: develop new
<BR>
sensory modalities, blend conscious and autonomic thought, overpower
<BR>
certain technical problems, observe its own thought processes at a
<BR>
relevant level of detail, and improve upon those processes at a
<BR>
fundamental level.  He also gives a good argument as to why none of
<BR>
these features are emergent.
<BR>
<P><EM>&gt; There is a wider, more important, question here though: What is a baby
</EM><BR>
<EM>&gt; born with? What have millions of years of evolution invented? Do we
</EM><BR>
<EM>&gt; have, as I would like to think, a superbly elegant, distributed
</EM><BR>
<EM>&gt; learning and representation forming machine, a general purpose pattern
</EM><BR>
<EM>&gt; extraction engine; or do we have a bunch of rules and hardwired
</EM><BR>
<EM>&gt; concepts with an afterthought of theorem prover on top? Has our
</EM><BR>
<EM>&gt; Darwinian history provided us with a database of rules and
</EM><BR>
<EM>&gt; combinatorics, or has it stumbled across a universal blank slate
</EM><BR>
<EM>&gt; automaton - itself more fit than any single, static apparatus?
</EM><BR>
<P>Unfortunately, it's just a bunch of rules with logic as an
<BR>
afterthought.  It would be handy if there were a general problem
<BR>
solver and we had somehow hit upon it.  But I see no reason to think
<BR>
that this is the case.  There are a lot of problems that we're quite
<BR>
obviously bad at, for obvious evolutionary reasons.  We might even be
<BR>
able to see obvious ways to fix some of our most obvious failings, if
<BR>
only we had the capacity to modify our autonomic thought processes in
<BR>
an intentional and relevant way.  We can't.
<BR>
<P>BTW, the fact that no such Holy Grail exists also provides a plausible
<BR>
explanation as to why AI has failed so often in the past.  In an
<BR>
important sense, were you right about what intelligence is like, AI
<BR>
would be easier than it is harder.
<BR>
<P><EM>&gt; Surely it's a bit of both - one can render the nature vs. nurture
</EM><BR>
<EM>&gt; dialectic, or any other, bland by saying it's a mix. The question
</EM><BR>
<EM>&gt; I'm posing though is as follows: Is intelligence something special,
</EM><BR>
<EM>&gt; is it something beyond a vast store of facts and rules? Is
</EM><BR>
<EM>&gt; instinctive knowledge necessary, or does instinct simply optimize
</EM><BR>
<EM>&gt; something deeper and more interesting? Can we construct a general
</EM><BR>
<EM>&gt; definition of what intelligence is, independent of it's utility in
</EM><BR>
<EM>&gt; some specific environment - and if we can is it possible to develop
</EM><BR>
<EM>&gt; an instance of that definition which would function 'intelligently'
</EM><BR>
<EM>&gt; no matter what universe we drop it into? My answer is, obviously,
</EM><BR>
<EM>&gt; yes, and finding that abstraction is the proper goal of AI and
</EM><BR>
<EM>&gt; cognitive science research. Think, for instance, about the concept
</EM><BR>
<EM>&gt; 'natural number.' What does it take to extract that from the world?
</EM><BR>
<EM>&gt; Surely it's universal to anything we'd recognize as intelligent -
</EM><BR>
<EM>&gt; but where does it come from, and by what process?
</EM><BR>
<P>I can see that it would disappoint you if the answer turned out to be
<BR>
that this feature emerged in our brains only due to some contingent
<BR>
evolutionary process.  Allow me to disappoint you a little further by
<BR>
suggesting that you read Eliezer's &quot;Algernon's Law,&quot; which argues that
<BR>
intelligence, of the kind in which we're interested, is an
<BR>
evolutionary *disadvantage*.  This gives us every reason to believe
<BR>
that not only are we NOT the general problem solver you might wish we
<BR>
were, but that, due to the fact that we tend to behave in a way that
<BR>
is evolutionarily advantageous, we're barely on the right track.
<BR>
<P><EM>&gt; Moravec, if I'm remembering correctly, estimates the raw computing
</EM><BR>
<EM>&gt; resources of the human brain at something like 10 teraflops. Give me
</EM><BR>
<EM>&gt; 'a few months' of time on a machine that big, and I'm confident I
</EM><BR>
<EM>&gt; could extract a reasonable working theory of objects as permanent, law
</EM><BR>
<EM>&gt; obeying things from raw sense data. You would argue, presumably, that
</EM><BR>
<EM>&gt; these months are spent on physical/neurological maturation and the
</EM><BR>
<EM>&gt; expression of world-describing genes - where as I would like to
</EM><BR>
<EM>&gt; believe they are spent on what I will call, for lack of a more
</EM><BR>
<EM>&gt; descriptive word, 'learning.'
</EM><BR>
<P>Actually, I believe that you couldn't do it at all.
<BR>
<P>Look, suppose you WERE somehow able to map out a somewhat
<BR>
comprehensive list of possible conceptual schemes which you would use
<BR>
to categorize the &quot;raw sense data.&quot;  How could you algorithmically
<BR>
determine which of these conceptual schemes worked better than some
<BR>
others?  Any others?  Our ancestors had a way: use it as a rule for
<BR>
action, see if it helps you breed.  You and your machines, even at
<BR>
10^21 ops/sec, would have nothing to test your values against.
<BR>
<P>Consider a search space in which you're trying to find local maximums.
<BR>
Now imagine trying to do it without any idea of the height of any
<BR>
point in the space.  Now try throwing 10^100 ops at the project.
<BR>
Doesn't help, does it?
<BR>
<P><EM>&gt; Baby's are a useful thing to ask questions about - they're are only
</EM><BR>
<EM>&gt; example of what a raw mind looks like - but we should keep in mind
</EM><BR>
<EM>&gt; that the human mind is not the only possible solution to the problem
</EM><BR>
<EM>&gt; of general intelligence. A brain is a useful thing to think about, but
</EM><BR>
<EM>&gt; an artificial mind might bare no resemblance to any natural
</EM><BR>
<EM>&gt; neurology. The design process and the engineering constraints differ
</EM><BR>
<EM>&gt; profoundly: Take the example of flight: The Concord, and artificial
</EM><BR>
<EM>&gt; bird, has no feathers, nor does it flap its wings for lift.
</EM><BR>
<P>I see no reason to think that there is a &quot;raw mind.&quot;  There are some
<BR>
minds, such as they are, but there is nothing out there to purify.
<BR>
(Eliezer and others call this mythical purificant &quot;mindstuff.&quot;)
<BR>
<P>To the extent that I can make this analogy in a totally non-moral way
<BR>
(I'll try), this is the difference between fascist eugenics and
<BR>
transhuman eugenics.  Fascist eugenics tries to breed out impurities,
<BR>
to bring us back to the one pure thing at our center; transhuman
<BR>
eugenics works to create something completely different, in nobody's
<BR>
image in particular.
<BR>
<P>[Again, I don't use this to imply anything morally about you or anyone
<BR>
who agrees with you, but merely to draw the distinction.]
<BR>
<P>-Dan
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-unless you love someone-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;-nothing else makes any sense-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e.e. cummings
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="3413.html">Eugene Leitl: "Re: A new level of sophistication in cyber security"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3411.html">Forrest Bishop: "any minute now"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3407.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3439.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="3439.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3412">[ date ]</A>
<A HREF="index.html#3412">[ thread ]</A>
<A HREF="subject.html#3412">[ subject ]</A>
<A HREF="author.html#3412">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:11:26 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
