<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: What is a codic cortex? (WAS: Coding a Transhuman A</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="What is a codic cortex? (WAS: Coding a Transhuman AI)">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>What is a codic cortex? (WAS: Coding a Transhuman AI)</H1>
<!-- received="Wed May 24 15:44:35 2000" -->
<!-- isoreceived="20000524214435" -->
<!-- sent="Wed, 24 May 2000 16:45:08 -0500" -->
<!-- isosent="20000524214508" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="What is a codic cortex? (WAS: Coding a Transhuman AI)" -->
<!-- id="392C4D46.E8240CF2@pobox.com" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20What%20is%20a%20codic%20cortex?%20(WAS:%20Coding%20a%20Transhuman%20AI)&In-Reply-To=&lt;392C4D46.E8240CF2@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Wed May 24 2000 - 15:45:08 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="3739.html">Eliezer S. Yudkowsky: "Fwd: Privacy threat alert (S.486/HR.2987)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3737.html">ABlainey@aol.com: "Re: A rather odd site"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3738">[ date ]</A>
<A HREF="index.html#3738">[ thread ]</A>
<A HREF="subject.html#3738">[ subject ]</A>
<A HREF="author.html#3738">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
What is a codic cortex?
<BR>
Does Deep Blue have a chess cortex?
<BR>
Does Kasparov have a sensory modality for chess?
<BR>
Is a spreadsheet a budgetary cortex?
<BR>
<P>The analogy between a codic cortex and a visual cortex is intended to be
<BR>
exact.  If there is anything of high-level thought in the visual cortex,
<BR>
the CaTAI model doesn't know about it (*).  The codic cortex doesn't
<BR>
translate goals into designs; it visualizes high-level features of code
<BR>
as low-level features of code.  More importantly, it's about noticing
<BR>
low-level features, then mid-level features, then high-level features. 
<BR>
These features aren't intrinsically goal-oriented any more than the edge
<BR>
of a knife, detected as a contrast between retinal neurons, is a design
<BR>
feature.  First comes the perception of edgeness and sharpness; then
<BR>
comes thoughts about the usefulness and intentionality of sharpness.  A
<BR>
codic cortex might be able to perceive extremely high-level features
<BR>
such as &quot;modularity&quot;, even have intuitions about qualities like
<BR>
&quot;elegance&quot; or at least &quot;density&quot;, without supporting modularity or
<BR>
elegance as goals.
<BR>
<P>A human, looking at a piece of code that plays tic-tac-toe, reads the
<BR>
code line by line and builds up an understanding of the higher
<BR>
structures - conditionals, recursion, search trees.  We use our
<BR>
knowledge, our concepts, about what each piece of code would do, to
<BR>
establish a logical argument that the whole will accomplish some
<BR>
higher-level purpose.  (Like all logical arguments, the code argument
<BR>
can easily be wrong, in which case the code needs to be debugged.)  We
<BR>
move from the code itself, to our knowledge of what each line is
<BR>
supposed to do, to our knowledge of what the whole does, to our
<BR>
knowledge of what the code is supposed to represent (i.e. tic-tac-toe). 
<BR>
Unless the codic structure is very small, we lose track of the
<BR>
correspondences; we are forced to analyze our abstract knowledge of what
<BR>
the code is supposed to do, rather than the code itself.
<BR>
<P>An AI with a codic modality would see all the code at once, and all the
<BR>
higher features at once, to the limits of working memory.  It would have
<BR>
all the non-intentional perceptions that a human programmer has
<BR>
consciously developed - or at least, that's the goal.  If a dereference
<BR>
makes no check for null pointers, that's a feature in the same way that
<BR>
a neural edge is a feature; if the pointers coming in are guaranteed
<BR>
non-null, or guaranteed to refer to some particular class of objects,
<BR>
that is a feature.  Actually, this is still too crystalline to be a true
<BR>
perception.  One way to do it would be to simulate the action of each
<BR>
line of code on, say, a hundred typical-special inputs and a thousand
<BR>
random inputs; this would give a stochastic, distributed image of what
<BR>
each line of code was doing.  And so on.  There are a lot of different
<BR>
methods that can be used; the general idea is to build up a perception
<BR>
of the code.  A codic cortex doesn't write code.  It sees code.  If the
<BR>
AI can imagine seeing a particular thing, the codic cortex can visualize
<BR>
code, but seeing the code - *noticing* the code - comes first.
<BR>
<P>After that comes forming concepts about the code, seeing how the code
<BR>
and the low-level features and the high-level features all correspond to
<BR>
this abstract shape called tic-tac-toe.  Only then can you move from
<BR>
knowing the shape of tic-tac-toe to visualizing a piece of code that
<BR>
fills it.
<BR>
<P>Is an optimizing compiler a codic cortex?  Is a *decompiler* a codic
<BR>
cortex?  No.  A codic cortex would contain some of the same features, do
<BR>
some of the same things, just as a visual cortex can do some of the same
<BR>
things as a CAD/CAM program or a video game.  Does Deep Blue have a
<BR>
sensory modality for chess?  No.  In Deep Blue, the goal and the mental
<BR>
image are identical, not integrated; more importantly, Deep Blue doesn't
<BR>
notice what it sees.  It can't be said to see the chess search tree
<BR>
because it isn't performing feature extraction on the search tree, only
<BR>
the individual boards.  Will Deep Blue notice if there are recurring
<BR>
features in all the chess boards, all the pixels, within the search
<BR>
tree?  Not as I understand Deep Blue's architecture.
<BR>
<P>Above all, a sensory modality is something that exists within a higher
<BR>
system.  A sensory modality by itself is helpless.  Deep Blue wasn't
<BR>
helpless, therefore Deep Blue is not a sensory modality.  As a
<BR>
definitional matter this is a non-sequitur; as a practical matter, it is
<BR>
very fundamentally and deeply true.  If you implement the higher layers
<BR>
directly as code, you can't form genuine, flexible higher layers.
<BR>
<P>Does Kasparov have a chess cortex?  Of course not.  He has a visual
<BR>
cortex.  He has managed to form extremely powerful concepts about the
<BR>
chess representations within his visual cortex.  What Deep Blue does
<BR>
through brute force searches, Kasparov does by thinking about the
<BR>
conceptual layer of the problem.  Kasparov perceives and reasons about
<BR>
deep underlying regularities in the game of chess, rather than the great
<BR>
search tree itself.  Deep Blue and Kasparov are two sides of the coin.
<BR>
<P>Is a spreadsheet a budgetary cortex?  Such uses must be reined in
<BR>
carefully if the term &quot;sensory modality&quot; is not to lose all meaning and
<BR>
become just another convenient buzzword.  Sensory modalities are part of
<BR>
a larger system.  They support concept formation and ultimately
<BR>
introspection.  A spreadsheet does neither.  The fact that a spreadsheet
<BR>
is very well suited to representing budgets may suggest that it would be
<BR>
a fine and profitable thing to have an AI with a genuine budgetary
<BR>
cortex containing some of the same structure as a modern spreadsheet. 
<BR>
But the spreadsheet itself is not a modality, it is a spreadsheet.
<BR>
<PRE>
--
(*):  Some books mention case studies in which people who lose their
entire visual cortex literally don't know that they're blind.  They not
only forget what it is like to see; they forget that the sense of sight
exists.  This is such a radical proposition that I'd want to see the
original research article before depending on its factuality; and, even
if true, it would hopefully only mean that visual thoughts needed to be
routed through the visual cortex, not that they were stored there.  A
similar (though less radical) problem is known to exist in which
neurological problems paralyze part of the body and the paralysis is denied.
-- 
        <A HREF="mailto:sentience@pobox.com?Subject=Re:%20What%20is%20a%20codic%20cortex?%20(WAS:%20Coding%20a%20Transhuman%20AI)&In-Reply-To=&lt;392C4D46.E8240CF2@pobox.com&gt;">sentience@pobox.com</A>    Eliezer S. Yudkowsky
              <A HREF="http://singinst.org/beyond.html">http://singinst.org/beyond.html</A>
</PRE>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="3739.html">Eliezer S. Yudkowsky: "Fwd: Privacy threat alert (S.486/HR.2987)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3737.html">ABlainey@aol.com: "Re: A rather odd site"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3738">[ date ]</A>
<A HREF="index.html#3738">[ thread ]</A>
<A HREF="subject.html#3738">[ subject ]</A>
<A HREF="author.html#3738">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:11:38 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
