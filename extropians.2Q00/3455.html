<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: SITE: Coding a Transhuman AI 2.0a</TITLE>
<META NAME="Author" CONTENT="Dan Fabulich (daniel.fabulich@yale.edu)">
<META NAME="Subject" CONTENT="Re: SITE: Coding a Transhuman AI 2.0a">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: SITE: Coding a Transhuman AI 2.0a</H1>
<!-- received="Sun May 21 04:00:19 2000" -->
<!-- isoreceived="20000521100019" -->
<!-- sent="Sun, 21 May 2000 05:25:01 -0400 (EDT)" -->
<!-- isosent="20000521092501" -->
<!-- name="Dan Fabulich" -->
<!-- email="daniel.fabulich@yale.edu" -->
<!-- subject="Re: SITE: Coding a Transhuman AI 2.0a" -->
<!-- id="Pine.GSO.4.10.10005210420040.18167-100000@morpheus.cis.yale.edu" -->
<!-- inreplyto="00052017274503.00501@localhost.localdomain" -->
<STRONG>From:</STRONG> Dan Fabulich (<A HREF="mailto:daniel.fabulich@yale.edu?Subject=Re:%20SITE:%20Coding%20a%20Transhuman%20AI%202.0a&In-Reply-To=&lt;Pine.GSO.4.10.10005210420040.18167-100000@morpheus.cis.yale.edu&gt;"><EM>daniel.fabulich@yale.edu</EM></A>)<BR>
<STRONG>Date:</STRONG> Sun May 21 2000 - 03:25:01 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="3456.html">Doug Skrecky: "folic acid and vascular health"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3454.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3439.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3465.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="3465.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3455">[ date ]</A>
<A HREF="index.html#3455">[ thread ]</A>
<A HREF="subject.html#3455">[ subject ]</A>
<A HREF="author.html#3455">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Matt Gingell cried out:
<BR>
<P><EM>&gt; Dan Fabulich wrote:
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &gt; Matt Gingell wondered:
</EM><BR>
<EM>&gt; &gt; ...
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &gt; BTW, the fact that no such Holy Grail exists also provides a plausible
</EM><BR>
<EM>&gt; &gt; explanation as to why AI has failed so often in the past.  In an
</EM><BR>
<EM>&gt; &gt; important sense, were you right about what intelligence is like, AI
</EM><BR>
<EM>&gt; &gt; would be easier than it is harder.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; How many attempts at heavier than air flight failed before we figured
</EM><BR>
<EM>&gt; that out? Just because we haven't found an answer yet or we haven't
</EM><BR>
<EM>&gt; got a fast enough machine to try it (a light enough engine) doesn't
</EM><BR>
<EM>&gt; mean it isn't out there. We've only had computers for 50 years and
</EM><BR>
<EM>&gt; already we've done things that even a hundred years ago would have
</EM><BR>
<EM>&gt; been generally thought impossible. Give AI a break. Obviously there
</EM><BR>
<EM>&gt; are huge holes left to be filled and principles yet to be discovered,
</EM><BR>
<EM>&gt; but it's a very young science.
</EM><BR>
<P>That claim was really just a sidebar designed to give my main argument
<BR>
more plausibility.  On its own, this explanation, as given, won't fly.
<BR>
<P><EM>&gt; &gt; Look, suppose you WERE somehow able to map out a somewhat
</EM><BR>
<EM>&gt; &gt; comprehensive list of possible conceptual schemes which you would use
</EM><BR>
<EM>&gt; &gt; to categorize the &quot;raw sense data.&quot;  How could you algorithmically
</EM><BR>
<EM>&gt; &gt; determine which of these conceptual schemes worked better than some
</EM><BR>
<EM>&gt; &gt; others?  Any others?  Our ancestors had a way: use it as a rule for
</EM><BR>
<EM>&gt; &gt; action, see if it helps you breed.  You and your machines, even at
</EM><BR>
<EM>&gt; &gt; 10^21 ops/sec, would have nothing to test your values against.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; When Newton developed with theory of gravitation, did he iterate
</EM><BR>
<EM>&gt; through the space of as possible physical laws till he found one that
</EM><BR>
<EM>&gt; matched his data? You seem to still be convinced that learning,
</EM><BR>
<EM>&gt; discovering patterns in facts, is a blind search.
</EM><BR>
<P>No.  Newton employed conceptual tools which his ancestors had stumbled
<BR>
upon and compared his results against those.  These were, as the
<BR>
saying goes, necessary, but not sufficient.  Yet they were necessary;
<BR>
he couldn't have done it without them.  Shoulders of giants and whatnot.
<BR>
<P><EM>&gt; A concept scheme is a theory about the world, a model of the way
</EM><BR>
<EM>&gt; things in the world work and the sort of laws they obey. Some ways of
</EM><BR>
<EM>&gt; looking at the world are objectively better than others, regardless of
</EM><BR>
<EM>&gt; their utility as a tool for perpetuating your own genes. Intelligence
</EM><BR>
<EM>&gt; is that which extracts those models from raw data - Feedback with the
</EM><BR>
<EM>&gt; world is a useful tool for finding those them, but it isn't the
</EM><BR>
<EM>&gt; only one.
</EM><BR>
<P>Now, you might be making a number of different kinds of claims when
<BR>
you say this; I would agree with you if you're making one kind of
<BR>
argument, and mostly disagree with you if you're making a certain
<BR>
other kind of argument.
<BR>
<P>One kind of argument you might be making goes like this: some
<BR>
conceptual schemes are useful independently of their utility in
<BR>
breeding, because there are *other* purposes we might have for a
<BR>
conceptual scheme which can justify one conceptual scheme over
<BR>
another, despite evolutionary disadvantages.  I'd have to agree with
<BR>
this.  I'm only making the smaller claim that the reason we happen to
<BR>
have the belief-fixing faculties we do is because they helped our
<BR>
ancestors breed; that this just happens to be the way Newton did it.
<BR>
<P>Another kind of argument you might be making goes like this: some
<BR>
conceptual schemes are just Truly Right, independent of any purpose we
<BR>
might have for them today or ever.  (This sounds more like the
<BR>
argument you're actually making.) I don't think making a statement
<BR>
like this matters.  Certainly, there are some conceptual schemes which
<BR>
are just right for the purposes which we have now, and we largely have
<BR>
to assume that we're mostly Right about our beliefs and purposes.  So
<BR>
we're going to have the beliefs we've got, whether we are in touch with
<BR>
the One Truth or whether they just suit our purposes.
<BR>
<P>With that having been said, however, I'd say you're on the wrong track
<BR>
to think that a &quot;pure mind&quot; abstracted from any goals would share our
<BR>
beliefs.  Better to say that we've got the right intentions, we've got
<BR>
the right purpose, and that any machine built to that purpose would
<BR>
also stumble across the same means of fulfilling it as we do.
<BR>
<P>Visions of elegance, simplicity, etc. are excellent.  I share them
<BR>
with you.  However, we got OUR beliefs about elegance through
<BR>
evolution; maybe that got us in touch with the one true Platonic
<BR>
Beauty; maybe it didn't.  Either way, there's no reason to think that
<BR>
an AI will stumble across Ockham's Razor and find it right for its own
<BR>
purposes (which it may or may not think of as 'objective') unless it
<BR>
shares ours, (in which case, they'll have to be hand coded in, at
<BR>
least at first) because being right is no explanation for how a mind
<BR>
comes to know something.  If you asked me &quot;how did you know that she
<BR>
was a brunette?&quot; and I replied &quot;because I was right,&quot; I'd have missed
<BR>
your point completely, wouldn't I?
<BR>
<P><EM>&gt; Heres a simple example of the sort of thing I'm talking about: 
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Suppose there exists some Vast set of character strings, and I've
</EM><BR>
<EM>&gt; posed you the challenge of characterizing that set from some finite
</EM><BR>
<EM>&gt; number of examples. The sample you've got contains instances like: (ab),
</EM><BR>
<EM>&gt; (aab), (abb), (aaabbb), (abbb), etc. 
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; The answer is obvious, or at least it would be with enough samples:
</EM><BR>
<EM>&gt; this is the set of strings beginning with one or more instances of 'a'
</EM><BR>
<EM>&gt; followed by one or more instances of 'b.' Of course, any other answer
</EM><BR>
<EM>&gt; is defensible: you could say we're looking at the set of all strings
</EM><BR>
<EM>&gt; built of characters in the alphabet and we just got a misleading
</EM><BR>
<EM>&gt; sample. Or you could say this set only contains the examples you've
</EM><BR>
<EM>&gt; seen. Both those answers are wrong though, in the same way epicycles
</EM><BR>
<EM>&gt; are wrong. It's my contention that there exists some general algorithm
</EM><BR>
<EM>&gt; for determining those 'right' answers.
</EM><BR>
<P>If it's an algorithm, it's incomplete.  There will be some undecidable
<BR>
questions, which are decidable on another stronger algorithm.  This
<BR>
may not bother you, but it should tell you that Truth is not an
<BR>
algorithm, that it cannot be reached, or even defined,
<BR>
algorithmically.
<BR>
<P>Epistemologically speaking, how would we know if we had stumbled upon
<BR>
the general algorithm, or whether we were just pursuing our own
<BR>
purposes again?  For that matter, why would we care?  Why not call our
<BR>
own beliefs Right out of elegance and get on with Coding a Transhuman
<BR>
AI?
<BR>
<P><EM>&gt; &gt; Consider a search space in which you're trying to find local maximums.
</EM><BR>
<EM>&gt; &gt; Now imagine trying to do it without any idea of the height of any
</EM><BR>
<EM>&gt; &gt; point in the space.  Now try throwing 10^100 ops at the project.
</EM><BR>
<EM>&gt; &gt; Doesn't help, does it?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; You do have a criterion: The representation of a theory should be as
</EM><BR>
<EM>&gt; small as possible, and it should generalize as little as possible while
</EM><BR>
<EM>&gt; describing as many examples as possible. It's Occam's Razor. I'll read
</EM><BR>
<EM>&gt; up on seed AI if you agree to read up on unsupervised learning
</EM><BR>
<EM>&gt; (learning without feedback or tagged examples.)
</EM><BR>
<P>Ahem.  And WHY do we have Ockham's Razor?  I've got my story.  What's
<BR>
yours?  Surely not &quot;because we're right about it&quot;?  That's missing the
<BR>
point.
<BR>
<P>Show me the faculty, I'll show you the feedback.  (Though I may have
<BR>
to point to history to do it.)
<BR>
<P><EM>&gt; &gt; I see no reason to think that there is a &quot;raw mind.&quot;  There are some
</EM><BR>
<EM>&gt; &gt; minds, such as they are, but there is nothing out there to purify.
</EM><BR>
<EM>&gt; &gt; (Eliezer and others call this mythical purificant &quot;mindstuff.&quot;)
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; A heart is a pump, an eye is a camera, why can't a brain be a baroque
</EM><BR>
<EM>&gt; biological instance of something simpler and more abstract?
</EM><BR>
<P>What's a raw pump?  What's a pure camera?  I think I don't see your point.
<BR>
<P><EM>&gt; &gt; To the extent that I can make this analogy in a totally non-moral way
</EM><BR>
<EM>&gt; &gt; (I'll try), this is the difference between fascist eugenics and
</EM><BR>
<EM>&gt; &gt; transhuman eugenics.  Fascist eugenics tries to breed out impurities,
</EM><BR>
<EM>&gt; &gt; to bring us back to the one pure thing at our center; transhuman
</EM><BR>
<EM>&gt; &gt; eugenics works to create something completely different, in nobody's
</EM><BR>
<EM>&gt; &gt; image in particular.
</EM><BR>
<EM>&gt; &gt; 
</EM><BR>
<EM>&gt; &gt; [Again, I don't use this to imply anything morally about you or anyone
</EM><BR>
<EM>&gt; &gt; who agrees with you, but merely to draw the distinction.]
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Thanks for qualifying that, but it's still a hell of a loaded
</EM><BR>
<EM>&gt; analogy. I prefer to think of blank-slate intelligence as a
</EM><BR>
<EM>&gt; egalitarian notion: we are all the same, differing only in our
</EM><BR>
<EM>&gt; experience and hardware resources, be we human, alien, or 
</EM><BR>
<EM>&gt; machine. The politics is irrelevant to the question, of course, but 
</EM><BR>
<EM>&gt; I'd still rather not be called a Nazi.
</EM><BR>
<P>I didn't intend to &quot;call you a Nazi.&quot;  One can share some beliefs with
<BR>
the Nazis without sharing all of them, and without suffering from any
<BR>
moral problems as a result.  I share lots of beliefs with the Nazis,
<BR>
but I also disagree with them on a variety of substantial issues.  I'm
<BR>
sure you do too.  The interesting thing to note here is not that it's
<BR>
the fascists who said it, but that the distinction exists and is
<BR>
interesting.
<BR>
<P>-Dan
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-unless you love someone-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;-nothing else makes any sense-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e.e. cummings
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="3456.html">Doug Skrecky: "folic acid and vascular health"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3454.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3439.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3465.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="3465.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3455">[ date ]</A>
<A HREF="index.html#3455">[ thread ]</A>
<A HREF="subject.html#3455">[ subject ]</A>
<A HREF="author.html#3455">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:11:28 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
