<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: SITE: Coding a Transhuman AI 2.0a</TITLE>
<META NAME="Author" CONTENT="Dan Fabulich (daniel.fabulich@yale.edu)">
<META NAME="Subject" CONTENT="Re: SITE: Coding a Transhuman AI 2.0a">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: SITE: Coding a Transhuman AI 2.0a</H1>
<!-- received="Sat May 20 14:50:49 2000" -->
<!-- isoreceived="20000520205049" -->
<!-- sent="Sat, 20 May 2000 16:51:20 -0400 (EDT)" -->
<!-- isosent="20000520205120" -->
<!-- name="Dan Fabulich" -->
<!-- email="daniel.fabulich@yale.edu" -->
<!-- subject="Re: SITE: Coding a Transhuman AI 2.0a" -->
<!-- id="Pine.GSO.4.10.10005201610130.28782-100000@morpheus.cis.yale.edu" -->
<!-- inreplyto="200005201511.IAA29458@finney.org" -->
<STRONG>From:</STRONG> Dan Fabulich (<A HREF="mailto:daniel.fabulich@yale.edu?Subject=Re:%20SITE:%20Coding%20a%20Transhuman%20AI%202.0a&In-Reply-To=&lt;Pine.GSO.4.10.10005201610130.28782-100000@morpheus.cis.yale.edu&gt;"><EM>daniel.fabulich@yale.edu</EM></A>)<BR>
<STRONG>Date:</STRONG> Sat May 20 2000 - 14:51:20 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="3439.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3437.html">QueeneMUSE@aol.com: "Re: Can I kill the &quot;original&quot;?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3419.html">hal@finney.org: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3440.html">Natasha Vita-More: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3438">[ date ]</A>
<A HREF="index.html#3438">[ thread ]</A>
<A HREF="subject.html#3438">[ subject ]</A>
<A HREF="author.html#3438">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Hal Finney challenged:
<BR>
<P><EM>&gt; Eliezer's coding cortext doesn't seem much more likely.  I don't see
</EM><BR>
<EM>&gt; how it could do more than strictly localized parsing and improvement,
</EM><BR>
<EM>&gt; technologies we already have.  Beyond that requires a deep understanding
</EM><BR>
<EM>&gt; of the purpose of code.  An unintelligent brain fragment won't be able
</EM><BR>
<EM>&gt; to achieve this.
</EM><BR>
<P>An unintelligent brain fragment could do part of the work, but it
<BR>
couldn't give itself something to do.  It needs the rest of the brain
<BR>
to do that.
<BR>
<P><EM>&gt; Is the coding cortext supposed to be more than an optimizing compiler?
</EM><BR>
<EM>&gt; If so, I'd like to hear what it is going to be able to do, and how it
</EM><BR>
<EM>&gt; will do it.
</EM><BR>
<P>The analogy between this an the intelligent coffee maker doesn't
<BR>
really hold.  This AI does have it as a goal to enhance its
<BR>
intelligence, but it won't have &quot;enhance intelligence&quot; as a bare goal,
<BR>
like &quot;make good coffee:&quot; it'll be informed in a very rich manner what
<BR>
it's trying to maximize.  Much of this will probably be done by hand.
<BR>
<P>Eliezer treats this objection in a section in which he challenges the
<BR>
idea that intelligence enhancement is a self-referential goal.
<BR>
<P>Quoting Eliezer from CaTAI:
<BR>
<P><EM>&gt; A surprisingly frequent objection to self-enhancement is that
</EM><BR>
<EM>&gt; intelligence, when defined as &quot;the ability to increase
</EM><BR>
<EM>&gt; intelligence&quot;, is a circular definition - one which would, they say,
</EM><BR>
<EM>&gt; result in a sterile and uninteresting AI.  Even if this were the
</EM><BR>
<EM>&gt; definition (it isn't), and the definition were circular (it wouldn't
</EM><BR>
<EM>&gt; be), the cycle could be broken simply by grounding the definition in
</EM><BR>
<EM>&gt; chess-playing ability or some similar test of ability.  However,
</EM><BR>
<EM>&gt; intelligence is not defined as the ability to increase intelligence;
</EM><BR>
<EM>&gt; that is simply the form of intelligent behavior we are most
</EM><BR>
<EM>&gt; interested in.  Intelligence is not defined at all.  What
</EM><BR>
<EM>&gt; intelligence is, if you look at a human, is more than a hundred
</EM><BR>
<EM>&gt; cytoarchitecturally (2) distinct areas of the brain, all of which
</EM><BR>
<EM>&gt; work together to create intelligence.  Intelligence is, in short,
</EM><BR>
<EM>&gt; modular, and the tasks performed by individual modules are different
</EM><BR>
<EM>&gt; in kind from the nature of the overall intelligence.  If the overall
</EM><BR>
<EM>&gt; intelligence can turn around and look at a module as an isolated
</EM><BR>
<EM>&gt; process, it can make clearly defined performance improvements -
</EM><BR>
<EM>&gt; improvements that eventually sum up to improved overall intelligence
</EM><BR>
<EM>&gt; - without ever confronting the circular problem of &quot;making myself
</EM><BR>
<EM>&gt; more intelligent&quot;.  Intelligence, from a design perspective, is a
</EM><BR>
<EM>&gt; goal with many, many subgoals.  An intelligence seeking the goal of
</EM><BR>
<EM>&gt; improved intelligence does not confront &quot;improved intelligence&quot; as a
</EM><BR>
<EM>&gt; naked fact, but a very rich and complicated fact adorned with less
</EM><BR>
<EM>&gt; complicated subgoals.
</EM><BR>
<P>The idea here is that a &quot;code&quot; cortex COULD get a handle on what it
<BR>
was trying to work on by virtue of the fact that we've hand coded in a
<BR>
lot of intelligence, unlike a good coffee maker with no clear idea as
<BR>
to what constitutes &quot;good coffee.&quot;
<BR>
<P>Part of what's easy to react strongly against in Eliezer's account is
<BR>
the tendency to assume that, like every other self-proclaimed
<BR>
visionary in AI, he's found the one simple thing you have to do in
<BR>
order to get AI (&quot;all you need is a code cortex...&quot;).  He hasn't.  If
<BR>
anything, he's found the many hard things you have to do in order to
<BR>
solve the hardest questions we've ever asked.  Sometimes I wonder if
<BR>
his idea isn't just the claim that, once we hand code an AI,
<BR>
transhuman AI will be the easy part.  (Of course, even this is a claim
<BR>
which can be strongly doubted.)
<BR>
<P>-Dan
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-unless you love someone-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;-nothing else makes any sense-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e.e. cummings
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="3439.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3437.html">QueeneMUSE@aol.com: "Re: Can I kill the &quot;original&quot;?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3419.html">hal@finney.org: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3440.html">Natasha Vita-More: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3438">[ date ]</A>
<A HREF="index.html#3438">[ thread ]</A>
<A HREF="subject.html#3438">[ subject ]</A>
<A HREF="author.html#3438">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:11:27 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
