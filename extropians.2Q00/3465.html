<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: SITE: Coding a Transhuman AI 2.0a</TITLE>
<META NAME="Author" CONTENT="Matt Gingell (mjg223@is7.nyu.edu)">
<META NAME="Subject" CONTENT="Re: SITE: Coding a Transhuman AI 2.0a">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: SITE: Coding a Transhuman AI 2.0a</H1>
<!-- received="Sun May 21 10:49:03 2000" -->
<!-- isoreceived="20000521164903" -->
<!-- sent="Sun, 21 May 2000 12:46:49 -0400" -->
<!-- isosent="20000521164649" -->
<!-- name="Matt Gingell" -->
<!-- email="mjg223@is7.nyu.edu" -->
<!-- subject="Re: SITE: Coding a Transhuman AI 2.0a" -->
<!-- id="00052112512200.00491@localhost.localdomain" -->
<!-- inreplyto="Pine.GSO.4.10.10005210420040.18167-100000@morpheus.cis.yale.edu" -->
<STRONG>From:</STRONG> Matt Gingell (<A HREF="mailto:mjg223@is7.nyu.edu?Subject=Re:%20SITE:%20Coding%20a%20Transhuman%20AI%202.0a&In-Reply-To=&lt;00052112512200.00491@localhost.localdomain&gt;"><EM>mjg223@is7.nyu.edu</EM></A>)<BR>
<STRONG>Date:</STRONG> Sun May 21 2000 - 10:46:49 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="3466.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3464.html">Zero Powers: "Re: transparancy / traffic cameras"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3455.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3509.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="3509.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3465">[ date ]</A>
<A HREF="index.html#3465">[ thread ]</A>
<A HREF="subject.html#3465">[ subject ]</A>
<A HREF="author.html#3465">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Dan Fabulich wrote:
<BR>
<P><EM>&gt; Another kind of argument you might be making goes like this: some
</EM><BR>
<EM>&gt; conceptual schemes are just Truly Right, independent of any purpose we
</EM><BR>
<EM>&gt; might have for them today or ever.  (This sounds more like the
</EM><BR>
<EM>&gt; argument you're actually making.) I don't think making a statement
</EM><BR>
<EM>&gt; like this matters.  Certainly, there are some conceptual schemes which
</EM><BR>
<EM>&gt; are just right for the purposes which we have now, and we largely have
</EM><BR>
<EM>&gt; to assume that we're mostly Right about our beliefs and purposes.  So
</EM><BR>
<EM>&gt; we're going to have the beliefs we've got, whether we are in touch with
</EM><BR>
<EM>&gt; the One Truth or whether they just suit our purposes.
</EM><BR>
<P>Well, take something like geometry: is it true that the interior
<BR>
angles of a triangle always add up to 180, or is that just an
<BR>
arbitrary decision we made because it happens to suit our particular
<BR>
purposes?
<BR>
<P>Consider this message: you're looking at a bunch of phosphors lit up
<BR>
on a screen, does it have any information content outside our
<BR>
idiosyncratic conventions? Independent of our desire to communicate,
<BR>
is any way of perceiving it as good as any other? I would say it has
<BR>
structure: that it is built from symbols drawn from a finite alphabet
<BR>
and that this is true regardless of the perceivers goal.
<BR>
<P>This is where the criterion of minimum description length comes in: if
<BR>
I generalize a little bit and allow pixels some fuzziness, then I can
<BR>
rerepresent this message at 7 bits per symbol - which is a much
<BR>
smaller encoding than a bitmap. This is a nice evaluation function
<BR>
for a hypothesis because it doesn't require feedback with the outside
<BR>
world. With a big enough sample I can get space savings classifying
<BR>
common strings into words, and then lists into structured instances of
<BR>
a grammar.
<BR>
<P>If we are to understand what intelligence is, we must construct a
<BR>
definition which is not particular to a design process, a set of
<BR>
goals, or a set of sensors and limbs. Implicit in this statement is
<BR>
the notion that the word 'intelligent' actually means something, that
<BR>
there's a meaningful line somewhere between intelligence and clever
<BR>
algorithms which fake universality by virtue of shear vastness.
<BR>
<P><EM>&gt; With that having been said, however, I'd say you're on the wrong track
</EM><BR>
<EM>&gt; to think that a &quot;pure mind&quot; abstracted from any goals would share our
</EM><BR>
<EM>&gt; beliefs.  Better to say that we've got the right intentions, we've got
</EM><BR>
<EM>&gt; the right purpose, and that any machine built to that purpose would
</EM><BR>
<EM>&gt; also stumble across the same means of fulfilling it as we do.
</EM><BR>
<P>Minds are imperfect and heuristic, they only approximate a truth which
<BR>
is, as you point out, uncomputable. A machine might out do us, as
<BR>
Newton was out done by Einstein, by finding a better model than
<BR>
ours. But any intelligent machine would have a concept of, say,
<BR>
integer at least as an special case of something (perhaps vastly)
<BR>
broader.
<BR>
<P><EM>&gt; Visions of elegance, simplicity, etc. are excellent.  I share them
</EM><BR>
<EM>&gt; with you.  However, we got OUR beliefs about elegance through
</EM><BR>
<EM>&gt; evolution; maybe that got us in touch with the one true Platonic
</EM><BR>
<EM>&gt; Beauty; maybe it didn't.  Either way, there's no reason to think that
</EM><BR>
<EM>&gt; an AI will stumble across Ockham's Razor and find it right for its own
</EM><BR>
<EM>&gt; purposes (which it may or may not think of as 'objective') unless it
</EM><BR>
<EM>&gt; shares ours, (in which case, they'll have to be hand coded in, at
</EM><BR>
<EM>&gt; least at first) because being right is no explanation for how a mind
</EM><BR>
<EM>&gt; comes to know something.  If you asked me &quot;how did you know that she
</EM><BR>
<EM>&gt; was a brunette?&quot; and I replied &quot;because I was right,&quot; I'd have missed
</EM><BR>
<EM>&gt; your point completely, wouldn't I?
</EM><BR>
<P>Ockham's razor would be one of the core principles of the general
<BR>
purpose learning system I'm interested in - hand coded rather than
<BR>
acquired, though not necessarily explicitly. Something is wired in,
<BR>
obviously I don't think you can just take a blank Turning machine tape
<BR>
and expect it to do something useful.
<BR>
<P><EM>&gt; If it's an algorithm, it's incomplete.  There will be some undecidable
</EM><BR>
<EM>&gt; questions, which are decidable on another stronger algorithm.  This
</EM><BR>
<EM>&gt; may not bother you, but it should tell you that Truth is not an
</EM><BR>
<EM>&gt; algorithm, that it cannot be reached, or even defined,
</EM><BR>
<EM>&gt; algorithmically.
</EM><BR>
<P>Sure - but this isn't a practical problem anymore than the
<BR>
incompleteness of number theory makes math useless. It's a good
<BR>
objection though. Saying intelligence is an algorithm was sloppy of
<BR>
me, I should say rather that intelligence is that which approximates
<BR>
some particular uncomputable function in a tractable way. This opens
<BR>
up the possibility of multiple viable solutions.
<BR>
<P><EM>&gt; Epistemologically speaking, how would we know if we had stumbled upon
</EM><BR>
<EM>&gt; the general algorithm, or whether we were just pursuing our own
</EM><BR>
<EM>&gt; purposes again?  For that matter, why would we care?  Why not call our
</EM><BR>
<EM>&gt; own beliefs Right out of elegance and get on with Coding a Transhuman
</EM><BR>
<EM>&gt; AI?
</EM><BR>
<P>We couldn't know, but if we got good results then we'd be pretty sure
<BR>
we were at least close. Whether you care depends on your motivation:
<BR>
I'm interested in intelligence because I like general solutions to
<BR>
problems more than I like special case ones, and you don't get a more
<BR>
general solution than AI. I futz around with this stuff out of
<BR>
intellectual curiosity, if the mind turns out to be necessarily ugly
<BR>
I'll go do something else. I don't really care about saving the world 
<BR>
from grey goo or the future of the human race or whatever.
<BR>
<P><EM>&gt; &gt; &gt; Consider a search space in which you're trying to find local maximums.
</EM><BR>
<EM>&gt; &gt; &gt; Now imagine trying to do it without any idea of the height of any
</EM><BR>
<EM>&gt; &gt; &gt; point in the space.  Now try throwing 10^100 ops at the project.
</EM><BR>
<EM>&gt; &gt; &gt; Doesn't help, does it?
</EM><BR>
<EM>&gt; &gt; 
</EM><BR>
<EM>&gt; &gt; You do have a criterion: The representation of a theory should be as
</EM><BR>
<EM>&gt; &gt; small as possible, and it should generalize as little as possible while
</EM><BR>
<EM>&gt; &gt; describing as many examples as possible. It's Occam's Razor. I'll read
</EM><BR>
<EM>&gt; &gt; up on seed AI if you agree to read up on unsupervised learning
</EM><BR>
<EM>&gt; &gt; (learning without feedback or tagged examples.)
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Ahem.  And WHY do we have Ockham's Razor?  I've got my story.  What's
</EM><BR>
<EM>&gt; yours?  Surely not &quot;because we're right about it&quot;?  That's missing the
</EM><BR>
<EM>&gt; point.
</EM><BR>
<P>We have it because it works, as a result of evolutionary feedback. 
<BR>
That doesn't mean it can't be captured in a simple way though - like 
<BR>
I said, that's one of the things I'd expect to hardwire.
<BR>
<P><EM>&gt; What's a raw pump?  What's a pure camera?  I think I don't see your point.
</EM><BR>
<P>We can build a simple artificial heart that can serve as a reasonable
<BR>
replacement for the real thing - it doesn't have to be constructed out
<BR>
of self replicating machines, it doesn't require millions of years of
<BR>
design work, etc. Its important property is that it moves blood around -
<BR>
it's other features are incidental. My point is that the heart is a very
<BR>
complicated instance of a fairly simple idea, and so might the brain be.
<BR>
<P><EM>&gt; I didn't intend to &quot;call you a Nazi.&quot;  One can share some beliefs with
</EM><BR>
<EM>&gt; the Nazis without sharing all of them, and without suffering from any
</EM><BR>
<EM>&gt; moral problems as a result.  I share lots of beliefs with the Nazis,
</EM><BR>
<EM>&gt; but I also disagree with them on a variety of substantial issues.  I'm
</EM><BR>
<EM>&gt; sure you do too.  The interesting thing to note here is not that it's
</EM><BR>
<EM>&gt; the fascists who said it, but that the distinction exists and is
</EM><BR>
<EM>&gt; interesting.
</EM><BR>
<P>Ah - so I sound a bit like Hitler, but that's ok since Hitler said lots
<BR>
of reasonable things in addition to the not-quite-so reasonable things
<BR>
he's more commonly associated with? Fair enough. I'll spare you my
<BR>
righteous indignation.
<BR>
<P>(I also like beer and sausages.)
<BR>
<P>-matt
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="3466.html">Matt Gingell: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3464.html">Zero Powers: "Re: transparancy / traffic cameras"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3455.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3509.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="3509.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3465">[ date ]</A>
<A HREF="index.html#3465">[ thread ]</A>
<A HREF="subject.html#3465">[ subject ]</A>
<A HREF="author.html#3465">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:11:28 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
