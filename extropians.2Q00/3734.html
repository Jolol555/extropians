<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: SITE: Coding a Transhuman AI 2.0a</TITLE>
<META NAME="Author" CONTENT="mjg223 (mjg223@is7.nyu.edu)">
<META NAME="Subject" CONTENT="Re: SITE: Coding a Transhuman AI 2.0a">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: SITE: Coding a Transhuman AI 2.0a</H1>
<!-- received="Wed May 24 11:47:06 2000" -->
<!-- isoreceived="20000524174706" -->
<!-- sent="Wed, 24 May 2000 13:47:00 -0400 (EDT)" -->
<!-- isosent="20000524174700" -->
<!-- name="mjg223" -->
<!-- email="mjg223@is7.nyu.edu" -->
<!-- subject="Re: SITE: Coding a Transhuman AI 2.0a" -->
<!-- id="Pine.OSF.4.21.0005241344540.31988-100000@is7.nyu.edu" -->
<!-- inreplyto="Pine.GSO.4.10.10005222221250.15546-100000@morpheus.cis.yale.edu" -->
<STRONG>From:</STRONG> mjg223 (<A HREF="mailto:mjg223@is7.nyu.edu?Subject=Re:%20SITE:%20Coding%20a%20Transhuman%20AI%202.0a&In-Reply-To=&lt;Pine.OSF.4.21.0005241344540.31988-100000@is7.nyu.edu&gt;"><EM>mjg223@is7.nyu.edu</EM></A>)<BR>
<STRONG>Date:</STRONG> Wed May 24 2000 - 11:47:00 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="3735.html">James Rogers: "Re: Musical note recognition ?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3733.html">Anders Sandberg: "Re: Fw: Will Florida Outlaw Naturopathy?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3580.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3419.html">hal@finney.org: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3734">[ date ]</A>
<A HREF="index.html#3734">[ thread ]</A>
<A HREF="subject.html#3734">[ subject ]</A>
<A HREF="author.html#3734">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Between puffs on the ontological crack pipe, Dan managed:
<BR>
<P><EM>&gt; [big old snip]
</EM><BR>
<P><EM>&gt; ... but despite our differing motivations, we will, at least, agree on
</EM><BR>
<EM>&gt; the Facts, right?  What are these except the beliefs which we cannot
</EM><BR>
<EM>&gt; imagine ourselves rejecting?  How could we tell the difference between
</EM><BR>
<EM>&gt; the two?  Why would we care about such a difference?
</EM><BR>
<P>I believe in an external reality which exists independent of our ability
<BR>
to know or perceive it. A fact isn't a belief we're really sure about,
<BR>
it's an accurate statement about what's going on outside your head.
<BR>
<P>How you gauge its accuracy is another question. On the whole, I'd tend
<BR>
to go with 'ability to predict,' rather than 'usefulness.' 
<BR>
<P><EM>&gt; I'm not pulling this &quot;radical interpreter&quot; stuff out of a hat, as it
</EM><BR>
<EM>&gt; were.  Have you read Quine on this question?
</EM><BR>
<P>I haven't read Quine on any question, I must confess. Not part of the
<BR>
engineering curriculum. I tried doing some Philosophy of Mind once. I
<BR>
got about as far as 'do we have minds when we're asleep' before going
<BR>
out for a cigarette and never coming back. 
<BR>
<P><EM>&gt; &gt; If motivation is made up but reality isn't, then it seems better to
</EM><BR>
<EM>&gt; &gt; describe the mind as something that parses the real world than a hack
</EM><BR>
<EM>&gt; &gt; that keeps you alive. 
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I'm rejecting the notion that a useful distinction can be made between
</EM><BR>
<EM>&gt; realism and anti-realism.  I'm a pragmatist about that question: it
</EM><BR>
<EM>&gt; just doesn't matter.  The whole distinction between &quot;made up&quot; and &quot;not
</EM><BR>
<EM>&gt; made up&quot; in this context is a useless artifact: there are no answers
</EM><BR>
<EM>&gt; there, and no need for answers.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Will you wear a different tie based on whether you have free will or
</EM><BR>
<EM>&gt; not?  Will you build a bridge differently on the basis of whether your
</EM><BR>
<EM>&gt; worldview is right for your own purposes or objectively right?  Will
</EM><BR>
<EM>&gt; you behave any differently at all if you decide that P is a
</EM><BR>
<EM>&gt; proposition you believe unflinchingly or if you think that P is a
</EM><BR>
<EM>&gt; Fact?
</EM><BR>
<P>If my purpose is to construct a bridge that doesn't fall down, and I
<BR>
have a theory which describes the world well enough that I can, then
<BR>
I'd say my theory is at least somewhat 'right.' 'Somewhat' because all
<BR>
we ever have are models, which reflect either more or less what's
<BR>
really going on.
<BR>
<P><EM>&gt; You won't even build an AI differently, I argue.  This question is
</EM><BR>
<EM>&gt; totally irrelevant as to whether there is a simple generic
</EM><BR>
<EM>&gt; truth-finding algoritm that we're running, or whether there is a nasty
</EM><BR>
<EM>&gt; complicated mess leading us to our conclusions.  We are willing to
</EM><BR>
<EM>&gt; agree that anything that follows our algorithm (or one like it) will
</EM><BR>
<EM>&gt; reach our conclusions.  We're even willing to agree that our algorithm
</EM><BR>
<EM>&gt; is mostly right.  The only added claim, a useless one, is that we got
</EM><BR>
<EM>&gt; to have this algorithm, and not another, because it's right.  I don't
</EM><BR>
<EM>&gt; see what you get out of saying this.
</EM><BR>
<P>We have come pretty far afield. I don't really want to talk about
<BR>
whether the chair's there or not. 
<BR>
<P>I don't know what you mean 'we've got to have this algorithm, and not
<BR>
another, because it's right.' What did I say that sounds like that?
<BR>
<P><EM>&gt; How is this different from drawing a line in the epistemological sand
</EM><BR>
<EM>&gt; and saying &quot;No uncertainty past this point!&quot;  I can always raise
</EM><BR>
<EM>&gt; useless questions like &quot;how do you know that's a structure, rather
</EM><BR>
<EM>&gt; than some arbitrary set?&quot;  Or is that a structure simply because you
</EM><BR>
<EM>&gt; CALL it a structure?
</EM><BR>
<P>I don't know, dude. If the Chinese nation dreams it's a butterfly
<BR>
flapping it's wings, does anyone get wet in the virtual rainstorm six
<BR>
months later? I know stepping outside the system makes you feel all
<BR>
bad an' shit, but if we're going to have a conversation we've got to
<BR>
have some base set to ground arguments in. 
<BR>
<P><EM>&gt; &gt; Out of curiosity, how would you explain Kasparov's ability to play a
</EM><BR>
<EM>&gt; &gt; decent game of chess against a computer analyzing 200 million
</EM><BR>
<EM>&gt; &gt; positions a second? Certainly not a regular occurrence on the plains
</EM><BR>
of
<BR>
<EM>&gt; &gt; Africa, what more general facility does it demonstrate?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Eh?  It can't just be his &quot;chess-playing&quot; facility?  I'm not aware of
</EM><BR>
<EM>&gt; anything *else* at which Kasparov is the best in the world, suggesting
</EM><BR>
<EM>&gt; that it is his chess-playing facility, and, apparently, nothing else,
</EM><BR>
<EM>&gt; that did the work.  :)
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Were I to assume that something more general was, in fact, at work,
</EM><BR>
<EM>&gt; I'd have to guess that his capacities to plan ahead, imagine in a
</EM><BR>
<EM>&gt; structured manner, and empathize with his opponent were all at work.
</EM><BR>
<EM>&gt; Shot in the dark on my part...
</EM><BR>
<P>Well it can't be his &quot;chess-playing&quot; facility, since there was never
<BR>
any pressure to evolve one. I must be something else more general,
<BR>
which was selected for but also happens to be applicable to
<BR>
chess. Monkeys didn't sit around playing tournaments for millions of
<BR>
years, or if they did it certainly didn't help them pass on their
<BR>
genes. (I was in a chess club in high-school. Believe me, no
<BR>
reproductive advantage there.)
<BR>
<P>Capacity to plan is one facility almost certainly involved, but he can't
<BR>
do nearly as complete a search as the machine. 'imagine in a
<BR>
structured manner' is provocative but too vague to comment on. I'm not
<BR>
clear what advantage is to be from empathy for a computer.
<BR>
<P><EM>&gt; &gt; This is a very anthropomorphic view - I'm looking for a definition
</EM><BR>
<EM>&gt; &gt; that transcends humans and evolution, the essential properties shared
</EM><BR>
<EM>&gt; &gt; by all possible intelligences. You seem to be saying there isn't such
</EM><BR>
<EM>&gt; &gt; a thing - or it's the null set.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; No *interesting* properties, other than stuff like &quot;X can pass the
</EM><BR>
<EM>&gt; Turing test,&quot; &quot;we'd probably call X intelligent,&quot; etc.
</EM><BR>
<P>Gah.
<BR>
<P><EM>&gt; If the code is written in a way that a &quot;programmer&quot; subsystem could
</EM><BR>
<EM>&gt; understand it, or, as Eliezer calls it, a &quot;codic cortex&quot; analogous to
</EM><BR>
<EM>&gt; the visual cortex, then each part may attempt to code itself better.
</EM><BR>
<EM>&gt; Each domdule can participate in improving the capacity of the rest of
</EM><BR>
<EM>&gt; the domdules.  That's how the system will improve, and improve itself
</EM><BR>
<EM>&gt; better and faster than any human could have done.  But you need a rich
</EM><BR>
<EM>&gt; system before this can take off.  That's where the hand-coding comes in.
</EM><BR>
<P>How does the machine decide what's an improvement? Out of the space of
<BR>
possible self-modifications, how does it distinguish qualitative
<BR>
improvements from junk? Putting aside the issue of universality, is
<BR>
there's a mechanism other than external feedback? 
<BR>
<P>-matt
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="3735.html">James Rogers: "Re: Musical note recognition ?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="3733.html">Anders Sandberg: "Re: Fw: Will Florida Outlaw Naturopathy?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="3580.html">Dan Fabulich: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="3419.html">hal@finney.org: "Re: SITE: Coding a Transhuman AI 2.0a"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#3734">[ date ]</A>
<A HREF="index.html#3734">[ thread ]</A>
<A HREF="subject.html#3734">[ subject ]</A>
<A HREF="author.html#3734">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:11:38 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
