<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: A Spring-Powered Theory of Consciousness (4 of 6)</TITLE>
<META NAME="Author" CONTENT="Jim Fehlinger (fehlinger@home.com)">
<META NAME="Subject" CONTENT="A Spring-Powered Theory of Consciousness (4 of 6)">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>A Spring-Powered Theory of Consciousness (4 of 6)</H1>
<!-- received="Mon Jun 19 22:17:14 2000" -->
<!-- isoreceived="20000620041714" -->
<!-- sent="Mon, 19 Jun 2000 23:57:49 -0400" -->
<!-- isosent="20000620035749" -->
<!-- name="Jim Fehlinger" -->
<!-- email="fehlinger@home.com" -->
<!-- subject="A Spring-Powered Theory of Consciousness (4 of 6)" -->
<!-- id="394EEBBD.BACDB580@home.com" -->
<STRONG>From:</STRONG> Jim Fehlinger (<A HREF="mailto:fehlinger@home.com?Subject=Re:%20A%20Spring-Powered%20Theory%20of%20Consciousness%20(4%20of%206)&In-Reply-To=&lt;394EEBBD.BACDB580@home.com&gt;"><EM>fehlinger@home.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon Jun 19 2000 - 21:57:49 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5579.html">Jim Fehlinger: "A Spring-Powered Theory of Consciousness (5 of 6)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5577.html">John  M Grigg: "Re: lol"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5578">[ date ]</A>
<A HREF="index.html#5578">[ thread ]</A>
<A HREF="subject.html#5578">[ subject ]</A>
<A HREF="author.html#5578">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Given that today's electronic technology is still one of relative
<BR>
scarcity (in terms of the economic limits on complexity),
<BR>
constructing a device possessing primary consciousness, using the
<BR>
principles of the TNGS, may not currently be feasible: &quot;In
<BR>
principle there is no reason why one could not by selective
<BR>
principles simulate a brain that has primary consciousness,
<BR>
provided that the simulation has the appropriate parts.
<BR>
But... no one has yet been able to simulate a brain system
<BR>
capable of concepts and thus of the **reconstruction** of
<BR>
portions of global mappings...  Add that one needs multiple
<BR>
sensory modalities, sophisticated motor appendages, and a lot of
<BR>
simulated neurons, and it is not at all clear whether
<BR>
presently-available supercomputers and their memories are up to
<BR>
the task&quot; (BABF pp. 193-194).
<BR>
<P>In a biological system, much of the physical complexity needed to
<BR>
support primary consciousness is inherent in the morphology of
<BR>
biological cells, tissues, and organs, and it isn't clear that
<BR>
this morphology can be easily dismissed: &quot;[Are] artifacts
<BR>
designed to have primary consciousness... **necessarily**
<BR>
confined to carbon chemistry and, more specifically, to
<BR>
biochemistry (the organic chemical or chauvinist position)[?]
<BR>
The provisional answer is that, while we cannot completely
<BR>
dismiss a particular material basis for consciousness in the
<BR>
liberal fashion of functionalism, it is probable that there will
<BR>
be severe (but not unique) constraints on the design of any
<BR>
artifact that is supposed to acquire conscious behavior.  Such
<BR>
constraints are likely to exist because there is every indication
<BR>
that an intricate, stochastically variant anatomy and synaptic
<BR>
chemistry underlie brain function and because consciousness is
<BR>
definitely a process based on an immensely intricate and unusual
<BR>
morphology&quot; (RP pp. 32-33).  Perhaps the kinds of advances
<BR>
projected for the coming decades by such writers as Ray Kurzweil,
<BR>
who predicts, based on a generalized Moore's Law, that a new
<BR>
technological paradigm (based on three-dimensional networks of
<BR>
carbon nanotubes, or something of the sort) will emerge when
<BR>
current semiconductor techniques reach their limits in a decade
<BR>
or two, will ease the current technical and economic limits on
<BR>
complexity and permit genuinely conscious artifacts to be
<BR>
constructed according to principles suggested by Edelman.
<BR>
<P>Edelman seems ambivalent about the desirability of constructing
<BR>
conscious artifacts: &quot;In principle... there is no reason to
<BR>
believe that we will not be able to construct such artifacts
<BR>
someday.  Whether we should or not is another matter.  The moral
<BR>
issues are fraught with difficult choices and unpredictable
<BR>
consequences.  We have enough to concern ourselves with in the
<BR>
human environment to justify suspension of judgment and thought
<BR>
on the matter of conscious artifacts for a bit.  There are more
<BR>
urgent tasks at hand&quot; (BABF pp. 194-195).  On the other hand,
<BR>
&quot;The results from computers hooked to NOMADs or noetic devices
<BR>
will, if successful, have enormous practical and social
<BR>
implications.  I do not know how close to realization this kind
<BR>
of thing is, but I do know, as usual in science, that we are in
<BR>
for some surprises&quot; (BABF p. 196).
<BR>
<P>Meanwhile, there is also the question of whether shortcuts can be
<BR>
taken to permit the high-level, linguistically-based logical and
<BR>
symbolic behavior of human beings to be &quot;grafted&quot; onto
<BR>
present-day symbol-manipulation machines such as digital
<BR>
computers, without duplicating all the baggage (as described by
<BR>
the TNGS) that allowed higher-order consciousness to emerge in
<BR>
the first place.  A negative answer to this question remains
<BR>
unproven, but despite such recent tours de force as IBM's &quot;Big
<BR>
Blue&quot; chess-playing system, Edelman is unpersuaded that
<BR>
traditional top-down AI will ever be able to produce
<BR>
general-purpose machines able to deal intelligently with the
<BR>
messiness and unpredictability of the world, while at the same
<BR>
time avoiding a correspondingly complex (and expensive) messiness
<BR>
in their own innards.  Edelman cites three maxims that summarize
<BR>
his position in this regard: 1. &quot;Being comes first, describing
<BR>
second...  [N]ot only is it impossible to generate being by mere
<BR>
describing, but, in the proper order of things, being precedes
<BR>
describing both ontologically and chronologically&quot;
<BR>
2. &quot;Doing... precedes understanding...  [A]nimals can solve
<BR>
problems that they certainly do not understand logically...  [W]e
<BR>
[humans] choose the right strategy before we understand why...
<BR>
[W]e use a [grammatical] rule before we understand what it is;
<BR>
and, finally...  we learn how to speak before we know anything
<BR>
about syntax&quot; 3. &quot;Selectionism precedes logic.&quot;  &quot;Logic is... a
<BR>
human activity of great power and subtlety... [but] [l]ogic is
<BR>
not necessary for the emergence of animal bodies and brains, as
<BR>
it obviously is to the construction and operation of a
<BR>
computer...  [S]electionist principles apply to brains
<BR>
and... logical ones are learned later by individuals with brains&quot;
<BR>
(UoC pp. 15-16).
<BR>
<P>Edelman speculates that the pattern-recognition capabilities
<BR>
granted to living brains by the processes of phylogenetic and
<BR>
somatic selection may exceed those of logic-based Turing
<BR>
machines: &quot;Clearly, if the brain evolved in such a fashion, and
<BR>
this evolution provided the biological basis for the eventual
<BR>
discovery and refinement of logical systems in human cultures,
<BR>
then we may conclude that, in the generative sense, selection is
<BR>
more powerful than logic.  It is selection -- natural and somatic
<BR>
-- that gave rise to language and to metaphor, and it is
<BR>
selection, not logic, that underlies pattern recognition and
<BR>
thinking in metaphorical terms.  Thought is thus ultimately based
<BR>
on our bodily interactions and structure, and its powers are
<BR>
therefore limited in some degree.  Our capacity for pattern
<BR>
recognition may nevertheless exceed the power to prove
<BR>
propositions by logical means...  This realization does not, of
<BR>
course, imply that selection can take the place of logic, nor
<BR>
does it deny the enormous power of logical operations.  In the
<BR>
realm of either organisms or of the synthetic artifacts that we
<BR>
may someday build, we conjecture that there are only two
<BR>
fundamental kinds -- Turing machines and selectional systems.
<BR>
Inasmuch as the latter preceded the emergence of the former in
<BR>
evolution, we conclude that selection is biologically the more
<BR>
fundamental process.  In any case, the interesting conjecture is
<BR>
that there appear to be only two deeply fundamental ways of
<BR>
patterning thought: selectionism and logic.  It would be a
<BR>
momentous occasion in the history of philosophy if a third way
<BR>
were found or demonstrated&quot; (UoC p. 214).
<BR>
<P>Edelman's latest book (_A Universe of Consciousness_ [UoC],
<BR>
coauthored with his colleague from the Neurosciences Institute,
<BR>
Giulio Tononi) continues to be based on the Theory of Neuronal
<BR>
Group Selection as developed by Edelman in his earlier books, but
<BR>
contains some new ideas and a shift in nomenclature reflecting a
<BR>
more information-theoretic and abstract point of view.  To
<BR>
introduce these new ideas, Edelman and Tononi give equations (in
<BR>
UoC Chap. 10 and Chap. 11) for some mathematically-defined
<BR>
quantities based on the notion of the statistical entropy of a
<BR>
system, &quot;a (logarithmic) function reflecting the number of
<BR>
possible patterns of activity that the system can take, weighted
<BR>
by their probability of occurrence&quot; (UoC p. 121).  Using
<BR>
statistical entropy as a basis, Edelman and Tononi go on to
<BR>
define: 1. the **integration** of a system, a measure of &quot;the
<BR>
loss of entropy that is due to the interactions among its
<BR>
elements&quot; due to the fact that &quot;if there are any interactions
<BR>
within the system, the number of states that the system can take
<BR>
will be **less** than would be expected from the number of states
<BR>
that its separate elements can take&quot; (UoC p. 121); 2. the
<BR>
**mutual information** among subsets of a system, measuring &quot;the
<BR>
total amount of statistical dependence (loss of entropy) between
<BR>
any chosen subset of elements and the rest of a system; 3. an
<BR>
**index of functional clustering**, measuring &quot;the relative
<BR>
strength of the interactions within a subset of elements compared
<BR>
to the interactions between that subset and the rest of the
<BR>
system&quot;, i.e., the integration of the subset divided by the
<BR>
mutual information between the subset and the rest of the system
<BR>
(UoC pp. 122-123); and finally 4. **neural complexity**, the
<BR>
&quot;averag[e of] the mutual information between each subset of a
<BR>
neural system and the rest of the system for all possible
<BR>
bipartitions of the system&quot; (UoC p. 130).  &quot;[T]he value of the
<BR>
average mutual information will be high if, on average, each
<BR>
subset can take on many different states **and** these states
<BR>
make a difference to the rest of the system&quot; (UoC p. 130).
<BR>
<P>A high value of neural complexity means that a system is both
<BR>
highly differentiated (consists of a large number of
<BR>
functionally-specialized subunits) and highly integrated (the
<BR>
activities of the subunits have a significant effect on the whole
<BR>
system): &quot;[H]igh values of complexity correspond to an optimal
<BR>
synthesis of functional specialization and functional integration
<BR>
within a system.  This is clearly the case with systems like the
<BR>
brain -- different areas and groups of neurons do different
<BR>
things (they are differentiated); at the same time they interact
<BR>
to give rise to a unified, conscious scene (they are integrated).
<BR>
By contrast, systems whose individual elements are either not
<BR>
integrated (such as a gas) or not specialized (like a homogeneous
<BR>
crystal) will have minimal complexity&quot; (UoC pp 130-131).
<BR>
<P>Edelman and Tononi have introduced the notions of neural
<BR>
complexity and mutual information deliberately to avoid problems
<BR>
associated with the application of traditional information theory
<BR>
to the brain: &quot;[A] number of applications of information theory
<BR>
in biology have been fraught with problems and have had a
<BR>
notoriously controversial history.  This is the case largely
<BR>
because at the heart of information theory as originally
<BR>
formulated lies the sensible notion of an external, intelligent
<BR>
observer who encodes messages using an alphabet of symbols.
<BR>
So-called information-processing views of the brain, however,
<BR>
have been severely criticized because they typically assume the
<BR>
existence in the world of previously **defined** information
<BR>
(begging the question of what information is) and often assume
<BR>
the existence of precise neural codes for which there is no
<BR>
evidence...  The standard approach would be to measure
<BR>
information by the number and probability of the states of the
<BR>
system that are discriminable from the point of view of an
<BR>
external observer.  To avoid the fallacy of assuming a
<BR>
'homunculus' watching the brain and interpreting its activity
<BR>
patterns from the outside, however, we must get rid of the
<BR>
privileged viewpoint of an external observer.  In other words,
<BR>
differences between activity patterns should be assessed only
<BR>
with reference to the system itself...  A noisy TV screen, for
<BR>
example, goes through a large number of 'activity patterns' that
<BR>
may look different to an external observer, but the TV, by
<BR>
itself, cannot tell the difference among them; they make no
<BR>
difference to it.  Since there is no homunculus watching the
<BR>
enchanted loom or TV screen of the brain, the only activity
<BR>
patterns that matter are those that make a difference to the
<BR>
brain itself&quot; (UoC pp. 126-127).
<BR>
<P>Edelman and Tononi define the quantity which they call mutual
<BR>
information precisely to eliminate the question-begging in the
<BR>
definition of information, and to obviate the need for an outside
<BR>
observer: &quot;How can we measure... differences that make a
<BR>
difference within a system like the brain?  A simple approach is
<BR>
to consider the system as its own 'observer'... all we need to do
<BR>
is to imagine dividing the system in two and considering how one
<BR>
part of the system affects the rest of the system, and vice
<BR>
versa&quot; (UoC pp. 127-128).  The metric of neural complexity
<BR>
further generalizes the notion of mutual information by averaging
<BR>
it across all such possible bipartitions, and considering the
<BR>
result as a measure of the overall information content of the
<BR>
system.  &quot;[A] complex brain is like a collection of specialists
<BR>
who talk to each other a lot&quot; (UoC p. 126).
<BR>
<P>The variation of complexity with extremes of integration and
<BR>
differentiation is illustrated (UoC p. 132 [Fig. 11.3]) by a set
<BR>
of successive frames of bitmaps representing the activity of a
<BR>
simulated primary visual cortical area, in which the connectivity
<BR>
in the simulation has been adjusted to correspond to 1. an old,
<BR>
deteriorated cortex, in which individual neuronal are still
<BR>
active but there has been a loss of inter-group connections; 2. a
<BR>
young, immature cortex in which each group is uniformly connected
<BR>
to every other; or 3. a normal adult cortex in which inter-group
<BR>
connectivity corresponds to that observed experimentally in the
<BR>
primary visual cortex (neuronal groups with similar orientation
<BR>
responsivity connected preferentially to each other, connection
<BR>
strength decreasing with increasing topographic distance).  The
<BR>
pictures of 1 show a random-dot pattern like snow on a TV screen,
<BR>
corresponding to high entropy but a lack of functional
<BR>
integration (the gaseous extreme); the pictures of 2 show
<BR>
alternating black and white bands rolling across the frames,
<BR>
corresponding to high integration but low entropy (the
<BR>
crystalline extreme) due to hypersynchronous firing like that in
<BR>
slow-wave sleep or generalized epilepsy; while the pictures of 3
<BR>
show continually changing patterned activity, corresponding to
<BR>
both high (but less than case 1) entropy **and** high (but less
<BR>
than case 2) functional integration, resulting in maximal
<BR>
complexity.  The patterned activity in a normal cortex can be
<BR>
more or less complex depending on its level of arousal, since the
<BR>
firing patterns of the thalamocortical neurons are responsive to
<BR>
the neuromodulatory effects of the diffusely projecting value
<BR>
systems.  The &quot;tonic&quot; pattern, typical of waking, undergoes a
<BR>
transition to a burst-pause pattern typical of slow-wave sleep,
<BR>
corresponding to a dramatic reduction in the activity of
<BR>
noradrenergic and serotoninergic systems during that sleep stage
<BR>
(UoC p. 134; see also UoC p. 91).
<BR>
<P>A further quantity, &quot;complexity matching&quot;, is not discussed in
<BR>
formal detail, but is defined as &quot;the change in neural complexity
<BR>
that occurs as a result of the encounter with external stimuli&quot;
<BR>
(UoC p.137).  This quantity reflects the fact that &quot;[f]or a small
<BR>
value of the **extrinsic** mutual information between a stimulus
<BR>
and a neural system, there is generally a large change in the
<BR>
**intrinsic** mutual information among subset of units within the
<BR>
neural system...  According to this analysis, extrinsic signals
<BR>
convey information not so much in themselves, but by virtue of
<BR>
how they modulate the intrinsic signals exchanged within a
<BR>
previously experienced neural system...  [H]igh values of
<BR>
complexity matching indicate a 'high degree of adjustment of
<BR>
inner to outer relations'...  The same stimulus, say, a Chinese
<BR>
character, can be meaningful to Chinese speakers and meaningless
<BR>
to English speakers even if the extrinsic information conveyed to
<BR>
the retina is the same.  Attempts to explain this difference that
<BR>
are based solely on the processing of a previously coded message
<BR>
in an information channel beg the question of where this
<BR>
information comes from.  The concept of matching in a selectional
<BR>
system easily resolves the issue&quot; (UoC pp. 137-138).
<BR>
<P>In Edelman's earlier books, the momentary state of the
<BR>
thalamocortical system of the brain of an organism exhibiting
<BR>
primary consciousness was characterized as a linked set of active
<BR>
global mappings, composed of the widely distributed,
<BR>
reentrantly-connected set of currently-active neuronal groups.
<BR>
This set of global mappings, one activation pattern selected out
<BR>
of all the possibilities of the secondary repertoire, was spoken
<BR>
of as constantly morphing into its successor in a probabilistic
<BR>
trajectory influenced both by the continued bombardment of new
<BR>
exteroceptive input (actively sampled through constant movement)
<BR>
and by the organism's past history (as reflected by the strengths
<BR>
of all the synaptic connections within and among the groups of
<BR>
the primary repertoire).  The evolving state of the
<BR>
thalamocortical system in the conscious brain is given a new
<BR>
characterization in Edelman's latest book by means of the
<BR>
&quot;dynamic core hypothesis&quot; (UoC Chap. 12), which is described
<BR>
using the notions of functional integration and complexity
<BR>
formalized in the previous two chapters.  Edelman and Tononi
<BR>
define a &quot;dynamic core&quot; as &quot;a cluster of neuronal groups that are
<BR>
strongly interacting among themselves and that have distinct
<BR>
functional borders with the rest of the brain at the time scale
<BR>
of fractions of a second&quot; (UoC p. 144).  The term was chosen to
<BR>
&quot;emphasize both its integration and its constantly changing
<BR>
composition.  A dynamic core is therefore a process, not a thing
<BR>
or a place...  [I]t is, in general, spatially distributed, as
<BR>
well as changing in composition, and thus cannot be localized to
<BR>
a single place in the brain&quot; (UoC p. 144).
<BR>
<P>The two tenets of the dynamic core hypothesis reframe the earlier
<BR>
picture of consciousness (that of an evolving succession of sets
<BR>
of active global mappings) as a dynamic core: &quot;1. A group of
<BR>
neurons can contribute directly to conscious experience only if
<BR>
it is part of a distributed functional cluster that, through
<BR>
reentrant interactions in the thalamocortical system, achieves
<BR>
high integration in hundreds of milliseconds.  2. To sustain
<BR>
conscious experience, it is essential that this functional
<BR>
cluster be highly differentiated, as indicated by high values of
<BR>
complexity&quot; (UoC p. 144).  &quot;While we envision that a functional
<BR>
cluster of sufficiently high complexity can be generated through
<BR>
reentrant interactions among neuronal groups distributed
<BR>
particularly within the thalamocortical system and possibly
<BR>
within other brain regions, such a cluster is neither coextensive
<BR>
with the entire brain nor restricted to any special subset of
<BR>
neurons&quot; (UoC p. 144).
<BR>
<P>The concept of consciousness as a dynamic core, and of the brain
<BR>
as its own observer, are used by Edelman and Tononi as a
<BR>
springboard to tackle the difficult problem of qualia (UoC
<BR>
Chap. 13): &quot;The specific quality, or 'quale' of subjective
<BR>
experience -- of color, warmth, pain, a loud sound -- has seemed
<BR>
beyond scientific explanation&quot; (UoC p. 157).  To provide a
<BR>
scientific basis for qualia, the authors first recast the dynamic
<BR>
core as an N-dimensional space, where N is the number of neuronal
<BR>
groups currently participating in the core (a &quot;large number, say,
<BR>
between 10^3 and 10^7&quot; [Uoc p. 165]): &quot;Since a functional cluster
<BR>
identifies a single, unified physical process, it follows that
<BR>
the activity of these N neuronal groups should be considered
<BR>
within a single reference space [a set of axes having a common
<BR>
origin]&quot; (UoC p. 165).  &quot;The number of points that can be
<BR>
differentiated in this N-dimensional space -- which make a
<BR>
difference to it -- is vast, as indicated by high values of
<BR>
complexity...  however, a large number of participating neuronal
<BR>
groups alone [a large value of N] is not a guarantee of high
<BR>
complexity...  If, for example, the firing of the N neuronal
<BR>
groups... were synchronized to an extreme degree, as is the case
<BR>
during epileptic seizures, the actual repertoire of neural states
<BR>
available to the dynamic core would be... just a few positions in
<BR>
the N-dimensional space&quot; (UoC p. 166).  &quot;[E]very discriminable
<BR>
point in the N-dimensional space defined by the dynamic core
<BR>
identifies a conscious state, while a trajectory joining points
<BR>
in this space would correspond to a sequence of conscious states
<BR>
occurring over time.  Contrary to common usage by many
<BR>
philosophers and scientists, we suggest that... **every conscious
<BR>
state deserves to be called a quale**&quot; (UoC p. 168).  The
<BR>
conscious experience of a quale therefore corresponds to the
<BR>
discrimination of one particular state (one particular point in
<BR>
the N-dimensional space) out of all the possible states of the
<BR>
dynamic core.
<BR>
<P>Note that the authors do **not** identify qualia with neuronal
<BR>
groups themselves.  A particular neuronal group in the visual
<BR>
cortex, for example, may fire when light of a particular
<BR>
wavelength impinges on the retina, but it can only contribute to
<BR>
a quale (a conscious discrimination) if it shares the same neural
<BR>
reference space (has a high degree of functional integration, or
<BR>
mutual information) with other neuronal groups.  &quot;Even then,
<BR>
there would still be no notion that the system is dealing with
<BR>
visual aspects of a stimulus, rather than... some other
<BR>
modality... [unless] the neural reference space include[s] other
<BR>
neuronal groups that are (or are not) responding to auditory,
<BR>
tactile, or proprioceptive inputs.  We would also need neuronal
<BR>
groups whose firing is correlated with the particular position
<BR>
your body is in and its relation to the environment -- the
<BR>
so-called body schema.  In addition, we would need neuronal
<BR>
groups whose firing is correlated with your sense of familiarity
<BR>
and of congruence with the situation you are in and neuronal
<BR>
groups indicating whether salient events are occurring.  And so
<BR>
on and so forth until there is a neural reference space that is
<BR>
sufficiently rich to allow discrimination of the conscious state
<BR>
corresponding to the pure perception of a given color from
<BR>
billions of other conscious states&quot; (UoC pp. 166-167).  This rich
<BR>
neural reference space must, in fact, be the N-dimensional space
<BR>
corresponding to the dynamic core of consciousness.
<BR>
<P>On the other hand, neuronal groups that are functionally
<BR>
disconnected from the dynamic core may be thought of as
<BR>
generating &quot;smaller neural spaces spanned by a few axes that have
<BR>
a separate origin [from the N-dimensional space of the dynamic
<BR>
core].  An example of such a small, functionally disconnected
<BR>
space may correspond to, for instance, neurons responding to the
<BR>
fluctuations of blood pressure&quot; (UoC pp. 165-166).  This
<BR>
independence from the dynamic core of consciousness accounts for
<BR>
the fact that while &quot;the firing of warm-sensitive neurons in the
<BR>
brain produces a quale of warmth,... the firing of neurons
<BR>
sensitive to blood pressure fails to produce any corresponding
<BR>
quale, or any subjective feeling of what it is like to have high
<BR>
blood pressure&quot; (UoC p. 158).
<BR>
<P>The number of qualia available to an individual organism,
<BR>
corresponding to the dimensionality of the N-dimensional space
<BR>
representing the dynamic core, varies among individual organisms
<BR>
depending on each organism's history and experience.  For
<BR>
example, the experience of a wine connoisseur, who has acquired
<BR>
the ability to discriminate Cabernets from Pinots where once
<BR>
there was only the ability to discriminate wine from water, has
<BR>
simply made available additional discriminatory dimensions to the
<BR>
connoisseur's dynamic core, &quot;thereby adding a large number of
<BR>
subtler differentiations among conscious states&quot; (UoC p. 174).
<BR>
Qualia can also be lost to conscious experience.  For example,
<BR>
neuronal groups in the fusiform gyrus, which selectively fire in
<BR>
response to color, may continue to particpate in the dynamic core
<BR>
even if damage to the retinas eliminates all sensory input
<BR>
corresponding to wavelengths of light.  These neuronal groups
<BR>
will even continue to be occasionally active in the visual cortex
<BR>
of a blind person in the absence of all such sensory input,
<BR>
contributing to qualia for colors in dreams, memory, and
<BR>
imagination.  However, if this area of the cortex is damaged,
<BR>
then a person will not only lose the capacity to respond to color
<BR>
as a sensory stimulus, but will lose the capacity to remember,
<BR>
imagine, or dream in color.  The lesion effectively reduces the
<BR>
dimensionality of the N-dimensional space of the dynamic core
<BR>
(UoC pp. 53, 160).
<BR>
<P>The intuitive notion of similarity and dissimilarity of conscious
<BR>
states corresponds to the notion of geometric distance between
<BR>
points in an N-dimensional space with a certain metric.  The
<BR>
metric of this space is chosen so that points along axes
<BR>
corresponding to the submodalities of a particular sensory
<BR>
modality are closer to each other than points along axes
<BR>
corresponding to different modalities -- in other words, the axes
<BR>
corresponding to submodalities of a particular modality are
<BR>
&quot;bundled&quot; together.  &quot;It is often remarked that red is as
<BR>
irreducible and different from blue as it can possibly be.  This
<BR>
irreducibility corresponds to the fact that different groups of
<BR>
neurons fire when we perceive red and when we perceive blue,
<BR>
thereby defining two irreducible dimensions of the N-dimensional
<BR>
space underlying conscious perception.  Yet we also know that as
<BR>
different as red and blue may seem subjectively, they are much
<BR>
closer to each other than they are, say, to the blaring of a
<BR>
trumpet.  In short, the phenomenal space obeys a certain metric
<BR>
within which certain conscious states are closer than others.
<BR>
According to our hypothesis, the topology and metric of this
<BR>
space should be described in terms of the appropriate neural
<BR>
reference -- the dynamic core -- and must be based on the
<BR>
interactions among the neuronal groups participating in it&quot; (UoC
<BR>
pp. 168-169).
<BR>
<P>The similarity between a human-made gadget or electronic device
<BR>
discriminating among stimuli and a person being asked to make the
<BR>
same discrimination (such as &quot;a photodiode that can differentiate
<BR>
between light and dark and provide an audible output, compared to
<BR>
a conscious human being performing the same task and giving a
<BR>
verbal report&quot; [UoC p. 32]) is misleading.  One might be tempted
<BR>
to ask &quot;Why should the simple differentiation between light and
<BR>
dark performed by the human being be associated with conscious
<BR>
experience, while that performed by the photodiode is not?&quot; (UoC
<BR>
p. 32).  The answer of Edelman and Tononi is that &quot;[T]o a
<BR>
photodiode, the discrimination between darkness and light is the
<BR>
only one available, and therefore it is only minimally
<BR>
informative.  To a human being, in contrast, an experience of
<BR>
complete darkness and an experience of complete light are two
<BR>
special conscious experiences selected out of an enormous
<BR>
repertoire, and their selection thus implies a correspondingly
<BR>
large amount of information and discrimination among potential
<BR>
actions&quot; (UoC pp. 32-33).  &quot;The enormous variety of discriminable
<BR>
states available to a conscious human being is clearly many
<BR>
orders of magnitude larger than those available to anything we
<BR>
have built.  Whether we can verbally describe these states
<BR>
satisfactorily or not, billions of such states are easily
<BR>
discriminable by the same person, and each of them is capable of
<BR>
bringing about different consequences&quot; (UoC p. 32).
<BR>
<P>It is a well-known fact of psychology that humans have a very
<BR>
limited capacity to keep more than a few distinct chunks of
<BR>
information (more than about seven digits, for example, or more
<BR>
than about four visualized objects) simultaneously in mind (UoC
<BR>
p. 26).  This has been ungenerously interpreted as meaning that
<BR>
the &quot;bandwidth&quot; of human consciousness is between 1 and 16 bits
<BR>
per second (UoC p. 150).  Edelman and Tononi assert that this is
<BR>
an incorrect interpretation: the bandwidth of consciousness
<BR>
should be calculated based on a definition of information as the
<BR>
discarding of alternatives: &quot;The ability to differentiate among a
<BR>
large repertoire of possibilities constitutes information, in the
<BR>
precise sense of 'reduction of uncertainty'.  Furthermore,
<BR>
conscious discrimination represents information **that makes a
<BR>
difference**, in the sense that the occurrence of a given
<BR>
conscious state can lead to consequences that are different, in
<BR>
terms of both thought and action, from those that might ensue
<BR>
from other conscious states&quot; (UoC pp. 29-30).  In the human
<BR>
brain, this ruling out of alternatives takes place within as
<BR>
little as 100 or 150 milliseconds: &quot;Since we can easily
<BR>
differentiate among billions of different conscious states within
<BR>
a fraction of a second, we have concluded that the
<BR>
informativeness of conscious experience must be extraordinarily
<BR>
high, indeed, better than any present-day engineer could dream
<BR>
of&quot; (UoC p. 150).
<BR>
<P>The limitation on the simultaneous conscious juggling of discrete
<BR>
&quot;chunks&quot;, claim Edelman and Tononi, &quot;is a limit not on the
<BR>
information content of conscious states, but merely on how many
<BR>
nearly independent entities can be discriminated within a
<BR>
**single** conscious state **without interfering with the
<BR>
integration and coherence of that state**&quot; (UoC p. 26).
<BR>
Consciousness is an inherently Gestalt phenomenon -- it &quot;wants&quot;
<BR>
to be integrated: &quot;In terms of the dynamic core, such a capacity
<BR>
limitation reflects an upper limit on **how many partially
<BR>
independent subprocesses can be sustained within the core without
<BR>
interfering with its integration and coherence**.  Indeed, it is
<BR>
likely that the same neural mechanisms responsible for the rapid
<BR>
integration of the dynamic core are also responsible for this
<BR>
capacity limitation&quot; (UoC p. 150).  It would seem that, just as
<BR>
it would be difficult to simulate primary consciousness on a
<BR>
digital computer, conscious brains return the compliment -- they
<BR>
have a hard time simulating the discrete registers and memory
<BR>
locations used by such machines during the course of computation
<BR>
(at least without resorting to pencil and paper).
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5579.html">Jim Fehlinger: "A Spring-Powered Theory of Consciousness (5 of 6)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5577.html">John  M Grigg: "Re: lol"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5578">[ date ]</A>
<A HREF="index.html#5578">[ thread ]</A>
<A HREF="subject.html#5578">[ subject ]</A>
<A HREF="author.html#5578">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Thu Jul 27 2000 - 14:13:51 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
