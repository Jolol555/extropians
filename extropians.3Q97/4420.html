<!-- received="Fri Sep 26 18:28:20 1997 MDT" -->
<!-- sent="Fri, 26 Sep 1997 18:20:36 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Singularity: Generation gap" -->
<!-- id="199709262138.WAA22713@hal.9000series.idiscover.net" -->
<!-- inreplyto="Singularity: Generation gap" -->
<title>extropians: Re: Singularity: Generation gap</title>
<h1>Re: Singularity: Generation gap</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Fri, 26 Sep 1997 18:20:36 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4420">[ date ]</a><a href="index.html#4420">[ thread ]</a><a href="subject.html#4420">[ subject ]</a><a href="author.html#4420">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4421.html">The Low Golden Willow: "Re: The Culture-List (Iain M. Banks)"</a>
<li> <b>Previous message:</b> <a href="4419.html">Geoff Smith: "Re: feedback and sharing knowledge"</a>
<li> <b>Maybe in reply to:</b> <a href="4352.html">Eliezer S. Yudkowsky: "Singularity: Generation gap"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="4446.html">Eric Watt Forste: "Re: Singularity: Generation gap"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Eric Watt Forste wrote:<br>
<i>&gt; </i><br>
<i>&gt; It *is* useful to use notions of the Singularity as Vinge did and</i><br>
<i>&gt; the Spike as Broderick did, when addressing a wider audience, the</i><br>
<i>&gt; sort of people who try our patience at times.  But these notions</i><br>
<i>&gt; are not so useful on a list like this one. Most people here already</i><br>
<i>&gt; know that things are getting weirder (and mostly better) and will</i><br>
<i>&gt; continue to do so at an accelerating rate at least until the</i><br>
<i>&gt; world has been changed beyond recognition.</i><br>
<p>
I think of the Singularity as protection against Failures Of The Imagination. <br>
Such failures can have *real* and *horrible* consequences.  I have gotten<br>
several letters from "Singularity skeptics" who think that enhanced<br>
intelligence isn't all *that* powerful, and technology can't progress *that*<br>
fast, and that it is therefore proper to IGNORE THE MOST ELEMENTARY<br>
PRECAUTIONS when attempting to create a transhuman intelligence!  One person<br>
seriously proposed that you could just "pull the plug" if it started looking<br>
dangerous!  Another, on this list, said that we should just "keep it bottled<br>
up" and only let it out "if it convinces us to".  I'm sorry, but those<br>
precautions would not suffice to keep *me* in, much less anything smarter. <br>
Pull the plug?  Hey, I'm already running on 200,000 computers around the<br>
world.  Only if it convinces us?  We all know humans are impossible to fool.<br>
<p>
If not for the "Singularity", I might not have realized that there are protein<br>
synthesis machines which may be hooked up to the Internet... that locked-goal<br>
Asimov circuits leak like sieves... and so on, and so on.<br>
<p>
<i>&gt;  &gt; don't expect to be in charge, and it looks to me like the logic of</i><br>
<i>&gt;  &gt; Libertarianism breaks down if you're omniscient For All Practical</i><br>
<i>&gt;  &gt; Purposes.</i><br>
<i>&gt; </i><br>
<i>&gt; Sorry, I'm calling you on this one.  "omniscient For All Practical</i><br>
<i>&gt; Purposes" looks like a phrase with null semantic content to me.</i><br>
<i>&gt; Could you please try rephrasing whatever it was you intended to</i><br>
<i>&gt; say?</i><br>
<p>
Omniscient FAPP means that you can visualize (and perfectly predict) all other<br>
"agents" in a game-theoretic scenario.  While multiple competing OFAPP agents<br>
are questionable (halting problem?), a Power deciding whether to force-upload<br>
humanity could well be OFAPP, game-theoretically speaking.<br>
<p>
(Note:  If external influences exist on a game scenario, OFAPP requires that<br>
you be able to predict the external influences as well.)<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4421.html">The Low Golden Willow: "Re: The Culture-List (Iain M. Banks)"</a>
<li> <b>Previous message:</b> <a href="4419.html">Geoff Smith: "Re: feedback and sharing knowledge"</a>
<li> <b>Maybe in reply to:</b> <a href="4352.html">Eliezer S. Yudkowsky: "Singularity: Generation gap"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="4446.html">Eric Watt Forste: "Re: Singularity: Generation gap"</a>
<!-- reply="end" -->
</ul>
