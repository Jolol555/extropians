<!-- received="Wed Aug 13 22:11:08 1997 MDT" -->
<!-- sent="Wed, 13 Aug 1997 22:23:13 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Galaxy Brain Problem" -->
<!-- id="199708140243.TAA20141@netcom11.netcom.com" -->
<!-- inreplyto="Galaxy Brain Problem" -->
<title>extropians: Re: Galaxy Brain Problem</title>
<h1>Re: Galaxy Brain Problem</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Wed, 13 Aug 1997 22:23:13 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1886">[ date ]</a><a href="index.html#1886">[ thread ]</a><a href="subject.html#1886">[ subject ]</a><a href="author.html#1886">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1887.html">Eliezer S. Yudkowsky: "Re: Emotions:  The Easy Part"</a>
<li> <b>Previous message:</b> <a href="1885.html">Eliezer S. Yudkowsky: "Re: EXTRO-3: Comments from Darren"</a>
<li> <b>Maybe in reply to:</b> <a href="1805.html">Rick Knight: "Galaxy Brain Problem"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1969.html">Robin Hanson: "Re: Galaxy Brain Problem"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Anders Sandberg wrote:<br>
<i>&gt; </i><br>
<i>&gt; On Wed, 13 Aug 1997, Geoff Smith wrote:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; As someone in another thread pointed out, game theory does not</i><br>
<i>&gt; &gt; apply to post-singularity entities.</i><br>
<i>&gt; </i><br>
<i>&gt; Huh? Could this someone please explain why it would not apply after</i><br>
<i>&gt; the singularity? Many situations in game theory will not be changed</i><br>
<i>&gt; if the players are ultra-intelligent (in fact, game theory often</i><br>
<i>&gt; assumes the players are very rational, more rational than most humans</i><br>
<i>&gt; are).</i><br>
<p>
That was me, in Re: [2] Freedom or death.  I suggested a non-Hofstadterian<br>
solution to the Power vs. Power Prisoner's Dilemna.  I also wondered if the<br>
Powers would use game theory at all, when deciding whether to respect our<br>
opinions.  In the question of forced uploading, we might appear as wholly<br>
deterministic phenomena in a one-shot PD.<br>
<p>
[Repost of relevant section:]<br>
<p>
I'm not sure that game theory applies to posthumans.  It's based on<br>
rather tenuous assumptions about lack of knowledge and conflicting goals.  It<br>
works fine for our cognitive architecture, but you can sort of see how it<br>
breaks down.  Take the Prisoner's Dilemna.  The famous Hofstadterian<br>
resolution is to assume that the actions of the other are dependent on yours,<br>
regardless of the lack of communication.  In a human Prisoner's Dilemna, this,<br>
alas, isn't true - but assuming that it is, pretending that it is, is the way out.<br>
<p>
While a Power might set up a segregated, logical line of reasoning that would,<br>
as a Turing machine, inevitably be the same as the reasoning used by the other<br>
partner... so that the two would inevitably arrive at the same decision.<br>
<p>
The problem is that this doesn't work for a "human vs. Power" Prisoner's<br>
Dilemna.  The Power isn't pretending anything.  It isn't acting out of respect<br>
for anyone's motives.  It isn't giving slack.  It isn't following a<br>
Tit-For-Tat strategy.  It *knows*.  A Power in a human/Power PD might be able<br>
to work out the human's entire line of logic, deterministically, in advance,<br>
and then - regardless of what the human would do - defect.  (Or it might<br>
cooperate.  There are no real-life Prisoners' Dilemnas.  Defecting always has<br>
continuing repercussions.)<br>
<p>
But in the case of involuntary uploading, the Power might well disregard our<br>
opinions entirely.  It *knows* what is wrong and what is right, in terms of<br>
ultimate meaning.  It *knows* we're wrong.  Unlike a human, it has no chance<br>
of being wrong - not of schizophrenia, not of being in an elaborate<br>
hallucinated test, *nothing*.  We can never acquire that logic, being unsuited<br>
to it by evolution, however seductive that logic may seem.<br>
<p>
Given that all Powers share exactly the same goals with no conflict arising<br>
even as a hypothetical, or that they can use the above<br>
identical-line-of-reasoning logic to ensure that no uncertainty ever arises...<br>
given that there is never a conflict of opinion... then the Powers have no<br>
need for game theory!  Even if some of the above conditions are violated,<br>
they'd still have no need for game theory with respect for humans.  What are<br>
we going to do?  Say, "Bad posthumans!  No biscuit!"  Does respect for<br>
another's motives apply when you can simulate, and intimately understand, a<br>
neural or atomic-level model of that person's brain?<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1887.html">Eliezer S. Yudkowsky: "Re: Emotions:  The Easy Part"</a>
<li> <b>Previous message:</b> <a href="1885.html">Eliezer S. Yudkowsky: "Re: EXTRO-3: Comments from Darren"</a>
<li> <b>Maybe in reply to:</b> <a href="1805.html">Rick Knight: "Galaxy Brain Problem"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1969.html">Robin Hanson: "Re: Galaxy Brain Problem"</a>
<!-- reply="end" -->
</ul>
