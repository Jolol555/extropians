<!-- received="Thu Sep 11 12:17:27 1997 MDT" -->
<!-- sent="Thu, 11 Sep 1997 13:38:52 -0400" -->
<!-- name="Hagbard Celine" -->
<!-- email="hagbard@ix.netcom.com" -->
<!-- subject="Re: Is cryopreservation a solution?" -->
<!-- id="199709111755.KAA00916@web2.calweb.com" -->
<!-- inreplyto="Is cryopreservation a solution?" -->
<title>extropians: Re: Is cryopreservation a solution?</title>
<h1>Re: Is cryopreservation a solution?</h1>
Hagbard Celine (<i>hagbard@ix.netcom.com</i>)<br>
<i>Thu, 11 Sep 1997 13:38:52 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3407">[ date ]</a><a href="index.html#3407">[ thread ]</a><a href="subject.html#3407">[ subject ]</a><a href="author.html#3407">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3408.html">Rick Knight: "Neo-Tech"</a>
<li> <b>Previous message:</b> <a href="3406.html">Lee Daniel Crocker: "Re: Free-Markets: Extro-Nazi's or Extro-Saints?"</a>
<li> <b>Maybe in reply to:</b> <a href="3294.html">Joao Pedro: "Is cryopreservation a solution?"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="3465.html">Anders Sandberg: "Re: Is cryopreservation a solution?"</a>
<li> <b>Reply:</b> <a href="3465.html">Anders Sandberg: "Re: Is cryopreservation a solution?"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Anders Sandberg wrote:<br>
<p>
<i>&gt; Holism can work the other way too: an almost correctly put together</i><br>
<i>&gt; brain would self-organize to a state very similar to the original</i><br>
<i>&gt; person as memories, personality, chemical gradients etc relaxed</i><br>
<i>&gt; towards a consistent state.</i><br>
<p>
It seems that some outside intervention would be necessary in this case.<br>
Is there any precedent for cellular self-organization? I mean to say, if<br>
the neurons are simply in the wrong place, what would make them move<br>
about spontaneously to their identity-creating positions? Perhaps after<br>
having reconstructed the brain it would then be possible to move neurons<br>
about, trial-and-error, based upon where activity is occurring and where<br>
it is not, but otherwise, it seems against the second law of<br>
thermodynamics. <br>
<p>
<i>&gt; It is funny to notice that the idea of holism is in some sense</i><br>
<i>&gt; deeply conservative. It claims that we can (and must) understand</i><br>
<i>&gt; the whole system without looking at the parts, and that attempts</i><br>
<i>&gt; to do so anyway will fail. The odd thing is that top-down explanations</i><br>
<i>&gt; (which are holistic) so far has not been very successful in helping</i><br>
<i>&gt; us understand the mind, the body and the universe despite centuries</i><br>
<i>&gt; of thinking, while bottom-up explanations of reductionism have worked</i><br>
<i>&gt; quite nicely even if they are limited.</i><br>
<p>
I am not a strict holist by any stretch. But, when from a materialist<br>
standpoint one attempts to use biology to explain a non-biological, or<br>
as-yet philosophical concept, holism becomes useful. <br>
<p>
In the case of identity, this utility arises because the sum of our<br>
biological parts yields a perceptually non-physical abstraction -- human<br>
identity. That is, common-sense would suggest that the sum of our<br>
biological parts will always be a biological machine. But in the case of<br>
identity, this is not true. Identity is not a part of the machine. You<br>
can't pick up, hold, or replace your identity in any physical sense.<br>
Identity exists above the biological machine, yet also affects it from<br>
that same meta-level.<br>
<p>
So holism is useful in this scenario only insofar as it offers a<br>
tentative explaination for identity based upon biology -- that is,<br>
identity arises when all the pieces are locked into a specific place,<br>
and in a specific way, birthing a more-than-biological thing.<br>
<p>
What would be the reductionist explaination for identity? Or for that<br>
matter, what are the other ways of explaining it?    <br>
<p>
 <br>
<i>&gt; Personally I see no problem with having complex and abstract</i><br>
<i>&gt; properties emerging from low-level systems, they do it all the</i><br>
<i>&gt; time. As I think it was Carl Sagan said: "there is no such</i><br>
<i>&gt; thing as *mere* matter"!. The simplest systems demonstrate</i><br>
<i>&gt; complex behaviors based on simple internal interactions; we</i><br>
<i>&gt; can choose on what level we want to study them, but it is often</i><br>
<i>&gt; easier to deal with the lower levels first and then deduce</i><br>
<i>&gt; how they interact to produce higher levels.</i><br>
<p>
Of course. But, I am arguing that the higher levels (more complex,<br>
indeed) are still biological. How does one deduce the biological<br>
interactions that produce an abstract identity? For that matter, what<br>
sorts of abstract properties do you know of that have emerged from<br>
non-abstract low-level systems? And are not these abstract properties<br>
more than the sum of the non-abstract system's parts? <br>
<p>
Correct me here, if necessary. It would seem you are suggesting that a<br>
pattern exists in the neural network of the brain, which if mapped,<br>
would allow us to fully repair a partially-reconstructed brain,<br>
including the pre-existing identity. I don't know enough neuroscience to<br>
comment, but what if there is no pattern? What if every neuron must be<br>
placed where it was in the original? This will severely limit our<br>
ability to repair the brain, and lends credence to a holistic view of<br>
identity since there is no systemic level between the neural arrangement<br>
and identity. <br>
<p>
One consequence of a bottom-up definition of identity is the increased<br>
likelihood that we will have the ability to alter identity as we wish.<br>
(Ah, autopotency...) I like the prospects of that, although what happens<br>
when you make a change to your identity that makes you more likely to<br>
want to change your identity in a way that makes you more likely to<br>
change your identity? Hofstadter fans take note.      <br>
<p>
<i>&gt; </i><br>
<i>&gt; &gt; This is not to say that our understanding of the brain will never reach</i><br>
<i>&gt; &gt; the point where we can "fudge" things a little. But, one neuron</i><br>
<i>&gt; &gt; incorrectly arranged may have little effect on identity -- say one step</i><br>
<i>&gt; &gt; on the identity continuum. Two neurons incorrectly arranged -- two</i><br>
<i>&gt; &gt; steps. Would three neurons incorrectly arranged have only a three-step</i><br>
<i>&gt; &gt; effect? Or would it be four steps? Or six? What about four neurons? An</i><br>
<i>&gt; &gt; exponential, geometric or one-to-one effect on identity?</i><br>
<i>&gt; </i><br>
<i>&gt; That depends on how you measure geometry, what local metric you use</i><br>
<i>&gt; in identity-space (see Sasha's excellent paper about it, on his website).</i><br>
<i>&gt; My guess is that the effect will depend a lot on which neurons are</i><br>
<i>&gt; erroneous, some are more important than the others. My experience</i><br>
<i>&gt; from neural networks suggests that they tend to be fairly stable as</i><br>
<i>&gt; you disrupt them until a certain point, and then they quickly break</i><br>
<i>&gt; down. So the identity left after a certain number of fudges</i><br>
<i>&gt; would look like this:</i><br>
<i>&gt; </i><br>
<i>&gt; ---------                 100% identity</i><br>
<i>&gt;          \</i><br>
<i>&gt;           |</i><br>
<i>&gt;           |</i><br>
<i>&gt;           |</i><br>
<i>&gt;           |</i><br>
<i>&gt;           |</i><br>
<i>&gt;            \</i><br>
<i>&gt;             ------------  Zero identity</i><br>
<i>&gt; ---Neural Change--------&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; This suggests that small errors are completely insignificant.</i><br>
<i>&gt; The big question is where the breakdown happens; we know far too</i><br>
<i>&gt; little to be able to say for certain. A guess is that it corresponds</i><br>
<i>&gt; to a fraction of the total cell number, and given what we know about</i><br>
<i>&gt; dementia, I would guess that you could probably have around 0.1-1%</i><br>
<i>&gt; neural change and still be safely yourself (with a possible</i><br>
<i>&gt; performance decrement).</i><br>
<p>
Hmmm. Reanimation has a very low margin of error then based upon the<br>
state of biostasis technology today. Pre-freeze cellular deterioration<br>
may make-up even more neuron damage than that, don't you think? In the<br>
absence of a pattern to work from in reconstruction, I don't give<br>
today's cryonics customers much of a chance at being themselves.<br>
<p>
Your above point does make good sense in that evolution is likely to<br>
have installed some built-in redundancy within the network to avoid<br>
total incapacitation after brain trauma. What are your thoughts on the<br>
reasons for the stability of neural networks? Is there actually a<br>
reorganization to "carry the load," so-to-speak, like you mentioned<br>
above? Or can mere redundancy explain the bulk of it? <br>
<p>
<p>
Interesting stuff,<br>
<p>
Hagbard<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3408.html">Rick Knight: "Neo-Tech"</a>
<li> <b>Previous message:</b> <a href="3406.html">Lee Daniel Crocker: "Re: Free-Markets: Extro-Nazi's or Extro-Saints?"</a>
<li> <b>Maybe in reply to:</b> <a href="3294.html">Joao Pedro: "Is cryopreservation a solution?"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="3465.html">Anders Sandberg: "Re: Is cryopreservation a solution?"</a>
<li> <b>Reply:</b> <a href="3465.html">Anders Sandberg: "Re: Is cryopreservation a solution?"</a>
<!-- reply="end" -->
</ul>
