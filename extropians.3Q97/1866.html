<!-- received="Wed Aug 13 15:53:17 1997 MDT" -->
<!-- sent="Wed, 13 Aug 1997 20:30:18 +0100" -->
<!-- name="Darren Reynolds" -->
<!-- email="extro@blue.demon.co.uk" -->
<!-- subject="Re: Galaxy brain problem" -->
<!-- id="3.0.1.32.19970813203018.006c2a74@pop3.demon.co.uk" -->
<!-- inreplyto="3.0.32.19970812182914.006eaf30@cinenet.net" -->
<title>extropians: Re: Galaxy brain problem</title>
<h1>Re: Galaxy brain problem</h1>
Darren Reynolds (<i>extro@blue.demon.co.uk</i>)<br>
<i>Wed, 13 Aug 1997 20:30:18 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1866">[ date ]</a><a href="index.html#1866">[ thread ]</a><a href="subject.html#1866">[ subject ]</a><a href="author.html#1866">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1867.html">Darren Reynolds: "Re: Emotions:  The Easy Part"</a>
<li> <b>Previous message:</b> <a href="1865.html">Darren Reynolds: "Re: Freedom or death? (Was: Re [2]: Extropy in the personal"</a>
<li> <b>In reply to:</b> <a href="1808.html">Dan Fabulich: "Re: Galaxy brain problem"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1904.html">Anders Sandberg: "Re: Galaxy brain problem"</a>
<li> <b>Reply:</b> <a href="1904.html">Anders Sandberg: "Re: Galaxy brain problem"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
At 18:29 12/08/97 -0700, Dan Fabulich wrote:<br>
<i>&gt;At 03:45 PM 8/11/97 -0700, Geoff Smith wrote:</i><br>
<i>&gt;&gt;It seems to me that the brains with duplication will be</i><br>
<i>&gt;&gt;conquered by the twice-as-massive brains with no backup at all.</i><br>
<i>&gt;</i><br>
<i>&gt;The backups could be their own autonomous (and autogamous) agents, in which</i><br>
<i>&gt;case the "backups" themselves would be the essential feature of an emergent</i><br>
<i>&gt;brain of enormous complexity.  It's hard to imagine anything centralized</i><br>
<i>&gt;"conquering" a system like that, especially if general relativity prevents</i><br>
<i>&gt;the centralized entity from moving faster than c.</i><br>
<p>
Suppose you have two designs for a system. Both have the same amount of<br>
energy. System 1 duplicates all data, whereas system 2 does not. Clearly,<br>
system 2 will have twice as much capacity for data, although not<br>
necessarily twice as much usefulness. (Both systems would prioritize<br>
resource usage to store the most valuable data first, and therefore system<br>
2's second half would have less value its first, which should be the same<br>
as system 1's total.)<br>
<p>
I'm no statistician, but you can figure it out mathematically. You want the<br>
system with the greatest total value over the systems' anticipated<br>
lifespan. The value would be a sum of the values of each datum stored<br>
successfully at each instant in time. If you can figure out the value of<br>
each datum, and the probability of it being destroyed at a given moment,<br>
then you can integrate to get the total value of the system. Better still,<br>
you might find that SOME data is worth duplicating, and some is not. There<br>
would probably be a series of variable "value thresholds", above which<br>
everything is duplicated, triplicated, or more. But how to place a value on<br>
the ability to answer "4" when the question is "2+2"?<br>
<p>
I doubt that we have the faintest idea at this time how to measure the<br>
risks, benefits or even the anticipated lifetime in the case of a<br>
super-intelligent system. The very thought of trying to work the equation<br>
out makes me long for the day when I need to make the choice. I never did<br>
get the hang of Calculus. Corporations face the same problem regularly, and<br>
the skill and judgement of the board in approximating these and other<br>
variables is an important part of what determines how much profit they make.<br>
<p>
Still, it's fun to speculate.<br>
<p>
Regards,<br>
Darren<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1867.html">Darren Reynolds: "Re: Emotions:  The Easy Part"</a>
<li> <b>Previous message:</b> <a href="1865.html">Darren Reynolds: "Re: Freedom or death? (Was: Re [2]: Extropy in the personal"</a>
<li> <b>In reply to:</b> <a href="1808.html">Dan Fabulich: "Re: Galaxy brain problem"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1904.html">Anders Sandberg: "Re: Galaxy brain problem"</a>
<li> <b>Reply:</b> <a href="1904.html">Anders Sandberg: "Re: Galaxy brain problem"</a>
<!-- reply="end" -->
</ul>
