<!-- received="Mon Aug 11 21:52:45 1997 MDT" -->
<!-- sent="Mon, 11 Aug 1997 22:54:09 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: [2] Freedom or death" -->
<!-- id="3.0.1.32.19970811190916.024e6c08@shell6.ba.best.com" -->
<!-- inreplyto="[2] Freedom or death" -->
<title>extropians: Re: [2] Freedom or death</title>
<h1>Re: [2] Freedom or death</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Mon, 11 Aug 1997 22:54:09 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1767">[ date ]</a><a href="index.html#1767">[ thread ]</a><a href="subject.html#1767">[ subject ]</a><a href="author.html#1767">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1768.html">Damien Broderick: "minds and waistlines"</a>
<li> <b>Previous message:</b> <a href="1766.html">Michael M. Butler: "PLONK and some other words (was Re: [2] Freedom or death)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Advance summary:<br>
<p>
1)  You cannot override the will of others.  Death isn't that important.<br>
2)  I can't guarantee the Powers will say the same.<br>
3)  But *you* are not a Power.<br>
<p>
[den Otter asks...]<br>
<i>&gt; Tell me, what crime is worse: kidnapping or murder? (rhetorical question!)</i><br>
<p>
WRONG!<br>
<p>
This question is *not* rhetorical.<br>
<p>
As I said:<br>
[<a href="http://tezcat.com/~eliezer/algernon_ethics.html">http://tezcat.com/~eliezer/algernon_ethics.html</a>]<br>
<p>
'Over time, I've learned to value free will over human life. No doubt,<br>
 this is and will remain a controversial statement. The truth is that<br>
 the intuitive reasons why I believe this are not derived from the<br>
 pure ethical logic of goal and subgoal. Life and death are simple<br>
 things, a binary duality. There's nothing palpably evil about death.<br>
 Certainly, nobody who believes in God or an immortal soul (11)<br>
 should, logically, look on death as more than an inconvenience. Six<br>
 thousand people die every hour. Out of the one hundred and fifty<br>
 thousand people who die every day, how many of them are children?<br>
 Ten thousand, I should think, at the very least. To quote Michael<br>
 Resnick's apotheosis of evil, Conrad Bland: "If there is a God, then<br>
 He has passed a death sentence on every human being from the<br>
 moment of conception. I am but a talented amateur." (12)'<br>
<p>
To summarize the rest - death is too cheap, and too simple, to really count as<br>
evil.  But the enormously complex entity called "The Human Mind" can be<br>
perverted in truly evil ways, rather than simply rendered wholly or partially<br>
nonfunctional.  So that, then, is Evil.<br>
<p>
Being a cognitive scientist, I see the human mind as having far more causal<br>
density - deep substance - than its biological substrate.  To die is simply to<br>
cease functioning; it doesn't really alter - much less *pervert* - the causal<br>
model, simply erases it.  Evil in my book is reserved for perverting the mind,<br>
because that's what my mind can get a grasp on.<br>
<p>
These are complex issues; in truth, I don't know how to properly articulate my<br>
intuitions on the subject.  I do trust my intuitions.  Not because of any<br>
dramatized conflict between Emotion and Rationality, but because my intuitions<br>
are almost always right, and are rarely in "conflict" with anything. <br>
Intuitions are reasons I haven't learned to articulate.  Which is why I'm<br>
going to deliver some complex ethical judgements, which - I admit it - are<br>
based largely on "unarticulated reasoning".<br>
<p>
My judgements on the cryonics and involuntary-uploading issue are both<br>
ambiguous.  Cryonic freezing takes place after the subject is *dead*. <br>
Therefore it does not really constitute kidnapping.  You aren't placing<br>
yourself in conflict with anyone's will, you're just picking up a dead body<br>
and sticking it in liquid nitrogen.  After revival, the corpsicle/newly<br>
created individual can decide to commit suicide - unless involuntarily<br>
uploaded; see below.<br>
<p>
Likewise, for involuntary uploading, a *human* cannot decree it.  But - In My<br>
Controversial Opinion - by the time we have uploading, it won't be humans who<br>
are doing it.  Posthuman Powers might be able to ethically override our wills<br>
with impunity, because they, in the time-worn justification that _we_ can<br>
never trust, really *would* be right!  The requirements of mutually respected<br>
motives are human protocols, derived from human history as a protection<br>
against human nature.  If Powers are never, ever mistaken - which could happen<br>
- they might invent different rules.<br>
<p>
I'm just not sure that game theory applies to posthumans.  It's based on<br>
rather tenuous assumptions about lack of knowledge and conflicting goals.  It<br>
works fine for our cognitive architecture, but you can sort of see how it<br>
breaks down.  Take the Prisoner's Dilemna.  The famous Hofstadterian<br>
resolution is to assume that the actions of the other are dependent on yours,<br>
regardless of the lack of communication.  In a human Prisoner's Dilemna, this,<br>
alas, isn't true - but assuming that it is, pretending that it is, is the way out.<br>
<p>
While a Power might set up a segregated, logical line of reasoning that would,<br>
as a Turing machine, inevitably be the same as the reasoning used by the other<br>
partner... so that the two would inevitably arrive at the same decision.<br>
<p>
The problem is that this doesn't work for a "human vs. Power" Prisoner's<br>
Dilemna.  The Power isn't pretending anything.  It isn't acting out of respect<br>
for anyone's motives.  It isn't giving slack.  It isn't following a<br>
Tit-For-Tat strategy.  It *knows*.  A Power in a human/Power PD might be able<br>
to work out the human's entire line of logic, deterministically, in advance,<br>
and then - regardless of what the human would do - defect.  (Or it might<br>
cooperate.  There are no real-life Prisoners' Dilemnas.  Defecting always has<br>
continuing repercussions.)<br>
<p>
But in the case of involuntary uploading, the Power might well disregard our<br>
opinions entirely.  It *knows* what is wrong and what is right, in terms of<br>
ultimate meaning.  It *knows* we're wrong.  Unlike a human, it has no chance<br>
of being wrong - not of schizophrenia, not of being in an elaborate<br>
hallucinated test, *nothing*.  We can never acquire that logic, being unsuited<br>
to it by evolution, however seductive that logic may seem.<br>
<p>
Given that all Powers share exactly the same goals with no conflict arising<br>
even as a hypothetical, or that they can use the above<br>
identical-line-of-reasoning logic to ensure that no uncertainty ever arises...<br>
given that there is never a conflict of opinion... then the Powers have no<br>
need for game theory!  Even if some of the above conditions are violated,<br>
they'd still have no need for game theory with respect for humans.  What are<br>
we going to do?  Say, "Bad posthumans!  No biscuit!"  Does respect for<br>
another's motives apply when you can simulate, and intimately understand, a<br>
neural or atomic-level model of that person's brain?<br>
<p>
Anyway, den Otter has, more or less by coincidence, managed to choose<br>
ambiguous cases, those of cryonics and uploading.<br>
<p>
If, however, somebody chooses to *destroy* their brain via explosives...<br>
If, however, *you* are faced with the decision of force-uploading someone...<br>
Or in any lesser conflict between free will and *your* principles...<br>
<p>
Tough luck.  You aren't a Power.  You don't know.  You are bound by the same<br>
rules of game theory that everyone else accepts to make life livable.  And if<br>
you violate those rules, then the rest of us will have to enforce them. <br>
You're just going to have to respect other people's opinions, even the<br>
"obviously wrong", the "obscene", the "indefensible" ones.  Even the fatal<br>
ones.  Because you and I are mortal and fallible, and - for all we know - the<br>
whole world could be an elaborate computer simulation, designed for the sole<br>
purpose of getting us to make this one wrong decision.  For all we know, a<br>
decision to force cryonics on others will provoke a worldwide backlash against<br>
all technology.<br>
<p>
My intuitions about this are hard to verbalize.  They involve terms, such as<br>
"causal density", which were invented less than a week ago.  What I do know is this:<br>
<p>
1)  You cannot override the will of others.  Death isn't that important.<br>
2)  I can't guarantee the Powers will say the same.<br>
3)  But *you* are not a Power.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1768.html">Damien Broderick: "minds and waistlines"</a>
<li> <b>Previous message:</b> <a href="1766.html">Michael M. Butler: "PLONK and some other words (was Re: [2] Freedom or death)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
