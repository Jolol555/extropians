<!-- received="Sat Aug 16 16:08:04 1997 MDT" -->
<!-- sent="Sat, 16 Aug 1997 19:37:41 +0100" -->
<!-- name="Darren Reynolds" -->
<!-- email="extro@blue.demon.co.uk" -->
<!-- subject="Re: Emotions:  The Easy Part" -->
<!-- id="3.0.1.32.19970816193741.0068b92c@pop3.demon.co.uk" -->
<!-- inreplyto="33F272F4.303B940B@pobox.com" -->
<title>extropians: Re: Emotions:  The Easy Part</title>
<h1>Re: Emotions:  The Easy Part</h1>
Darren Reynolds (<i>extro@blue.demon.co.uk</i>)<br>
<i>Sat, 16 Aug 1997 19:37:41 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2082">[ date ]</a><a href="index.html#2082">[ thread ]</a><a href="subject.html#2082">[ subject ]</a><a href="author.html#2082">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2083.html">Darren Reynolds: "Re: [2] Freedom or death"</a>
<li> <b>Previous message:</b> <a href="2081.html">The Low Golden Willow: "ECON: Jane Jacobs Epiphany"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2175.html">Sarah Marr: "Re: Emotions:  The Easy Part"</a>
<li> <b>Reply:</b> <a href="2175.html">Sarah Marr: "Re: Emotions:  The Easy Part"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
At 21:52 13/08/97 -0500, Eliezer S. Yudkowsky wrote:<br>
<i>&gt;Finally, although this is getting into the hard problem of conscious</i><br>
<i>&gt;experience, I don't think the physiological stuff can explain the subjective</i><br>
<i>&gt;aspect.  Is adrenaline conscious?</i><br>
<p>
Yeah, but you can say that about "consciousness" itself. The topic is<br>
wandering off into the mist a bit here, but I do believe that the "I" in me<br>
is an illusion; I'm not really there. It's the only way I seem to be able<br>
to explain away reality. Not very satisfactory, I know, but until someone<br>
offers me something better ...<br>
<p>
<p>
<i>&gt;Emotions aren't mysterious forces.  They can't hide from us.  Turing machines</i><br>
<i>&gt;are deterministic and wholly causal; any force operating on Deep Thought</i><br>
would<br>
<i>&gt;be explicable as the sum of its parts.</i><br>
<p>
Obvious question: why do you think we are any different?<br>
<p>
<p>
<i>&gt;&gt; When Deep Blue won the series recently, I wondered whether it felt a most</i><br>
<i>&gt;&gt; primitive sense of achievement, in the same way that our own primitive</i><br>
<i>&gt;&gt; ancestors might have felt a sense of achievement if they had lifted their</i><br>
<i>&gt;&gt; first stone, and successfully hurled it at a passing antelope. Yes, the</i><br>
<i>&gt;&gt; thing is just a mathematical algorithm. But so, apparently, are we.</i><br>
<i>&gt;</i><br>
<i>&gt;It didn't.  Deep Blue may have emergent chess stratagems from the complex</i><br>
<i>&gt;physical-process-level chess exploration algorithms.  I don't think there was</i><br>
<i>&gt;any form of feedback for successful and unsuccessful moves.  In training</i><br>
mode ...<br>
<p>
Yeah, but this misses the point. There IS feedback in the form of<br>
selection. If Deep Blue makes bad moves, IBM will trash it. There doesn't<br>
have to be anything in the code. It's the same feedback (and the only<br>
feedback) that our own evolutionary path had until very recently.<br>
<p>
Forget the physical substrate, and think only of the whole logical system:<br>
inputs and ouputs. If one system produces all the same outputs for a given<br>
series of inputs as another, then the systems are identical. If they don't<br>
consist of the same stuff, that doesn't stop the SYSTEMs being identical.<br>
One could be plastic, silicon and Cobol whilst the other is water, fat and<br>
DNA.<br>
<p>
A fun discussion this. :o)<br>
<p>
<p>
<i>&gt;Except from a functionalist perspective, there wouldn't be much</i><br>
<i>&gt;internal difference between "pleasure" and "pain" - just negative and</i><br>
positive<br>
<i>&gt;numbers.</i><br>
<p>
Right. Whereas in humans, the internal difference between pleasure and pain<br>
is ... er, what is the internal difference in humans exactly?<br>
<p>
<p>
<i>&gt;If Deep Blue were reversed to make the worst possible move, it might</i><br>
<i>&gt;not require much reprogramming... i.e., the distinction between pleasure and</i><br>
<i>&gt;pain would be surface, rather than deep causal.</i><br>
<p>
Hmm, that's a good point. But it is possible that this may have more to do<br>
with our level of understanding of the system rather than the depth of the<br>
distinction.<br>
<p>
<p>
<i>&gt;Emotions did indeed evolve because they are evolutionary advantages.</i><br>
Although<br>
<i>&gt;Deep Blue's weightings for piece value may be "evolutionary" in some sense, I</i><br>
<i>&gt;don't think the term can really apply in the sense you use it.  Linear</i><br>
numbers<br>
<i>&gt;aren't complex enough to "evolve"; evolving sequences of instructions, as in</i><br>
<i>&gt;the TIERRA environment, are another matter.</i><br>
<p>
Again, I think that this misses the point. I argue that if humans alter a<br>
system in a way which leads to greater levels of reproduction for that<br>
system, then the system has evolved. The agent causing the evolution is<br>
irrelevant. The code doesn't have to learn from its mistakes in chess.<br>
There merely has to be an environment which prefers good moves, with<br>
penalties and rewards that affect reproductive success.<br>
<p>
<p>
<i>&gt;Deep Blue, again, is not that level of AI.</i><br>
<p>
It doesn't have to be.<br>
<p>
<p>
<i>&gt;So if I have a program,</i><br>
<i>&gt;</i><br>
<i>&gt;struct Wasp {</i><br>
<i>&gt;	float wing_tone;</i><br>
<i>&gt;	float air_speed;</i><br>
<i>&gt;	float sting_angle;</i><br>
<i>&gt;};</i><br>
<i>&gt;</i><br>
<i>&gt;and I adjust the three variables you mentioned, the program has emotions?</i><br>
<p>
I guess we'd have to ask it. How can you tell if I am angry?<br>
<p>
<p>
<i>&gt;Any emotions</i><br>
<i>&gt;that are observer-dependent, I am not interested in.</i><br>
<p>
Precisely. You are interested only in your own emotions. That's all you can<br>
be. In fact, you ought (I don't mean this nastily at all, but<br>
scientifically) to be interested in only yourself, because you can't be<br>
sure that anything else exists. There are two types of entity: you, and<br>
everything else. Other humans, dogs, wasps and computer programs are all<br>
"everything else", and you really can't say very much about them. Your own<br>
body probably falls into the category of "everything else", too, which is a<br>
bit unnerving.<br>
<p>
What do you think?<br>
<p>
Regards,<br>
Darren<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2083.html">Darren Reynolds: "Re: [2] Freedom or death"</a>
<li> <b>Previous message:</b> <a href="2081.html">The Low Golden Willow: "ECON: Jane Jacobs Epiphany"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2175.html">Sarah Marr: "Re: Emotions:  The Easy Part"</a>
<li> <b>Reply:</b> <a href="2175.html">Sarah Marr: "Re: Emotions:  The Easy Part"</a>
<!-- reply="end" -->
</ul>
