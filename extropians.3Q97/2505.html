<!-- received="Wed Aug 27 04:45:41 1997 MDT" -->
<!-- sent="Wed, 27 Aug 1997 11:37:13 +0000" -->
<!-- name="Nicholas Bostrom" -->
<!-- email="bostrom@mail.ndirect.co.uk" -->
<!-- subject="Re: Goo prophylaxis (was: Hanson antiproliferation method?)" -->
<!-- id="199708271038.LAA00971@andromeda.ndirect.co.uk" -->
<!-- inreplyto="Goo prophylaxis (was: Hanson antiproliferation method?)" -->
<title>extropians: Re: Goo prophylaxis (was: Hanson antiproliferation method?)</title>
<h1>Re: Goo prophylaxis (was: Hanson antiproliferation method?)</h1>
Nicholas Bostrom (<i>bostrom@mail.ndirect.co.uk</i>)<br>
<i>Wed, 27 Aug 1997 11:37:13 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2505">[ date ]</a><a href="index.html#2505">[ thread ]</a><a href="subject.html#2505">[ subject ]</a><a href="author.html#2505">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2506.html">Nicholas Bostrom: "Re: Goo prophylaxis"</a>
<li> <b>Previous message:</b> <a href="2504.html">Anders Sandberg: "Re: Smoothing the Path[Was:Re: Singularity Passivity]"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
den Otter:<br>
<p>
<i>&gt; Nicholas Bostrom &lt;bostrom@mail.ndirect.co.uk&gt; wrote:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; Safe Libertarian Future:</i><br>
<i>&gt; &gt; The scenario assumes that many humans value freedom and independent </i><br>
<i>&gt; &gt; personal existence higher than anything else. When nanotechnology </i><br>
<i>&gt; &gt; approaches, they realise that if freedom is allowed in a world with </i><br>
<i>&gt; &gt; strong nanotech, then some mad person will certainly design the </i><br>
<i>&gt; &gt; doomsday virus. So they realise they have to give up on freedom. But </i><br>
<i>&gt; &gt; then some bright person comes up with the idea that all people upload </i><br>
<i>&gt; &gt; and that only a robot is left with the ability to operate in the real </i><br>
<i>&gt; &gt; world. The whole system is hardwired so that the robot only executes </i><br>
<i>&gt; &gt; instructions that have been agreed upon by the majority of the </i><br>
<i>&gt; &gt; uploads. In their virtual reality, the uploads can do anything they </i><br>
<i>&gt; &gt; want: each one has unlimited individual freedom. The only thing they </i><br>
<i>&gt; &gt; can't do in the virtual reality is to mass murder a lot of other </i><br>
<i>&gt; &gt; uploads (the virtual physics doesn't allow destructive nanomachines </i><br>
<i>&gt; &gt; to be built, for example). The uploads cannot influence the external </i><br>
<i>&gt; &gt; world either, except when a majority decision can be made. But for </i><br>
<i>&gt; &gt; many decisions, this should be feasible: e.g. colonising the galaxy </i><br>
<i>&gt; &gt; to provide more Lebensraum etc. One can even imagine refinements of </i><br>
<i>&gt; &gt; this scheme such that each individual would have his own robot that </i><br>
<i>&gt; &gt; he could to what he liked with; though this presupposes that the </i><br>
<i>&gt; &gt; robots could be built in such a way that nobody could use their robot </i><br>
<i>&gt; &gt; to do anything that would endanger the computer on which they all </i><br>
<i>&gt; &gt; existed.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; This is the only way I can think of that a very nearly </i><br>
<i>&gt; &gt; completely libertarian society, without any guardian or international </i><br>
<i>&gt; &gt; government, can exist long after the arrival of strong nanotech.</i><br>
<i>&gt; </i><br>
<i>&gt; There will always be (plenty) of people who'll refuse to get uploaded</i><br>
<i>&gt; into some virtual reality asylum, what about them? Would they be</i><br>
<i>&gt; forced to upload (upload or die!)?</i><br>
<p>
Yes.<br>
<p>
<i>&gt;IMHO a democratic system</i><br>
<i>&gt; like the one above is almost by definition a severe handicap in case</i><br>
<i>&gt; of a conflict with "free" outsiders, because while the democrates are</i><br>
<i>&gt; busy debating and voting, the enemy has already launched his proton</i><br>
<i>&gt; torpedoes or whatever. </i><br>
<p>
There is a good chance we will never be attacked by aliens. And if we <br>
were, then those aliens would surely be smart enough not to attack us <br>
unless they were certain that they could easily win whatever we did. <br>
If alien invasion really were an issue, we could develop an automated <br>
missile launch system or something like that.<br>
<p>
<i>&gt; What would happen if someone trashed the robot (the only outside link),</i><br>
<i>&gt; would the VRs be trapped in their "dreamworld" forever?</i><br>
<p>
Well, I spoke of "the robot" figuratively. In reality this would <br>
consist of millions of von Neumann probes expanding our computer in <br>
all directions.<br>
<p>
<i>&gt; Anyway, I think the only way you can stay reasonably free *and* safe is</i><br>
<i>&gt; when everybody (possibly in small like-minded groups) leaves earth and goes</i><br>
<i>&gt; in different directions. A 1.000.000 lightyears or so seem (with the</i><br>
<i>&gt; current laws of physics) like a pretty safe barrier.</i><br>
<p>
Well then you would agree that we need some temporary accomodation <br>
for the next million years or so.<br>
<p>
------------------------------------------------<br>
Nicholas Bostrom<br>
bostrom@ndirect.co.uk<br>
<p>
         *Visit my transhumanist web site at*<br>
            <a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2506.html">Nicholas Bostrom: "Re: Goo prophylaxis"</a>
<li> <b>Previous message:</b> <a href="2504.html">Anders Sandberg: "Re: Smoothing the Path[Was:Re: Singularity Passivity]"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
