<!-- received="Thu Aug 21 22:56:27 1997 MDT" -->
<!-- sent="Thu, 21 Aug 1997 23:28:51 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Emotions:  The Easy Part" -->
<!-- id="199708212327.QAA28945@igc3.igc.apc.org" -->
<!-- inreplyto="Emotions:  The Easy Part" -->
<title>extropians: Re: Emotions:  The Easy Part</title>
<h1>Re: Emotions:  The Easy Part</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Thu, 21 Aug 1997 23:28:51 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2330">[ date ]</a><a href="index.html#2330">[ thread ]</a><a href="subject.html#2330">[ subject ]</a><a href="author.html#2330">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2331.html">The Low Golden Willow: "Re: extropian kids (opinions from a non-parent)"</a>
<li> <b>Previous message:</b> <a href="2329.html">Eliezer S. Yudkowsky: "Sandberg 1, Yudkowsky 0"</a>
<li> <b>Maybe in reply to:</b> <a href="1803.html">Eliezer S. Yudkowsky: "Emotions:  The Easy Part"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2323.html">Len Flatley: "Re: extropian kids (opinions from a non-parent)"</a>
<li> <b>Reply:</b> <a href="2323.html">Len Flatley: "Re: extropian kids (opinions from a non-parent)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Darren Reynolds wrote:<br>
<i>&gt; </i><br>
<i>&gt; At 21:52 13/08/97 -0500, Eliezer S. Yudkowsky wrote:</i><br>
<p>
<i>&gt; &gt;Emotions aren't mysterious forces.  They can't hide from us.  Turing machines</i><br>
<i>&gt; &gt;are deterministic and wholly causal; any force operating on Deep Thought</i><br>
<i>&gt; would</i><br>
<i>&gt; &gt;be explicable as the sum of its parts.</i><br>
<i>&gt; </i><br>
<i>&gt; Obvious question: why do you think we are any different?</i><br>
<p>
We aren't.  Our emotions aren't hidden - it's obvious to anyone, internally<br>
and externally, that we have these things called "emotions".  You were<br>
proposing that Deep Thought had emotions and we didn't know about it.  My<br>
response was that emotions are not subjective philosophical constructs that we<br>
can assign or deny at our whim; they are cognitive processes that objectively<br>
exist - in all known cases, obviously so.  From the medial forebrain bundle,<br>
to the amygdala, to the mammillary bodies... the entire limbic system... there<br>
are numerous parts of the brain specialized for handling emotions.  Emotions<br>
don't just happen, they are evolutionarily designed to serve specific<br>
functions.  Insofar as Deep Thought doesn't evolve, wasn't designed to feel,<br>
and contains no hidden processes - which assumptions I debated in the previous<br>
letter - it is not likely to have emotions remotely analogous to those which<br>
take up so much explicit processing power in our own minds.<br>
<p>
<i>&gt; &gt;&gt; When Deep Blue won the series recently, I wondered whether it felt a most</i><br>
<i>&gt; &gt;&gt; primitive sense of achievement, in the same way that our own primitive</i><br>
<i>&gt; &gt;&gt; ancestors might have felt a sense of achievement if they had lifted their</i><br>
<i>&gt; &gt;&gt; first stone, and successfully hurled it at a passing antelope. Yes, the</i><br>
<i>&gt; &gt;&gt; thing is just a mathematical algorithm. But so, apparently, are we.</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt;It didn't.  Deep Blue may have emergent chess stratagems from the complex</i><br>
<i>&gt; &gt;physical-process-level chess exploration algorithms.  I don't think there was</i><br>
<i>&gt; &gt;any form of feedback for successful and unsuccessful moves.  In training</i><br>
<i>&gt; mode ...</i><br>
<i>&gt; </i><br>
<i>&gt; Yeah, but this misses the point. There IS feedback in the form of</i><br>
<i>&gt; selection. If Deep Blue makes bad moves, IBM will trash it. There doesn't</i><br>
<i>&gt; have to be anything in the code. It's the same feedback (and the only</i><br>
<i>&gt; feedback) that our own evolutionary path had until very recently.</i><br>
<p>
I am not interested in debating what evolution "really" is.  The evolution you<br>
name had no effect on Deep Blue's design.  Deep Blue was designed by a bunch<br>
of humans; it was not designed by selection of any type.  The weightings may<br>
have been evolutionary; the architecture was not.<br>
<p>
<i>&gt; &gt;Except from a functionalist perspective, there wouldn't be much</i><br>
<i>&gt; &gt;internal difference between "pleasure" and "pain" - just negative and</i><br>
<i>&gt; positive</i><br>
<i>&gt; &gt;numbers.</i><br>
<i>&gt; </i><br>
<i>&gt; Right. Whereas in humans, the internal difference between pleasure and pain</i><br>
<i>&gt; is ... er, what is the internal difference in humans exactly?</i><br>
<p>
Pleasure is handled by the medial forebrain bundle... though this is only one<br>
kind of pleasure; "satiation" pleasure is handled by an entirely different<br>
area.  I don't recall where the pain center(s) are, but they are *elsewhere*. <br>
So in response, pleasure and pain are handled by entirely different sections<br>
of the brain, feel entirely different from a subjective standpoint, have<br>
wholly different effects, and in general are totally different subsystems. <br>
Ask not how they differ; ask how they are alike.<br>
<p>
<i>&gt; &gt;Emotions did indeed evolve because they are evolutionary advantages.</i><br>
<i>&gt; Although</i><br>
<i>&gt; &gt;Deep Blue's weightings for piece value may be "evolutionary" in some sense, I</i><br>
<i>&gt; &gt;don't think the term can really apply in the sense you use it.  Linear</i><br>
<i>&gt; numbers</i><br>
<i>&gt; &gt;aren't complex enough to "evolve"; evolving sequences of instructions, as in</i><br>
<i>&gt; &gt;the TIERRA environment, are another matter.</i><br>
<i>&gt; </i><br>
<i>&gt; Again, I think that this misses the point. I argue that if humans alter a</i><br>
<i>&gt; system in a way which leads to greater levels of reproduction for that</i><br>
<i>&gt; system, then the system has evolved. The agent causing the evolution is</i><br>
<i>&gt; irrelevant. The code doesn't have to learn from its mistakes in chess.</i><br>
<i>&gt; There merely has to be an environment which prefers good moves, with</i><br>
<i>&gt; penalties and rewards that affect reproductive success.</i><br>
<p>
I disagree.  Programs designed by humans tend to work one way.  Programs<br>
designed by evolutionary computation work a completely different way.  This is<br>
what makes the distinction between "evolved" and "designed" programs useful. <br>
The narrowness and genuine distinguishing power of the term is what makes<br>
evolutionary computation a science.  What practical purpose is served by<br>
broadening the term as you suggest?<br>
<p>
<i>&gt; Precisely. You are interested only in your own emotions. That's all you can</i><br>
<i>&gt; be. In fact, you ought (I don't mean this nastily at all, but</i><br>
<i>&gt; scientifically) to be interested in only yourself, because you can't be</i><br>
<i>&gt; sure that anything else exists.</i><br>
<p>
I couldn't fail to disagree with you less!  I can study emotions though<br>
experiment.  I can study the neural architectures of the areas of the brain<br>
which subserve emotions.  I can study subjective cost-benefit ratios, and how<br>
our causal handling of them differs from Deep Blue's linear branching. <br>
Emotions are REAL and objective, not subjective.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2331.html">The Low Golden Willow: "Re: extropian kids (opinions from a non-parent)"</a>
<li> <b>Previous message:</b> <a href="2329.html">Eliezer S. Yudkowsky: "Sandberg 1, Yudkowsky 0"</a>
<li> <b>Maybe in reply to:</b> <a href="1803.html">Eliezer S. Yudkowsky: "Emotions:  The Easy Part"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2323.html">Len Flatley: "Re: extropian kids (opinions from a non-parent)"</a>
<li> <b>Reply:</b> <a href="2323.html">Len Flatley: "Re: extropian kids (opinions from a non-parent)"</a>
<!-- reply="end" -->
</ul>
