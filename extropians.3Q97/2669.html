<!-- received="Sat Aug 30 22:22:48 1997 MDT" -->
<!-- sent="Sat, 30 Aug 1997 20:49:38 -0700 (PDT)" -->
<!-- name="John K Clark" -->
<!-- email="johnkc@well.com" -->
<!-- subject="Black Goo" -->
<!-- id="199708310349.UAA08335@well.com" -->
<!-- inreplyto="" -->
<title>extropians: Black Goo</title>
<h1>Black Goo</h1>
John K Clark (<i>johnkc@well.com</i>)<br>
<i>Sat, 30 Aug 1997 20:49:38 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2669">[ date ]</a><a href="index.html#2669">[ thread ]</a><a href="subject.html#2669">[ subject ]</a><a href="author.html#2669">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2670.html">Anders Sandberg: "Re: NANO: Goo color"</a>
<li> <b>Previous message:</b> <a href="2668.html">GBurch1@aol.com: "META: dropped posts"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
-----BEGIN PGP SIGNED MESSAGE-----<br>
<p>
Some people say (correctly I think) that we can know almost nothing about <br>
what things will be like after the singularity, BUT then they tacitly assume <br>
that the rate of change after The Spike will level out into a plateau of <br>
sorts, a very high plateau to be sure but a plateau nevertheless. I see no <br>
reason why that should be true and think we should expect an accelerating <br>
rate of change for the indefinite future.<br>
 <br>
Let's assume sometime after the singularity, around breakfast, a suicidal nut <br>
case makes some black goo with the intention of destroying the world. About <br>
the only thing I can say with any confidence about the world after the <br>
singularity is that life will be fast and things will change, so by lunch <br>
time this black goo would seem comically old fashioned, positively stone age<br>
compared to the vastly improved nanotechnology that had been developed in the <br>
following 4 hours, an eon of subjective time. The black goo might look pretty <br>
scary in the first few milliseconds after it was made, but after a geological <br>
age (a minute or two) it would seem more pitiful than scary, like a man armed <br>
with a flint ax trying to concur the world.<br>
       <br>
In the long run intelligence will always beat stupidity, so to be really <br>
dangerous, life destroying dangerous, you'd need to make an intelligence <br>
hostile to all life that could improve itself faster than any machine anybody <br>
else could build. That's a very tall order and far beyond the capacity of  <br>
"goo" of any color that some Bozo cooked up in his basement.<br>
<p>
<p>
                                             John K Clark    johnkc@well.com<br>
<p>
-----BEGIN PGP SIGNATURE-----<br>
Version: 2.6.i<br>
<p>
iQCzAgUBNAjmz303wfSpid95AQGuZQTw7jrE680ZoiOV1sEdHKgy7/ZWdJ1S7JWT<br>
Ul3+jbT3swRXul01ZKf0kHxom2w2Egwx6gteZLL6+OTYvIwoJ/s0GfM62Oi8MJlc<br>
IbXSDUnEOOz5dI/6b9Kv3LSS18rl0pErVAILQORXcBFx6Zo93RV1FZYY2IG2GSR9<br>
AbjeGnCi44KjVVMRmdOW2Rbw+bxVXffmv1Gmv/cepQJ8j2x2Pjo=<br>
=2zLi<br>
-----END PGP SIGNATURE-----<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2670.html">Anders Sandberg: "Re: NANO: Goo color"</a>
<li> <b>Previous message:</b> <a href="2668.html">GBurch1@aol.com: "META: dropped posts"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
