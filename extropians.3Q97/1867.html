<!-- received="Wed Aug 13 16:19:47 1997 MDT" -->
<!-- sent="Wed, 13 Aug 1997 21:52:32 +0100" -->
<!-- name="Darren Reynolds" -->
<!-- email="extro@blue.demon.co.uk" -->
<!-- subject="Re: Emotions:  The Easy Part" -->
<!-- id="3.0.1.32.19970813215232.006c2a74@pop3.demon.co.uk" -->
<!-- inreplyto="33F0FC96.ADAB463E@pobox.com" -->
<title>extropians: Re: Emotions:  The Easy Part</title>
<h1>Re: Emotions:  The Easy Part</h1>
Darren Reynolds (<i>extro@blue.demon.co.uk</i>)<br>
<i>Wed, 13 Aug 1997 21:52:32 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1867">[ date ]</a><a href="index.html#1867">[ thread ]</a><a href="subject.html#1867">[ subject ]</a><a href="author.html#1867">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1868.html">Geoff Smith: "Re: Galaxy Brain Problem"</a>
<li> <b>Previous message:</b> <a href="1866.html">Darren Reynolds: "Re: Galaxy brain problem"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
At 19:15 12/08/97 -0500, Eliezer S. Yudkowsky wrote:<br>
<p>
<i>&gt;But if a human tried to imitate Deep Thought - which is how we describe</i><br>
it, by<br>
<i>&gt;putting ourselves in its place - he would have to suppress his emotions,</i><br>
<i>&gt;because Deep Thought doesn't have any emotions.</i><br>
<p>
I only recently began to read your posts, but already find each one<br>
fascinating.<br>
<p>
A question for you.<br>
<p>
Is it not possible that our emotion is an illusion which emerges from our<br>
programmed reaction to certain stimuli? A tiger approaches; we appear to<br>
get frightened. Perhaps objectively we merely go through physiological<br>
changes which prepare us to escape injury. <br>
<p>
If that were the case, then is it not also possible that Deep Thought and<br>
its successors have similar illusionary emotions? Suppose Deep Blue wins a<br>
match. Whilst all the while appearing the same, staid machine predictably<br>
pulling bytes from RAM and processing them, it deviates slightly and pulls<br>
a different set of bytes to usual. Nothing odd to the designer there. But<br>
then there is nothing odd about a little adrenalin escaping into our blood<br>
stream, and our heart rate going up in the example with the tiger.<br>
<p>
When Deep Blue won the series recently, I wondered whether it felt a most<br>
primitive sense of achievement, in the same way that our own primitive<br>
ancestors might have felt a sense of achievement if they had lifted their<br>
first stone, and successfully hurled it at a passing antelope. Yes, the<br>
thing is just a mathematical algorithm. But so, apparently, are we.<br>
<p>
I suspect that "emotions" have evolved because they are an evolutionary<br>
advantage. Deep Blue has more chance of survival if it wins matches, and<br>
hence suffers selection pressures of the same kind that our ancestors did.<br>
If each system is a logical parallel of the other, then are "emotions" not<br>
possible results in both cases?<br>
<p>
If we accept that emotions are illusionary, then I doubt that the<br>
capability to experience emotion has a Boolean value. Probably, all systems<br>
have emotion, but today, most humans are capable of perceiving that emotion<br>
only in certain animals.<br>
<p>
<p>
<i>&gt;Modern computers are emotionless, and utterly unintelligent.  Their level of</i><br>
<i>&gt;"pattern" ranges from undirected physical processes such as Deep Thought, to</i><br>
<i>&gt;bacterium-level organisms such as a word processor, to the insect-level</i><br>
<i>&gt;mind-boggling complexity of Windows 95.</i><br>
<p>
You seem to be implying that insects don't have emotions. Have you never<br>
seen an angry wasp? The tone of its beating wings rises, it flies faster,<br>
and its sting hangs lower.<br>
<p>
If I understand you correctly, then doesn't the fact that you don't see<br>
emotion in insects whilst I do, prove that beauty is in the eye of the<br>
beholder?<br>
<p>
Emotions in computers may be even easier than you intended to point out!<br>
<p>
<p>
<i>&gt;The only reason why we don't already have emotional computers - it is a</i><br>
simple<br>
<i>&gt;problem - is that there are no perceived profits.  As soon as somebody</i><br>
figures<br>
<i>&gt;out how to make it sell, we'll have emotional computers.</i><br>
<p>
Nice one!<br>
<p>
Regards,<br>
Darren<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1868.html">Geoff Smith: "Re: Galaxy Brain Problem"</a>
<li> <b>Previous message:</b> <a href="1866.html">Darren Reynolds: "Re: Galaxy brain problem"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
