<!-- received="Wed Aug 13 11:54:37 1997 MDT" -->
<!-- sent="Wed, 13 Aug 1997 10:21:24 -0700 (PDT)" -->
<!-- name="Geoff Smith" -->
<!-- email="geoffs@unixg.ubc.ca" -->
<!-- subject="Re: Galaxy Brain Problem" -->
<!-- id="9707138715.AA871500452@ccmgate.platinum.com" -->
<!-- inreplyto="9707128714.AA871442474@ccmgate.platinum.com" -->
<title>extropians: Re: Galaxy Brain Problem</title>
<h1>Re: Galaxy Brain Problem</h1>
Geoff Smith (<i>geoffs@unixg.ubc.ca</i>)<br>
<i>Wed, 13 Aug 1997 10:21:24 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1844">[ date ]</a><a href="index.html#1844">[ thread ]</a><a href="subject.html#1844">[ subject ]</a><a href="author.html#1844">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1845.html">Tony B. Csoka: "Re: Galaxy brain death"</a>
<li> <b>Previous message:</b> <a href="1843.html">Anders Sandberg: "Re: Re [2]: Extropy in the personal sphere"</a>
<li> <b>In reply to:</b> <a href="1805.html">Rick Knight: "Galaxy Brain Problem"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1886.html">Eliezer S. Yudkowsky: "Re: Galaxy Brain Problem"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Tue, 12 Aug 1997, Rick Knight wrote:<br>
<p>
<i>&gt;      Geoff Smith wrote:</i><br>
<i>&gt;      </i><br>
<i>&gt;      Is it paranoia to think that the universe will only become more </i><br>
<i>&gt;      Darwinian after the singularity?</i><br>
<i>&gt;      </i><br>
<i>&gt;      </i><br>
<i>&gt;      Rick Knight responds:</i><br>
<i>&gt;      </i><br>
<i>&gt;      Perhaps a more pertinent question is are you devoting a single </i><br>
<i>&gt;      synaptic flash of energy towards this manufactured notion?  Pardon me </i><br>
<i>&gt;      but, unless you're being tongue-in-cheek (and there is no evidence in </i><br>
<i>&gt;      your post to that effect), what could be the possible relevence of </i><br>
<i>&gt;      such a question?  It sure beats agoraphobia for unreasonable </i><br>
<i>&gt;      disorienting fears!</i><br>
<p>
I'm sorry if my question is more provocative than it is substantial, but<br>
I'm afraid my inadequate synapses cannot fire up a connection between my<br>
question and agoraphobia!  "Paranoia" was an unfortunate and hasty word<br>
choice.  What I am really saying is that I am *concerned* for my security<br>
in the future.  How could anyone seeking indefinite lifespan not be<br>
concerned?  <br>
	It is not that I think anarchy and natural selection are in<br>
themselves bad things, but a high selection gradient can be a little<br>
unsettling, especially for a person of lesser synaptic integrity such as <br>
myself.  What would cause a high selection gradient?  When/if we run low<br>
on matter.  When the very building blocks of our existence(unless you<br>
believe in a soul) are in scarce supply.<br>
	The only immediate solution I can see to this problem is strategic<br>
alliances.  I won't eat your galaxy brain if you don't eat mine.  But what<br>
means to we have to study and prepare ourselves for these alliances?  As<br>
someone in another thread pointed out, game theory does not apply to<br>
post-singularity entities.  What can we do for security in a future of<br>
uncertainty?<br>
<p>
<p>
geoff.  <br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1845.html">Tony B. Csoka: "Re: Galaxy brain death"</a>
<li> <b>Previous message:</b> <a href="1843.html">Anders Sandberg: "Re: Re [2]: Extropy in the personal sphere"</a>
<li> <b>In reply to:</b> <a href="1805.html">Rick Knight: "Galaxy Brain Problem"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1886.html">Eliezer S. Yudkowsky: "Re: Galaxy Brain Problem"</a>
<!-- reply="end" -->
</ul>
