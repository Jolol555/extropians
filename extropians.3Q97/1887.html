<!-- received="Wed Aug 13 22:11:09 1997 MDT" -->
<!-- sent="Wed, 13 Aug 1997 21:52:40 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Emotions:  The Easy Part" -->
<!-- id="199708140243.TAA20141@netcom11.netcom.com" -->
<!-- inreplyto="Emotions:  The Easy Part" -->
<title>extropians: Re: Emotions:  The Easy Part</title>
<h1>Re: Emotions:  The Easy Part</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Wed, 13 Aug 1997 21:52:40 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1887">[ date ]</a><a href="index.html#1887">[ thread ]</a><a href="subject.html#1887">[ subject ]</a><a href="author.html#1887">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1888.html">Peter C. McCluskey: "Re: Skitzoid"</a>
<li> <b>Previous message:</b> <a href="1886.html">Eliezer S. Yudkowsky: "Re: Galaxy Brain Problem"</a>
<li> <b>Maybe in reply to:</b> <a href="1803.html">Eliezer S. Yudkowsky: "Emotions:  The Easy Part"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2330.html">Eliezer S. Yudkowsky: "Re: Emotions:  The Easy Part"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Darren Reynolds wrote:<br>
<i>&gt; </i><br>
<i>&gt; A question for you.</i><br>
<i>&gt; </i><br>
<i>&gt; Is it not possible that our emotion is an illusion which emerges from our</i><br>
<i>&gt; programmed reaction to certain stimuli? A tiger approaches; we appear to</i><br>
<i>&gt; get frightened. Perhaps objectively we merely go through physiological</i><br>
<i>&gt; changes which prepare us to escape injury.</i><br>
<p>
As such, not very likely.  I have my own theories about how emotions work; it<br>
has to do with the existence of cognitive objects called "goals"; most of the<br>
interesting stuff comes from our two-tiered architecture, the pleasure/pain<br>
system's centrality, and the coordination between goal-system and worldview.<br>
<p>
Point being... no, this doesn't seem very likely.  Aside from the subjective<br>
view, the emotional processes are too complex to be purely reflexive or<br>
behavioristic.  The amount of grief parents experience over the death of a<br>
child correlates roughly with the amount of effort wasted, from a reproductive<br>
cost-benefit view.  This is evolutionary, rather than conscious, in origin...<br>
and demonstrates that evolution can control the emotions in detail. <br>
Evolutionary psychology explains emotions far more complex and devious than<br>
fright and flight. <br>
<p>
The subjective aspect of emotions does come from an entire orchestra of goals,<br>
viewpoint alterations, belief changes, and physiological responses which<br>
stimulate further changes... but all the cognitive stuff is still there; it is<br>
an integral part.<br>
<p>
Finally, although this is getting into the hard problem of conscious<br>
experience, I don't think the physiological stuff can explain the subjective<br>
aspect.  Is adrenaline conscious?<br>
<p>
<i>&gt; If that were the case, then is it not also possible that Deep Thought and</i><br>
<i>&gt; its successors have similar illusionary emotions? Suppose Deep Blue wins a</i><br>
<i>&gt; match. Whilst all the while appearing the same, staid machine predictably</i><br>
<i>&gt; pulling bytes from RAM and processing them, it deviates slightly and pulls</i><br>
<i>&gt; a different set of bytes to usual. Nothing odd to the designer there. But</i><br>
<i>&gt; then there is nothing odd about a little adrenalin escaping into our blood</i><br>
<i>&gt; stream, and our heart rate going up in the example with the tiger.</i><br>
<p>
Emotions aren't mysterious forces.  They can't hide from us.  Turing machines<br>
are deterministic and wholly causal; any force operating on Deep Thought would<br>
be explicable as the sum of its parts.<br>
<p>
<i>&gt; When Deep Blue won the series recently, I wondered whether it felt a most</i><br>
<i>&gt; primitive sense of achievement, in the same way that our own primitive</i><br>
<i>&gt; ancestors might have felt a sense of achievement if they had lifted their</i><br>
<i>&gt; first stone, and successfully hurled it at a passing antelope. Yes, the</i><br>
<i>&gt; thing is just a mathematical algorithm. But so, apparently, are we.</i><br>
<p>
It didn't.  Deep Blue may have emergent chess stratagems from the complex<br>
physical-process-level chess exploration algorithms.  I don't think there was<br>
any form of feedback for successful and unsuccessful moves.  In training mode<br>
- if it possesses such a thing - it might be said that it experiences<br>
"pleasure" at a successful move.  In truth, I think this is torturing the<br>
metaphor.  Except from a functionalist perspective, there wouldn't be much<br>
internal difference between "pleasure" and "pain" - just negative and positive<br>
numbers.  If Deep Blue were reversed to make the worst possible move, it might<br>
not require much reprogramming... i.e., the distinction between pleasure and<br>
pain would be surface, rather than deep causal.<br>
<p>
<i>&gt; I suspect that "emotions" have evolved because they are an evolutionary</i><br>
<i>&gt; advantage. Deep Blue has more chance of survival if it wins matches, and</i><br>
<i>&gt; hence suffers selection pressures of the same kind that our ancestors did.</i><br>
<i>&gt; If each system is a logical parallel of the other, then are "emotions" not</i><br>
<i>&gt; possible results in both cases?</i><br>
<p>
Emotions did indeed evolve because they are evolutionary advantages.  Although<br>
Deep Blue's weightings for piece value may be "evolutionary" in some sense, I<br>
don't think the term can really apply in the sense you use it.  Linear numbers<br>
aren't complex enough to "evolve"; evolving sequences of instructions, as in<br>
the TIERRA environment, are another matter.<br>
<p>
Deep Blue, again, is not that level of AI.  It is a brute-force mechanism.  I<br>
don't believe it possesses evolvable heuristics as did EURISKO, or evolvable<br>
concepts as did AM, or evolvable anything except weightings.  And weightings,<br>
not possessing much in the way of causal force - though this is true only for<br>
Deep Blue and the like - are not emotions.  At the very best, they could be<br>
instincts or reflexes, possessing no cognitive correlates.<br>
<p>
It is possible that the weightings of piece value have far and subtle<br>
consequences in Deep Blue's Great Search Tree.  Kasparov has stated that Deep<br>
Blue had a new kind of intelligence.  It could be that all the interacting<br>
weightings had the same kind of emergent effect as all our interacting neural<br>
weightings... although on a much smaller scale!  It would take a lot of<br>
looking, and specialized tools, to detect those patterns.  The fundamental<br>
questions are these:<br>
<p>
1.  Are the patterns complex?  Are they non-linear, non-algebraic?  Are they<br>
divisible into sub-patterns?  Are the patterns "causal" - composed of the<br>
complex interaction of sub-patterns?  Are they Turing complete?<br>
<p>
2.  Do the patterns evolve?  Can they reproduce?  Can they mix?  Do they compete?<br>
<p>
Although not an expert on search trees, I very much suspect that search trees,<br>
or at least A* and variants, are not that complicated.  They may be<br>
Turing-complete in theory, although I actually doubt it... but that Turing<br>
completeness may not be translatable into a level where patterns could<br>
"evolve" and the best patterns would win games.  A lot of technically Turing<br>
complete processes' Universal Turing Machines are *very* far from the level in<br>
which they are implemented.<br>
<p>
It is possible.  There are theories in which "hedonistic neurons" - neurons<br>
that want to fire as often as possible - provide the basic organizing<br>
principle for the brain.  We are faced not merely with that, but with<br>
requiring that the most often-firing neurons are the ones which have the best<br>
solutions to some cortical-level problem.  On the other hand, Deep Blue's<br>
basic level is much more directly semantic than the basic neural level.<br>
<p>
<i>&gt; If we accept that emotions are illusionary, then I doubt that the</i><br>
<i>&gt; capability to experience emotion has a Boolean value. Probably, all systems</i><br>
<i>&gt; have emotion, but today, most humans are capable of perceiving that emotion</i><br>
<i>&gt; only in certain animals.</i><br>
<p>
Again, I disagree.  Once I argued over whether a thermostat had meaning.  I<br>
took the position that it was meaningless, because meaning was a far cognitive<br>
function, or even a far physical function.  Likewise, emotion as we perceive<br>
it is a functional or surface matter, often depending on how fuzzy and lovable<br>
something *looks*.  Emotion, as that which produces the functions, is a<br>
cognitive matter.  The subjective aspect of emotions is a far cognitive<br>
matter, impinging on things that, quite frankly, I don't think any mortal<br>
being will ever understand.  But we don't need the subjective aspect, any more<br>
than we need conscious experience for human-equivalent intelligence.<br>
<p>
<i>&gt; &gt;Modern computers are emotionless, and utterly unintelligent.  Their level of</i><br>
<i>&gt; &gt;"pattern" ranges from undirected physical processes such as Deep Thought, to</i><br>
<i>&gt; &gt;bacterium-level organisms such as a word processor, to the insect-level</i><br>
<i>&gt; &gt;mind-boggling complexity of Windows 95.</i><br>
<i>&gt; </i><br>
<i>&gt; You seem to be implying that insects don't have emotions. Have you never</i><br>
<i>&gt; seen an angry wasp? The tone of its beating wings rises, it flies faster,</i><br>
<i>&gt; and its sting hangs lower.</i><br>
<p>
So if I have a program,<br>
<p>
struct Wasp {<br>
	float wing_tone;<br>
	float air_speed;<br>
	float sting_angle;<br>
};<br>
<p>
and I adjust the three variables you mentioned, the program has emotions?<br>
<p>
Likewise, if I videotape a wasp and play the videotape, does the videotape<br>
have emotions?  Certainly you'll ascribe emotions to the thing you see on<br>
videotape, even though it's simply a recorded image.<br>
<p>
It's not how it looks... it's the functional part.<br>
<p>
<i>&gt; If I understand you correctly, then doesn't the fact that you don't see</i><br>
<i>&gt; emotion in insects whilst I do, prove that beauty is in the eye of the</i><br>
<i>&gt; beholder?</i><br>
<i>&gt; </i><br>
<i>&gt; Emotions in computers may be even easier than you intended to point out!</i><br>
<p>
Computer programs work exactly the same, regardless of what we perceive in<br>
them.  The same, to some extent, might be said of reality... except that we're<br>
a part of it.  The way we ascribe emotions is a fruitful area of cognitive<br>
science.  It is not, however, a fruitful area of philosophy.  Any emotions<br>
that are observer-dependent, I am not interested in.  Likewise for other AI<br>
"achievements" that require me to believe the symbol G0025 represents a<br>
hamburger merely because the string "hamburger" is attached.<br>
<p>
Profitable computer emotions will require functional capability, not merely<br>
surface resemblance.  Rick Knight doesn't want to comfort a forlorn computer<br>
program... well, that's not what I had in mind.  More like a program that<br>
could be frightened by an imminent crash and save your data, or could "want"<br>
to send mail and take all necessary actions to do it.  Once you have the goal<br>
system for setting up those chains, doglike emotions are only a short step away.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1888.html">Peter C. McCluskey: "Re: Skitzoid"</a>
<li> <b>Previous message:</b> <a href="1886.html">Eliezer S. Yudkowsky: "Re: Galaxy Brain Problem"</a>
<li> <b>Maybe in reply to:</b> <a href="1803.html">Eliezer S. Yudkowsky: "Emotions:  The Easy Part"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2330.html">Eliezer S. Yudkowsky: "Re: Emotions:  The Easy Part"</a>
<!-- reply="end" -->
</ul>
