<!-- received="Fri Aug 29 12:45:57 1997 MDT" -->
<!-- sent="Fri, 29 Aug 1997 11:44:36 -0700" -->
<!-- name="Eric Watt Forste" -->
<!-- email="arkuat@pobox.com" -->
<!-- subject="Goo prophylaxis" -->
<!-- id="199708291844.LAA26868@idiom.com" -->
<!-- inreplyto="" -->
<title>extropians: Goo prophylaxis</title>
<h1>Goo prophylaxis</h1>
Eric Watt Forste (<i>arkuat@pobox.com</i>)<br>
<i>Fri, 29 Aug 1997 11:44:36 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2603">[ date ]</a><a href="index.html#2603">[ thread ]</a><a href="subject.html#2603">[ subject ]</a><a href="author.html#2603">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2604.html">YakWaxx@aol.com: "Re: Government's war against itself???"</a>
<li> <b>Previous message:</b> <a href="2602.html">Eric Watt Forste: "Goo prophylaxis"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Nicholas Bostrom writes:<br>
<i> &gt; Given the values, it may or may not be trivial to translate them</i><br>
<i> &gt; into micro-level descriptions of the value-optimal physical state.</i><br>
<p>
I suspect that an entity driven by values that were trivial to<br>
translate into micro-level descriptions of the value-optimal physical<br>
state would quickly succumb to competition from strategies of<br>
entities with more complex values with more unpredictable outcomes.<br>
<p>
An entity with more complex values would not find it trivial to do<br>
calculations of optimality, and would likely refrain from destroying<br>
computing hardware for fear of inadvertently introducing a suboptimal<br>
overall state. Particularly since market interactions allow one to<br>
freely exploit as much computing power as one can pay for (and I'm<br>
including human attention and cognition in here as well... many of<br>
us on the list are professionals paid for our personal computing<br>
time).<br>
<p>
<i> &gt; That depends on what the values are. But whatever they are, we can</i><br>
<i> &gt; be pretty sure they are more likely to be made real by a </i><br>
<i> &gt; superinelligence who holds those values than by one who doesn't.</i><br>
<i> &gt; (Unless the values intrinsically involve, say,  respect for</i><br>
<i> &gt; independent individuals or such ethical stuff.) The superintelligence</i><br>
<i> &gt; realizes this and decides to junk the other computers in the</i><br>
<i> &gt; universe, if it can, since they are in the way when optimising the</i><br>
<i> &gt; SI's values.</i><br>
<p>
I don't know.  I see existing hardware as a resource to be used,<br>
not as raw feedstock atoms.  People fantasize a lot about ab-initio<br>
scenarios, but ab-initio stuff on a large scale is usually much<br>
less economical than bootstrapping from real, existing, computational<br>
resources.  Current economic theory (I still favor the Austrian<br>
school) looks abstract, general, *and* solid enough to me that I<br>
doubt superintelligences are going to be able to evade its dictates.<br>
<p>
<pre>
--
Eric Watt Forste ++ arkuat@pigdog.org ++ expectation foils perception -pcd
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2604.html">YakWaxx@aol.com: "Re: Government's war against itself???"</a>
<li> <b>Previous message:</b> <a href="2602.html">Eric Watt Forste: "Goo prophylaxis"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
