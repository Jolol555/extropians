<!-- received="Thu Sep 25 23:15:05 1997 MDT" -->
<!-- sent="Thu, 25 Sep 1997 20:51:50 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Singularity: Generation gap" -->
<!-- id="199709260008.UAA21470@freenet.npiec.on.ca" -->
<!-- inreplyto="Singularity: Generation gap" -->
<title>extropians: Re: Singularity: Generation gap</title>
<h1>Re: Singularity: Generation gap</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Thu, 25 Sep 1997 20:51:50 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4400">[ date ]</a><a href="index.html#4400">[ thread ]</a><a href="subject.html#4400">[ subject ]</a><a href="author.html#4400">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4401.html">Mitchell Porter: "What should you ask a wish machine to do for you?"</a>
<li> <b>Previous message:</b> <a href="4399.html">Eliezer S. Yudkowsky: "Re: Singularity: Generation gap"</a>
<li> <b>Maybe in reply to:</b> <a href="4352.html">Eliezer S. Yudkowsky: "Singularity: Generation gap"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="4403.html">Mikael Johansson: "Re: Singularity: Generation gap"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
The Low Golden Willow wrote:<br>
<i>&gt; </i><br>
<i>&gt; Er, what were you thinking of by "Your generation"?  I'm 22.  And when</i><br>
<i>&gt; the Singularity could be something like the Blight I don't think fear of</i><br>
<i>&gt; it is connected to discomfort with computers.  Or fear of being</i><br>
<i>&gt; uploaded.  It's more a fear of being wiped out.</i><br>
<p>
Yes, but *why* does someone fear being wiped out?<br>
Maybe it's indirect - xenophobes to fiction to popular "Borg" image to fear -<br>
but the root still lies in xenophobia.  And someone totally comfortable with<br>
computers is less likely to have an *irrational* fear, or to have a big<br>
emotional investment in that fear.<br>
<p>
I don't think I'd like to be at Ground Zero of a Blight either.  This means<br>
I'll be careful - not that I'll be frightened of the Singularity.  Are you<br>
frightened of your car because a crash at 60 mph would wipe you out?  No,<br>
because you're used to going very fast.  Speed doesn't scare you.<br>
<p>
But take someone who's never been in a car out on the Interstate...<br>
<p>
Re: Generations - since my postulated threshold was learning to use a computer<br>
before puberty (the usual maturation window), then you might well be of a<br>
different generation.  Or maybe not.  It was just a suggestion.  I think some<br>
study or other demonstrated that "generation gaps" actually were occurring at<br>
4-or-5 year intervals, or something like that.<br>
<p>
My little brother (age 12) is reading Broderick's "The Spike".  Will he have a<br>
still different attitude towards Singularity when he grows older?  Time will tell.<br>
<p>
The early-adopter issue may confuse it also.  Is everyone of the same age in<br>
the same generation?<br>
<p>
And finally, you might not be a counterexample to my basic example.  If you<br>
reject the Transcension on purely rational grounds (no fear) and I adopt it on<br>
equally rational grounds, then this would exactly support my point.<br>
<p>
<i>&gt; } You can use computers, but can you freely and without prejudice regard them as</i><br>
<i>&gt; } human?  Not that my Plus was human, but there may be a hardwired maturation</i><br>
<i>&gt; </i><br>
<i>&gt; Regard them as human?  Half the time I regard myself as "an AI wannabee</i><br>
<i>&gt; trapped in a male human body".</i><br>
<p>
There 'ya go.  I agree fully.<br>
<p>
<i>&gt; The difference between the Culture and</i><br>
<i>&gt; the Singularity isn't one of AI rights, or technical capability, or</i><br>
<i>&gt; maximum intelligence.  It's one of ethics, of the behaviors of</i><br>
<i>&gt; civilization surviving through and beyond a rapid transition.  If I was</i><br>
<i>&gt; in the Culture I'd happily fork, or even destructively upload, and grow</i><br>
<i>&gt; my way up to Mind status.</i><br>
<p>
What is the "Culture", and who is this author whose books I clearly need to buy?<br>
<p>
But going on your description of the Culture, it sounds like an Abort - a "go<br>
two inches into the Singularity and freeze" scenario.  Or a Kadath where<br>
humans may upgrade at leisure.<br>
<p>
The former sounds like a simple failure of imagination.  The latter depends on<br>
Power Ethics, which I'm tired of talking about.  Flip a coin.<br>
<p>
<i>&gt; But I'd be happy knowing that the Amish were still toiling the soil in</i><br>
<i>&gt; Pennsylvania.  Not because I have great love for the Amish, but because</i><br>
<i>&gt; a Mind-civilization which had that much respect for pre-existing</i><br>
<i>&gt; sentience would be a profoundly safe and libertarian (liberal) society</i><br>
<i>&gt; for anyone else.  The image of Singularity I get these days is one of</i><br>
<i>&gt; everything going totally haywire.  But I don't see any proof that's</i><br>
<i>&gt; inevitable.  So I root for civlization.</i><br>
<i>&gt; </i><br>
<i>&gt; } And I don't find it frightening at all to contemplate stepping entirely into</i><br>
<i>&gt; } Other Reality.  I grew up there, after all - it's as much home to me as</i><br>
<i>&gt; </i><br>
<i>&gt; There's a difference between "stepping into" and "being sucked up".</i><br>
<p>
You're acclimated to uploads.  You're not (instinctively) frightened at all. <br>
You're just worried about preserving your Libertarian principles.  You don't<br>
want to coerce the Amish and you don't want to be coerced.  Well, good for<br>
you.  If I was in charge of uploading humanity, you'd better believe it would<br>
be voluntary - except *maybe* in case of death, and even then as a static<br>
copy.  But I don't expect to be in charge, and it looks to me like the logic<br>
of Libertarianism breaks down if you're omniscient For All Practical Purposes.<br>
<p>
<i>&gt; } Singularity.  Then I continued the calculation to find that 3 years after</i><br>
<i>&gt; } human-equivalent AIs, AIs reach infinite speed, and I called *that* the</i><br>
<i>&gt; </i><br>
<i>&gt; } You call me a Rapturist.  "Humanity becomes something unimaginably different</i><br>
<i>&gt; </i><br>
<i>&gt; Funny that.  "Infinite speed"?</i><br>
<p>
Sure.  Just like a Schwartzchild singularity is infinitely dense and<br>
infinitely curved.  Needless to say, I expect infinite speeds to be achieved<br>
as the result of a single breakthrough, not an infinite Moore's series.  But<br>
yeah, I have no problem with the concept of thinking infinitely fast.  So I<br>
can't visualize it!  All that means is that my semantic primitives are<br>
deficient.  Like, wow, I never would have guessed.  I can't wait to fix the problem.<br>
<p>
<i>&gt; } But I grew up, partially, in Other Reality, where all Laws are transient as</i><br>
<i>&gt; } clouds, blown about on the winds of technology.  All except one:  Moore's Law.</i><br>
<i>&gt; }  Things always get faster, more powerful, more complex.  Always.  Where Other</i><br>
<i>&gt; </i><br>
<i>&gt; Always?  This law which didn't exist a few decades ago?</i><br>
<p>
1)  Moore's Law has held for, what, 30 generations?  That's a human millennium<br>
or thereabout, right?  Long enough for me.<br>
<p>
2)  I didn't exist a few decades ago either.  I'm not about to speculate about<br>
things outside my experience.  (Ha, ha, ha.)<br>
<p>
<i>&gt; And do you</i><br>
<i>&gt; really not believe in sigmoid curves?</i><br>
<p>
Sigmoids apply to *specific* technologies, not fields.  Much less enhanced<br>
intelligence.  Maybe, if there's an unbreakable upper limit - which I very<br>
strongly doubt - then the Singularity will reach a ceiling and instantaneously<br>
flatten.  That's not a sigmoid, that's a hyperbolic and a constant function<br>
spliced together.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4401.html">Mitchell Porter: "What should you ask a wish machine to do for you?"</a>
<li> <b>Previous message:</b> <a href="4399.html">Eliezer S. Yudkowsky: "Re: Singularity: Generation gap"</a>
<li> <b>Maybe in reply to:</b> <a href="4352.html">Eliezer S. Yudkowsky: "Singularity: Generation gap"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="4403.html">Mikael Johansson: "Re: Singularity: Generation gap"</a>
<!-- reply="end" -->
</ul>
