<!-- received="Tue Jul 22 13:47:29 1997 MDT" -->
<!-- sent="Tue, 22 Jul 1997 12:05:40 -0700 (PDT)" -->
<!-- name="Mark Crosby" -->
<!-- email="crosby_m@rocketmail.com" -->
<!-- subject="Re: Complexity" -->
<!-- id="3.0.1.32.19970722010533.00a21ea0@best.com" -->
<!-- inreplyto="Complexity" -->
<title>extropians: Re: Complexity</title>
<h1>Re: Complexity</h1>
Mark Crosby (<i>crosby_m@rocketmail.com</i>)<br>
<i>Tue, 22 Jul 1997 12:05:40 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#903">[ date ]</a><a href="index.html#903">[ thread ]</a><a href="subject.html#903">[ subject ]</a><a href="author.html#903">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0904.html">Hal Finney: "Re: Afterlife"</a>
<li> <b>Previous message:</b> <a href="0902.html">James Rogers: "Re: Terence McKenna"</a>
<li> <b>Maybe in reply to:</b> <a href="0875.html">John K Clark: "Complexity"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
I wrote that the AIC and "depth" measures of complexity are strictly<br>
syntactic, whereas semantics basically involves categorization and<br>
context.  <br>
<p>
John K Clark replied:<br>
&lt;I don't think the two are as strictly segregated as some think and I<br>
don't you need humans of even intelligence for semantics.  A sequence<br>
of amino acids MEANS a shape, the way the resulting protein folds up<br>
in water at the temperature and acidity found in life.&gt;<br>
<p>
OK.  I should have written that these two measures are *mostly*<br>
syntactic.<br>
<p>
<a href="http://turing.pacss.binghamton.edu/dietrich/table-of-contents.html">http://turing.pacss.binghamton.edu/dietrich/table-of-contents.html</a><br>
provides an outline of a new book called _Thinking Computers and<br>
Virtual Persons_ edited by Eric Dietrich.  One of the papers is called<br>
"Syntactic Semantics" by William J. Rapaport.  The following is from<br>
the summary of this paper:<br>
<p>
&lt; Rapaport argues, contra Searle, that computers can understand<br>
natural language and thus be humanly intelligent . . . Rapaport agrees<br>
that computers are syntactical, but argues that syntax suffices for<br>
semantics, hence computers are not purely syntactical. The internal<br>
relationships between representations turns out to be sufficient for<br>
semantics. Rapaport's arguments are made all the more compelling<br>
because he and his colleagues have a system that understands some<br>
English.&gt;<br>
<p>
This seems similar to your argument using the DNA code.  I haven’t<br>
read the Rapaport paper, and you both seem to have a point that there<br>
is some semantics in the syntax, however, I intuitively question<br>
whether *that’s all* there is to semantics. (In the same spirit that I<br>
question statements like "evolution is stupid" or "evolution is<br>
blind".  And, no, I’m not talking about God or ‘spirits’, I’m talking<br>
about levels of control in cybernetics.)<br>
<p>
I wrote (without really thinking through the implications) that<br>
complexity could possibly be measured by the number of different<br>
functional relationships a system can maintain, and might further be<br>
weighted by the number of functions it can perform simultaneously.<br>
<p>
John replied:<br>
&lt;Not a useful definition because we can't know how many relationships<br>
a  system can have.&gt;<br>
<p>
A good point - I wasn’t really thinking about how *all* those<br>
relationships could be *measured*.  Instead, I relied on Bruce<br>
Edmonds' somewhat more formal definition of complexity: "It is<br>
proposed that complexity can usefully be applied only to constructions<br>
within a given language".<br>
<p>
John replied:<br>
&lt;A language doesn't need an observer, consider the Genetic code. The<br>
nucleotide triplet CAU in messenger RNA  is a "real" object it MEANS<br>
the amino acid histidine but I grant you, only in the context<br>
(language) of life, their are no special chemical characteristics that<br>
relate one to the other.&gt;<br>
<p>
Perhaps ‘life’ is the observer of that language and organisms are the<br>
speakers.  My point, and perhaps Edmonds’ point, is that when we<br>
examine any descriptive language (or formal system) *from the outside*<br>
there is always something at another level that the language fails to<br>
describe.<br>
<p>
My quote from Edmonds was:<br>
&lt;A definition of complexity is proposed which can be summarised as<br>
"that property of a language expression which makes it difficult to<br>
formulate its overall behaviour even when given almost complete<br>
information about its atomic components and their inter-relations.&gt;<br>
<p>
John replied:<br>
&lt;This could be AIC or depth, I can't tell which because Edmonds<br>
doesn't say what he means by "difficult". <br>
<p>
One of the main points of my post was that there are other measures of<br>
complexity besides just the two you mentioned.  Edmonds specifically<br>
describes at least five measures: Computational Complexity, Kolmogorov<br>
Complexity, Bennett's Logical Depth, Löfgren's Interpretation and<br>
Descriptive Complexity, Kauffman's number of conflicting constraints.<br>
<p>
John continued:<br>
&lt;At any rate, by this definition the gibberish produced by a monkey<br>
would certainly be more complex than any article in the journal<br>
"Nature". I want a definition that is not the opposite of our<br>
intuitive understanding of the word, I haven't found it.&gt;<br>
<p>
Because, in the monkey example at least, you haven’t described the<br>
system completely enough.  The monkey has no knowledge of the language<br>
for which the typewriter is a recording device.  So, the monkey plays<br>
with it and produces gibberish.  If I were to sit down at a piano<br>
(having no musical knowledge), I’d just produce a cacophony rather<br>
than a symphony because I don’t understand the language of the piano.<br>
<p>
I’m reminded of something Hara Ra wrote in the post you were<br>
originally responding to:<br>
&lt; Mathematical theorems and conjectures are statements of how to<br>
generate and investigate strings finite in length, grammar and syntax<br>
which describe the mathematical objects and operations in relation to<br>
same (the strings representing the objects only!)&gt;<br>
<p>
I’m not very competent at either chemistry or math, but you seem to be<br>
seeking something similar to describe human-level intelligence.  I’m<br>
suggesting that there are many levels of complexity operating in<br>
nature and that a reduction of the kind you are suggesting might be<br>
useful for some purposes (possibly for making upload copies) but<br>
wouldn’t suffice for more complex (oh oh, there’s that word again!)<br>
goals (such as the ‘automorphing’ of that upload once it was<br>
established).<br>
<p>
Mark Crosby<br>
<p>
_____________________________________________________________________<br>
Sent by RocketMail. Get your free e-mail at <a href="http://www.rocketmail.com">http://www.rocketmail.com</a><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0904.html">Hal Finney: "Re: Afterlife"</a>
<li> <b>Previous message:</b> <a href="0902.html">James Rogers: "Re: Terence McKenna"</a>
<li> <b>Maybe in reply to:</b> <a href="0875.html">John K Clark: "Complexity"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
