<!-- received="Mon Jul  7 15:26:48 1997 MDT" -->
<!-- sent="Mon, 7 Jul 1997 15:04:21 -0600" -->
<!-- name="Brent Allsop" -->
<!-- email="allsop@swttools.fc.hp.com" -->
<!-- subject="Re: Uploading" -->
<!-- id="199707072104.AA255809461@raptor.fc.hp.com" -->
<!-- inreplyto="Uploading" -->
<title>extropians: Re: Uploading</title>
<h1>Re: Uploading</h1>
Brent Allsop (<i>allsop@swttools.fc.hp.com</i>)<br>
<i>Mon, 7 Jul 1997 15:04:21 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#170">[ date ]</a><a href="index.html#170">[ thread ]</a><a href="subject.html#170">[ subject ]</a><a href="author.html#170">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0171.html">Dan Hook: "Re: ETHICS: reprogramming others, trials, justice (was Re:  remorse, w"</a>
<li> <b>Previous message:</b> <a href="0169.html">Max More: "Re: Huh?!"</a>
<li> <b>Maybe in reply to:</b> <a href="3847.html">YakWaxx@aol.com: "Uploading"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0201.html">YakWax@aol.com: "Re: Uploading"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Hal Finney &lt;hal@rain.org&gt; reponded:<br>
<p>
<i>&gt; I agree that if a theory were developed which accurately predicted</i><br>
<i>&gt; the subjective sensations for a variety of brain types, conditions,</i><br>
<i>&gt; and circumstances, confirmed in their accuracy by the people</i><br>
<i>&gt; experiencing these events, that it would become widely accepted as a</i><br>
<i>&gt; valid theory of consciousness...</i><br>
<p>
	Great.<br>
<p>
<i>&gt; (It would, however, be difficult to know how to extend the theory to</i><br>
<i>&gt; the more interesting cases of less-conscious beings, like animals.)</i><br>
<p>
	Why would they be any different?  I'm sure there are probably<br>
some insects (worms?) that have no conscious experience or phenomenal<br>
subjective experience at all, having purely abstract intelligence.  In<br>
order to know what echo-location is like for a bat we simply must<br>
reproduce the proper neural corelate (or whatever gives the bat it's<br>
experience) in our conscious world.<br>
<p>
	Right now we have no idea whether a worm feels pain or not and<br>
what such might be like if it does.  But, once we understand what<br>
consciousness is we will be able to both know the answer to this<br>
question and to eventually be able to modify our brain so that it,<br>
too, can produce similar sensations so we can know what it is (or<br>
isn't) like to be a worm.<br>
<p>
<i>&gt; Perhaps so.  In an earlier debate on this topic, I suggested that</i><br>
<i>&gt; technologically mediated mind-reading might similarly allow direct</i><br>
<i>&gt; perception of other people's consciousness.  However it raises</i><br>
<i>&gt; complicated issues of whether I can be said to actually perceive</i><br>
<i>&gt; your consciousness in the same way you perceive it, or whether I am</i><br>
<i>&gt; inherently filtering it through my own consciousness, so that</i><br>
<i>&gt; ultimately what I experience is not identical to what you do.</i><br>
<p>
	For more complex sensations such as love I'm sure this will be<br>
very true.  But, for more basic sensations like red, or the taste of<br>
salt..., I doubt which sensation our brain uses to represent such<br>
changes much due to experience.  The important thing is that more<br>
complex feelings, though much more involved, fleeting, complexly<br>
interdependent..., are built out of the same kind of phenomenally<br>
subjective conscious stuff.  Knowing what red is will go a long way<br>
towards telling us what love is.<br>
<p>
<i>&gt; It reminds me of a disagreement I had a few years ago on</i><br>
<i>&gt; comp.ai.philosophy (where discussing such issues is their bread and</i><br>
<i>&gt; butter) about whether computer simulations of conscious brains would</i><br>
<i>&gt; be conscious.</i><br>
<p>
	Yes, comp.ai.philosophy is a great place.  I've had great<br>
conversations with Marvin Minsky, Hans Morovec, and many others over<br>
there.<br>
<p>
<i>&gt; The person I was debating with maintained that the people might not</i><br>
<i>&gt; be conscious while in the computer, that the downloading process</i><br>
<i>&gt; actually would end up inserting false memories of earlier conscious</i><br>
<i>&gt; experiences.  While logically possible, I think the discrepancy</i><br>
<i>&gt; between this point of view and the sense people had of their</i><br>
<i>&gt; experiences would make it effectively unsupportable.</i><br>
<p>
	Why?  If this was true then I could argue that the artificial<br>
fictional memories implanted in Arnold's mind during Total Recall<br>
prove that the fictional experience was real simply because it<br>
"struck" Arnold that they were real.  Of course, even though Arnold<br>
would not be able to distinguish memories of real events from<br>
artificially implanted memories having no corresponding reality or<br>
history, he could know, through other evidences, which were real and<br>
which were fictional.  Just as we will know that memories reverse<br>
uploaded from an "abstract" simulation of ourselves are not real<br>
memories of real feelings we actually had but that they are merely<br>
from abstract, unfeeling, simulations.<br>
<p>
	Brent Allsop<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0171.html">Dan Hook: "Re: ETHICS: reprogramming others, trials, justice (was Re:  remorse, w"</a>
<li> <b>Previous message:</b> <a href="0169.html">Max More: "Re: Huh?!"</a>
<li> <b>Maybe in reply to:</b> <a href="3847.html">YakWaxx@aol.com: "Uploading"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0201.html">YakWax@aol.com: "Re: Uploading"</a>
<!-- reply="end" -->
</ul>
