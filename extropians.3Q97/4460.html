<!-- received="Sat Sep 27 22:26:10 1997 MDT" -->
<!-- sent="Sat, 27 Sep 1997 21:41:53 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Singularity: Generation gap" -->
<!-- id="199709272321.AAA09993@andromeda.ndirect.co.uk" -->
<!-- inreplyto="Singularity: Generation gap" -->
<title>extropians: Re: Singularity: Generation gap</title>
<h1>Re: Singularity: Generation gap</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sat, 27 Sep 1997 21:41:53 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4460">[ date ]</a><a href="index.html#4460">[ thread ]</a><a href="subject.html#4460">[ subject ]</a><a href="author.html#4460">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4461.html">Raiford C. Dudley Jr.: "Re: Singularity: The Weird Future? (fwd)"</a>
<li> <b>Previous message:</b> <a href="4459.html">Eliezer S. Yudkowsky: "Re: What should you ask a wish machine to do for you?"</a>
<li> <b>Maybe in reply to:</b> <a href="4352.html">Eliezer S. Yudkowsky: "Singularity: Generation gap"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Eric Watt Forste wrote:<br>
<i>&gt; </i><br>
<i>&gt;  &gt; Omniscient FAPP means that you can visualize (and perfectly predict)</i><br>
<i>&gt;  &gt; all other "agents" in a game-theoretic scenario.  While multiple</i><br>
<i>&gt;  &gt; competing OFAPP agents are questionable (halting problem?), a Power</i><br>
<i>&gt;  &gt; deciding whether to force-upload humanity could well be OFAPP,</i><br>
<i>&gt;  &gt; game-theoretically speaking.</i><br>
<i>&gt; </i><br>
<i>&gt; Thank you for the very clear and precise definition.  I know</i><br>
<i>&gt; that I tend to be more skeptical about scalar comparisons</i><br>
<i>&gt; between the computational power of the kinds of Turing machines</i><br>
<i>&gt; that we know how to make and the computational power of the</i><br>
<i>&gt; kinds of Turing machines that we are than you do.  If I</i><br>
<i>&gt; swallowed Moravec's estimates of these things whole, I'd share</i><br>
<i>&gt; your concern.  But I don't know whether or not we know how to</i><br>
<i>&gt; measure the computational power of chordate nervous systems in</i><br>
<i>&gt; a way that we can compare directly to the computational power</i><br>
<i>&gt; of our silicon abacuses.</i><br>
<p>
It's also possible that our actions in ambiguous cases may be<br>
quantum-unpredictable, some arbitrary function of which neurons randomly fire.<br>
 But, and I'd like to adjust my definition a bit in retrospect (so much for<br>
"clear"), OFAPP should state that you can perfectly predict the<br>
*probabilities* of all outcomes.<br>
<p>
<i>&gt; There's research going on to implement sophisticated neural nets</i><br>
<i>&gt; in silicon hardware, and that research route might lead to an</i><br>
<i>&gt; omniscient FAPP entity someday even if we are fundamentally different</i><br>
<i>&gt; from abacuses.</i><br>
<p>
Especially if our decisions are not determined by random quantum collapses,<br>
but by genuine and predictable rational reasoning.<br>
<p>
<i>&gt; But a review of the taxonomy of neural net</i><br>
<i>&gt; architectures (I usually distinguish between feedforward and</i><br>
<i>&gt; recursive architectures, and make a second distinction depending</i><br>
<i>&gt; on whether the learning algorithm uses feedback from the exterior</i><br>
<i>&gt; environment or not) makes it clear that chordate nervous systems</i><br>
<i>&gt; are both recursive in architecture and (to some extent or another)</i><br>
<i>&gt; seek out and use reinforcing behavior from the environment in the</i><br>
<i>&gt; learning algorithms.  It seems to me that we have very little</i><br>
<i>&gt; technical experience building and training nets that use *all* the</i><br>
<i>&gt; architectural tricks, or in other words, building and training nets</i><br>
<i>&gt; that even remotely resemble ourselves or even other real animals.</i><br>
<p>
"Neural nets are not built in imitation of the human brain.  They are built in<br>
imitation of a worm's brain, and when we have neural nets down straight we'll<br>
have a long way to go." (self-quote).<br>
<p>
<i>&gt; While I agree with you in wanting to guard against failures of</i><br>
<i>&gt; imagination, venturing real predictions in a field as new and</i><br>
<i>&gt; inchoate as this one is folly.  I consider Moravec's predictions</i><br>
<i>&gt; to be an enjoyable form of play, but I don't let them keep me up</i><br>
<i>&gt; at night.</i><br>
<p>
You'll note that I said Powers could be OFAPP.  I was just pointing out that<br>
our ethical systems derive a great deal of their pattern from:<br>
<p>
(1), the possibility that you are wrong no matter how sure you are of yourself<br>
("The ends do not justify the means")<br>
<p>
(2), the fact that someone else might know more than you do no matter how dumb<br>
you think they are ("Respect the opinions of others")<br>
<p>
(3), the Hofstadterian Prisoners-Dilemna resolution, that your decision<br>
process is partially duplicated in others ("what if everyone else decides to<br>
do the same thing?").<br>
<p>
Note that *all* *three* break down under even an *approximation* to OFAPP. <br>
For all I know, they break down under first-stage transhumanity, no Powers necessary.<br>
<p>
Our ethical laws are a paradox.  They are very "fragile" derivatives of human<br>
nature, in the sense that a slight alteration in nature would produce a large<br>
difference in result.  (My definition of "slight" may differ from yours.)  But<br>
we think of them as absolute, because only an absolute injunction can overcome<br>
our nature to break ethical rules for what seem like rational and altruistic reasons.<br>
<p>
But Anders rational result-by-probability multipliers don't obey 1<br>
<p>
Knowledge-rich Powers may not obey 2 (or at least may not see any reason to),<br>
plus 2 is a derivative of one in the sense that you don't estimate the<br>
*probability* that someone knows more than you do.<br>
<p>
And perhaps only a slight increase in emotional sophistication is necessary to<br>
void the partial illusion of 3.  One who *knows* that others are not reasoning<br>
the same way, and can guess the outcome with near-certainty, may "defect" in a<br>
non-iterated PD or any non-iterated game.  (Force-uploading is "defecting", in<br>
a sense.)<br>
<p>
Finally, an increase in *self*-knowledge - and boy, will that be easy to<br>
program - voids a lot of the *strength* of ethical rules.  Again, ours are<br>
absolute only because our nature is to incorrectly ignore them, especially in<br>
political issues.  So even if all 3 remain, they may be voidable at will.<br>
<p>
<i>&gt; But you may well know more neural-net theory than I do (because</i><br>
<i>&gt; I'm guessing that you may well have more math than I do), so</i><br>
<i>&gt; maybe I'll adjust my paranoia upward a notch or two.  As the</i><br>
<i>&gt; Bears song goes, "fear is never boring."  ;)</i><br>
<p>
As long as you refuse to act on it, there is no such thing as too much paranoia.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4461.html">Raiford C. Dudley Jr.: "Re: Singularity: The Weird Future? (fwd)"</a>
<li> <b>Previous message:</b> <a href="4459.html">Eliezer S. Yudkowsky: "Re: What should you ask a wish machine to do for you?"</a>
<li> <b>Maybe in reply to:</b> <a href="4352.html">Eliezer S. Yudkowsky: "Singularity: Generation gap"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
