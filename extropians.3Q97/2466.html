<!-- received="Mon Aug 25 16:31:31 1997 MDT" -->
<!-- sent="Mon, 25 Aug 1997 17:19:58 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Goo prophylaxis (was: Hanson antiproliferation method?)" -->
<!-- id="199708252129.WAA24790@andromeda.ndirect.co.uk" -->
<!-- inreplyto="Goo prophylaxis (was: Hanson antiproliferation method?)" -->
<title>extropians: Re: Goo prophylaxis (was: Hanson antiproliferation method?)</title>
<h1>Re: Goo prophylaxis (was: Hanson antiproliferation method?)</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Mon, 25 Aug 1997 17:19:58 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2466">[ date ]</a><a href="index.html#2466">[ thread ]</a><a href="subject.html#2466">[ subject ]</a><a href="author.html#2466">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2467.html">Anders Sandberg: "Re: Goo prophylaxis (was: Hanson antiproliferation"</a>
<li> <b>Previous message:</b> <a href="2465.html">Eliezer S. Yudkowsky: "Re: Emotions, The Easy Part"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2470.html">Anders Sandberg: "Re: Goo prophylaxis"</a>
<li> <b>Reply:</b> <a href="2470.html">Anders Sandberg: "Re: Goo prophylaxis"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Anders Sandberg wrote:<br>
<i>&gt; </i><br>
<i>&gt; On Sun, 24 Aug 1997, Eliezer S. Yudkowsky wrote:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; Very true.  The power of evolutionary computing lies in its blind speed.</i><br>
<i>&gt; &gt; Let's say we have a million-organism brew that evolves for a thousand</i><br>
<i>&gt; &gt; generations.  Now let's say we have a hundred thousand programmers, each of</i><br>
<i>&gt; &gt; whom chooses an organism out of the stew and redesigns it.  The latter</i><br>
<i>&gt; &gt; situation will proceed more slowly than the first, and be vastly more</i><br>
<i>&gt; &gt; expensive, but it will also be far more powerful and capable of reaching</i><br>
<i>&gt; &gt; loftier goals.</i><br>
<i>&gt; </i><br>
<i>&gt; Well, that depends on how much redesign the programmers could do. If</i><br>
<i>&gt; you have ever looked at genetically evolved programs, you know they</i><br>
<i>&gt; are a mess. Structured programming? Bah! Sphagetti code? Not even</i><br>
<i>&gt; that - angel hair code! (slightly overcooked too...)</i><br>
<p>
Cycles:<br>
  1A.  Programmer writes a piece of code to do something.<br>
  2A.  Code evolves into something small, dense and tight.<br>
  1B.  Code leaps into new evolutionary space and discovers new algorithm.<br>
  2B.  Programmer expands algorithm and generalizes it.<br>
<p>
I think that evolutionary programming is mainly useful for three things.  At a<br>
low level of power and sophistication, it's good for optimizing pre-existing<br>
programs.  At a high level of sophistication, it can invent new algorithms. <br>
And at a very high level, it acts as a symbiont to programmers.  A programmer<br>
hacks up an inefficient but general algorithm, and then the EP automatically<br>
uses it as raw material and incorporates it into all existing programmings. <br>
To put it another way, imagine that when somebody had invented the quicksort,<br>
all programs in the world used the quicksort a few moments later.  In effect,<br>
this would let the programmer write code on an enormously higher level.  As if<br>
the language itself was sentient.<br>
<p>
But we don't have that kind of EP yet.<br>
<p>
<i>&gt; &gt; Very, very true.  A lot of people on this list seem to lack a deep-seated</i><br>
<i>&gt; &gt; faith in the innate perversity of the universe.  I shudder to think what would</i><br>
<i>&gt; &gt; happen if they went up against a perverse Augmented human.  Field mice under a</i><br>
<i>&gt; &gt; lawn mower.</i><br>
<i>&gt; </i><br>
<i>&gt; I don't think the universe is perverse. We just like to think it is,</i><br>
<i>&gt; since it takes the blame :-)</i><br>
<p>
I believe the universe is perverse - because that belief helps me outwit it.<br>
<p>
<i>&gt; &gt; "Who will guard the guardians?" - maybe nanotechnology would give us a perfect</i><br>
<i>&gt; &gt; lie detector.  Nanotechnology in everyone's hands would be just like giving</i><br>
<i>&gt; &gt; every single human a complete set of "launch" buttons for the world's nuclear</i><br>
<i>&gt; &gt; weapons.  Like it or not, nanotechnology cannot be widely and freely</i><br>
<i>&gt; &gt; distributed or it will end in holocaust.  Nanotechnology will be controlled by</i><br>
<i>&gt; &gt; a single entity or a small group... just as nuclear weapons are today.</i><br>
<i>&gt; </i><br>
<i>&gt; It is this assumption I want to challenge. If it has the tremendous</i><br>
<i>&gt; destructive potential you assume, it is a fairly logical assumption.</i><br>
<i>&gt; But can you really back it up with some hard calculations?</i><br>
<p>
How much time would it take for a nanomachine to construct a nuclear weapon? <br>
I think we can assume that nano is at least as destructive as nuke.<br>
<p>
<i>&gt; You might be worrying about an imaginary ultra-danger, which will</i><br>
<i>&gt; suggest a course of action which is less than optimal but sounds</i><br>
<i>&gt; plausible Remember that we humans consistently overestimate the risks</i><br>
<i>&gt; of huge disasters and underestimate small, common disasters, and that</i><br>
<i>&gt; fear is the best way of making people flock to an "obvious solution",</i><br>
<i>&gt; especially if it is nicely authoritarian.</i><br>
<p>
Okay, so I'll list my "Top ten ways to destroy all human life using only a few<br>
thousand dollars worth of current technology."  Or maybe not.<br>
<p>
You'll just have to take my word on this one.  We're too damn close to the<br>
brink already.  If nanotechnology can build cities, it can destroy them.  If<br>
nanotechnology can heal a broken arm, it can cause subtle lesions in the<br>
prefrontal cortex and amygdala.  Where nanotech is concerned, the trick isn't<br>
taking over the world - it's doing it so that nobody else notices.<br>
<p>
Remember that the Bad Guys aren't operating under the same restrictions as the<br>
Good Guys.  The wannabe dictators will use directed evolution on a scale no<br>
Good Guy would dare to contemplate.  And evolving mutually competing predators<br>
will go a lot faster than mutually supporting immune systems.<br>
<p>
Our immune systems are unimaginably more sophisticated than a virus or a<br>
bacterium, using controlled evolution to combat natural evolution.  And yet we<br>
still suffer from colds and diseases.  The only reason that the viruses<br>
haven't killed us outright is that it isn't good strategy.<br>
<p>
I want to repeat this, because it's important.  Our immune systems are the<br>
closest analogue to proposed nano-immunities.  The mismatch in available power<br>
and sophistication is enormous.  Our immune systems learn from experience, use<br>
controlled and directed evolution, have memories... everything but the ability<br>
to consciously design things.  And yet the viruses waltz casually through our<br>
bodies, because it's so much easier to destroy than create.  Sometimes a virus<br>
which has deliberately evolved not to kill the host will be expelled from the<br>
body after a few weeks.  Lethal viruses who've taken the gloves off thrive for<br>
years under continuous assualt by our immune systems and the designed<br>
pharmaceuticals of science.  And really lethal viruses, the truly destructive<br>
ones, kill us directly.<br>
<p>
Imagine a virus as sophisticated as the immune system.  No contest.  The sight<br>
would be pathetic, or even moving if the immune system managed to stay on its<br>
feet for a few milliseconds.  And, thanks to the relative speeds of evolved<br>
destruction and evolved protection, the viruses will actually be more sophisticated.<br>
<p>
It is easier to destroy than create!<br>
<p>
<i>&gt; I think you are partially right, nanotech will be dangerous, but we</i><br>
<i>&gt; have to estimate the threat levels and what countermeasures that can</i><br>
<i>&gt; be created before we jump to conclusions about future politics. For</i><br>
<i>&gt; example, if decent immune systems can be created then the dishwasher</i><br>
<i>&gt; goo scenario is unlikely, and if relatively few have the twisted</i><br>
<i>&gt; genius and expertise to design Hollywood goo then it is a potential</i><br>
<i>&gt; danger but with a likeliehood of occuring that is low enough for some</i><br>
<i>&gt; planning to be done (like moving outwards, which ought to be feasible</i><br>
<i>&gt; at the assumed tech level). We need to get some estimates of these</i><br>
<i>&gt; factors.</i><br>
<p>
Well... I'm not competent to estimate the percentage of the population with<br>
the genius and expertise to design death goo.  The "twisted" part can pretty<br>
much be taken for granted.  And I truly don't think death goo would be that<br>
hard to design.  If any human is even capable of designing an immune system,<br>
then the average educated person will be capable of breaking it, given time<br>
and effort.  Any twisted genius will go through it like tissue paper.<br>
<p>
As with nuclear weapons, the issue is really quite simple.  This is in<br>
occasion where the destructive power of X is not diminished by spreading it<br>
among more hands.  Military and governmental power is diminished by<br>
distribution; hence the U.S. government.  Nuclear power is diminished by MAD;<br>
any number of Nuclear Powers greater than two decreases stability.  The<br>
situation will be pretty much the same with nanotechnology... except that a<br>
first strike will have a different probability of succeeding.  If that<br>
probability is high enough, MAD won't work and nano should be confined to a<br>
*single* group.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2467.html">Anders Sandberg: "Re: Goo prophylaxis (was: Hanson antiproliferation"</a>
<li> <b>Previous message:</b> <a href="2465.html">Eliezer S. Yudkowsky: "Re: Emotions, The Easy Part"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2470.html">Anders Sandberg: "Re: Goo prophylaxis"</a>
<li> <b>Reply:</b> <a href="2470.html">Anders Sandberg: "Re: Goo prophylaxis"</a>
<!-- reply="end" -->
</ul>
