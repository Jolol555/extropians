<!-- received="Wed Aug 13 16:35:48 1997 MDT" -->
<!-- sent="Wed, 13 Aug 1997 14:42:47 -0700 (PDT)" -->
<!-- name="Geoff Smith" -->
<!-- email="geoffs@unixg.ubc.ca" -->
<!-- subject="Re: Galaxy Brain Problem" -->
<!-- id="3.0.1.32.19970813215232.006c2a74@pop3.demon.co.uk" -->
<!-- inreplyto="Pine.GSO.3.95.970813205252.21100B-100000@void.nada.kth.se" -->
<title>extropians: Re: Galaxy Brain Problem</title>
<h1>Re: Galaxy Brain Problem</h1>
Geoff Smith (<i>geoffs@unixg.ubc.ca</i>)<br>
<i>Wed, 13 Aug 1997 14:42:47 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1868">[ date ]</a><a href="index.html#1868">[ thread ]</a><a href="subject.html#1868">[ subject ]</a><a href="author.html#1868">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1869.html">Darren Reynolds: "Re: Re: [2] Freedom or death"</a>
<li> <b>Previous message:</b> <a href="1867.html">Darren Reynolds: "Re: Emotions:  The Easy Part"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Wed, 13 Aug 1997, Anders Sandberg wrote:<br>
<p>
<i>&gt; On Wed, 13 Aug 1997, Geoff Smith wrote:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; As someone in another thread pointed out, game theory does not</i><br>
<i>&gt; &gt; apply to post-singularity entities. </i><br>
<i>&gt; </i><br>
<i>&gt; Huh? Could this someone please explain why it would not apply after</i><br>
<i>&gt; the singularity? Many situations in game theory will not be changed</i><br>
<i>&gt; if the players are ultra-intelligent (in fact, game theory often</i><br>
<i>&gt; assumes the players are very rational, more rational than most humans</i><br>
<i>&gt; are).</i><br>
<i>&gt; </i><br>
<p>
I was wondering how this idea was put into my head, so I looked<br>
back in the list a bit, and found this quote from Eliezer S. Yudkows:<br>
<p>
<p>
<i>&gt; I'm just not sure that game theory applies to posthumans.  It's based on</i><br>
<i>&gt; rather tenuous assumptions about lack of knowledge and conflicting</i><br>
<i>&gt; goals.  It works fine for our cognitive architecture, but you can sort</i><br>
<i>&gt; of see how it breaks down.  Take the Prisoner's Dilemna.  The famous</i><br>
<i>&gt; Hofstadterian resolution is to assume that the actions of the other are</i><br>
<i>&gt; dependent on yours, regardless of the lack of communication.  In a human</i><br>
<i>&gt; Prisoner's Dilemna, this, alas, isn't true - but assuming that it is,</i><br>
<i>&gt; pretending that it is, is the way out.</i><br>
<p>
So this is the man to talk to ;)<br>
<p>
I'm not knowledgeable enough in this area to lean either way.<br>
<p>
<p>
geoff.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1869.html">Darren Reynolds: "Re: Re: [2] Freedom or death"</a>
<li> <b>Previous message:</b> <a href="1867.html">Darren Reynolds: "Re: Emotions:  The Easy Part"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
