<!-- received="Sun Sep 21 13:54:50 1997 MDT" -->
<!-- sent="21 Sep 1997 20:29:54 +0200" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: Identity-formalism" -->
<!-- id="199709211757.KAA10549@s27.term1.sb.rain.org" -->
<!-- inreplyto="Tue, 16 Sep 1997 14:51:29 -0200"" -->
<title>extropians: Re: Identity-formalism</title>
<h1>Re: Identity-formalism</h1>
Anders Sandberg (<i>asa@nada.kth.se</i>)<br>
<i>21 Sep 1997 20:29:54 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4133">[ date ]</a><a href="index.html#4133">[ thread ]</a><a href="subject.html#4133">[ subject ]</a><a href="author.html#4133">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4134.html">Natasha V. More: "WIRED Magazine: Future of Fitness"</a>
<li> <b>Previous message:</b> <a href="4132.html">Hal Finney: "Re: copying related probability question"</a>
<li> <b>In reply to:</b> <a href="3729.html">Prof. Jose Gomes Filho: "Re: Is cryopreservation a solution?"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="4153.html">Wei Dai: "Re: copying related probability question"</a>
<li> <b>Reply:</b> <a href="4153.html">Wei Dai: "Re: copying related probability question"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
"Prof. Jose Gomes Filho" &lt;gomes@dpx.cnen.gov.br&gt; writes:<br>
<p>
<i>&gt; At 20:03 11/09/97 -0700, Geoff Smith &lt;geoffs@unixg.ubc.ca&gt; wrote:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt;.....................</i><br>
<i>&gt; &gt;I would say there is a self, but it is not a state.  It is a history of</i><br>
<i>&gt; &gt;states and changes of state, with the potential for even more</i><br>
<i>&gt; &gt;history-making.</i><br>
<i>&gt; &gt;....................</i><br>
<i>&gt; </i><br>
<i>&gt; Using the avaiable language's auxiliary of all, named "mathematics", we</i><br>
<i>&gt; could just say:  self(t), t in ( -oo , +oo ) &lt;considering, simplifiedly, t</i><br>
<i>&gt; real and not complex...&gt;.</i><br>
<p>
Hmm, as a mathematician (well, I started out as a mathematician before<br>
falling in love with brains and computers) I think this is too simplistic.<br>
<p>
The above definition simply assumes that there is something known as<br>
the self during all the time, which may change. But it doesn't tell<br>
us much, and it is not tied to anything in the physical world.<br>
<p>
My suggestion is something like this: self() is a function that<br>
acts on the current state of a system capable of computing it<br>
that produces something called 'sense of identity' (SoI):<br>
<p>
self: state -&gt; SoI<br>
<p>
Note that as the state changes, so does the sense of identity. Hmm,<br>
I see I have not been general enough here: the self-function need<br>
not be universal, it is unique for each system (I identify myself<br>
with my actions, you might identify with your memes and somebody<br>
might identify with their body). So if we assume the existence<br>
of some kind of abstract "superself-function" which for any system<br>
gives us its sense of identity when in a certain state we get:<br>
<p>
self: state x state -&gt; SoI<br>
<p>
This means that self(X,Y) is system X's estimation of system Y's<br>
sense of self. In practice X can of course never calculate this,<br>
only self(X,X), since it has only access to its own state, but<br>
it can make an estimate of self(Y,Y) which of course doesn't have <br>
to be the least close to self(Y,Y). Two estimates would be<br>
self(X,Y'), an estimate of how one would feel if one was like Y,<br>
and self(Y',Y'), an estimate of how someone like Y would feel<br>
about itself.<br>
<p>
Note that self(X,X) is history-dependent if the system has a<br>
memory of its past. This information is included in its state X. <br>
<p>
Also note that most people seems to assume self(X,X) never changes. <br>
I would say this is because 1) self(X,X) is rather slow-changing<br>
over time, and 2) because it makes a lot of sense to make self(X,X)<br>
ones mental origo ('me') when one compares oneself with other<br>
and potential selves. <br>
<p>
Now, let's apply this to some transhumanistic problems. Let X(t)<br>
be my state over time. self(X(t),X(t)) would be my sense of<br>
identity. Through experience I know that I tend to identify with<br>
my past selves at least a certain time T back, so we get (assuming<br>
some kind of distance metric in the "sense of identity space"):<br>
<p>
<i>| self(X(t),X(t)) - Self(X(t-d),X(t-d)) | &lt; epsilon, 0 &lt; d &lt; T</i><br>
<p>
where epsilon is a constant and d is how far in the past I look.<br>
In fact, I would say that normally the distance is less than<br>
epsilon*d, suggesting that self(X(t),X(t)) is continous. <br>
<p>
Notation: I will henceforth call <br>
<i>| self(X(t),X(t)) - self(X(t),X(s)) |, the distance between</i><br>
me at time t and me at time s, as evaluated at time t for dist(t,s).<br>
<p>
I notice that I can only evaluate self(X,Y) when I'm conscious.<br>
When I'm not conscious I will not do this, but <br>
self(conscious, nonconscious) seems to still be less than epsilon. <br>
So I consider myself sleeping in the past as myself.<br>
<p>
What about the future? My state X(t) is evolving, and it is<br>
quite possible for dist(X(t),X(t+d)) to exceed any<br>
bound if I'm really lucky/unlucky (depending on view). That<br>
means I can become someone more different from my current self<br>
than I am from a stranger. This frightens many people. However,<br>
since X(t) is more or less continous and self(X(t),X(t)) seems<br>
to be continous and fairly resilent to noticeable changes in<br>
my body, mind and environment, it seems likely that barring<br>
any surprises I will remain myself (as estimated by me<br>
today) at least for some time. <br>
<p>
If our states are evolving in a chaotic manner, which seems likely,<br>
then dist(t,t+d) ~ exp(lambda*d), where d is the time in the future <br>
and 0 &lt; lambda our "identity lyapunov constant" (which may not be a <br>
constant either, but let's keep things simple right now). <br>
<p>
Since our past seems to become "not me" in the far past, the above <br>
formula does not hold for d &lt; -T, and we get a suggestion that X(t) <br>
is not only chaotic in the positive time sense but also in the negative<br>
time sense - i.e we have a whole spectrum of lyapunov constants<br>
of all signs.<br>
<p>
Alexander Chislenko suggested (in his excellent paper "Drifting Identities"<br>
<a href="http://www.lucifer.com/~sasha/articles/Identity.txt">http://www.lucifer.com/~sasha/articles/Identity.txt</a>, from which I<br>
have borrowed many ideas) that we have an identity horizon, <br>
beyond which we would not recognize versions of ourselves as ourselves.<br>
However, these horizons need not be real: just like somebody falling<br>
into a black hole doesn't see any event horizon, they might receede<br>
as we move closer to them. Others may remain very constant - <br>
I do not consider a cloud of ionized plasma to be me, and I doubt<br>
I would do it even if I was standing close to an armed nuclear wepon.<br>
So "death" can be considered moving across a horizon. <br>
<p>
There are several kinds of death. The usual kind consists of having<br>
our states X(t) move towards a non-living, highly entropic attractor<br>
and loosing cohesion. Most people seem to think this part of state<br>
space is delineated by discontinuities in state, but as anybody who has<br>
actually seen another person die slowly knows, it can be a very<br>
gradual process with no clear discontinuities. <br>
<p>
In fact, this may explain why some dying people accept their death:<br>
the horizons recede as they die, and they no longer consider their<br>
inevitable death as a loss of identity. Compare this to the behavior<br>
of Timothy Leary. <br>
<p>
Another kind of death is "death forward": we change so much we are no<br>
longer recognizable to ourselves, and become new persons. Note that<br>
this already happens all the time: I doubt my 5-year old self would <br>
have recognized me as I am now as itself: our appearance, values and<br>
ways of thinking are simply to different. <br>
dist(5 years, 25 years) &gt; D_max. And the same goes for me:<br>
I have a hard time identifying with the little human who thought<br>
frozen puddles was a conspiracy and that jumping from a pier into<br>
deep water to see what would happen very similar to myself, so<br>
dist(25 years, 5 years) &gt; D_max. Since different people X evaluate<br>
self(X,Y) differently, some might regard all their previous states<br>
(including some quite non-human states such as a blastula) as<br>
themselves, while others regard only the latest as themselves.<br>
Both are right, since they apply different evaluating functions<br>
self(X, ) to their pasts. <br>
<p>
However, in the future we might change even more dramatically,<br>
by becoming immortal transhumans, posthuman jupiter brains or<br>
open standards. I would guess that it is very likely that many<br>
of the horizons will receede quite quickly as we approach them.<br>
Some might remain, and that suggests that there can be jumps in<br>
identity.<br>
<p>
One such example is destructive uploading: our minds are digitized<br>
in a destructive manner and a new entity, the uploaded version is<br>
created. So, will dist(human, upload) be too large for us to<br>
regard as ourselves? That seems to depend a lot on how we evaluate<br>
it; some people identify with their physical body and might<br>
hence regard the difference as immense, while others who identify<br>
with their mind would regard it as smaller. There doesn't seem<br>
to exist any reason to think the difference cannot be well within<br>
the identity horizon for some people. <br>
<p>
If uploading is to be regarded as successful, the upload should<br>
consider itself to be the previous person: dist(upload, human)<br>
should be small enough. "Small enough" is commonly suggested to<br>
be roughly equal to the ordinary changes in identity during one's<br>
life; as we have seen, the definition of "one's life" may be a<br>
bit tricky since our remote pasts may actually be too alien. <br>
Perhaps a better definition should be that the maximal allowable<br>
change in identity should be on the order of the identity<br>
changes during our self-perceived past:<br>
<p>
dist(upload, human(t)) &lt; O(dist(human(t), human(t-d)) for all d<br>
so that dist(human(t), human(t-d)) &lt; D_max(t).<br>
<p>
Note that this can be far less than D_max(t), since most of the<br>
past may have been rather unchanging, with the exception of becoming<br>
the person in the first place.<br>
<p>
Since the upload will have roughly the same mental structure and<br>
hence evaluating capabilities, self(upload, human) ~ self(human, upload),<br>
at least right after the uploading. After a while the distance<br>
will likely grow.<br>
<p>
Non-destructive uploading poses another problem: suppose a person X<br>
is copied into an upload Y, are they the same person? The main problem<br>
here is that people tend to get confused by semantics: there is a<br>
difference between being an independent *being* with an individual<br>
consciousness (I don't experience what anybody else experiences, and<br>
neither do they experience what I experience), being an *individual*<br>
with a sense of selfhood (i.e. self(X,X) exists), and being a *person*, <br>
which is a legal term rather than a philosophical concept.<br>
A conscious system is a being (let's ignore borganisms for the<br>
moment), and likely also an individual and if it is lucky, a person.<br>
<p>
Now, let's look at X and Y. Both are beings (assuming uploads have<br>
consciousness), but neither will experience the experiences of the<br>
other (footnote: a simple way of proving this is to run Y on a deterministic<br>
computer with a deterministic environment (non-determinism can at <br>
least briefly be emulated by a look-up table with random numbers): <br>
since Y would by definition experience the same things each time the<br>
"Y program" was run, it cannot experience anything X is experiencing),<br>
so X and Y will be different beings. However, both X and Y will<br>
evaluate their selves self(X,X) and self(Y,Y) to almost the same<br>
sense of identity, so they will be the same individual. Legally,<br>
they might be anything and change personhood by changing jurisdiction.<br>
<p>
So, it seems that if a person is copied (xoxed, forked or something<br>
similar) we will end up with a number of different beings, but the<br>
same individual. These beings will of course diverge at a rate<br>
determined by their lyapunov constants, and in the long run become<br>
different individuals. <br>
<p>
Finally, what about merging (as described in Greg Egan's short story<br>
"Closer" <a href="http://www.midnight.com.au/eidolon/issue_09/09_closr.htm">http://www.midnight.com.au/eidolon/issue_09/09_closr.htm</a>)?<br>
In this case two beings X and Y merge to form Z, a composite being<br>
with parts from both and possibly new emergent properties. It is<br>
not obvious how large dist(X,Z), dist(Y,Z), dist(Z,X) and dist(Z,Y)<br>
would become. A wild guess is that since Z would contain at least<br>
some of the identity of X, dist(X,Z) would be on the order of<br>
dist(X,Y)/2; this is likely more than most people would accept as<br>
themselves, so it seems likely Z is regarded as a new individual<br>
by X and Y. Z, on the other hand, can trace its past through the<br>
life of X and Y, and might after a while with its new valuations<br>
regard itself as a continuation of both X and Y with a preserved<br>
identity.<br>
<p>
<p>
<p>
<pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
asa@nada.kth.se                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="4134.html">Natasha V. More: "WIRED Magazine: Future of Fitness"</a>
<li> <b>Previous message:</b> <a href="4132.html">Hal Finney: "Re: copying related probability question"</a>
<li> <b>In reply to:</b> <a href="3729.html">Prof. Jose Gomes Filho: "Re: Is cryopreservation a solution?"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="4153.html">Wei Dai: "Re: copying related probability question"</a>
<li> <b>Reply:</b> <a href="4153.html">Wei Dai: "Re: copying related probability question"</a>
<!-- reply="end" -->
</ul>
