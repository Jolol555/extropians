<!-- received="Thu Sep  4 18:21:54 1997 MDT" -->
<!-- sent="Thu, 4 Sep 1997 16:45:54 -0700" -->
<!-- name="Hal Finney" -->
<!-- email="hal@rain.org" -->
<!-- subject="Re: Goo prophylaxis:consensus" -->
<!-- id="199709042345.QAA03814@crypt.hfinney.com" -->
<!-- inreplyto="Goo prophylaxis:consensus" -->
<title>extropians: Re: Goo prophylaxis:consensus</title>
<h1>Re: Goo prophylaxis:consensus</h1>
Hal Finney (<i>hal@rain.org</i>)<br>
<i>Thu, 4 Sep 1997 16:45:54 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2838">[ date ]</a><a href="index.html#2838">[ thread ]</a><a href="subject.html#2838">[ subject ]</a><a href="author.html#2838">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2839.html">Eric Watt Forste: "Re: Goo prophylaxis:consensus"</a>
<li> <b>Previous message:</b> <a href="2837.html">Prof. Jose Gomes Filho: "Nano again..."</a>
<li> <b>Maybe in reply to:</b> <a href="2771.html">Nicholas Bostrom: "Goo prophylaxis:consensus"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2839.html">Eric Watt Forste: "Re: Goo prophylaxis:consensus"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Forrest Bishop wrote:<br>
<i>&gt;    As one who considers the burning of the Library at Alexandria to be </i><br>
<i>&gt; the most </i><br>
<i>&gt; enormous crime in history I find your position _extremely_ distressing. </i><br>
<i>&gt; I think</i><br>
<i>&gt; an SI would at least have the brainpower to know that it cannot know </i><br>
<i>&gt; the </i><br>
<i>&gt; consequences of its information destruction. (The notion that it could </i><br>
<i>&gt; reconstruct that information is garbage, IMO.)</i><br>
<p>
I seem to recall a proposal by Drexler for a "brute force" route to AI<br>
via simulation.  He had some estimate for the total number of compute<br>
cycles it would take to simulate the entire history of life on earth.<br>
You wouldn't try to actually replicate our history, but you would<br>
be in effect simulating an alternate history where life might evolve<br>
differently.  The point was, supposedly using nanotech computers you could<br>
in a reasonable time evolve AI's which would be as intelligent as humans.<br>
<p>
This would imply that by running it a bit longer than the first appearance<br>
of intelligence, you could simulate a whole history for that AI species:<br>
its empires, its revolutions, its artists and engineers.  They'd have<br>
their own "library at alexandria", and many other tragedies and triumphs.<br>
Each simulation would be as rich and significant as our own human history.<br>
<p>
Once you've done this once, you could do it again.  And again, and again.<br>
Each time, with a few months or years of nanotech simulation (or a<br>
few more cubic meters of nanotech computers), you have created as much<br>
complexity and interesting information as the entire history of mankind.<br>
Plus, you have access to it at a far more detailed level.  Nothing will<br>
be lost.<br>
<p>
If this is really possible (I'm not sure the numbers work) then it<br>
suggests to me that the loss of information caused by the destruction of<br>
some portion of the human race is insignificant compared to the amount<br>
which will be created and manipulated routinely by a nanotech culture.<br>
<i>&gt;From that perspective, the loss of Alexandria or the loss of the United</i><br>
States would both be no more important in practice than the loss of<br>
structure when a bird plucks a worm from your lawn.<br>
<p>
Hal<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2839.html">Eric Watt Forste: "Re: Goo prophylaxis:consensus"</a>
<li> <b>Previous message:</b> <a href="2837.html">Prof. Jose Gomes Filho: "Nano again..."</a>
<li> <b>Maybe in reply to:</b> <a href="2771.html">Nicholas Bostrom: "Goo prophylaxis:consensus"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2839.html">Eric Watt Forste: "Re: Goo prophylaxis:consensus"</a>
<!-- reply="end" -->
</ul>
