<!-- received="Tue Sep  2 13:19:18 1997 MDT" -->
<!-- sent="Tue, 2 Sep 1997 11:57:15 -0700" -->
<!-- name="Hal Finney" -->
<!-- email="hal@rain.org" -->
<!-- subject="Re: Goo prophylaxis" -->
<!-- id="199709021857.LAA26135@crypt.hfinney.com" -->
<!-- inreplyto="Goo prophylaxis" -->
<title>extropians: Re: Goo prophylaxis</title>
<h1>Re: Goo prophylaxis</h1>
Hal Finney (<i>hal@rain.org</i>)<br>
<i>Tue, 2 Sep 1997 11:57:15 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2742">[ date ]</a><a href="index.html#2742">[ thread ]</a><a href="subject.html#2742">[ subject ]</a><a href="author.html#2742">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2743.html">Guru George: "Re: PHIL: Eliminative Materialism"</a>
<li> <b>Previous message:</b> <a href="2741.html">Eric Watt Forste: "POLITICS: costs of researching votes, etc."</a>
<li> <b>Maybe in reply to:</b> <a href="2689.html">YakWaxx@aol.com: "Goo prophylaxis"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2750.html">CurtAdams@aol.com: "Re: Goo prophylaxis"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Most people are pretty reasonable, IMO.  They're not perfect, but they're<br>
not raving lunatics or frothing sociopaths.  Most of them, if you gave<br>
them the power to destroy the world, would refrain.<br>
<p>
This is not true of everyone.  If everyone had the power to destroy the<br>
world, it would be destroyed.  This is one of the things we fear about<br>
widespread nanotech.<br>
<p>
But initially it won't be widespread.  Initially it will be controlled<br>
only by one or a few groups.  What will they do with it?<br>
<p>
Are they likely to launch a pre-emptive strike to destroy the competition<br>
and seal their own advantage?  Will they create a goo which eats everything<br>
in the world, turning it into resources under the control of the initiating<br>
group?<br>
<p>
I don't think these are likely scenarios.  The game theory argument is<br>
going to be unpersuasive to most people.  Yes, by doing so they could<br>
increase the odds of their future success, they could arguably put<br>
themselves in a position to control the entire universe.  However this<br>
inducement will not be enough to make most people destroy innocent<br>
lives all over the world, people whose only crime is to be potential<br>
future competitors.  This is not consistent with most people's ethical<br>
sense.<br>
<p>
It is perhaps more likely that one or more of the initial nano powers will<br>
try to preemptively take protective action.  Eliezer proposes dispersing<br>
the entire population into space, cities in flight, where distance and<br>
vacuum will hopefull provide a moat of security around each enclave.<br>
Nicholas suggests involuntarily uploading everyone into a VR where the<br>
laws of physics are such that they can't hurt each other.<br>
<p>
I doubt that even these supposedly benevolant schemes will be attractive<br>
to the first groups with nanotech.  The engineering problems to be solved<br>
are huge, so unless you believe in a strongly peaked Singularity, it is<br>
doubtful that either of these ideas is practical.  They also require an<br>
unusual level of confidence and arrogance, reaching out to drastically<br>
change the entire structure of human society.  Here on this list we can<br>
toy with ideas, but few people would be willing to implement these plans.<br>
<p>
What I suspect will happen is less dramatic.  I would expect the labs to<br>
begin working almost immediately on countermeasures for gray goo and<br>
similar nanotech based destructive devices.  Unfortunately, this will<br>
probably require the creation in the lab of destructive goo.  The labs<br>
will undoubtedly be built around a principle of defense in depth, to<br>
prevent the escape of destructive nanotech.<br>
<p>
Drexler talks about active shields, technologies which can defend against<br>
harmful goo.  We've had a lot of discussion here recently on how this<br>
might work.  It seems that one of the issues is which side gets an<br>
initial advantage.  Hopefully, by deploying anti-goo technologies ahead of<br>
time, it will give people a fighting chance by the time the really nasty<br>
nanotech attacks begin.  This is Drexler's picture of initial nanotech<br>
development from over ten years ago, and it still seems more plausible<br>
to me than many of the extreme scenarios we've been discussing.<br>
<p>
BTW, the recent issue of the Foresight newsletter (www.foresight.org)<br>
suggests that they will be moving into a "phase 2" in terms of their<br>
purpose and objectives.  Up until now they've mostly been working to<br>
convince people that nanotech was plausible and would come into existence<br>
some time in the next few decades.<br>
<p>
Now they feel that this has largely been accomplished, and they want<br>
to begin working in earnest on what was always their ultimate goal.<br>
They want to have the kinds of discussions we've been having here,<br>
to talk about the dangers of nanotech, and what kinds of approaches<br>
to nano would leave us with the best chance for success and survival.<br>
<p>
Obviously this is difficult, on several levels; we may disagree on even<br>
basic goals, with the extropians' ideal future being the rifkinites'<br>
dystopia, and vice versa (as was pointed out here).  I suspect that<br>
a consensus would be for a much more moderate and controlled future,<br>
with less access to technology, than we would like.  Still I think<br>
people deserve a voice in the future they are going to live in, and<br>
they at least need to understand what the options and alternatives are.<br>
It will be interesting to see how Foresight goes about its new task.<br>
<p>
Hal<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2743.html">Guru George: "Re: PHIL: Eliminative Materialism"</a>
<li> <b>Previous message:</b> <a href="2741.html">Eric Watt Forste: "POLITICS: costs of researching votes, etc."</a>
<li> <b>Maybe in reply to:</b> <a href="2689.html">YakWaxx@aol.com: "Goo prophylaxis"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2750.html">CurtAdams@aol.com: "Re: Goo prophylaxis"</a>
<!-- reply="end" -->
</ul>
