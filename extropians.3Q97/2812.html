<!-- received="Thu Sep  4 00:30:44 1997 MDT" -->
<!-- sent="Wed, 3 Sep 1997 23:07:48 -0700 (PDT)" -->
<!-- name="The Low Golden Willow" -->
<!-- email="phoenix@ugcs.caltech.edu" -->
<!-- subject="Re: Goo prophylaxis:consensus" -->
<!-- id="199709040607.XAA28891@blend.ugcs.caltech.edu" -->
<!-- inreplyto="bostrom@mail.ndirect.co.uk" -->
<title>extropians: Re: Goo prophylaxis:consensus</title>
<h1>Re: Goo prophylaxis:consensus</h1>
The Low Golden Willow (<i>phoenix@ugcs.caltech.edu</i>)<br>
<i>Wed, 3 Sep 1997 23:07:48 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2812">[ date ]</a><a href="index.html#2812">[ thread ]</a><a href="subject.html#2812">[ subject ]</a><a href="author.html#2812">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2813.html">Anders Sandberg: "Re: Rifkin speaks in SCI AM"</a>
<li> <b>Previous message:</b> <a href="2811.html">The Low Golden Willow: "Re: Give a man a fish... Give a man a nuke..."</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Sep 3,  2:33pm, "Nicholas Bostrom" wrote:<br>
<p>
} still disagreement on several issus. But perhaps we reached a near<br>
} consensus on the following non-trivial points?<br>
 <br>
Nope.<br>
<p>
} 1. Provided that technological research continues, nanotechnology will<br>
} eventually be developed.<br>
 <br>
I see no reason to assume Drextech (Drexler-Merkle nanotech) must be<br>
developable.  Using machine-phase chemistry to replace diffusion<br>
transport is a big step, not to mention doing so robustly.  Plus the<br>
control and power requirements.<br>
<p>
} 2. An immune system wouldn't work unless it was global.<br>
 <br>
Like the global biological immune system against flesh-eating bacteria,<br>
amoebae, and other yummies?<br>
<p>
} 3. In the absence of a global immune system, if everybody could make<br>
} their own nanotech machines then all life on earth would soon become<br>
} extinct.<br>
 <br>
No.<br>
<p>
} 4. In the absence of ethical motives, the benefits would outweigh the<br>
} costs for a nanotech power that chose to eliminate the competition or<br>
} prevent it from arising, provided it had the ability to do so.<br>
<p>
Unproven.  Even if the game theory arguments seem unassailable, I see<br>
that as no guarantee that "the first nanopower" would obey it.  People<br>
follow their own emotional morality -- which may yet be right.  Note the<br>
current survival of the Third World, despite First World technological<br>
superiority.  Wouldn't it be beneficial to clear out the useless<br>
populations, remove their ecological impact, get some more living space<br>
for us?<br>
<p>
I resist believing that game theory has fully recapitulated and<br>
surpassed the sophistication of our evolved morality. (see Hayek and<br>
Churchland.) <br>
<p>
And if the first nanopower is a human, they'll probably not want to wipe<br>
out all possible companionship.  Most of my friends would be appalled at<br>
the genocide of the rest of humanity.  Great life I'd have.<br>
<p>
Merry part,<br>
 -xx- Damien R. Sullivan X-) &lt;*&gt; <a href="http://www.ugcs.caltech.edu/~phoenix">http://www.ugcs.caltech.edu/~phoenix</a><br>
<p>
"It is a proud and lonely thing to be a prince of Amber, incapable of<br>
trust.  I wasn't real fond of it just then, but there I was."<br>
  -- Roger Zelazny, one of the Amber books, Corwin<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2813.html">Anders Sandberg: "Re: Rifkin speaks in SCI AM"</a>
<li> <b>Previous message:</b> <a href="2811.html">The Low Golden Willow: "Re: Give a man a fish... Give a man a nuke..."</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
