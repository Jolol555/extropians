<!-- received="Tue Aug 12 18:48:04 1997 MDT" -->
<!-- sent="Tue, 12 Aug 1997 19:15:21 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Emotions:  The Easy Part" -->
<!-- id="199708130022.TAA26717@dfw-ix10.ix.netcom.com" -->
<!-- inreplyto="" -->
<title>extropians: Emotions:  The Easy Part</title>
<h1>Emotions:  The Easy Part</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Tue, 12 Aug 1997 19:15:21 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1803">[ date ]</a><a href="index.html#1803">[ thread ]</a><a href="subject.html#1803">[ subject ]</a><a href="author.html#1803">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1804.html">CALYK@aol.com: "Re: Patriarchy and the Age of Information"</a>
<li> <b>Previous message:</b> <a href="1802.html">Forrest Bishop: "Fwd: backlinks ready for testing"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1887.html">Eliezer S. Yudkowsky: "Re: Emotions:  The Easy Part"</a>
<li> <b>Maybe reply:</b> <a href="1887.html">Eliezer S. Yudkowsky: "Re: Emotions:  The Easy Part"</a>
<li> <b>Maybe reply:</b> <a href="2330.html">Eliezer S. Yudkowsky: "Re: Emotions:  The Easy Part"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
"Emotions are the easy part.  We'll have human-equivalent computational<br>
emotions long before we have human-equivalent reasoning."<br>
	-- Eliezer S. Yudkowsky, official prediction.<br>
<p>
I hereby open fire on the stereotype of the emotionless AI.<br>
<p>
There's an old story that some AI ancient once told a graduate student to take<br>
a summer off and "solve the problem of vision", figuring that it would be easy<br>
in comparision with the problem of general reasoning.  Decades later, Marr<br>
finally cracked the visual cortex wide open, explaining the computational<br>
algorithms used to process vision, and even assigning specific neurons to<br>
specific tasks.  We're far from knowing everything about vision... but<br>
definite and irrevocable progress has been made.<br>
<p>
We may have been wrong about taking off one summer... but we were completely<br>
*right* about vision being easy in comparision to general reasoning.  We still<br>
don't have the *faintest* idea of what algorithms are being used... much less<br>
assigning individual neurons to their processing!<br>
<p>
The point of all this?  To provide proper context for my statement that, in<br>
comparision to general reasoning, emotions are easy.  I might even take a<br>
summer off one of these days...<br>
<p>
Dogs have emotions.  Only humans have sophisticated general reasoning. <br>
Emotions are more primitive than reasoning - not in the primal sense, but in<br>
the computational.  They evolved over billions rather than millions of years. <br>
They are simple, relatively inflexible.  In many cases, emotions have been<br>
observed to correspond with particular areas of the brain.<br>
<p>
All this leads up to my statement:  We will have emotional computers before we<br>
have human-equivalent AIs.  In fact, we will have completely cracked the<br>
problem of emotion, in toto, before we have human-equivalent AIs.<br>
<p>
The stereotype of emotionless machines, therefore, is misplaced on<br>
technological grounds.  We might - I say *might* - be able to divorce the<br>
emotional processes from the rational, and deliberately build emotionless<br>
machines.  Perhaps not.  The emotions started out as instincts; they are<br>
probably the single longest-evolved part of the brain.  So they might be so<br>
simple and physiological - so directly programmed - as to be easily untangled.<br>
 Or they might act as a coordination center for the rest of the brain.  It<br>
could go either way; probably both.<br>
<p>
I'm sure we all know the origins of the common stereotype.  For some reason,<br>
machines and computers got associated with pure logic.  Now, that's not the<br>
case - never has been, probably never will be.  Deep Thought is not a human<br>
missing the emotions of its primate ancestors.  Deep Thought is far, far from<br>
the level of an emotional dog.  Deep Thought would have to be considerably<br>
more complex to qualify as an insect.  A bacterium, maybe, if not simply a<br>
physical process.<br>
<p>
But if a human tried to imitate Deep Thought - which is how we describe it, by<br>
putting ourselves in its place - he would have to suppress his emotions,<br>
because Deep Thought doesn't have any emotions.  He would also have to<br>
suppress his long-term strategical reasoning, his memories, his chunking<br>
ability, his ability to perceive complex spatial relationships, his linguistic<br>
capability... but we can't even imagine suppressing those.  Those abilities<br>
are too intimately bound up with ourselves, being recently evolved and<br>
therefore indirectly programmed and therefore developmentally tangled with all<br>
other systems... not to mention processing the self-symbol-subsystem.  While<br>
we can suppress our emotions; we do it all the time.<br>
<p>
The height of the stereotypical silliness occurs when a machine is portrayed<br>
as having deeply repressed emotions.  The idea is so silly it's not even<br>
wrong.  It derives from viewing humans who behave "mechanically"; they often<br>
have deeply repressed emotions.  The idea that you can generalize to all<br>
mechanisms... it reminds me of a cartoon I have on my door, a Mr. Boffo.  The<br>
caption is "Most-Broken Award"; the cartoon shows a cuckoo-clock bird going: <br>
"What time is it?"<br>
<p>
My argument, in summary, is this.<br>
Modern computers are emotionless, and utterly unintelligent.  Their level of<br>
"pattern" ranges from undirected physical processes such as Deep Thought, to<br>
bacterium-level organisms such as a word processor, to the insect-level<br>
mind-boggling complexity of Windows 95.<br>
<p>
When our processing power and programming capability reaches "dog" level,<br>
we'll start seeing emotional computers.<br>
<p>
A while later, we'll see emotional AIs, and then very shortly thereafter<br>
posthuman AIs, who may or may not have anything recognizable as emotions.<br>
<p>
The only reason why we don't already have emotional computers - it is a simple<br>
problem - is that there are no perceived profits.  As soon as somebody figures<br>
out how to make it sell, we'll have emotional computers.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1804.html">CALYK@aol.com: "Re: Patriarchy and the Age of Information"</a>
<li> <b>Previous message:</b> <a href="1802.html">Forrest Bishop: "Fwd: backlinks ready for testing"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1887.html">Eliezer S. Yudkowsky: "Re: Emotions:  The Easy Part"</a>
<li> <b>Maybe reply:</b> <a href="1887.html">Eliezer S. Yudkowsky: "Re: Emotions:  The Easy Part"</a>
<li> <b>Maybe reply:</b> <a href="2330.html">Eliezer S. Yudkowsky: "Re: Emotions:  The Easy Part"</a>
<!-- reply="end" -->
</ul>
