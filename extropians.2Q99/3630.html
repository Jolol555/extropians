<!-- received="Fri Jun 11 07:16:18 1999 MDT" -->
<!-- sent="Fri, 11 Jun 1999 15:12:21 +0200" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: freedom vs even distribution" -->
<!-- id="199906111315.GAA27123@geocities.com" -->
<!-- inreplyto="freedom vs even distribution" -->
<!-- version=1.10, linesinbody=34 -->
<html><head><title>extropians: Re: freedom vs even distribution</title>
<meta name=author content="den Otter">
<link rel=author rev=made href="mailto:neosapient@geocities.com" title ="den Otter">
</head><body>
<h1>Re: freedom vs even distribution</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Fri, 11 Jun 1999 15:12:21 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3630">[ date ]</a><a href="index.html#3630">[ thread ]</a><a href="subject.html#3630">[ subject ]</a><a href="author.html#3630">[ author ]</a>
<!-- next="start" -->
<li><a href="3631.html">[ Next ]</a><a href="3629.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3592.html">Dan Clemmensen</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3644.html">Anders Sandberg</a>
</ul>
<!-- body="start" -->



<hr>
<br>
<a name="3644qlink1">&gt; From: Dan Clemmensen &lt;Dan@Clemmensen.ShireNet.com&gt;<br>

<p>
<a href="3592.html#3630qlink1">&gt; I've read their entire "civilization", but not "Lessons of History."</a><br>
<i>&gt; I think that the conclusion overlooks the likelihood of altruism</i><br>
<i>&gt; among the potential immortals. If even one of the immortals has</i><br>
<i>&gt; even the slightest amount of altruism, then the technology will</i><br>
<i>&gt; be be disseminated to the populace as a whole. </i><br>

<p>
Assuming that the other immortals allow it (there could be a codex
against proliferation of transhuman tech, much like the current one
against the proliferation of nuclear weapons). Violators could face
destruction, which is a horrible threat to a (potential) immortal. With
</a>
<a name="3644qlink2">(near-)perfect surveillance being easy for posthumans, any "illegal"
development on earth could easily be nipped in the but.</a> 

<p>
<a name="3644qlink3"><i>&gt; The argument against</i><br>
<a href="3592.html#3630qlink2">&gt; this assumes a zero-sum game in which a gain by the unwashed masses</a><br>
<i>&gt; equates to a loss by an immortal. Information doesn't work this</i><br>
<i>&gt; way.</i><br>

<p>
Information is power, and by allowing millions of people to become
god-like too, you multiply the risk of something going wrong by
(approximately) the same amount. To the already fully autonomous
posthumans this might not seem like a very good idea; there's more
to lose than to gain. 
</a>

<p>
No, posthuman altruism (if there is such a thing) would probably
result in (covert) uploading of "normal" humans, and letting
them "dream" their own version of a "perfect" future, while the
original uploads guard reality.
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3631.html">[ Next ]</a><a href="3629.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3592.html">Dan Clemmensen</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3644.html">Anders Sandberg</a>
</ul>
</body></html>
