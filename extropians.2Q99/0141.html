<!-- received="Mon Apr  5 01:32:56 1999 MDT" -->
<!-- sent="Mon, 06 Apr 1998 02:34:55 -0500" -->
<!-- name="Darin Sunley" -->
<!-- email="umsunley@cc.umanitoba.ca" -->
<!-- subject="Re: Subjective counterfactuals" -->
<!-- id="3528859F.1EE@cc.umanitoba.ca" -->
<!-- inreplyto="Subjective counterfactuals" -->
<!-- version=1.10, linesinbody=82 -->
<html><head><title>extropians: Re: Subjective counterfactuals</title>
<meta name=author content="Darin Sunley">
<link rel=author rev=made href="mailto:umsunley@cc.umanitoba.ca" title ="Darin Sunley">
</head><body>
<h1>Re: Subjective counterfactuals</h1>
Darin Sunley (<i>umsunley@cc.umanitoba.ca</i>)<br>
<i>Mon, 06 Apr 1998 02:34:55 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#141">[ date ]</a><a href="index.html#141">[ thread ]</a><a href="subject.html#141">[ subject ]</a><a href="author.html#141">[ author ]</a>
<!-- next="start" -->
<li><a href="0142.html">[ Next ]</a><a href="0140.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0135.html">Dan Fabulich</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0188.html">Dan Fabulich</a>
</ul>
<!-- body="start" -->

<p>
Dan Fabulich wrote:

<p>
<a href="0135.html#0141qlink1">&gt; no ordinary playback will pass any reasonable Turing test.</a><br>

<p>
<a name="0155qlink1">It seems to me that thsi whole debate becomes a littel cleaarer when
stated in terms of the ontological levels of the agents involved.</a>

<p>
An example of an ontological level is Searle's old saw about a
simulation of a thunderstorm making the inside of the computer wet.
I wouldn't expect a simulation of a thunderstorm, or playback thereof,
to make the inside of my computer wet. But I WOULD expect it to make any
virtual cows in the same simulation  virtually wet. The virtual cows and
the virtual thunderstorm are on the same ontological level. The virtual
cows and thunderstorm are all one level DOWN from the level of the
hardware running the simulation, and of the programmer who write the
simulation.

<p>
<a name="0155qlink2">Isn't the whole idea of a Turing test that it be done between agents on
the same ontological level? When we program a computer to attempt a
Turing test we are equipping it with sensors and knowledge about our
ontological level, and the ability to communicate to our ontological
level. We are, in short, attempting to keep the computer's processing in
the same ontological level as the hardware, instead of doing processing
in a domain one ontological level down.</a>

<p>
<a name="0188qlink1"><i>&gt;From the view of agents within simulations we create as being one</i><br>
ontological level down, we can imagine the agents that are one
ontological level UP fomr us. Some theologians call them Gods.

<p>
The framework of ontological levels gives some insights to classical
theological arguments. Aquinas's arguments from First Cause can be
restated to say "Within a given ontological level, there is a first
effect, caused by an agent in a higher ontological level." Anselm's
argument can be characterized as "There is a highest ontological level."
Aquinas and Anselm characterized these statements as proves, but in fact
they are statements, which, if both proven, would serve as a prove for
the existance of God.</a>

<p>
Returning to the point of this thread, the framework of ontological
levels sheds some light on the problem of the computability of
consciousness, especially as regards the consciousness of playbacks of
conscious agents.

<p>
<a name="0155qlink3">Consciousness is a label assigned by one agent to another.</a>
<a name="0155qlink4">Postulate an ontological level containing two agents, each of whom
believe the other to be conscious. Let them make a recording of their
interactions, to the greatest level of detail their environment allows,
to their analog of the Heisenberg limit. Let one of them program a
simulation, containing two other agents. Neither of the first two agents
believes that either of the agents in the simulation is conscious.</a>

<p>
<a name="0155qlink5"><i>&gt;From OUR point of view, one ontological level up form these agents,</i><br>
neither seems conscious. Both are, from our point of view, completely
deterministic, and we see no menanigful distinction between them and
their recording.</a>

<p>
<a name="0155qlink6"><i>&gt;From THEIR point of view however, the recording seems dramatically less</i><br>
conscious then they are. Both agents in the original level would pass
Turing tests adminstered by the other. Neither of the agents in the
recording would pass a Turing test adminitstered by agents from the
original level.</a> 

<p>
Playbacks are all one ontological level down from the original
matieriel, from the point of view of agents within the original
matieriel. From the point of view of agents one ontological level UP
form the original matieriel, a playback and the original would be on the
same level. Similarly, could a being on the original meaningfully
distinguish an agent one level up from a recording of an agent one level
up? 

<p>
I may be running in a simulation right now. The beings that created the
simulation are one level above me. I may create a simulation. The beings
within that simulation are one level below me. From my point of view, a
"playback" of a human being is at the same ontological level as a
simulation I write.

<p>
Tangent: How about an "Inverse Turing Test"? Where we try to write a
computer program that can distinguish humans from computers? :)

<p>
Darin Sunley
<br>
umsunley@cc.umanitoba.ca
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0142.html">[ Next ]</a><a href="0140.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0135.html">Dan Fabulich</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0188.html">Dan Fabulich</a>
</ul>
</body></html>
