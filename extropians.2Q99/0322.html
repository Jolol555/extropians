<!-- received="Fri Apr  9 09:33:38 1999 MDT" -->
<!-- sent="Fri, 9 Apr 1999 10:32:58 -0500" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@conemsco.com" -->
<!-- subject="RE: Geniebusters" -->
<!-- id="000d01be829e$3f3564a0$352501c0@mfg130" -->
<!-- inreplyto="199904090117.SAA25587@gull.prod.itd.earthlink.net" -->
<!-- version=1.10, linesinbody=88 -->
<html><head><title>extropians: RE: Geniebusters</title>
<meta name=author content="Billy Brown">
<link rel=author rev=made href="mailto:bbrown@conemsco.com" title ="Billy Brown">
</head><body>
<h1>RE: Geniebusters</h1>
Billy Brown (<i>bbrown@conemsco.com</i>)<br>
<i>Fri, 9 Apr 1999 10:32:58 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#322">[ date ]</a><a href="index.html#322">[ thread ]</a><a href="subject.html#322">[ subject ]</a><a href="author.html#322">[ author ]</a>
<!-- next="start" -->
<li><a href="0323.html">[ Next ]</a><a href="0321.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0308.html">Lyle Burkhead</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Lyle Burkhead wrote:
<br>
<i>&gt; Apparently you missed that post.</i><br>

<p>
Yes, I did.  My apologies.

<p>
<a href="0308.html#0322qlink1">&gt; There are three questions at issue here. In decreasing order</a><br>
<i>&gt; of importance,</i><br>
<i>&gt; they are:</i><br>
<i>&gt;</i><br>
<i>&gt; 1. Whether my method of argument is valid and useful.</i><br>

<p>
I would say that it is.  I think it is only one of several techniques that
need to be used in making predictions, but it is a very useful tool for
pruning back wild speculations.

<p>
<a href="0308.html#0322qlink2">&gt; 2. Whether my conclusions are true.</a><br>

<p>
Some of them are.  Others seem suspicious to me, and a few seem completely
wrong - although I may just be misinterpreting you.

<p>
<a href="0308.html#0322qlink3">&gt; 3. Who I am, whether I know anything about computer science or anything</a><br>
<i>&gt; else, whether I did a good job of applying the principle of calibration,</i><br>
<i>&gt; etc.</i><br>
<i>&gt;</i><br>
<i>&gt; Question #3 is of no relevance. Maybe I'm an idiot. That has nothing to do</i><br>
<i>&gt; with it. As I said on the home page of geniebusters, Eric Drexler is not</i><br>
<i>&gt; the issue here. Neither am I.</i><br>

<p>
Agreed.

<p>
<a href="0308.html#0322qlink4">&gt; Question #2 is more important. Is someone going to design the entire</a><br>
<i>&gt; "universal assembler," in advance, in such a way that once set in motion</i><br>
<i>&gt; the whole thing suddenly comes into existence with explosive force? Are</i><br>
<i>&gt; robots going to build skyscrapers for free? Is there going to be an event</i><br>
<i>&gt; that is incommensurable with everything else? or not?</i><br>

<p>
Obviously not - or at least, not if it is being done by humans.  I would
expect something more like the computer revolution in that case.  Getting a
really sudden change requires you to make very strong assumptions about the
progress of AI, which would be rather speculative at this point.

<p>
<a href="0064.html#0322qlink5">&gt; &gt; Ah. So you do still read the list. I wondered.</a><br>
<i>&gt;</i><br>
<i>&gt; You wondered? In other words you were on the list two years</i><br>
<i>&gt; ago?</i><br>

<p>
No, I just read the archives.

<p>
<a href="0064.html#0322qlink6">&gt; However, this afternoon I find myself writing yet another</a><br>
<i>&gt; post to the list, the fourth one this week, in spite of my repeated</i><br>
efforts
<br>
<a href="0308.html#0322qlink7">&gt; to stop. I may have to join some kind of 12-step program for postaholics.</a><br>
<i>&gt; Maybe a Higher Power will help me do what I can't do myself.</i><br>

<p>
We're all addicts here.  :-)

<p>
<a href="0308.html#0322qlink8">&gt; &gt; Your assertion that any software capable of replacing a human</a><br>
<i>&gt; &gt; would demand a salary</i><br>
<i>&gt;</i><br>
<i>&gt; I'm not asserting this for software capable of replacing *a* human, just</i><br>
<i>&gt; for software that replaces humans who make unsupervised decisions</i><br>
requiring
<br>
<i>&gt; human judgment.</i><br>

<p>
"Artificial Intelligence: The science of making computers do things that, if
they were done by humans, would require intelligence."

<p>
I think there is a lot more potential for AI here than you seem to expect.
However, this is a complex topic that deserves a real discussion, so I'll
save it for a later post.

<p>
<a href="0308.html#0322qlink9">&gt; That section isn't supposed to be read in isolation. It is part of the</a><br>
<i>&gt; argument leading up to Exercise 5 in Section 7, and it also has to be read</i><br>
<i>&gt; in conjunction with sections 12 and 13. The question is whether the</i><br>
<i>&gt; Assembler Breakthrough is going to happen. To get a handle on that</i><br>
<i>&gt; question, I'm trying to establish the level of complexity involved in "a</i><br>
<i>&gt; system that can make anything, including copies of itself." If a</i><br>
nanosystem
<br>
<a href="0308.html#0322qlink10">&gt; can make anything, in the sense required for the Assembler Breakthrough,</a><br>
<i>&gt; then it will amount to the same thing as an industrial economy.</i><br>

<p>
Once again, this is long enough that it really needs its own post.  I'll get
to it as soon as I can.

<p>
Billy Brown, MCSE+I
<br>
bbrown@conemsco.com
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0323.html">[ Next ]</a><a href="0321.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0308.html">Lyle Burkhead</a>
<!-- nextthread="start" -->
</ul>
</body></html>
