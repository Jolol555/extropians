<!-- received="Wed Jun 23 14:30:38 1999 MDT" -->
<!-- sent="Wed, 23 Jun 1999 16:27:00 -0400" -->
<!-- name="Harvey Newstrom" -->
<!-- email="newstrom@newstaffinc.com" -->
<!-- subject="Re: Qualia and the Galactic Loony Bin" -->
<!-- id="001601bebdb6$f5020380$f01924cf@ibmus2.ibm.com" -->
<!-- inreplyto="Qualia and the Galactic Loony Bin" -->
<!-- version=1.10, linesinbody=178 -->
<html><head><title>extropians: Re: Qualia and the Galactic Loony Bin</title>
<meta name=author content="Harvey Newstrom">
<link rel=author rev=made href="mailto:newstrom@newstaffinc.com" title ="Harvey Newstrom">
</head><body>
<h1>Re: Qualia and the Galactic Loony Bin</h1>
Harvey Newstrom (<i>newstrom@newstaffinc.com</i>)<br>
<i>Wed, 23 Jun 1999 16:27:00 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4387">[ date ]</a><a href="index.html#4387">[ thread ]</a><a href="subject.html#4387">[ subject ]</a><a href="author.html#4387">[ author ]</a>
<!-- next="start" -->
<li><a href="4388.html">[ Next ]</a><a href="4386.html">[ Previous ]</a>
<b>In reply to:</b> <a href="4380.html">hal@finney.org</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Hal &lt;hal@finney.org&gt; wrote:
<br>
<a href="4380.html#4387qlink1">&gt; This is where I think I have confused you, because I tried to describe</a><br>
<i>&gt; another kind of experiment and you were still thinking in terms of the</i><br>
<i>&gt; first one.</i><br>

<p>
Yes, I think I got confused trying to follow too many examples at once.  My
fault for using a primitive meat-brain.  One of these days I plan to upgrade
it.

<p>
<a href="4380.html#4387qlink2">&gt; The overlaying supposition I am trying to support is that none of</a><br>
<i>&gt; these explanations where consciousness is based solely on information</i><br>
<i>&gt; hold water.  All have serious inconsistencies and all are implausible</i><br>
<i>&gt; if you look at them closely enough.  There must be something wrong with</i><br>
<i>&gt; our fundamental reasoning on these problems.</i><br>

<p>
Agreed.  I think information alone is like a book.  It contains data, but it
is not "functional".  I also have recently realized that a function brain
with no history, memory or experience is also not functional.  I think we
need both hardware, a operating system, and a database (to use computer
analogies).  Maybe all three are required.  One or two of the three is not
sufficient to simulate a consciousness.

<p>
<a href="4380.html#4387qlink3">&gt; Does that help give you an overview of what I am trying to do here?</a><br>

<p>
Yes.  Thanks for your patience.  A communications failure like this often
leads to unnecessary disagreements, when actually there is not a
disagreement on ideas, but a difference in perception as to what is being
discussed.

<p>
<a href="4380.html#4387qlink4">&gt; You have to go through the steps of the thought experiment and the</a><br>
<i>&gt; reasoning behind them to see how we come to the conclusion, you can't</i><br>
<i>&gt; just deny it because it seems implausible.  You need to reject the</i><br>
<i>&gt; premise of the experiment, or one of the steps involved.</i><br>

<p>
Actually, I do disagree with this methodology.  Sometimes we can't detect
the exact moment where the failure occurs, but we still can predict that
there will be total failure by the end of the events.  If I bleed to death,
you can't predict exactly which drop of blood loss will be fatal, but the
sum total effect is predictable.  I had the same problem with your
experiments.  I think the end points are pretty well defined and can be
debated.  The intermediate examples are fuzzy and are difficult to debate.
I don't think this invalidates the final conclusions.  I think it merely
shows that we do not have total and perfect knowledge of the entire system.

<p>
<a name="4451qlink1"><a href="4380.html#4387qlink5">&gt; The premise is that if you run a brain (or perhaps a brain simulation)</a><br>
<i>&gt; through an experience where you supply the inputs and it processes</i><br>
<i>&gt; the data normally, it will be conscious.  I think you will agree with</i><br>
<i>&gt; that.  We also have to assume that the brain can be made to run in</i><br>
<i>&gt; a deterministic way, without any physical randomness.  This would be</i><br>
<i>&gt; more plausible in the case of a brain simulation but perhaps it could</i><br>
<i>&gt; be arranged with a real brain.</i><br>

<p>
I agree that a brain that processes inputs normally should be defined as
conscious.  I disagree that brains are deterministic and will respond to
identical input with identical output every time.  The internal state of the
brain is not the same.  If you override a brain's internal states to the
point that they are controlled by external minds and not by the brain
itself, I begin to doubt that the brain is conscious.</a>

<p>
<a href="4380.html#4387qlink6">&gt; It follows that if you run a brain through the same experience twice,</a><br>
<i>&gt; with the same initial state and the same inputs, it will be conscious</i><br>
<i>&gt; both times.  That follows because it satisfies the conditions of the</i><br>
<i>&gt; premise, and the premise says that whenever you satisfy those conditions</i><br>
<i>&gt; it will be conscious.  It is an intermediate conclusion of the argument,</i><br>
<i>&gt; and you have to reject the premise to reject this conclusion.</i><br>

<p>
This I agree with, given your assumptions.  If you don't force the brain to
think each thought at each step of the way, but rather let it process the
input on its own, then I agree it is conscious.

<p>
<a name="4451qlink2">I am not sure I agree with the assumption that the same input into the same
brain will always produce the same results.  Most theories of creativity
involve randomizing factors in the brain.  Maybe the left brain will always
come up with the same response, but I believe that the right brain uses
randomization to enhance creativity.  You can force its random factors to
replay as well, but you would have to do this at each step of the thought
process.  Suppose your input was, "name something Clinton hasn't screwed
up"?  Would the brain produce the same output every time?  This question has
no determinate answer.  I think the creative randomizing brain would think
up something different every time.</a>

<p>
<a name="4451qlink3"><a href="4380.html#4387qlink7">&gt; We now introduce the notion of making a recording of all the internal</a><br>
<i>&gt; activity of the brain during the first run, each neural firing or whatever</i><br>
<i>&gt; other information was appropriate depending on how the brain works.</i><br>
<i>&gt;</i><br>
<i>&gt; We then add a comparison during the second run of the activity generated</i><br>
<i>&gt; during that run with the recorded activity from the first run.  Imagine</i><br>
<i>&gt; comparing this neural activity at each neuron.  The two sets of data</i><br>
<i>&gt; will match, by the premises of the experiment, because we are exactly</i><br>
<i>&gt; repeating the first run.</i><br>

<p>
I don't think this is possible.  Neurons fire trigger each other by
releasing neurotransmitters into a liquid solution that exists between them.
When the concentration of chemical gets high enough, those neurons that
detect the concentration will fire.  It would be impossible to make this
chemical diffuse across a liquid medium in exactly the same way every time.
Each molecule diffuses randomly.  It will work roughly the same way, but</a> its
exact precision is indeterminate.  The restoration of these chemicals to the
neural stores are also random.  The components to make the chemicals float
around randomly in the blood.  They are picked up randomly as needed by
individual cells.  There is no way that this supply and demand will always
work out the same.  Different neurons will be supplied slightly differently
with each run.  The only way to totally control the brain, which is 90%
liquid, is to control every molecule as it bounces around in the liquid.

<p>
I therefore think the example is interesting but impossible.  I agree in
general with your theory, but in reality do not think it could ever really
work out the way you said.  Your philosophical argument is whether the
repeated sequence is conscious if it is pre-determined.  This is about to
get into questions of determinism and free-will, a religious debate.  I
think the randomizing function will make each identical run unique, and each
brain will come up with unique responses, and each will be conscious.

<p>
<a name="4451qlink4"><a href="4380.html#4387qlink8">&gt; Finally, at each neural element we substitute the recorded data for the</a><br>
<i>&gt; actively generated data.  We block the actively generated signals</i><br>
<i>&gt; somehow, and replace them with the signals from the recorded first run.</i><br>
<i>&gt; Since these two signals are completely identical, it follows that there</i><br>
<i>&gt; will be no difference in the behavior of the brain.  Substituting one</i><br>
<i>&gt; data value for another identical one is of no functional significance.</i><br>

<p>
This is the point where I am positive that the brain is not conscious.  To
do this, you must suppress all of the brains own neuron firing, and control
every neuron externally.  True, you can make the brain act like it would
have anyway, but you can also make it act unnaturally.  The
control/consciousness of this brain is now in the hands of the programmer
controlling each neuron, and not with the brain.  The brain has become a
meat puppet.</a>

<p>
Of course, these answers seem obvious to me.  You seem to think that if both
brains act identically, how can one be conscious and one not be?  I think
you are not going back far enough to the roots of where thoughts come from.
The information being processed by the real brain comes from inside.  The
information being processed by the meat-puppet comes from outside and must
be transferred into the brain somehow.  Yes, we can make the information
coming from the outside look like the information on the inside, but it is
coming from a different source.  That source is the external brains.  This
controlled brain will never act conscious if the external brains don't cause
it to do so.  It is not an independent consciousness that can act
independently from the other brains.  If the experimenter has a heart attack
and dies, the brain stops working because its controller has stop giving it
instructions.

<p>
I seem to be repeating myself, but this seems obvious to me.  If a brain is
self-directed it is conscious.  If it only does what external brains tell it
to do, then it is not conscious.

<p>
<a href="4380.html#4387qlink9">&gt; The point is that we took a situation where the brain was conscious,</a><br>
<i>&gt; by the premise of causal connectivity based functionalism, and by</i><br>
<i>&gt; substituting one set of signals for an identical set of signals, which is</i><br>
<i>&gt; arguably no substitution at all, we produced a brain which is passively</i><br>
<i>&gt; running a replay.  So either this seemingly ineffectual substitution</i><br>
<i>&gt; has eliminated consciousness, which seems hard to understand, or passive</i><br>
<i>&gt; replays are as conscious as functional brains, which you deny.</i><br>

<p>
You have to block the brain's own neuron firing mechanisms.  You have to
prevent the brain from thinking on its own so that you can substitute the
replay.  Whatever function or structure or data you are blocking to have
this effect is the root of consciousness.  You block this source of
consciousness, and replace it with a stream of consciousness coming from the
outside programmers.  It is not clearly defined what you are blocking that
prevents the brain from thinking on its own.  I do know that cutting up the
neurons and separating them will have this effect of stopping thought.  We
also call that step "death".  I believe that you are first killing the brain
so that there are no brainwaves and it is not conscious at that point.  Then
you are piping in the brain patterns as defined by external brains.

<p>
The problem with your whole theory, I think, is that you are blocking
something in the original brain, without clearly defining what you are
blocking.  Then you are claiming that the replay brain has all the
consciousness of the original.  If it does not, it seems clear to me that
whatever you blocked was a required component of consciousness.
<pre>
--
Harvey Newstrom &lt;<a href="mailto://newstrom@newstaffinc.com">mailto://newstrom@newstaffinc.com</a>&gt; &lt;<a href="http://newstaffinc.com">http://newstaffinc.com</a>&gt;
Author, Consultant, Engineer, Legal Hacker, Researcher, Scientist.
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="4388.html">[ Next ]</a><a href="4386.html">[ Previous ]</a>
<b>In reply to:</b> <a href="4380.html">hal@finney.org</a>
<!-- nextthread="start" -->
</ul>
</body></html>
