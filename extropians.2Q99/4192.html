<!-- received="Fri Jun 18 19:36:24 1999 MDT" -->
<!-- sent="Fri, 18 Jun 1999 18:33:49 -0700" -->
<!-- name="hal@finney.org" -->
<!-- email="hal@finney.org" -->
<!-- subject="Re: Qualia and the Galactic Loony Bin" -->
<!-- id="199906190133.SAA18116@finney.org" -->
<!-- inreplyto="Qualia and the Galactic Loony Bin" -->
<!-- version=1.10, linesinbody=86 -->
<html><head><title>extropians: Re: Qualia and the Galactic Loony Bin</title>
<meta name=author content="hal@finney.org">
<link rel=author rev=made href="mailto:hal@finney.org" title ="hal@finney.org">
</head><body>
<h1>Re: Qualia and the Galactic Loony Bin</h1>
<i>hal@finney.org</i><br>
<i>Fri, 18 Jun 1999 18:33:49 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4192">[ date ]</a><a href="index.html#4192">[ thread ]</a><a href="subject.html#4192">[ subject ]</a><a href="author.html#4192">[ author ]</a>
<!-- next="start" -->
<li><a href="4193.html">[ Next ]</a><a href="4191.html">[ Previous ]</a>
<b>In reply to:</b> <a href="4069.html">Wei Dai</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="4199.html">Damien Broderick</a>
</ul>
<!-- body="start" -->

<p>
Wei Dai, &lt;weidai@eskimo.com&gt;, writes:
<br>
<a href="4069.html#4192qlink1">&gt; On Tue, Jun 15, 1999 at 06:01:46PM -0500, Eliezer S. Yudkowsky wrote:</a><br>
<i>&gt; &gt; Of course not.  If we can't comprehend the First Cause or qualia</i><br>
<i>&gt; &gt; ourselves, because our reasoning processes deal only with</i><br>
<i>&gt; &gt; Turing-computable ontologies, I would hardly expect us to be able to</i><br>
<i>&gt; &gt; explain qualia to a skeptical Turing-computable being!  Remember, qualia</i><br>
<i>&gt; &gt; are enormously improbable; the only reason we're allowed to have them is</i><br>
<i>&gt; &gt; because of the enormous number of races combined with the Anthropic</i><br>
<i>&gt; &gt; Principle.  To any skeptic, the probability that our race is</i><br>
<i>&gt; &gt; congenitally brain-damaged would exceed the probability of our having</i><br>
<i>&gt; &gt; actual qualia.  You'd have to open up the neurons and demonstrate that</i><br>
<i>&gt; &gt; we're messing with Weird Physics, after which the whole qualia business</i><br>
<i>&gt; &gt; would be more plausible.</i><br>
<i>&gt;</i><br>
<i>&gt; Can you go through your reasons for believing that qualia are enormously</i><br>
<i>&gt; improbable? I understand it has something to do with the seeming</i><br>
<i>&gt; impossibility of finding a definition for "instantiation" of a</i><br>
<i>&gt; computation, but what is the connection exactly?</i><br>

<p>
<a name="4238qlink1">I will take the liberty of replying since Eliezer has not.  I think
this is a very interesting question, and I feel that I do understand
his reasoning on this point.  He is welcome to correct or clarify any
mistakes I make.</a>

<p>
<a name="4333qlink1"><a name="4238qlink2"><a name="4228qlink1">I believe Eliezer's logic is that if it is impossible to define whether
a computation</a> is instantiated,<a name="4333qlink2"> then there is no "fact of the matter" as
to whether any given computation is instantiated in any given system.</a>
But he takes as given, based on his personal experience, that it is
a definite fact that his qualia exist.  It follows that qualia cannot
result merely from computation.</a></a>

<p>
<a name="4238qlink3"><a name="4199qlink1">Now I am speculating a bit, but I believe that Eliezer's position
is that qualia are not a crucial and necessary aspect of the mental
structure of a successful organism.  It should be entirely possible
for intelligent beings to evolve and succeed without qualia, because
all that is really necessary is the ability to model the environment,
extrapolate possible events, come up with plans, and so on, all of which
seem to be computational tasks (hence not requiring qualia).</a></a>

<p>
<a name="4238qlink4">If this is the case, there is no reason for qualia to evolve.  Given that
we (or at least Eliezer!) have qualia, this must be what Gould calls a
"spandrel", a sort of evolutionary by-product which has no functional or
adaptive significance.</a>  Since qualia are not selected for, they probably
<a name="4238qlink5">do not normally appear as the result of the evolutionary process, and
so most intelligent races would not have them.</a>  As Eliezer says above,
<a name="4238qlink6">the only reason we have qualia is because the universe is so large that
some few intelligent races have ended up with them by happenstance,
and then he invokes the anthropic principle to say that we must be one
of that minority, since we are conscious.</a>

<p>
<a name="4238qlink7"><a href="4069.html#4192qlink2">&gt; Also, do you think it is possible to program a Turing-computable being to</a><br>
<i>&gt; behave as if it has qualia even though it doesn't really? If so how can we</i><br>
<i>&gt; tell we are not such beings? I'm thinking "I have qualia" but that is what</i><br>
<i>&gt; such a being would be thinking as well. Conversely, how do we know it's</i><br>
<i>&gt; not the case that non-sentient animals also have qualia, but they just</i><br>
<i>&gt; can't communicate that fact to us? </i><br>

<p>
Based on Eliezer's model, it would seem impossible to faithfully
replicate all of the characteristics of qualia in a computational being.
Although he might be able to approximate or mimic the behavior of a
being who actually had qualia, there would inevitably be some subtle
differences which could in principle be detected (although it might take
a super-Turing being to tell the difference?).</a>

<p>
<a name="4238qlink8"><a href="4069.html#4192qlink3">&gt; When you talk about non-Turing-computable "Weird Physics" are you thinking</a><br>
<i>&gt; of physics that would be computable under a more powerful model of</i><br>
<i>&gt; computation (for example TM with various oracles), or something entirely</i><br>
<i>&gt; different?</i><br>

<p>
I think to answer this you have to go back to his basic argument
regarding the vagueness of instantiation.  If you accept that, then
augmenting the TM with various oracles would still seem to leave the
problem of indeterminacy of when such an augmented TM is instantiated.
Now, if the qualia are somehow in the oracle itself, so that it doesn't
matter what kind of TM is attached to it, then this would be strong
enough to allow for qualia.  But if the existence of qualia depends on a
particular program being run in the TM, in addition to any functionality
in the oracle, then the difficulties with instantiating that TM crop up,
so this would not be strong enough.</a>

<p>
<a name="4238qlink9">Personally, I largely agree with Eliezer's reasoning here (at least as I
have interpreted it) but I question the premise about uncertainty of
implementation.  This is where I look for an answer.
</a>

<p>
Hal
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="4193.html">[ Next ]</a><a href="4191.html">[ Previous ]</a>
<b>In reply to:</b> <a href="4069.html">Wei Dai</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="4199.html">Damien Broderick</a>
</ul>
</body></html>
