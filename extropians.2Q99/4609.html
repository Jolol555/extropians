<!-- received="Sun Jun 27 16:14:49 1999 MDT" -->
<!-- sent="Sun, 27 Jun 1999 01:54 +0000" -->
<!-- name="jmcasey@pacific.net.sg" -->
<!-- email="jmcasey@pacific.net.sg" -->
<!-- subject="Re: The Copy Question" -->
<!-- id="199906272214.GAA08260@pop1.pacific.net.sg" -->
<!-- inreplyto="The Copy Question" -->
<!-- version=1.10, linesinbody=13 -->
<html><head><title>extropians: Re: The Copy Question</title>
<meta name=author content="jmcasey@pacific.net.sg">
<link rel=author rev=made href="mailto:jmcasey@pacific.net.sg" title ="jmcasey@pacific.net.sg">
</head><body>
<h1>Re: The Copy Question</h1>
<i>jmcasey@pacific.net.sg</i><br>
<i>Sun, 27 Jun 1999 01:54 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4609">[ date ]</a><a href="index.html#4609">[ thread ]</a><a href="subject.html#4609">[ subject ]</a><a href="author.html#4609">[ author ]</a>
<!-- next="start" -->
<li><a href="4610.html">[ Next ]</a><a href="4608.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="4340.html">Harvey Newstrom</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->


<OL>
  <li>  If your "self" can't be uploaded , economics suggests that you'll be indifferent to whether minds can be uploaded, unless others pay you for a copy. "You're" going to die anyway. The benefits of upoading won't accrue to you but to your survivors -- and then only if you've developed your facilities to the degree that the virtual you could add value to life as your survivors know it. (Oh, and of course, to your relatives, who might want to keep you the way I keep a picture of my grandmother.)

  <li>  Kevin Kelly in his "Out of Control" summarises complexity theory as involving a swarm of "stupid" units acting independently and allowing intelligence to emerge. His idea, as I understand it, is that top-down control can't create intelligence, because a structure built in this way is inherently unstable. Rather, a collection of low-level, simple algorithms gets overlaid with a "meta" algorithm, usually no "smarter" in any qualitative way than the low-level ones (though its input/output fields address the lower-level algorithms rather than raw experience). A group of these then come under the purview of an even higher-order algorithm, which itself is simple in terms of its functions. Ultimately the structure becomes complex and consciousness emerges, but in fact there's no organising principle, just the emergence which couldn't have been predicted.


</OL>
<p>
So far so good, except that if our consciousness is only an emergent property of the swarm of simple algorithms that evolution and personal experience have given each of us, *any* change in the population of that swarm should also change consciousness. Perhaps the change will be minute, but isn't it more likely that in uploading we miss key pieces that affect the whole, unless, for instance, we put the entire mind into an exact replica of the body it inhabited prior to copying? And if so, have we not obviated the need for uploading in the first place? What's the point of putting the mind of a dying human into the body of another human that's dying in exactly the same way?

<p>
Perhaps there's a solution to this if, instead, we put the mind into a simulator, where it perceives itself to have the same body. Then, with the miracle of software, we "cure" the virtual body's cancer, say. Indeed, extending this logic, we could enhance anything about this virtual organism step by step until its virtual physical form corresponded perfectly to the actual physical form into which it will be downloaded later. But the conundrum remains: by changing its form we change its mind, and we have no idea which particular changes will change consciousness so much as to destroy the organisation of the erstwhile mind.

<p>
<a name="4627qlink1">Remembering that these are just copies and not people, we could experiment endlessly until we found which particular changes worked and which didn't.</a> Even so, I suspect that the complex nature of mind (and in particular, body-mind) means we will have to go through this process every single time we upload someone -- that we won't find many shortcuts. The problem is reducing the instruction set: matters we consider trivial from a top-down perspective, such as the mechanisms of moving the legs, may be so tied in with the concept of self in a particular individual (in ways even the individual doesn't comprehend) that altering them may alter the organisation of that individual's mind beyond the point of stability.

<p>
(On the other hand, my starting point in this extrapolation is a book that's five years old by now. Perhaps, if the theory has moved past this problem by now, someone on this list can illuminate me as to my error.)<br>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="4610.html">[ Next ]</a><a href="4608.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="4340.html">Harvey Newstrom</a>
<!-- nextthread="start" -->
</ul>
</body></html>
