<!-- received="Mon Jun 14 23:37:36 1999 MDT" -->
<!-- sent="Mon, 14 Jun 1999 22:34:56 -0700" -->
<!-- name="hal@finney.org" -->
<!-- email="hal@finney.org" -->
<!-- subject="Re: TECH: Fractal Tardis Brains" -->
<!-- id="199906150534.WAA02824@finney.org" -->
<!-- inreplyto="TECH: Fractal Tardis Brains" -->
<!-- version=1.10, linesinbody=53 -->
<html><head><title>extropians: Re: TECH: Fractal Tardis Brains</title>
<meta name=author content="hal@finney.org">
<link rel=author rev=made href="mailto:hal@finney.org" title ="hal@finney.org">
</head><body>
<h1>Re: TECH: Fractal Tardis Brains</h1>
<i>hal@finney.org</i><br>
<i>Mon, 14 Jun 1999 22:34:56 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3855">[ date ]</a><a href="index.html#3855">[ thread ]</a><a href="subject.html#3855">[ subject ]</a><a href="author.html#3855">[ author ]</a>
<!-- next="start" -->
<li><a href="3856.html">[ Next ]</a><a href="3854.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3796.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3904.html">James Rogers</a>
</ul>
<!-- body="start" -->

<p>
<a name="3921qlink1">Eliezer S. Yudkowsky, &lt;sentience@pobox.com&gt;, writes, quoting me:
<br>
<a href="3796.html#3855qlink1">&gt; &gt; Moravec has an interesting thought experiment in which a CA model like</a><br>
<i>&gt; &gt; Conway's game of Life runs for long enough to evolve living organisms</i><br>
<i>&gt; &gt; which then develop intelligence.  Would you say that this is possible?</i><br>
<i>&gt;</i><br>
<i>&gt; Plain intelligence?  Sure.  Probably the vast majority of races across</i><br>
</a><i>&gt; the Reality are non-conscious until they Transcend.  We're exceptions,</i><br>
<i>&gt; but, quite obviously, the only consciously observed exceptions in the</i><br>
<i>&gt; absence of a Singularity.</i><br>

<p>
That's an interesting possibility.  So they could evolve intelligence,
emotions, and many of the other trappings of life similar to ours, but
they would be lacking qualia.  They would not be conscious in the sense
that we are.  They would process information, they would have models of
the world as we do, it would probably be meaningful for them to use the
words "I" and "me" in conversations.  But something would be different.

<p>
<a name="3921qlink2">How would this difference manifest itself?  Would it be possible to
convince such a being that we humans have some "spark", some kind
of primary, irreducible experience of reality, that they don't have?</a>
<a name="3921qlink3">Suppose they aren't sure initially that there can be any experience
of reality beyond what they have.  What empirical test can we offer,
what capability would we have that they do not?  If they were blind, we
could talk about how we can tell what is happening at a distance without
having to go and touch it.  What can we say if they are blind to qualia?</a>

<p>
<a name="3921qlink4">Maybe, after all, I don't have qualia in the sense that Eliezer does.
Perhaps my basic sense of the universe is fundamentally different
from his.  We both react to the same universe and so there is a certain
basic commonality of representation and reasoning, but perhaps the raw,
nitty gritty irreducible nature of reality is totally different for us.
How could we detect this lack on my part?</a>

<p>
<a href="3796.html#3855qlink2">&gt; The one thing that got seared into my memory by my attempt to formalize</a><br>
<i>&gt; instantiation is never, ever, ever believe in epiphenomena.  *Anything*</i><br>
<i>&gt; real has to be experimentally detectable, including the property of</i><br>
<i>&gt; reality itself.  Anything "real" has exhibit different behavior than</i><br>
<i>&gt; things that are "not real".  Otherwise you've got epiphenomena, a zombie</i><br>
<i>&gt; theory of reality.  A Turing computation, being a Platonic object,</i><br>
<i>&gt; proceeds just the same whether it's "instantiated" or "not instantiated"</i><br>
<i>&gt; in our reality.  This is probably the fundamental reason why nobody will</i><br>
<i>&gt; ever define instantiation; zombie theories of reality are as silly as</i><br>
<i>&gt; zombie theories of consciousness.</i><br>

<p>
<a name="3921qlink5">Consider a situation like in The Matrix, where people have their
brains directly interfaced to a computer simulation.  Are objects in
the simulation "real"?  I would assume not.  But what is the different
behavior which would reveal their unreality?  Are you saying that there
is some way of distinguishing any simulation from reality?  What is
the trick?
</a>

<p>
Hal
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3856.html">[ Next ]</a><a href="3854.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3796.html">Eliezer S. Yudkowsky</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3904.html">James Rogers</a>
</ul>
</body></html>
