<!-- received="Thu Apr  8 19:17:26 1999 MDT" -->
<!-- sent="Thu, 8 Apr 1999 18:16:52 -0700" -->
<!-- name="Lyle Burkhead" -->
<!-- email="lybrhed@earthlink.net" -->
<!-- subject="Re:  Lyle Burkhead and that "nit-brained newbie"" -->
<!-- id="199904090117.SAA25587@gull.prod.itd.earthlink.net" -->
<!-- inreplyto="Lyle Burkhead and that "nit-brained newbie"" -->
<!-- version=1.10, linesinbody=142 -->
<html><head><title>extropians: Re:  Lyle Burkhead and that "nit-brained newbie"</title>
<meta name=author content="Lyle Burkhead">
<link rel=author rev=made href="mailto:lybrhed@earthlink.net" title ="Lyle Burkhead">
</head><body>
<h1>Re:  Lyle Burkhead and that "nit-brained newbie"</h1>
Lyle Burkhead (<i>lybrhed@earthlink.net</i>)<br>
<i>Thu, 8 Apr 1999 18:16:52 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#308">[ date ]</a><a href="index.html#308">[ thread ]</a><a href="subject.html#308">[ subject ]</a><a href="author.html#308">[ author ]</a>
<!-- next="start" -->
<li><a href="0309.html">[ Next ]</a><a href="0307.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0064.html">Lyle Burkhead</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0322.html">Billy Brown</a>
</ul>
<!-- body="start" -->

<p>
Billy, I didn't read your geniebusters thread when it first appeared,
because I thought it would just be more of the same.  A few days later
(Friday, April 2) I did read it, and replied to it.  Apparently you missed
that post.  When I saw that your remarks were rational, I let the personal
stuff go and just replied to the points you raised.  You might do the same.
 It's message #2559 in the archive, but for your convenience I will
reproduce it below.  

<p>
<a href="0258.html#0308qlink1">&gt; I refer interested readers to parts 15-21 of Geniebusters, </a><br>
<i>&gt; where Mr. Burkhead argues without evidence </i><br>
<i>&gt; that no possible AI system can ever result in dramatic cost savings </i><br>
<i>&gt; for any kind of organization.</i><br>

<p>
What?!  I also refer interested parties to sections 15-21 of Geniebusters. 
Interested parties, if there are any, can judge for themselves whether I
made any such claim.  

<p>
<a href="0258.html#0308qlink2">&gt; Now, Mr. Burkhead, if you are actually interested in </a><br>
<i>&gt; discussing the issue I will be happy to do so. </i><br>

<p>
It depends on what you mean by "the issue."  You have to reply to something
I actually said.  When you make up something like "no possible AI system
can ever result in dramatic cost savings for any kind of organization,"
that's a waste of everybody's time.  

<p>
This is the post that you seem to have missed:    

<hr>

<p>
<a name="0322qlink1">There are three questions at issue here. In decreasing order of importance,
they are: 


<OL>
  <li>  Whether my method of argument is valid and useful.</a> 

<a name="0322qlink2">  <li>  Whether my conclusions are true.</a> 

<a name="0322qlink3">  <li>  Who I am, whether I know anything about computer science or anything
else, whether I did a good job of applying the principle of calibration,
etc. 


</OL>
<p>
Question #3 is of no relevance. Maybe I'm an idiot. That has nothing to do
with it. As I said on the home page of geniebusters, Eric Drexler is not
</a>
the issue here. Neither am I. 

<p>
<a name="0322qlink4">Question #2 is more important. Is someone going to design the entire
"universal assembler," in advance, in such a way that once set in motion
the whole thing suddenly comes into existence with explosive force? Are
robots going to build skyscrapers for free? Is there going to be an event
that is incommensurable with everything else? or not?</a> 

<p>
Question #1 is the most important one. How can we get a handle on question
#2? Is there a general procedure for approaching such questions, so that
they become matters of fact and proof rather than matters of opinion and
speculation? 

<p>
After writing the above, I finally looked at Billy Brown's comments and
found, to my surprise, that they are rational. If he had made a rational
comment in the first place, matters would have taken a different course. 

<p>
<a href="0064.html#0308qlink3">&gt; Ah. So you do still read the list. I wondered. </a><br>

<p>
You wondered? In other words you were on the list two years ago? Anyway I
don't read the list. Twirlip of Greymist pointed out to me that Anders had
mentioned geniebusters on the list, so I looked at his comment and your
reply. Then I read the earlier posts in that thread, and sampled some other
threads in the archive. Apart from that, I haven't read the list since I
retired from it. However, this afternoon I find myself writing yet another
post to the list, the fourth one this week, in spite of my repeated efforts
<a name="0322qlink7">to stop. I may have to join some kind of 12-step program for postaholics.
Maybe a Higher Power will help me do what I can't do myself.</a> 

<p>
<a href="0064.html#0308qlink4">&gt; Certain sections of your site make it seem as if </a><br>
<i>&gt; your method is to assume that any revolutionary claim </i><br>
<i>&gt; must be false, and then look for an excuse to back up </i><br>
<i>&gt; your assumption. This is just as wrong-headed as </i><br>
<i>&gt; assuming that all such claims are true.</i><br>

<p>
You don't say which sections you are referring to, or what "makes it seem"
that way. I am concerned here with certain specific claims made by Eric
Drexler in Engines of Creation. I don't even dispute all of the claims made
in that book, let alone "any" revolutionary claim. If someone wants to
claim that we will, in the foreseeable future, be able to make our bodies
much more self-correcting than they are now, and that our cells and organs
will become (asymptotically) perfect, I would not have a problem with that
claim, even though it could be considered "revolutionary." If someone wants
to claim that it is possible (and practicable) to make artificial
organisms, I don't have a problem with that either. The assumption that I
do make, and defend, is that new technologies, however revolutionary they
may appear to be, will occur within a certain unchanging framework. 

<p>
<a name="0322qlink8"><a href="0064.html#0308qlink5">&gt; Your assertion that any software capable of replacing a human </a><br>
<i>&gt; would demand a salary </i><br>

<p>
I'm not asserting this for software capable of replacing *a* human, just
for software that replaces humans who make unsupervised</a> decisions requiring
human judgment. Some humans are not on "our" side of the line. Many
employees are, for all practical purposes, automatons, and can be replaced
by machines.  This happens all the time and will continue. My assertion is
that the realm of automation will always exist within a larger realm that
is not automated. The boundary between agents and automatons will always be
there, and agents will always have to be dealt with as agents. They will
always expect to get paid (unless you can find some other way to motivate
them). I think what I'm going to have to do is illustrate this point with
an animated diagram, where you can see something changing within a
framework that does not change. I also need to put in a lot more
interstitial material explaining what I'm doing. (In other words the
revised version will be longer, not shorter. It isn't going to be an
article, it's going to be a book. Sorry, Robin ;-) 

<p>
<a href="0064.html#0308qlink6">&gt; The comparison of a truly general-purpose nanomanufacturing system </a><br>
<i>&gt; to a modern industrial nation is such an oversimplification that it has </i><br>
<i>&gt; no predictive value. There are too many differences between the behaviors</i><br>
<i>&gt; of the two systems, and the implications of these differences need to be </i><br>
<i>&gt; assessed individually and in detail if you want to make a meaningful</i><br>
prediction. 

<p>
<a name="0327qlink1"><a name="0322qlink9">That section isn't supposed to be read in isolation. It is part of the
argument leading up to Exercise 5 in Section 7, and it also has to be read
in conjunction with sections 12 and 13. The question is whether the
Assembler Breakthrough is going to happen. To get a handle on that
question, I'm trying to establish the level of complexity involved in "a
system that can make anything, including copies of itself." If</a> a nanosystem
</a>
<a name="0327qlink2"><a name="0322qlink10">can make anything, in the sense required for the Assembler Breakthrough,
then it will amount to the same thing as an industrial economy.</a></a> 

<p>
It would of course be possible to have a nanosystem that could make
anything in a certain domain, maybe an extensive domain, without being able
to make anything in general. But that's not what you need for the Assembler
Breakthrough. If anyone has a proof (i.e. a structured argument, not some
impressionistic remarks) that the Breakthrough can happen with a less
general system, I'd like to see it.

<hr>

<p>
So, that's it.  There is no personal abuse in the above.  I quoted what you
said and replied to it.  If you raise similar points I will reply in a
similar vein.  Any further discussion of geniebusters should take place in
the geniebusters thread.  

<p>
Lyle 
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="0309.html">[ Next ]</a><a href="0307.html">[ Previous ]</a>
<b>In reply to:</b> <a href="0064.html">Lyle Burkhead</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="0322.html">Billy Brown</a>
</ul>
</body></html>
