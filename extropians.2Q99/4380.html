<!-- received="Wed Jun 23 12:40:00 1999 MDT" -->
<!-- sent="Wed, 23 Jun 1999 11:37:10 -0700" -->
<!-- name="hal@finney.org" -->
<!-- email="hal@finney.org" -->
<!-- subject="Re: Qualia and the Galactic Loony Bin" -->
<!-- id="199906231837.LAA07349@finney.org" -->
<!-- inreplyto="Qualia and the Galactic Loony Bin" -->
<!-- version=1.10, linesinbody=110 -->
<html><head><title>extropians: Re: Qualia and the Galactic Loony Bin</title>
<meta name=author content="hal@finney.org">
<link rel=author rev=made href="mailto:hal@finney.org" title ="hal@finney.org">
</head><body>
<h1>Re: Qualia and the Galactic Loony Bin</h1>
<i>hal@finney.org</i><br>
<i>Wed, 23 Jun 1999 11:37:10 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4380">[ date ]</a><a href="index.html#4380">[ thread ]</a><a href="subject.html#4380">[ subject ]</a><a href="author.html#4380">[ author ]</a>
<!-- next="start" -->
<li><a href="4381.html">[ Next ]</a><a href="4379.html">[ Previous ]</a>
<b>In reply to:</b> <a href="4377.html">Harvey Newstrom</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Harvey Newstrom, &lt;newstrom@newstaffinc.com&gt;, writes:
<br>
<a href="4377.html#4380qlink1">&gt; Hal &lt;hal@finney.org&gt; wrote:</a><br>
<i>&gt; &gt; I may have been unclear about this "replay".  The brain is entirely</i><br>
<i>&gt; &gt; normal and fully functioning.  It's just that we "happen" to be giving it</i><br>
<i>&gt; &gt; exactly the same inputs we did on an earlier run.</i><br>
<i>&gt;</i><br>
<i>&gt; I must have missed something.  If the second brain is normal and fully</i><br>
<i>&gt; functioning, how is it different than the first brain or any brain?  What is</i><br>
<i>&gt; being replayed here?  The test?  I though you had something hooked up to the</i><br>
<i>&gt; brain and was forcing it to replay the same thoughts it had earlier.  If its</i><br>
<i>&gt; thoughts are being dictated from the outside, it is not conscious or</i><br>
<i>&gt; generating thought.  If its thoughts are being self-generated from the</i><br>
<i>&gt; inside, then it is conscious and generating thought.</i><br>
<i>&gt;</i><br>
<i>&gt; I guess I am having a hard time following all your examples.  What exactly</i><br>
<i>&gt; is it that you are trying to prove?  Every example seems to devolve into a</i><br>
<i>&gt; discussion about the minutia of the example.  What is the overlaying</i><br>
<i>&gt; supposition you are trying to support?</i><br>

<p>
I am sorry that my examples have seemed confusing.  What has happened is
that I gave the first experiment of the divided brain.  You responded that
there is no consciousness because there is no flow of information, it is
just a pattern which is being imposed from the outside.

<p>
I said then, if that were the case, that the flow of information and
connection and causality were the true determining factor of consciousness,
then how would you explain what is happening in this *other* experiment?

<p>
<a name="4387qlink1">This is where I think I have confused you, because I tried to describe
another kind of experiment and you were still thinking in terms of the
first one.</a>

<p>
<a name="4387qlink2">The overlaying supposition I am trying to support is that none of
these explanations where consciousness is based solely on information
hold water.  All have serious inconsistencies and all are implausible
if you look at them closely enough.  There must be something wrong with
our fundamental reasoning on these problems.</a>

<p>
The divided-brain experment shows that this is true with regard to the
pattern model, and the other experiment which I have tried to describe
to you yesterday shows that this is true with regard to the information
processing, connection based, causality based model that you support.

<p>
<a name="4387qlink3">Does that help give you an overview of what I am trying to do here?</a>

<p>
My message with the *second* experiment, the one which attempts to lay
the groundwork for showing that causality is not a coherent concept for
a model of consciousness, is at
<br>
<a href="http://www.lucifer.com/exi-lists/extropians/4335.html">http://www.lucifer.com/exi-lists/extropians/4335.html</a>.  I will explain
it again and perhaps try to eliminate any superfluous elements.

<p>
<a name="4388qlink1">The point is to show that there is no difference between a causally
connected brain and a passively replaying one, or at least that the
difference is so trivial that it is almost inconceivable that it could be
the cause of consciousness.  I know that you disagree with this stated in
such bald terms, but realize that it is a conclusion, not an assumption.
</a>

<p>
<a name="4387qlink4">You have to go through the steps of the thought experiment and the
reasoning behind them to see how we come to the conclusion, you can't
just deny it because it seems implausible.  You need to reject the
premise of the experiment, or one of the steps involved.
</a>

<p>
<a name="4387qlink5">The premise is that if you run a brain (or perhaps a brain simulation)
through an experience where you supply the inputs and it processes
the data normally, it will be conscious.  I think you will agree with
that.  We also have to assume that the brain can be made to run in
a deterministic way, without any physical randomness.  This would be
more plausible in the case of a brain simulation but perhaps it could
be arranged with a real brain.
</a>

<p>
<a name="4387qlink6">It follows that if you run a brain through the same experience twice,
with the same initial state and the same inputs, it will be conscious
both times.  That follows because it satisfies the conditions of the
premise, and the premise says that whenever you satisfy those conditions
it will be conscious.  It is an intermediate conclusion of the argument,
and you have to reject the premise to reject this conclusion.
</a>

<p>
<a name="4387qlink7">We now introduce the notion of making a recording of all the internal
activity of the brain during the first run, each neural firing or whatever
other information was appropriate depending on how the brain works.

<p>
We then add a comparison during the second run of the activity generated
during that run with the recorded activity from the first run.  Imagine
comparing this neural activity at each neuron.  The two sets of data
will match, by the premises of the experiment, because we are exactly
repeating the first run.
</a>

<p>
<a name="4387qlink8">Finally, at each neural element we substitute the recorded data for the
actively generated data.  We block the actively generated signals
somehow, and replace them with the signals from the recorded first run.
Since these two signals are completely identical, it follows that there
will be no difference in the behavior of the brain.  Substituting one
data value for another identical one is of no functional significance.
</a>

<p>
Now we claim that the result is not "actually" a case of functional
information processing, but merely a passive replay.  Each neuron is
given inputs from a recording, and its responses go nowhere.  This is
essentially the same as the case where you said there would be no
consciousness.

<p>
<a name="4387qlink9"><a name="4383qlink1">The point is that we took a situation where the brain was conscious,
by the premise of causal connectivity based functionalism, and by
substituting one set of signals for an identical set of signals, which is
arguably no substitution at all, we produced a brain which is passively
running a replay.  So either this seemingly ineffectual substitution
has eliminated consciousness, which seems hard to understand, or passive
replays are as conscious as functional brains, which you deny.
</a>

<p>
Hal
</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="4381.html">[ Next ]</a><a href="4379.html">[ Previous ]</a>
<b>In reply to:</b> <a href="4377.html">Harvey Newstrom</a>
<!-- nextthread="start" -->
</ul>
</body></html>
