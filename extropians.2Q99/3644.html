<!-- received="Fri Jun 11 08:30:18 1999 MDT" -->
<!-- sent="11 Jun 1999 16:30:07 +0200" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: freedom vs even distribution" -->
<!-- id="b493dzyhksw.fsf@mdj.nada.kth.se" -->
<!-- inreplyto="Fri, 11 Jun 1999 15:12:21 +0200" -->
<!-- version=1.10, linesinbody=63 -->
<html><head><title>extropians: Re: freedom vs even distribution</title>
<meta name=author content="Anders Sandberg">
<link rel=author rev=made href="mailto:asa@nada.kth.se" title ="Anders Sandberg">
</head><body>
<h1>Re: freedom vs even distribution</h1>
Anders Sandberg (<i>asa@nada.kth.se</i>)<br>
<i>11 Jun 1999 16:30:07 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3644">[ date ]</a><a href="index.html#3644">[ thread ]</a><a href="subject.html#3644">[ subject ]</a><a href="author.html#3644">[ author ]</a>
<!-- next="start" -->
<li><a href="3645.html">[ Next ]</a><a href="3643.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3630.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
"den Otter" &lt;neosapient@geocities.com&gt; writes:

<p>
<a href="3630.html#3644qlink1">&gt; ----------</a><br>
<i>&gt; &gt; From: Dan Clemmensen &lt;Dan@Clemmensen.ShireNet.com&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; I've read their entire "civilization", but not "Lessons of History."</i><br>
<i>&gt; &gt; I think that the conclusion overlooks the likelihood of altruism</i><br>
<i>&gt; &gt; among the potential immortals. If even one of the immortals has</i><br>
<i>&gt; &gt; even the slightest amount of altruism, then the technology will</i><br>
<i>&gt; &gt; be be disseminated to the populace as a whole. </i><br>
<i>&gt; </i><br>
<a name="3765qlink1"><i>&gt; Assuming that the other immortals allow it (there could be a codex</i><br>
<i>&gt; against proliferation of transhuman tech, much like the current one</i><br>
<i>&gt; against the proliferation of nuclear weapons). </i><br>

<p>
We might not allow selling nuclear weapons to Saddam, but free spread
of medical technology is encouraged.</a> And of course, the scientific
<a name="3765qlink2">community is displaying its discoveries for all to see.</a> 

<p>
<a href="3630.html#3644qlink2">&gt; With (near-)perfect surveillance being easy for posthumans, any "illegal"</a><br>
<i>&gt; development on earth could easily be nipped in the but. </i><br>

<p>
I wonder about perfect surveillance; I was arguing with Nick Boström a
bit about it after TransVision, and I wonder if you really could
develop a perfect surveillance system when the enemy might be
posthuman too.<a name="3685qlink1"> To make things even worse, a defecting posthuman might
make scratch monkey posthumans that would do the defection "of their
own free will" with no trace to the originator.</a>

<p>
<a href="3630.html#3644qlink3">&gt; &gt; The argument against</a><br>
<i>&gt; &gt; this assumes a zero-sum game in which a gain by the unwashed masses</i><br>
<i>&gt; &gt; equates to a loss by an immortal. Information doesn't work this</i><br>
<i>&gt; &gt; way.</i><br>
<i>&gt; </i><br>
<a name="3765qlink3"><i>&gt; Information is power, and by allowing millions of people to become</i><br>
<i>&gt; god-like too, you multiply the risk of something going wrong by</i><br>
<i>&gt; (approximately) the same amount. To the already fully autonomous</i><br>
<i>&gt; posthumans this might not seem like a very good idea; there's more</i><br>
<i>&gt; to lose than to gain. </i><br>

<p>
I disagree, I would say you gain much more. An industrialized third
world would increase utility production worldwide tremendously - both
as direct producer and as trading parters to everybody else. Compare
to the Marshall plan: everybody was better off afterwards despite the
increased destructive abilities of Europe (remember that de Gaulle
proudly boasted that his missiles could be turned in any direction,
including his former benefactors).</a> Somehow I think posthuman
economics is much more like Wired's New Economy rather than Malthusian
zero-sums.

<p>
<a name="3765qlink4">For your scenario to hold, the risk posed by each posthuman to each
other posthuman must be bigger than the utility of each posthuman to
each other posthuman.</a> But network economics seems to suggest that the
<a name="3765qlink5">utilities increase with the number of participants, which means that
they would become bigger than the risks as the number of posthumans
grow.</a>


<hr>
<pre>
Anders Sandberg                                      Towards Ascension!
asa@nada.kth.se                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
</pre>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3645.html">[ Next ]</a><a href="3643.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3630.html">den Otter</a>
<!-- nextthread="start" -->
</ul>
</body></html>
