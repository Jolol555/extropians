<!-- received="Sun Jun 13 05:34:28 1999 MDT" -->
<!-- sent="Sun, 13 Jun 1999 12:26:15 +0200" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: freedom vs even distribution" -->
<!-- id="199906131134.EAA00637@geocities.com" -->
<!-- inreplyto="freedom vs even distribution" -->
<!-- version=1.10, linesinbody=74 -->
<html><head><title>extropians: Re: freedom vs even distribution</title>
<meta name=author content="den Otter">
<link rel=author rev=made href="mailto:neosapient@geocities.com" title ="den Otter">
</head><body>
<h1>Re: freedom vs even distribution</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Sun, 13 Jun 1999 12:26:15 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3765">[ date ]</a><a href="index.html#3765">[ thread ]</a><a href="subject.html#3765">[ subject ]</a><a href="author.html#3765">[ author ]</a>
<!-- next="start" -->
<li><a href="3766.html">[ Next ]</a><a href="3764.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3644.html">Anders Sandberg</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3997.html">Anders Sandberg</a>
</ul>
<!-- body="start" -->



<hr>
<br>
<a name="3997qlink1">&gt; From: Anders Sandberg &lt;asa@nada.kth.se&gt;<br>

<p>
<a href="3644.html#3765qlink1">&gt; &gt; Assuming that the other immortals allow it (there could be a codex</a><br>
<i>&gt; &gt; against proliferation of transhuman tech, much like the current one</i><br>
<i>&gt; &gt; against the proliferation of nuclear weapons). </i><br>
<i>&gt; </i><br>
<i>&gt; We might not allow selling nuclear weapons to Saddam, but free spread</i><br>
<i>&gt; of medical technology is encouraged. </i><br>

<p>
Yes, but that's because medical technology is relatively harmless. If
you allow powerful transhuman tech such as intelligence enhancement techniques and
mature nanotech to proliferate you can hardly control 
what it will be used for, and you can bet your life that it will be used 
for "evil" sooner or later. Rather sooner than later, btw.
</a>

<p>
And of course, the scientific
<br>
<a name="3997qlink2"><a href="3644.html#3765qlink2">&gt; community is displaying its discoveries for all to see. </a><br>

<p>
But posthumans could much easily keep their advances to themselves.</a>

<p>
<a href="3644.html#3765qlink3">&gt; &gt; Information is power, and by allowing millions of people to become</a><br>
<i>&gt; &gt; god-like too, you multiply the risk of something going wrong by</i><br>
<i>&gt; &gt; (approximately) the same amount. To the already fully autonomous</i><br>
<i>&gt; &gt; posthumans this might not seem like a very good idea; there's more</i><br>
<i>&gt; &gt; to lose than to gain. </i><br>
<i>&gt; </i><br>
<a name="3997qlink3"><i>&gt; I disagree, I would say you gain much more. An industrialized third</i><br>
<i>&gt; world would increase utility production worldwide tremendously - both</i><br>
<i>&gt; as direct producer and as trading parters to everybody else. Compare</i><br>
<i>&gt; to the Marshall plan: everybody was better off afterwards despite the</i><br>
<i>&gt; increased destructive abilities of Europe (remember that de Gaulle</i><br>
<i>&gt; proudly boasted that his missiles could be turned in any direction,</i><br>
<i>&gt; including his former benefactors). </i><br>

<p>
Well, there's one crucial difference between humans and 
posthumans: the former *must* cooperate in order to survive 
and to get ahead, while the latter are fully autonomous, 
self-contained systems. A human can't damage society
</a>
too much without ultimately damaging his own future, while
a (real) posthuman superintelligence could wipe out all life on
earth (or anywhere else), and still thrive for all eternity. 

<p>
<a name="3997qlink4"><a href="3644.html#3765qlink4">&gt; For your scenario to hold, the risk posed by each posthuman to each</a><br>
<i>&gt; other posthuman must be bigger than the utility of each posthuman to</i><br>
<i>&gt; each other posthuman. </i><br>

<p>
And that's exactly what would be the case; other entities are useful
because of their added computing/physical power, but if you can
add infinite amounts of "slave" modules to your brain/body, why bother
with unpredictable, potentially dangerous "peers"?</a> Of course, it is
<a name="3997qlink5">unlikely that there would be just one supreme posthuman, so they'd
have to compromize and declare a "pax posthumana" based on
MAD, very much like the cold war I presume.</a> Posthumans can 
easily be compared to countries (or worlds), after all. New members
would almost certainly have a (very) destabilizing effect -- as is
indeed the case with the proliferation of nuclear weapons (and 
they would further reduce the amount of available resources), so 
the top dogs would surely think twice before allowing anyone 
to reach their level of development.

<p>
<a name="3997qlink6"><i>&gt; But network economics seems to suggest that the</i><br>
<a href="3644.html#3765qlink5">&gt; utilities increase with the number of participants, which means that</a><br>
<i>&gt; they would become bigger than the risks as the number of posthumans</i><br>
<i>&gt; grow.</i><br>

<p>
If a posthuman is so smart, and already (practically) immortal, surely
it could develop all its utilities by itself in due time? Economies are
typically a construct of highly limited creatures that must specialize 
and cooperate to survive.
</a>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3766.html">[ Next ]</a><a href="3764.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3644.html">Anders Sandberg</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="3997.html">Anders Sandberg</a>
</ul>
</body></html>
