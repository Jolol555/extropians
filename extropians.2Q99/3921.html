<!-- received="Tue Jun 15 17:01:08 1999 MDT" -->
<!-- sent="Tue, 15 Jun 1999 18:01:46 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Qualia and the Galactic Loony Bin" -->
<!-- id="3766DB57.AA3D76D@pobox.com" -->
<!-- inreplyto="" -->
<!-- version=1.10, linesinbody=84 -->
<html><head><title>extropians: Qualia and the Galactic Loony Bin</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Qualia and the Galactic Loony Bin</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Tue, 15 Jun 1999 18:01:46 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3921">[ date ]</a><a href="index.html#3921">[ thread ]</a><a href="subject.html#3921">[ subject ]</a><a href="author.html#3921">[ author ]</a>
<!-- next="start" -->
<li><a href="3922.html">[ Next ]</a><a href="3920.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3855.html">hal@finney.org</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="4069.html">Wei Dai</a>
</ul>
<!-- body="start" -->

<p>
hal@finney.org wrote:
<br>
<i>&gt; </i><br>
<a href="3855.html#3921qlink1">&gt; Eliezer S. Yudkowsky, &lt;sentience@pobox.com&gt;, writes, quoting me:</a><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; Plain intelligence?  Sure.  Probably the vast majority of races across</i><br>
<i>&gt; &gt; the Reality are non-conscious until they Transcend.  We're exceptions,</i><br>
<i>&gt; &gt; but, quite obviously, the only consciously observed exceptions in the</i><br>
<i>&gt; &gt; absence of a Singularity.</i><br>
<i>&gt; </i><br>
<i>&gt; That's an interesting possibility.  So they could evolve intelligence,</i><br>
<i>&gt; emotions, and many of the other trappings of life similar to ours, but</i><br>
<i>&gt; they would be lacking qualia.  They would not be conscious in the sense</i><br>
<i>&gt; that we are.  They would process information, they would have models of</i><br>
<i>&gt; the world as we do, it would probably be meaningful for them to use the</i><br>
<i>&gt; words "I" and "me" in conversations.  But something would be different.</i><br>

<p>
They'd have complete personalities, and be likable or evil or whatever. 
They just wouldn't have Penrose, Chalmers, and Dennett duking it out in
the philosophy journals.  They might have Godel's theorem and
technophobes combining to yield Lucas; anti-mechanists; even talk of
sentience not being Turing-computable - but you wouldn't see any
discussion of qualia.  Instead, you'd just see the standard "Can a
machine really have emotions?", debate over the self-symbol and the
event-loop of the mind, and, for sufficiently sophisticated
technophobes, the Argument from Godel.  You wouldn't see people arguing
about the irreducible redness of red.  I think.

<p>
<a href="3855.html#3921qlink2">&gt; How would this difference manifest itself?  Would it be possible to</a><br>
<i>&gt; convince such a being that we humans have some "spark", some kind</i><br>
<i>&gt; of primary, irreducible experience of reality, that they don't have?</i><br>

<p>
<a name="4069qlink1">Of course not.  If we can't comprehend the First Cause or qualia
ourselves, because our reasoning processes deal only with
Turing-computable ontologies, I would hardly expect us to be able to
explain qualia to a skeptical Turing-computable being!  Remember, qualia
are enormously improbable; the only reason we're allowed to have them is
because of the enormous number of races combined with the Anthropic
Principle.  To any skeptic, the probability that our race is
congenitally brain-damaged would exceed the probability of our having
actual qualia.  You'd have to open up the neurons and demonstrate that
we're messing with Weird Physics, after which the whole qualia business
would be more plausible.</a>

<p>
<a href="3855.html#3921qlink3">&gt; Suppose they aren't sure initially that there can be any experience</a><br>
<i>&gt; of reality beyond what they have.  What empirical test can we offer,</i><br>
<i>&gt; what capability would we have that they do not?  If they were blind, we</i><br>
<i>&gt; could talk about how we can tell what is happening at a distance without</i><br>
<i>&gt; having to go and touch it.  What can we say if they are blind to qualia?</i><br>

<p>
We can talk about the irreducible redness of red until they lock us in
the Galactic Loony Bin.

<p>
<a href="3855.html#3921qlink4">&gt; Maybe, after all, I don't have qualia in the sense that Eliezer does.</a><br>
<i>&gt; Perhaps my basic sense of the universe is fundamentally different</i><br>
<i>&gt; from his.  We both react to the same universe and so there is a certain</i><br>
<i>&gt; basic commonality of representation and reasoning, but perhaps the raw,</i><br>
<i>&gt; nitty gritty irreducible nature of reality is totally different for us.</i><br>
<i>&gt; How could we detect this lack on my part?</i><br>

<p>
We can't - our I/O is representable as static data, even though the
internals aren't.  The only way I have to know that you're conscious is
by analogizing your output to my output and assuming that they have the
same internal cause.  Philosophically, there's no way for me to know
that you're conscious, short of opening up your brain (with tools left
behind by the Transcendent who used to have my office) and having a look.

<p>
<a href="3855.html#3921qlink5">&gt; Consider a situation like in The Matrix, where people have their</a><br>
<i>&gt; brains directly interfaced to a computer simulation.  Are objects in</i><br>
<i>&gt; the simulation "real"?  I would assume not.  But what is the different</i><br>
<i>&gt; behavior which would reveal their unreality?  Are you saying that there</i><br>
<i>&gt; is some way of distinguishing any simulation from reality?  What is</i><br>
<i>&gt; the trick?</i><br>

<p>
No, the distinction between simulation/reality isn't what I'm talking
about.  There might be some real, definite, unarguable definition of
"instantiation", just for non-Turing processes.  I don't know that there
is no definition of instantiation, just that there's no definition
within the Turing continuum.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3922.html">[ Next ]</a><a href="3920.html">[ Previous ]</a>
<b>In reply to:</b> <a href="3855.html">hal@finney.org</a>
<!-- nextthread="start" -->
<b>Next in thread:</b> <a href="4069.html">Wei Dai</a>
</ul>
</body></html>
