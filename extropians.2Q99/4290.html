<!-- received="Tue Jun 22 07:35:15 1999 MDT" -->
<!-- sent="Tue, 22 Jun 1999 14:33:21 +0000" -->
<!-- name="Nick Bostrom" -->
<!-- email="bostrom@ndirect.co.uk" -->
<!-- subject="Re: Volume of Human-Equivalent Intelligence" -->
<!-- id="199906221335.OAA29167@sioux.hosts.netdirect.net.uk" -->
<!-- inreplyto="19990622.231156.5095.12.rgvandewalker@juno.com" -->
<!-- version=1.10, linesinbody=71 -->
<html><head><title>extropians: Re: Volume of Human-Equivalent Intelligence</title>
<meta name=author content="Nick Bostrom">
<link rel=author rev=made href="mailto:bostrom@ndirect.co.uk" title ="Nick Bostrom">
</head><body>
<h1>Re: Volume of Human-Equivalent Intelligence</h1>
Nick Bostrom (<i>bostrom@ndirect.co.uk</i>)<br>
<i>Tue, 22 Jun 1999 14:33:21 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#4290">[ date ]</a><a href="index.html#4290">[ thread ]</a><a href="subject.html#4290">[ subject ]</a><a href="author.html#4290">[ author ]</a>
<!-- next="start" -->
<li><a href="4291.html">[ Next ]</a><a href="4289.html">[ Previous ]</a>
<b>In reply to:</b> <a href="4284.html">Raymond G. Van De Walker</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
Some comments on Raymond G. Van De Walker's epistle:

<p>
<a href="4284.html#4290qlink1">&gt; Let me show you the numbers:  The human brain has 10 billion</a><br>
<i>&gt; neurons.</i><br>

<p>
Most recent estimtes put it at 100 billion.

<p>
<a href="4284.html#4290qlink2">&gt; Now, I thought about ways to reduce this by editing the system,</a><br>
<i>&gt; but they won't work.  Like most real computing systems, the majority of</i><br>
<i>&gt; the logic (&gt;95%) by weight or volume is I/O.  (The cerebrum, cerebellum,</i><br>
<i>&gt; gyrii, and most of the encephalon)  Neural networks are great for I/O:</i><br>
<i>&gt; they're robust and compact compared to the digital systems they replace. </i><br>
<i>&gt; You would not want to use anything else to construct the phenomenal</i><br>
<i>&gt; systems of a robot.</i><br>

<p>
Hmm. One way of estimating the brain's processing power is like this:

<p>
The human brain contains about 10^11 neurons. Each neuron has about
5*10^3 synapses, and signals are transmitted along these synapses at
an average frequency of about 10^2 Hz. Each signal contains, say, 5
bits. This equals 10^17 ops.

<p>
Another method, used by Moravec, is to look at one part of the 
nervous system whose function we can replicate on computers today 
(the retina). Then we multiply the resources needed for this 
computation by the factor by which the total brain is larger than the 
retina. This gives us the figure 10^14 ops for the brain, three 
magnitudes less than the first estimate.

<p>
The second estimate presupposes that we can make some optimizations. 
Maybe intelligent design and new computational elements enable us to 
do several orders of magnitude better than mother nature with the 
same number of ops. Why couldn't the same be possible with regard to 
memory requirements? There is no evidence that the brain's memory 
system is not highly redundant, so that with highly reliable 
artificial (or simulated) neurons one get away with a lot less 
memory. We simply don't know.

<p>
Another problem with the multiplicative approach - multiplying 
neurons, synapeses per neuron, and resources per synapse - is that it 
might turn out that a lot of the brain's computing power is in 
higher-order interactions in the dendritic trees. Signals may not 
only be added, but multiplied, and with interesting time-integration 
effects that might be used by the brain. The multiplicative approach 
neglects these potential effects, and may thus understimate the 
brain's computing power. 

<p>
<a href="4284.html#4290qlink3">&gt; 	 Using Drexler's estimates for random-access memory</a><br>
<i>&gt; (20MBytes/cubic micron), we can fit 305 of 64K computers in a cubic</i><br>
<i>&gt; micron.  The computers therefore take roughly 9.8x10^4 cubic microns.</i><br>

<p>
Does this take into account that there might be a need for cooling?

<p>
<a href="4284.html#4290qlink4">&gt; 	11,300 cubic microns is small.  It's a cube about 22.5 microns on</a><br>
<i>&gt; a side, say a quarter-millimeter on a side, about 1/8 the size of a</i><br>
<i>&gt; crystal of table salt.  17,300 cubic microns (storing synaptic addresses)</i><br>
<i>&gt; is still small, about 25.9 microns on a side.  Even 34,600 cubic microns</i><br>
<i>&gt; (double everything) is small, maybe 32.6microns on a side, the size of a</i><br>
<i>&gt; crystal of table salt.</i><br>

<p>
As an estimate of precisely how small advance nanotech could make a 
human-equivalent computer I think we have to take this grain of 
salt with a grain of salt so to speak. An uncertainty interval of a 
couple of orders of magnitude does not seem unreasonable. 


<p>
Nick Bostrom
<br>
<a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a>      n.bostrom@lse.ac.uk
Department of Philosophy, Logic and Scientific Method
London School of Economics
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="4291.html">[ Next ]</a><a href="4289.html">[ Previous ]</a>
<b>In reply to:</b> <a href="4284.html">Raymond G. Van De Walker</a>
<!-- nextthread="start" -->
</ul>
</body></html>
