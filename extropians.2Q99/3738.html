<!-- received="Sat Jun 12 21:03:05 1999 MDT" -->
<!-- sent="Sat, 12 Jun 1999 22:03:37 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Indestructable Humans, Libertarian migrations, etc." -->
<!-- id="37631F86.33F51FE@pobox.com" -->
<!-- inreplyto="Indestructable Humans, Libertarian migrations, etc." -->
<!-- version=1.10, linesinbody=24 -->
<html><head><title>extropians: Re: Indestructable Humans, Libertarian migrations, etc.</title>
<meta name=author content="Eliezer S. Yudkowsky">
<link rel=author rev=made href="mailto:sentience@pobox.com" title ="Eliezer S. Yudkowsky">
</head><body>
<h1>Re: Indestructable Humans, Libertarian migrations, etc.</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sat, 12 Jun 1999 22:03:37 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3738">[ date ]</a><a href="index.html#3738">[ thread ]</a><a href="subject.html#3738">[ subject ]</a><a href="author.html#3738">[ author ]</a>
<!-- next="start" -->
<li><a href="3739.html">[ Next ]</a><a href="3737.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="3713.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
</ul>
<!-- body="start" -->

<p>
There is only one weapon, and that weapon is intelligence.  A
<br>
"transhuman" is a transhuman mind.  Anything else is a side issue, a<br>
technological party trick.  Neanderthals with machine guns could not
have held off Cro-Magnons with clubs... because the Cro-Magnons wouldn't
be armed with clubs for long.  Unless a nanowar is too fast and too
deadly for any research to take place, the war will be won by the side
with the better neurohackers.

<p>
I don't say "better AI", because then the war is won by the AI, period. 
Intelligence is its own purpose and has its own goals; even if
superhuman intelligence is created by one side in a conflict, it will
very rapidly become its own side as soon as it passes a certain level of intelligence.

<p>
A free nanotechnological society is dynamically unstable; any research
lab possesses the resources to create either military goo or a computer
with far-transhuman capacity.  A nanotech society is supersaturated with
the means of transition to the two stable states, death and Singularity.
 Such a society would not last long enough for legal issues to become relevant.
<pre>
-- 
           sentience@pobox.com          Eliezer S. Yudkowsky
        <a href="http://pobox.com/~sentience/tmol-faq/meaningoflife.html">http://pobox.com/~sentience/tmol-faq/meaningoflife.html</a>
Running on BeOS           Typing in Dvorak          Programming with Patterns
Voting for Libertarians   Heading for Singularity   There Is A Better Way
</pre>
<!-- body="end" -->
<p>
<ul>
<!-- next="start" -->
<li><a href="3739.html">[ Next ]</a><a href="3737.html">[ Previous ]</a>
<b>Maybe in reply to:</b> <a href="3713.html">Robert J. Bradbury</a>
<!-- nextthread="start" -->
</ul>
</body></html>
