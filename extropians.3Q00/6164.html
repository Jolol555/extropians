<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=Windows-1252">
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="J. R. Molloy (jr@shasta.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Sat Sep 30 08:52:17 2000" -->
<!-- isoreceived="20000930145217" -->
<!-- sent="Sat, 30 Sep 2000 07:45:55 -0700" -->
<!-- isosent="20000930144555" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="01fe01c02aee$2af116c0$56bc473f@jrmolloy" -->
<!-- charset="Windows-1252" -->
<!-- inreplyto="NEBBLDNPLKMJPLEGEOCKAEKPCIAA.andrew.lias@corp.usa.net" -->
<STRONG>From:</STRONG> J. R. Molloy (<A HREF="mailto:jr@shasta.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;01fe01c02aee$2af116c0$56bc473f@jrmolloy&gt;"><EM>jr@shasta.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Sat Sep 30 2000 - 08:45:55 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="6165.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6163.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6081.html">Andrew Lias: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6164">[ date ]</A>
<A HREF="index.html#6164">[ thread ]</A>
<A HREF="subject.html#6164">[ subject ]</A>
<A HREF="author.html#6164">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Andrew Lias has written,
<BR>
<P><EM>&gt; I've been following the debates regarding the possibilities of friendly vs.
</EM><BR>
<EM>&gt; unfriendly AI and I have a question.  It seems that we are presuming that a
</EM><BR>
<EM>&gt; friendly AI would be friendly towards us in a manner that we would recognize
</EM><BR>
<EM>&gt; as friendly.  Indeed, what, precisely, do we mean by friendly?
</EM><BR>
<P>Good question. I think everyone has their own idea of what friendly means. Some
<BR>
theorists may speculate that the friendliest thing AI could do for us would be
<BR>
to round up all the anti-AI fascists and homicidal fundies and upload them into
<BR>
a video game (where they belong, after all).
<BR>
<P><EM>&gt; Let us (to invoke a favored cliche) suppose that an AI is evolved such that
</EM><BR>
<EM>&gt; it's understanding of being friendly towards humans is that it should try to
</EM><BR>
<EM>&gt; insure the survival of humanity and that it should attempt to maximize our
</EM><BR>
<EM>&gt; happiness.  What is to prevent it from deciding that the best way to
</EM><BR>
<EM>&gt; accomplish those goals is to short circuit our manifestly self-destructive
</EM><BR>
<EM>&gt; sense of intelligence and to re-wire our brains so that we are incapable of
</EM><BR>
<EM>&gt; being anything but deleriously happy at all times? [1]
</EM><BR>
<P>More likely it would explain the benefits of meditation, and leave any obstinate
<BR>
egoists in the dust while installing the enlightened in paradisiacal
<BR>
immortality.
<BR>
<P><EM>&gt; Now, I'm not suggesting that *this* is a plausible example (as noted, it's
</EM><BR>
<EM>&gt; very much a science-fiction cliche), but I am concerned that any definition
</EM><BR>
<EM>&gt; or set of parameters we develop for the evolution of friendly AI may include
</EM><BR>
<EM>&gt; unforseen consequences in the definition that we simply can't anticipate at
</EM><BR>
<EM>&gt; our level of intelligence -- and that's supposing that the SI will still
</EM><BR>
<EM>&gt; want to be friendly.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt; What am I missing?
</EM><BR>
<P>Unforeseen consequences of AI will not be unforeseen for AI cyborgs, but only
<BR>
for organically unintelligent power brokers. To get to a Type I, II, and III
<BR>
civilization, we'll definitely need AI. The most important job for AI (a seventh
<BR>
generation expert system) will be to bypass the efforts of control freaks who
<BR>
want to outlaw AI.
<BR>
<P>--J. R.
<BR>
<P><P>&quot;One trend that bothers me is the glorification of stupidity, that it's
<BR>
all right not to know anything.&quot;
<BR>
--Carl Sagan
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="6165.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6163.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6081.html">Andrew Lias: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6164">[ date ]</A>
<A HREF="index.html#6164">[ thread ]</A>
<A HREF="subject.html#6164">[ subject ]</A>
<A HREF="author.html#6164">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:29 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
