<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Fwd: Earthweb from transadmin</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: Fwd: Earthweb from transadmin">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Fwd: Earthweb from transadmin</H1>
<!-- received="Mon Sep 18 22:56:42 2000" -->
<!-- isoreceived="20000919045642" -->
<!-- sent="Tue, 19 Sep 2000 00:42:03 -0400" -->
<!-- isosent="20000919044203" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Fwd: Earthweb from transadmin" -->
<!-- id="39C6EE9B.64D3A93E@pobox.com" -->
<!-- inreplyto="14790.58394.758038.73971@bahgwah.nyu.edu" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Fwd:%20Earthweb%20from%20transadmin&In-Reply-To=&lt;39C6EE9B.64D3A93E@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon Sep 18 2000 - 22:42:03 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5243.html">Samantha Atkins: "Re: just me"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5241.html">Spudboy100@aol.com: "Re: God &amp; stuff (was: Re: just me)"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5237.html">Matt Gingell: "Re: Fwd: Earthweb from transadmin"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5250.html">Jason Joel Thompson: "Re: Fwd: Earthweb from transadmin"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5250.html">Jason Joel Thompson: "Re: Fwd: Earthweb from transadmin"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5258.html">Matt Gingell: "Re: Fwd: Earthweb from transadmin"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5242">[ date ]</A>
<A HREF="index.html#5242">[ thread ]</A>
<A HREF="subject.html#5242">[ subject ]</A>
<A HREF="author.html#5242">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Matt Gingell wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt;  You seem to think there's a dense, more-or-less coherent, domain of
</EM><BR>
<EM>&gt;  attraction with a happily-ever-after stable point in the middle. A
</EM><BR>
<EM>&gt;  blotch of white paint with some grey bleed around the edges. If we
</EM><BR>
<EM>&gt;  start close to the center, our AI can orbit around as much as it
</EM><BR>
<EM>&gt;  likes but never escape into not-nice space. The boundaries are fuzzy,
</EM><BR>
<EM>&gt;  but that's unimportant: I can know the Empire State Building is a
</EM><BR>
<EM>&gt;  skyscraper without knowing exactly how many floors it has, or having
</EM><BR>
<EM>&gt;  any good reason for believing a twelve story apartment building
</EM><BR>
<EM>&gt;  isn't. So long as we're careful to start somewhere unambiguous, we
</EM><BR>
<EM>&gt;  don't have to worry about formal definitions or prove nothing nasty
</EM><BR>
<EM>&gt;  is going to happen.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt;  Eugene, on the other hand, seems to think this is not the case.
</EM><BR>
<EM>&gt;  Rather, the friendly and unfriendly subspaces are embedded in each
</EM><BR>
<EM>&gt;  other, and a single chaotic step in the wrong direction shoots a
</EM><BR>
<EM>&gt;  thread off into unpredictability. More like intertwined balls of
</EM><BR>
<EM>&gt;  turbulent Cantor-String, knotted together at infinitely many
</EM><BR>
<EM>&gt;  meta-stable catastrophe points, than like the comforting whirlpool of
</EM><BR>
<EM>&gt;  nice brains designing ever nicer ones. Your final destination is
</EM><BR>
<EM>&gt;  still a function of where you start, but it's so sensitive to initial
</EM><BR>
<EM>&gt;  conditions it depends more on the floating point rounding model you
</EM><BR>
<EM>&gt;  used on your first build than it does on anything you actually though
</EM><BR>
<EM>&gt;  about.
</EM><BR>
<P>Let's distinguish between trajectories of intelligence and cognitive
<BR>
architecture, and trajectories of motives.  My basic point about motivations
<BR>
is that the center of Friendly Space is both reasonably compact and
<BR>
referenceable by a reasonably intelligent human; it's something that seems
<BR>
possible, at least in theory, for us to point to and say:  &quot;Look where I'm
<BR>
pointing, not at my finger.&quot;
<BR>
<P>We can't specify cognitive architectures; lately I've been thinking that even
<BR>
my underlying quantitative definition of goals and subgoals may not be
<BR>
something that should be treated as intrinsic to the definition of
<BR>
friendliness.  But suppose we build a Friendliness Seeker, that starts out
<BR>
somewhere in Friendly Space, and whose purpose is to stick around in Friendly
<BR>
Space regardless of changes in cognitive architecture.  We tell the AI up
<BR>
front and honestly that we don't know everything, but we do want the AI to be
<BR>
friendly.  What force would knock such an AI out of Friendly Space?
<BR>
<P>The rule of thumb I use is that there'll be at least one thing I don't know
<BR>
about, and therefore I have to come up with a *simple* underlying strategy
<BR>
that looks able to handle the things I do know about even if I hadn't thought
<BR>
of them.
<BR>
<P><EM>&gt;  The later seems more plausible, my being a pessimist, the trajectory
</EM><BR>
<EM>&gt;  to hell being paved with locally-good intentions, etc.
</EM><BR>
<P>Matt Gingell makes my quotes file yet again.  &quot;The trajectory to hell is paved
<BR>
with locally-good intentions.&quot;
<BR>
<P>The underlying character of intelligence strikes me as being pretty
<BR>
convergent.  I don't worry about nonconvergent behaviors; I worry that
<BR>
behaviors will converge to somewhere I didn't expect.
<BR>
<P><EM>&gt;  &gt; Evolution is the degenerate case of intelligent design in which intelligence
</EM><BR>
<EM>&gt;  &gt; equals zero.  If I happen to have a seed AI lying around, why should it be
</EM><BR>
<EM>&gt;  &gt; testing millions of unintelligent mutations when it could be testing millions
</EM><BR>
<EM>&gt;  &gt; of intelligent mutations?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt;  Intelligent design without intelligence is exactly what makes
</EM><BR>
<EM>&gt;  evolution such an interesting bootstrap: It doesn't beg the question
</EM><BR>
<EM>&gt;  of how to build an intelligent machine by assuming you happen to have
</EM><BR>
<EM>&gt;  one lying around already.
</EM><BR>
<P>Intelligence isn't a binary quality.  You use whatever intelligence you have
<BR>
lying around, even if it's just a little tiny bit.  Just being able to mutate
<BR>
descriptors and reversible features is a step above mutating the raw data, and
<BR>
that doesn't require any intelligence above the level of sensory modalities.
<BR>
<P><EM>&gt;  Eugene is talking, I think, about parasitic memes and mental illness
</EM><BR>
<EM>&gt;  here, not space invaders.
</EM><BR>
<P>If you're asking whether the Sysop will be subject to spontaneously arising
<BR>
computer viruses, my answer is that the probability can be reduced to an
<BR>
arbitrarily low level, just like nanocomputers and thermal vibration errors. 
<BR>
Remember the analogy between human programmers and blind painters.  Our
<BR>
pixel-by-pixel creations can be drastically changed by the mutation of a
<BR>
single pixel; the Mona Lisa would remain the Mona Lisa.  I think that viruses
<BR>
are strictly problems of the human level.
<BR>
<P>I think that parasitic memes and mental illness are definitely problems of the
<BR>
human level.
<BR>
<P>If you're asking how a human can design motivations for a seed AI so that the
<BR>
seed AI grows into a Sysop which is not evilly exploitable by a formerly-human
<BR>
superintelligence running on a Java sandbox inside the Sysop, the best answer
<BR>
I can give is that we should politely ask the Sysop not to be evilly
<BR>
exploitable.  There are some issues that are simply beyond what the Sysop
<BR>
Programmers can reasonably be expected to handle in detail.  This doesn't mean
<BR>
the issues aren't solvable.
<BR>
<P><EM>&gt;  But it's rather like thinking a
</EM><BR>
<EM>&gt;  friendly, symbiotic, strain of bacteria is likely to eventually
</EM><BR>
<EM>&gt;  evolve into friendly people. The first few steps might preserve
</EM><BR>
<EM>&gt;  something of our initial intent, but none of the well-meaning
</EM><BR>
<EM>&gt;  intermediates is going to have any more luck anticipating the
</EM><BR>
<EM>&gt;  behavior of a qualitatively better brain than you or I.
</EM><BR>
<P>It depends on to what extent the behavior of a qualitatively better brain
<BR>
impacts motives, and to what extent the changes are transmitted from
<BR>
generation to generation.  If the AI's motives are defined only in terms of
<BR>
inertia, with no attempt at external-reference semantics, then changes will
<BR>
build up.  If the AI's motives are defined by pointing to Friendly Space, so
<BR>
that the AI knows even in the early stages that there is such a thing as a
<BR>
&quot;wrong&quot; motive, then the behaviors of better brains should hopefully affect
<BR>
planning more than outcomes.  Eventually the AI finally reaches the level of
<BR>
superintelligence necessary to perceive Friendly Space cleanly and clearly and
<BR>
to pick out a really good, solid spot inside it.
<BR>
<P>--              --              --              --              -- 
<BR>
Eliezer S. Yudkowsky                          <A HREF="http://singinst.org/">http://singinst.org/</A> 
<BR>
Research Fellow, Singularity Institute for Artificial Intelligence
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5243.html">Samantha Atkins: "Re: just me"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5241.html">Spudboy100@aol.com: "Re: God &amp; stuff (was: Re: just me)"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5237.html">Matt Gingell: "Re: Fwd: Earthweb from transadmin"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5250.html">Jason Joel Thompson: "Re: Fwd: Earthweb from transadmin"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5250.html">Jason Joel Thompson: "Re: Fwd: Earthweb from transadmin"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5258.html">Matt Gingell: "Re: Fwd: Earthweb from transadmin"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5242">[ date ]</A>
<A HREF="index.html#5242">[ thread ]</A>
<A HREF="subject.html#5242">[ subject ]</A>
<A HREF="author.html#5242">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:28 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
