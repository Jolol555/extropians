<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Eugene's nuclear threat</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: Eugene's nuclear threat">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Eugene's nuclear threat</H1>
<!-- received="Fri Sep 29 16:35:16 2000" -->
<!-- isoreceived="20000929223516" -->
<!-- sent="Fri, 29 Sep 2000 18:24:03 -0400" -->
<!-- isosent="20000929222403" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Eugene's nuclear threat" -->
<!-- id="39D51683.2F3838B2@pobox.com" -->
<!-- inreplyto="4.2.0.58.20000929172320.00d31b70@mason.gmu.edu" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Eugene's%20nuclear%20threat&In-Reply-To=&lt;39D51683.2F3838B2@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Fri Sep 29 2000 - 16:24:03 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="6117.html">Barbara Lamar: "Re: Symbiotic Evolution (was Why would AI want to be friendly?)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6115.html">J. R. Molloy: "Re: Posthuman domain names auction"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6106.html">Robin Hanson: "Re: Eugene's nuclear threat"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6123.html">Robin Hanson: "Re: Eugene's nuclear threat"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6116">[ date ]</A>
<A HREF="index.html#6116">[ thread ]</A>
<A HREF="subject.html#6116">[ subject ]</A>
<A HREF="author.html#6116">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Robin Hanson wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Eliezer S. Yudkowsky wrote:
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt;And scientists and engineers are, by and large, benevolent - they may
</EM><BR>
<EM>&gt; &gt;express that benevolence in unsafe ways, but I'm willing to trust to good
</EM><BR>
<EM>&gt; &gt;intentions.  After all, it's not like I have a choice.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; A week earlier, Eliezer S. Yudkowsky wrote:
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt;On the contrary; the Earthweb is only as friendly as it is smart.  I really
</EM><BR>
<EM>&gt; &gt;don't see why the Earthweb would be benevolent.  Benevolent some of the time,
</EM><BR>
<EM>&gt; &gt;yes, but all of the time? 
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; These two quotes seem worth comparing.
</EM><BR>
<P>Okay, let's.  Scientists and engineers are benevolent most of the time, and so
<BR>
is the Earthweb.  I might trust a *mature* Earthweb to make *one* decision
<BR>
correctly, just as I would trust a *successful* AI engineer to program *one*
<BR>
Friendly AI.  If we had to trust a thousand teams to create a thousand AIs,
<BR>
we'd be doomed.  And I certainly wouldn't trust a randomly selected AI
<BR>
researcher to competently program a Friendly AI, just as I wouldn't trust a
<BR>
randomly selected AI researcher to successfully build an AI at all - this
<BR>
being the analogy to trusting a young Earthweb.
<BR>
<P><EM>&gt; Eliezer distrusts the scenario of
</EM><BR>
<EM>&gt; EarthWeb becoming smarter and smarter, with more and more input from
</EM><BR>
<EM>&gt; advanced software as available, because people can be emotional.   But he
</EM><BR>
<EM>&gt; thinks we should trust researchers creating a big bang AI, because
</EM><BR>
<EM>&gt; scientists are benevolent.
</EM><BR>
<P>I don't think we should trust them because they're benevolent; I said we could
<BR>
probably trust them to *be* benevolent.  If we build a scenario that relies on
<BR>
the winning researchers being benevolent, it's not a scenario-killing
<BR>
improbability - most people *are* benevolent.  *Competence* is another issue
<BR>
entirely.
<BR>
<P><EM>&gt; This seems to me the Spock theory of who to
</EM><BR>
<EM>&gt; trust - let scientists run things because they aren't emotional, and don't
</EM><BR>
<EM>&gt; let markets run things, because they might let just any emotional
</EM><BR>
<EM>&gt; person have influence.
</EM><BR>
<P>Please stop deliberately distorting my words.  I don't trust scientists to run
<BR>
*anything*, any more than I trust democracy or an Earthweb.  Building a
<BR>
Friendly AI isn't &quot;running&quot; anything.  Internally, it's an AI project, not a
<BR>
social interaction of any kind.
<BR>
<P><EM>&gt; Of course if people realized that scientists knew better and should be
</EM><BR>
<EM>&gt; trusted, then non-scientists wouldn't bet against scientists in a market,
</EM><BR>
<EM>&gt; and scientists would effectively run an EarthWeb.
</EM><BR>
<P>The scenario you describe has an unfortunately low probability of coming to
<BR>
pass.  (My mother once recommended this to me as a more polite alternative to
<BR>
&quot;You're wrong.&quot;)
<BR>
<P><EM>&gt; But since people are
</EM><BR>
<EM>&gt; too stupid to know who their rightful betters [bettors? -- Eliezer] are,
</EM><BR>
<EM>&gt; Eliezer prefers to impose on them the choice they wouldn't choose for
</EM><BR>
<EM>&gt; themselves.
</EM><BR>
<P>Robin, you - like myself - have *got* to learn to resist that parting shot. 
<BR>
Did that really add anything to the discussion?
<BR>
<P>I would no more trust scientists to run the planet than I would trust a group,
<BR>
or an Earthweb.  And may I note that I would expect benevolent intentions from
<BR>
the person on the street almost as often as I would expect it from AI
<BR>
scientists - say, 70% vs. 80%.  When I said that I believed the intentions of
<BR>
researchers were good, I didn't mean that the rest of humanity was bad.
<BR>
<P>--              --              --              --              -- 
<BR>
Eliezer S. Yudkowsky                          <A HREF="http://singinst.org/">http://singinst.org/</A> 
<BR>
Research Fellow, Singularity Institute for Artificial Intelligence
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="6117.html">Barbara Lamar: "Re: Symbiotic Evolution (was Why would AI want to be friendly?)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6115.html">J. R. Molloy: "Re: Posthuman domain names auction"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6106.html">Robin Hanson: "Re: Eugene's nuclear threat"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6123.html">Robin Hanson: "Re: Eugene's nuclear threat"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6116">[ date ]</A>
<A HREF="index.html#6116">[ thread ]</A>
<A HREF="subject.html#6116">[ subject ]</A>
<A HREF="author.html#6116">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:27 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
