<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="hal@finney.org (hal@finney.org)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Tue Sep  5 11:58:07 2000" -->
<!-- isoreceived="20000905175807" -->
<!-- sent="Tue, 5 Sep 2000 10:57:42 -0700" -->
<!-- isosent="20000905175742" -->
<!-- name="hal@finney.org" -->
<!-- email="hal@finney.org" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="200009051757.KAA12023@finney.org" -->
<!-- inreplyto="Why would AI want to be friendly?" -->
<STRONG>From:</STRONG> <A HREF="mailto:hal@finney.org?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;200009051757.KAA12023@finney.org&gt;"><EM>hal@finney.org</EM></A><BR>
<STRONG>Date:</STRONG> Tue Sep 05 2000 - 11:57:42 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4272.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4270.html">Emlyn O'Regan: "Re: Bugs in Anarchy was: Bugs in Free-Markets."</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="5641.html">J. R. Molloy: "Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="4272.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4271">[ date ]</A>
<A HREF="index.html#4271">[ thread ]</A>
<A HREF="subject.html#4271">[ subject ]</A>
<A HREF="author.html#4271">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Eliezer writes:
<BR>
<EM>&gt; If, as seems to be the default scenario, all supergoals are ultimately
</EM><BR>
<EM>&gt; arbitrary, then the superintelligence should do what we ask it to, for lack of
</EM><BR>
<EM>&gt; anything better to do.
</EM><BR>
<P>Before it is a super-intelligence, it will be as smart as a genius human.
<BR>
<P>Before it is a genius, it will be as smart as an ordinary human.
<BR>
<P>Before it is a human, it will be as smart as a very dumb human being
<BR>
(albeit perhaps with good coding skills, an idiot savant of programming).
<BR>
<P>And before that, it will be a smart coding aid.
<BR>
<P>In those early phases, you are going to have to direct it.  It could
<BR>
no more choose its own goals than Eurisko or Cyc.
<BR>
<P>Even as it approaches human level, it's not going to be able to
<BR>
spontaneously decide what to do.  This isn't something that will just
<BR>
emerge.  It has to be designed in.  The creators must decide how the
<BR>
program will allocate its resources, what will guide its decisions about
<BR>
what to work on.
<BR>
<P>I don't see how you can transition from a system which had designed-in
<BR>
structures which control how it spends its time, to one which lacks such,
<BR>
without it spinning its wheels.
<BR>
<P>The very notion of a system which &quot;chooses its own goals&quot; seems
<BR>
contradictory.  Choice presupposes a ranking system, which implies a
<BR>
pre-ordained goal structure.  Choosing your own goals is like proving
<BR>
your own axioms, or lifting yourself by your bootstraps.  It's not going
<BR>
to get off the ground.
<BR>
<P>You can build the computer to do what you tell it to, and you probably
<BR>
would need to do this, in the early stages.  It's just not clear what
<BR>
you replace it with when you decide to turn it off.
<BR>
<P>Hal
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4272.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4270.html">Emlyn O'Regan: "Re: Bugs in Anarchy was: Bugs in Free-Markets."</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="5641.html">J. R. Molloy: "Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="4272.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4271">[ date ]</A>
<A HREF="index.html#4271">[ thread ]</A>
<A HREF="subject.html#4271">[ subject ]</A>
<A HREF="author.html#4271">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:37:13 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
