<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Franklin Wayne Poley (culturex@vcn.bc.ca)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Wed Sep 27 15:58:51 2000" -->
<!-- isoreceived="20000927215851" -->
<!-- sent="Wed, 27 Sep 2000 14:59:05 -0700 (PDT)" -->
<!-- isosent="20000927215905" -->
<!-- name="Franklin Wayne Poley" -->
<!-- email="culturex@vcn.bc.ca" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="Pine.GSO.4.21.0009271454190.20746-100000@vcn.bc.ca" -->
<!-- inreplyto="14801.39429.386763.901647@lrz.uni-muenchen.de" -->
<STRONG>From:</STRONG> Franklin Wayne Poley (<A HREF="mailto:culturex@vcn.bc.ca?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;Pine.GSO.4.21.0009271454190.20746-100000@vcn.bc.ca&gt;"><EM>culturex@vcn.bc.ca</EM></A>)<BR>
<STRONG>Date:</STRONG> Wed Sep 27 2000 - 15:59:05 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5926.html">Franklin Wayne Poley: "Re: Robots, but philosophers (or, Hal-2001)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5924.html">J. R. Molloy: "Re: Why would an AI want to be friendly"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5879.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="5964.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5925">[ date ]</A>
<A HREF="index.html#5925">[ thread ]</A>
<A HREF="subject.html#5925">[ subject ]</A>
<A HREF="author.html#5925">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
On Tue, 26 Sep 2000, Eugene Leitl wrote:
<BR>
<P><EM>&gt; Franklin Wayne Poley writes:
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt;  &gt; Everyone who has gone to high school knows that there are underachievers
</EM><BR>
<EM>&gt;  &gt; and overachievers. It is a matter of motivation having a weak correlation
</EM><BR>
<EM>&gt;  &gt; with intelligence.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Show me a live intelligent being completely lacking motivation.
</EM><BR>
<EM>&gt; 
</EM><BR>
The issue here was whether a machine would be 'motivated' and my reply is
<BR>
that it will act according to its programming. If you program it to
<BR>
simulate human motivation it will do so. If you program
<BR>
friendly/unfriendly AI that is what you will get. If you give your
<BR>
machines lots of autonomy and genetic programs that take them where you
<BR>
can't possibly anticipate then that too will yield in accordance with the
<BR>
programming...and heaven help all of us. AI machines can get out of
<BR>
control just as any other machines can.
<BR>
FWP
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5926.html">Franklin Wayne Poley: "Re: Robots, but philosophers (or, Hal-2001)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5924.html">J. R. Molloy: "Re: Why would an AI want to be friendly"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5879.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="5964.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5925">[ date ]</A>
<A HREF="index.html#5925">[ thread ]</A>
<A HREF="subject.html#5925">[ subject ]</A>
<A HREF="author.html#5925">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:16 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
