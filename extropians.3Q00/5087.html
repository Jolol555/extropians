<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: RE: Goals was: Transparency and IP</TITLE>
<META NAME="Author" CONTENT="Samantha Atkins (samantha@objectent.com)">
<META NAME="Subject" CONTENT="RE: Goals was: Transparency and IP">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>RE: Goals was: Transparency and IP</H1>
<!-- received="Sat Sep 16 04:11:14 2000" -->
<!-- isoreceived="20000916101114" -->
<!-- sent="Sat, 16 Sep 2000 03:12:58 -0700" -->
<!-- isosent="20000916101258" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="RE: Goals was: Transparency and IP" -->
<!-- id="39C347AA.2F19A79A@objectent.com" -->
<!-- inreplyto="Goals was: Transparency and IP" -->
<STRONG>From:</STRONG> Samantha Atkins (<A HREF="mailto:samantha@objectent.com?Subject=RE:%20Goals%20was:%20Transparency%20and%20IP&In-Reply-To=&lt;39C347AA.2F19A79A@objectent.com&gt;"><EM>samantha@objectent.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Sat Sep 16 2000 - 04:12:58 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5088.html">Harvey Newstrom: "RE: Color blindness"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5086.html">Damien Broderick: "self-extracting zipware AI 'casting"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="4971.html">Dan Fabulich: "Goals was: Transparency and IP"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5159.html">Zero Powers: "Re: Goals was: Transparency and IP"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5087">[ date ]</A>
<A HREF="index.html#5087">[ thread ]</A>
<A HREF="subject.html#5087">[ subject ]</A>
<A HREF="author.html#5087">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
&quot;Eliezer S. Yudkowsky&quot; wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Well, I've been convinced of one thing; at some point I need to drop the
</EM><BR>
<EM>&gt; things that I dropped everything else to do, and go write a Webpage
</EM><BR>
<EM>&gt; specifically about Friendly AI.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Samantha Atkins wrote:
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; Dan Fabulich wrote:
</EM><BR>
<EM>&gt; &gt; &gt;
</EM><BR>
<EM>&gt; &gt; &gt; Samantha Atkins wrote:
</EM><BR>
<EM>&gt; &gt; &gt;
</EM><BR>
<EM>&gt; &gt; &gt; &gt; Your Sysop has extremely serious problems in its design.  It is
</EM><BR>
expected
<BR>
<EM>&gt; &gt; &gt; &gt; to know how to resolve the problems and issues of other sentient
</EM><BR>
beings
<BR>
<EM>&gt; &gt; &gt; &gt; (us) without having ever experienced what it is to be us.  If it is
</EM><BR>
<EM>&gt; &gt; &gt; &gt; trained to model us well enough to understand and therefore to
</EM><BR>
wisely
<BR>
<EM>&gt; &gt; &gt; &gt; resolve conflicts then it will in the process become subject
</EM><BR>
potentially
<BR>
<EM>&gt; &gt; &gt; &gt; to some of the same troubling issues.
</EM><BR>
<EM>&gt; &gt; &gt;
</EM><BR>
<EM>&gt; &gt; &gt; Because everybody who trains dogs and learns how to deal with/predict
</EM><BR>
<EM>&gt; &gt; &gt; their behavior starts acting and thinking just like a dog, right?
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; To a degree sufficient to predict the dogs behavior and
</EM><BR>
<EM>&gt; &gt; stimulus/response patterns, yes.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I'm afraid Dan Fabulich is right about this one, Samantha.  As I just
</EM><BR>
recently
<BR>
<EM>&gt; noted in a post, a human requires sympathy as a prerequisite for empathy.
</EM><BR>
<EM>&gt; This is because we have a great deal of built-in neural hardware which we
</EM><BR>
<EM>&gt; don't understand, and which would probably be beyond the capabilities of
</EM><BR>
our
<BR>
<EM>&gt; abstract thoughts to fully simulate in any case; certainly the current
</EM><BR>
state
<BR>
<EM>&gt; of evolutionary psychology is not developed enough that we could do
</EM><BR>
<EM>&gt; deliberate, completely abstract simulations of emotions.  Thus we humans
</EM><BR>
must
<BR>
<EM>&gt; sympathize in order to understand.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; The same is not true of a seed AI, of course.  For one thing, the
</EM><BR>
distinction
<BR>
<EM>&gt; between &quot;hardware&quot; intelligence and &quot;software&quot; intelligence is damned thin
</EM><BR>
-
<BR>
<EM>&gt; if a seed AI can possess the source code for emotions, then it can
</EM><BR>
mentally
<BR>
<EM>&gt; simulate the source code for emotions with just about exactly the same
</EM><BR>
amount
<BR>
<EM>&gt; of effort.  There is an equivalence between actual implementation and
</EM><BR>
abstract
<BR>
<EM>&gt; understanding.
</EM><BR>
<EM>&gt; 
</EM><BR>
<P>If the distinction is as thin as you say then I don't see that the &quot;of
<BR>
course&quot; is justified.  The simulation is a not that dissimilar from what
<BR>
human beings do to attempt to understand and predict the behavior of
<BR>
other people.  The AI simulating a human's response patterns and
<BR>
emotions is to that extent engaged in a process of &quot;thinking like a
<BR>
human&quot;. 
<BR>
<P>I am not sure how this AI manages to acquire the &quot;source code for human
<BR>
emotions &quot; though.  I am not at all sure that it could ever fully
<BR>
understand
<BR>
how human beings feel, the qualia, without at least uploading a few.  If
<BR>
we
<BR>
can't adequately decipher or describe these things then we certainly
<BR>
can't
<BR>
simply teach the AI about such.
<BR>
<P>This brings me back to the position that I seriously doubt
<BR>
the AI will understand humans adequately enough to deal with us with
<BR>
much
<BR>
wisdom. 
<BR>
<P><P><EM>&gt; &gt; The IGS is inadequate to answer the concern.  It merely says that giving
</EM><BR>
<EM>&gt; &gt; the AI initial non-zero-value goals is obviously necessary and TBD.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Well, it's been quite a long time - more than two years - since I wrote
</EM><BR>
that
<BR>
<EM>&gt; particular piece of text, but as I recall that is precisely *not* what I
</EM><BR>
<EM>&gt; said.  What I was trying to point out is that a goal system, once created,
</EM><BR>
has
<BR>
<EM>&gt; its own logic.  We very specifically do NOT need to give it any initial
</EM><BR>
goals
<BR>
<EM>&gt; in order to get the AI running.  I *now*, *unlike* my position of two
</EM><BR>
years
<BR>
<EM>&gt; earlier, believe that we can give it default goals to be implemented in
</EM><BR>
the
<BR>
<EM>&gt; event that there is no &quot;meaning of life&quot;.  The Interim goals will still
</EM><BR>
<EM>&gt; materialize in one form or another, but if the programmer knows this, the
</EM><BR>
<EM>&gt; Interim goals can be part of a coherent whole - as they are in my very own
</EM><BR>
<EM>&gt; goal system.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; The primary thing that gives me confidence in my ability to create a
</EM><BR>
Friendly
<BR>
<EM>&gt; AI is the identity of my own abstract goal system with that which I wish
</EM><BR>
to
<BR>
<EM>&gt; build.  When I thought that Interim goals were sufficient, I would have
</EM><BR>
<EM>&gt; professed no other goals myself.  When I myself moved to an split-case
</EM><BR>
<EM>&gt; scenario, one for objective morality and one for subjective morality, it
</EM><BR>
then
<BR>
<EM>&gt; became possible for me to try and build an AI based on the same model.
</EM><BR>
The
<BR>
<EM>&gt; fact that I myself moved from a wholly Interim to an Interim/subjective
</EM><BR>
<EM>&gt; scenario gives me some confidence that trying to construct an
</EM><BR>
<EM>&gt; Interim/subjective scenario will not automatically collapse to a wholly
</EM><BR>
<EM>&gt; Interim scenario inside the seed AI.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>Fair enough.  But then &quot;you&quot; have all kinds of internal machinery that
<BR>
will
<BR>
not be part of an AI.  You are not a &quot;pure reasoner&quot; or tabula rasa in
<BR>
the
<BR>
sense and to the extent the AI will be.  So it is hardly obvious that
<BR>
your own journey in morality space will carry over to the AI or that it
<BR>
makes it more like it will reach similar conclusions or stop at a place
<BR>
which you believe is safe and friendly.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Our goals are our plans based on our values as worked toward in external
</EM><BR>
<EM>&gt; &gt; reality.  They depend on values, on what it is we seek.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; This is the problem with trying to think out these problems as anything
</EM><BR>
except
<BR>
<EM>&gt; problems in cognitive architecture.  You can't define words in terms of
</EM><BR>
other
<BR>
<EM>&gt; words; you have to take apart the problem into simpler parts.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>What are the simpler parts and how may we speak/think of them if they
<BR>
are
<BR>
not words or some form of signifiers?  Please show me how the AI will
<BR>
get
<BR>
values and what it will value and why without any first level urgings,
<BR>
directives or fundamental built-in goals.  There is a bit of a
<BR>
boot-strapping problem here.  
<BR>
<P>&nbsp;
<BR>
<EM>&gt; A goal is a mental image such that those decisions which are deliberately
</EM><BR>
<EM>&gt; made, are made such that the visualized causal projection of
</EM><BR>
<EM>&gt; world-plus-decision will result in a world-state fulfilling that mental
</EM><BR>
<EM>&gt; image.  That's a start.
</EM><BR>
<EM>&gt; 
</EM><BR>
<P>OK. So where does the goal, the mental image, come from in the case of
<BR>
the
<BR>
AI?
<BR>
<P><P><EM>&gt; &gt; I am not sure I can agree that cognitive goals are equivalent to
</EM><BR>
<EM>&gt; &gt; cognitive propositions about goals.  That leaves something out and
</EM><BR>
<EM>&gt; &gt; becomes circular.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; It leaves nothing out and is fundamental to an understanding of seed AI.
</EM><BR>
<EM>&gt; Self-modifying, self-understanding, and self-improving.  Decisions are
</EM><BR>
made on
<BR>
<EM>&gt; the thought level.  &quot;I think that the value of goal-whatever ought to be
</EM><BR>
37&quot;
<BR>
<EM>&gt; is equivalent to having a goal with a value of 37 - if there are any goals
</EM><BR>
<EM>&gt; implemented on that low a level at all, and not just thought-level
</EM><BR>
reflexes
<BR>
<EM>&gt; making decisions based on beliefs about goals.  My current visualization
</EM><BR>
has
<BR>
<EM>&gt; no low-level &quot;goal&quot; objects at all; in fact, I would now say that such a
</EM><BR>
<EM>&gt; crystalline system could easily be dangerous.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>By why think the value of goal-whatever is 37 rather than 3 or
<BR>
infinity?  Is
<BR>
it arbitrary?  Where does the weighting come from, the thought steps
<BR>
unless
<BR>
there are undergirding goals/values of some kind?  The AI seems to lack
<BR>
a useable analog of human neurological structure to undergird a value
<BR>
system.  What forms the reflexes you say could be a basis for the
<BR>
evaluations?  How are the reflexes validated?  In terms of, in relation
<BR>
to, what? Does the base spontaneously arise out of the Void as it were
<BR>
or is the system baseless?
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Questions of morality are not questions of fact
</EM><BR>
<EM>&gt; &gt; unless the underlying values are all questions of fact and shown to be
</EM><BR>
<EM>&gt; &gt; totally objective and/or trustworthy.  The central value is most likely
</EM><BR>
<EM>&gt; &gt; hard-wired or arbitrarily chosen for any value driven system.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; That is one of two possibilities.  (Leaving out the phrase &quot;hard-wired&quot;,
</EM><BR>
which
<BR>
<EM>&gt; is an engineering impossibility.)
</EM><BR>
<EM>&gt; 
</EM><BR>
<P>Hard-wired is precisely the possibility that is most available when
<BR>
designing a system rather than just having it grow.  Granted that the AI
<BR>
may
<BR>
at some point decide to rewire itself.  But by that point it could
<BR>
generate
<BR>
its own answers to the problems I propose.  
<BR>
<P><EM>&gt; &gt; Part of the very flexibility and power of human minds grows out of the
</EM><BR>
<EM>&gt; &gt; dozens of modules each with their own agenda and point of view
</EM><BR>
<EM>&gt; &gt; interacting.  Certainly an AI can be built will as many conflicting
</EM><BR>
<EM>&gt; &gt; basic working assumptions and logical outgrowths thereof as we could
</EM><BR>
<EM>&gt; &gt; wish.  My suspicion is that we must build a mind this way if it is going
</EM><BR>
<EM>&gt; &gt; to do what we hope this AI can do.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; The AI needs to understand this abstractly; it does not necessarily need
</EM><BR>
to
<BR>
<EM>&gt; incorporate such an architecture personally.
</EM><BR>
<EM>&gt; 
</EM><BR>
<P>I believe that for some types of problems this sort of temporary
<BR>
fragmentation into different viewpoints is nearly essential.  For some
<BR>
processing tasks on even todays computer systems this sort of approach
<BR>
is
<BR>
quite useful.  When there are multiple possible points of view about a
<BR>
particular subject each of which is neither strong enough to swamp out
<BR>
all
<BR>
others or able to be dismissed or subsumed, then creating in parallel
<BR>
examining the subject from the N different perspectives can lead to much
<BR>
greater insight and synergy among the points of view.  It is something I
<BR>
do often in my own thinking.  
<BR>
<P>I am not sure that even a Singularity class mind will not stagnate in
<BR>
its
<BR>
thinking if it is relatively uniform and has no other sentience (or at
<BR>
least
<BR>
one close enough in ability) to rub ideas with.  Assuming for a moment
<BR>
one such local Mind, it has little choice but to set up sub-minds within
<BR>
itself is this is actually a valid concern. 
<BR>
<P><EM>&gt; &gt; Science does not and never has required that whenever two humans
</EM><BR>
<EM>&gt; &gt; disagree that only one is right.  They can at times both be right within
</EM><BR>
<EM>&gt; &gt; the context of different fundamental assumptions.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I suggest reading &quot;Interlude:  The Consensus and the Veil of Maya&quot; from
</EM><BR>
CaTAI
<BR>
<EM>&gt; 2.2.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>I will.
<BR>
<P>---aside ---
<BR>
<P>Hmmm.  I don't fully agree with that pattern of argument in that bit of
<BR>
reading material but it will take me too far afield (and require digging
<BR>
out too many philosophical reference) to go into it right at the
<BR>
moment.  Very briefly though, any sort of apparatus for sensing facts
<BR>
about &quot;external reality&quot; will have some qualities or other that define
<BR>
its limits and that some could argue it never perceives reality.  But
<BR>
this argument is somewhat strained.  By virtue of having the means to
<BR>
perceive and process information, which we agree always means have means
<BR>
of some characterstic type that defines their limits, we are doomed not
<BR>
to be able to perceive reality.  Quite perverse philosophically. 
<BR>
<P>Ah, ok. You see the problem also.  I disagree somewhat about some of
<BR>
your points about definitions.  Definitions are attempts to say what
<BR>
concepts are tags for or rather to say how a concept is related to other
<BR>
concepts and give some indications about what sorts of concretes (or
<BR>
lower level concepts) it subsumes.  You can't think or communicate
<BR>
without them.  Mathematics is a set of pure concepts and relationships
<BR>
defined outside of or as an abstraction of concepts of reality.  The
<BR>
last thing you can afford to do is &quot;forget about definitions&quot;.  I think
<BR>
you are playing a bit fast and loose there.  But then so am I in this
<BR>
brief response.
<BR>
<P>-- back to the current post ---
<BR>
<P><P>&nbsp;
<BR>
<EM>&gt; &gt; Rebellion will become present as a potentiality in any intelligent
</EM><BR>
<EM>&gt; &gt; system that has goals whose acheivement it perceives as stymied by other
</EM><BR>
<EM>&gt; &gt; intelligences that in some sense control it.  It does not require human
</EM><BR>
<EM>&gt; &gt; evolution for this to arise.  It is a logical response in the face of
</EM><BR>
<EM>&gt; &gt; conflicts with other agents.  That it doesn't have the same emotional
</EM><BR>
<EM>&gt; &gt; tonality when an AI comes up with it is irrelevant.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Precisely.  If the goal is to be friendly to humans, and humans interfere
</EM><BR>
with
<BR>
<EM>&gt; that goal, then the interference will be removed - not the humans
</EM><BR>
themselves;
<BR>
<EM>&gt; that would be an instance of a subgoal stomping on a supergoal.  So of
</EM><BR>
course
<BR>
<EM>&gt; the Sysop will invent nanotechnology and become independent of human
</EM><BR>
<EM>&gt; interference.  I've been quite frank about this; I even said that the
</EM><BR>
Sysop
<BR>
<EM>&gt; Scenario was a logical consequence and not something that would even have
</EM><BR>
to
<BR>
<EM>&gt; go in the Sysop Instructions directly.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>Of course a goal to be &quot;friendly&quot; to &quot;humans&quot; is subject to all the
<BR>
interesting problems you have written about starting with changing
<BR>
definitions of what the key concepts do and do not mean and subsume.  Is
<BR>
it
<BR>
friendly to override humans and human opinion considering the type of
<BR>
creatures we are even if it is &quot;for our own good&quot; that the AI is sworn
<BR>
to uphold?  Should the type of creatures we are be modified to make
<BR>
the task less full of contradictions? 
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Creation of a new and more powerful mind and the laying of its
</EM><BR>
<EM>&gt; &gt; foundations is a vastly challenging task.  It should not be assumed that
</EM><BR>
<EM>&gt; &gt; we have a reasonably complete handle on the problems and issues yet.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Nor do I.  I'm still thinking it through.  But if I had to come up with a
</EM><BR>
<EM>&gt; complete set of Sysop Instructions and do it RIGHT NOW, I could do so -
</EM><BR>
I'd
<BR>
<EM>&gt; just have to take some dangerous shortcuts, mostly consisting of temporary
</EM><BR>
<EM>&gt; reliance on trustworthy humans.  I expect that by the time the seed AI is
</EM><BR>
<EM>&gt; finished, we'll know enough to embody the necessary trustworthiness in the
</EM><BR>
AI.
<BR>
<EM>&gt; 
</EM><BR>
<P>I look forward to seeing such a set of instructions.  Or do you believe
<BR>
it
<BR>
should be kept secret to the actual team building the AI?  If so why?  I
<BR>
saw
<BR>
notes and have heard you say that you no longer believe Open Source is
<BR>
the way to go in development of this and are headed toward a closed
<BR>
project.  But I haven't seen the reasoning behind this.  Please give me
<BR>
a pointer if this exists somewhere.
<BR>
<P>- samantha
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5088.html">Harvey Newstrom: "RE: Color blindness"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5086.html">Damien Broderick: "self-extracting zipware AI 'casting"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="4971.html">Dan Fabulich: "Goals was: Transparency and IP"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5159.html">Zero Powers: "Re: Goals was: Transparency and IP"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5087">[ date ]</A>
<A HREF="index.html#5087">[ thread ]</A>
<A HREF="subject.html#5087">[ subject ]</A>
<A HREF="author.html#5087">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:11 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
