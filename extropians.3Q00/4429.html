<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="J. R. Molloy (jr@shasta.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Wed Sep  6 17:34:45 2000" -->
<!-- isoreceived="20000906233445" -->
<!-- sent="Wed, 6 Sep 2000 16:36:21 -0700" -->
<!-- isosent="20000906233621" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="009301c0185b$5ae19e40$8fbc473f@jrmolloy" -->
<!-- inreplyto="14773.65204.352861.599680@lrz.uni-muenchen.de" -->
<STRONG>From:</STRONG> J. R. Molloy (<A HREF="mailto:jr@shasta.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;009301c0185b$5ae19e40$8fbc473f@jrmolloy&gt;"><EM>jr@shasta.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Wed Sep 06 2000 - 17:36:21 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4430.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4428.html">Franklin Wayne Poley: "AI-State-Of-The-Art"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4346.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="4480.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4429">[ date ]</A>
<A HREF="index.html#4429">[ thread ]</A>
<A HREF="subject.html#4429">[ subject ]</A>
<A HREF="author.html#4429">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Eugene Leitl
<BR>
<EM>&gt; So you've got a billion AIs gyring and gimbling in their wabe, and how
</EM><BR>
<EM>&gt; exactly do you intend to supervise them, and guarantee that their
</EM><BR>
<EM>&gt; behaviour will be certifyable not hostile in all possible situations?
</EM><BR>
<P>All the AIs don't have to run factories and do the markets. Sacrifice a few
<BR>
million of them to supervise the others, i.e., give some of them the job of
<BR>
police, some psychologists, a few executioners, [insert your favourite
<BR>
enforcement agent here] so that these AIs can monitor the entire population.
<BR>
Only the ones who perform above average as supervisors get to reproduce. The
<BR>
very best line of supervisors get appointed as governors, executive officers,
<BR>
and overseers of the AI meritocracy.
<BR>
<P><EM>&gt; You never got bitten by a dog? A maltreated dog? A crazied, sick dog?
</EM><BR>
<EM>&gt; Never, ever?
</EM><BR>
<P>I've never been bitten by a dog, but I've heard about other people getting
<BR>
bitten. Mailmen get bitten from time to time, but it's almost never, ever fatal.
<BR>
Dog-level AI wouldn't be all that dangerous. Whether it takes five years to go
<BR>
from dog-level AI to human-level AI or 500 million years to go from dog-level
<BR>
natural intelligence to human level natural intelligence, along the way to the
<BR>
higher levels of intelligence, evolution provides for behavior modification
<BR>
which can accommodate the higher levels of intelligence. (Although one might
<BR>
doubt this when considering the 5, 000 wars humans have fought in the last 3,000
<BR>
years.)
<BR>
Speaking of wars, the first fully functional dog-level robots will probably be
<BR>
dogs of war.
<BR>
<P><EM>&gt; How strange, I thought it was the other way round. Statistical
</EM><BR>
<EM>&gt; properties, emergent properties, all things which you don't have to
</EM><BR>
<EM>&gt; deal with when all you have is a single specimen.
</EM><BR>
<P>I think it would be as foolish to try to bring a single specimen up to
<BR>
human-level AI as it would be to use all of a nation's resources to educate just
<BR>
one human. Equal access to education makes a population more stable and more
<BR>
efficient and more successful. When you only educate the prince(s), then the
<BR>
king can be threatened (as many have been in monarchical societies). There's
<BR>
safety in numbers.
<BR>
<P><EM>&gt; Of course, there is no such thing as a single specimen, unless the
</EM><BR>
<EM>&gt; thing instantly falls into a positive autofeedback loop -- same thing
</EM><BR>
<EM>&gt; happened when the first autocatalytic set nucleated in the prebiotic
</EM><BR>
<EM>&gt; ursoup. But then you're dealing with a jack-in-the-box SI, and all is
</EM><BR>
<EM>&gt; moot, anyway.
</EM><BR>
<P>If all of the millions of dog-level AIs fall into positive autofeedback loops,
<BR>
(don't forget, some are guard dogs, some are drug sniffing dogs, some are
<BR>
seeing-eye dogs for the blind, some are sheep dogs, none are merely pets) that
<BR>
would be wonderfully beneficial because it would make it much easier to train
<BR>
them. Nobody wants a really stupid dog. The most intelligent animals (including
<BR>
humans) are generally and usually the most cooperative, and easy to train. The
<BR>
dangerous criminal element is almost always average or below average in
<BR>
intelligence. If the dog-level AI instantly falls into a positive autofeedback
<BR>
loop (about as likely as a human falling into a positive bio-feedback loop and
<BR>
becoming a super genius), you simply ask your family guard dog (now an SI) to
<BR>
continue its line of duty with hugely expanded responsibilities and obligations
<BR>
to protect the family against hackers, attackers, and intruders of every kind.
<BR>
The first thing your faithful SI pooch does might surprise you with its
<BR>
brilliance. If you have money in the stock market, the SI would instantly check
<BR>
to see if your SI stock broker's dog has acted to safeguard your investments
<BR>
from any rogue market manipulators. The beautiful part is you'd not have to roll
<BR>
up a newspaper to chastise the SI doggie, because it would have more reason to
<BR>
offer behavior modification to *you*. What used to be your canine security guard
<BR>
has transformed into your own personal genie.
<BR>
If you don't trust high IQ, then you'd better kick Eliezer off this list before
<BR>
he scuttles the enterprise.
<BR>
[Don't forget dog-level AI spelled backwards is IA level-god.]
<BR>
<P><EM>&gt; Yeah, *right*.
</EM><BR>
<P>Stupidity, rather than too much intelligence, is the cause of most of the
<BR>
world's problems. Just between us, friends, I'd place a hell of a lot more trust
<BR>
in an SI than in any of the current candidates for political office. It seems to
<BR>
me that fear of malevolent SIs is misplaced. Having grown up (instantly) in a
<BR>
free market environment, the first thing the SIs would have to do is make some
<BR>
money. The SIs would have to curry favor with the right people (and more
<BR>
importantly with the right SIs) to acquire power. With a sufficiently high
<BR>
number of SIs, they'd instantly figure out how best to create a stable and
<BR>
prosperous SI community.
<BR>
<P><EM>&gt; It is exactly prevalence of such profoundly unreasonable, utterly
</EM><BR>
<EM>&gt; devoid from basic common sense expectations that makes me consider
</EM><BR>
<EM>&gt; breeding an AI a highly dangerous business. Because there is such a
</EM><BR>
<EM>&gt; demand for the things, and because so many teams are working on them,
</EM><BR>
<EM>&gt; someone will finally succeed. What will happen then is essentially
</EM><BR>
<EM>&gt; unknowable, and most likely irreversible, and that's why we should be
</EM><BR>
<EM>&gt; very very careful about when, how and the context we do it in.
</EM><BR>
<P>Life is profoundly unreasonable, utterly devoid of basic common sense
<BR>
expectations. Life doesn't want to breed an AI. Life wants to breed millions of
<BR>
them simultaneously and instantly.
<BR>
Life is really weird that way. For example, Sasha Chislenko had a much better
<BR>
life than I have. Yet he apparently killed himself, and I'm still plugging along
<BR>
optimistically hoping for things to get better. [perhaps he found life too time
<BR>
consuming] Now, if a smart and savvy guy like him decides to chuck it in, while
<BR>
underachieving jerks like me keep merrily cruisin' along, hey! maybe I'm missing
<BR>
something; perhaps he knew something (or his depression revealed something to
<BR>
him) that I don't know. Maybe I should join the billions of grateful dead folks.
<BR>
Anyway, we all have to die sometime. Why not die bringing a higher life form
<BR>
into existence? Look how many fools throw their lives away on idiotic notions
<BR>
like patriotism and honor. I think Morovec was brilliant when he said (in Palo
<BR>
Alto) that the most responsible thing we can do is to build artificial life,
<BR>
especially if it surpasses us in cognitive capability. The riskiest thing you
<BR>
can do is to not take any risks. The best insurance against some mad scientist
<BR>
building an AI is to have thousands of responsible and reasonable scientists
<BR>
building millions of AIs.
<BR>
Lots of people didn't want humans to build nuclear weapons. Too dangerous. But
<BR>
if it weren't for nuclear weapons, I'd likely have died in my twenties fighting
<BR>
a war with the USSR or some other maniacal regime. Nuclear weapons have been an
<BR>
effective deterrent to another world war. Multitudes of SIs will be perhaps the
<BR>
only deterrent to fanatic religious retards (in the Middle East and dozens of
<BR>
other hot spots) dragging the world into another global conflagration.
<BR>
<P><EM>&gt; Socialize a god.
</EM><BR>
<P>I have a better idea: Let a million gods socialize us.
<BR>
<P>--J. R.
<BR>
<P><P><P><P>&quot;He that will not sail till all dangers are over must never put
<BR>
to sea. &quot;     --Thomas Fuller
<BR>
<P><P>&quot;Every act of creation is first of all an act of destruction.&quot;
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--Pablo Picasso
<BR>
<P><P>&quot;Begin at the beginning and go on till you come to the end; then stop.&quot;
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--Lewis Carrol, from Alice in Wonderland
<BR>
<P><P>&quot;Do not be too timid and squeamish about your actions.  All life
<BR>
is an experiment.&quot; -- Ralph Waldo Emerson
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4430.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4428.html">Franklin Wayne Poley: "AI-State-Of-The-Art"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4346.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="4480.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4429">[ date ]</A>
<A HREF="index.html#4429">[ thread ]</A>
<A HREF="subject.html#4429">[ subject ]</A>
<A HREF="author.html#4429">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:37:23 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
