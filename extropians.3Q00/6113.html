<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="J. R. Molloy (jr@shasta.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Fri Sep 29 15:59:41 2000" -->
<!-- isoreceived="20000929215941" -->
<!-- sent="Fri, 29 Sep 2000 15:00:12 -0700" -->
<!-- isosent="20000929220012" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="004301c02a60$b04df560$56bc473f@jrmolloy" -->
<!-- inreplyto="14804.18583.690077.538245@lrz.uni-muenchen.de" -->
<STRONG>From:</STRONG> J. R. Molloy (<A HREF="mailto:jr@shasta.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;004301c02a60$b04df560$56bc473f@jrmolloy&gt;"><EM>jr@shasta.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Fri Sep 29 2000 - 16:00:12 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="6114.html">Dehede011@aol.com: "Re: Capitalists and concentration camps"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6112.html">Spudboy100@aol.com: "Re: Capitalists and concentration camps"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6046.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6113">[ date ]</A>
<A HREF="index.html#6113">[ thread ]</A>
<A HREF="subject.html#6113">[ subject ]</A>
<A HREF="author.html#6113">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Eugene Leitl writes,
<BR>
<P><EM>&gt; Now that is pretty harsh. Particularly considering the fact that
</EM><BR>
<EM>&gt; everybody capable of thought is immured in a belief system. Say, how
</EM><BR>
<EM>&gt; about a nice cask of amontillado?
</EM><BR>
<P>I don't believe that everybody capable of thought is immured in a belief system.
<BR>
Perhaps you haven't discovered thoughtful people who have freed themselves from
<BR>
belief, but that doesn't mean they don't exist. One example that I know is Jiddu
<BR>
Krishnamurti. He believed (in the sense that I have used the word) nothing at
<BR>
all.
<BR>
<A HREF="http://www.kfa.org">http://www.kfa.org</A>
<BR>
<P><EM>&gt; Is that your understanding of humanity? I was actually thinking about
</EM><BR>
<EM>&gt; an inoculated Petri dish when I was writing this. The lowest common
</EM><BR>
<EM>&gt; denominator of life: make mutated copies of yourself.
</EM><BR>
<P>Then we can at least assign this as an attribute of AI without
<BR>
anthropomorphizing?
<BR>
Could one say, &quot;If I were in the AIs' shoes, I'd reproduce myself like crazy.&quot;?
<BR>
<P>&lt;I concur with what you write about reproduction&gt;
<BR>
<P><EM>&gt; As to common sense, I presume this means street smarts. Ability to
</EM><BR>
<EM>&gt; make the right decisions rapidly in face of incomplete and/or
</EM><BR>
<EM>&gt; conflicting data. Darwinian systems are known to be able to handle
</EM><BR>
<EM>&gt; that very nicely, why, they've grown up in the street.
</EM><BR>
<P>I think Darwinian systems would do even better among machines.
<BR>
<P><EM>&gt; The notion that AIs are going to be crystal clear citadels of pure
</EM><BR>
<EM>&gt; thought must appear ludicrous. Because that notion does not make any
</EM><BR>
<EM>&gt; sense in an evolutionary theatre.
</EM><BR>
<P>Right. Evolution is not always rational. Sometimes it's more intelligent to be
<BR>
a-rational (that is, supersede rationalism altogether).
<BR>
<P><EM>&gt;  &gt; &gt; Because the copy process is much faster
</EM><BR>
<EM>&gt;  &gt; &gt; than adding new nodes (even if you have nanotechnology) you have
</EM><BR>
<EM>&gt;  &gt; &gt; instant resource scarcity and hence competition for limited
</EM><BR>
<EM>&gt;  &gt; &gt; resources. Those individua with lesser fitness will have to go to the
</EM><BR>
<EM>&gt;  &gt; &gt; great bit bucket in the sky.
</EM><BR>
<EM>&gt;  &gt;
</EM><BR>
<EM>&gt;  &gt; So AI individua will be *very* friendly toward each other. The question
</EM><BR>
then is,
<BR>
<EM>&gt;
</EM><BR>
<EM>&gt; Huh? Your logic seems to be working on very different principles from
</EM><BR>
<EM>&gt; mine. I just told you that AIs will have to compete and die just as we
</EM><BR>
<EM>&gt; do, and you say &quot;they will be very friendly to each other&quot;. Remind me
</EM><BR>
<EM>&gt; to never become your friend, will you?
</EM><BR>
<P>I was looking at AI reproduction in terms of sexual reproduction. AIs could have
<BR>
sex couldn't they? Sure they could and would compete. They'd compete for mates,
<BR>
just as we do. So, just like us, they'd get *very* friendly as sexual partners,
<BR>
while remaining competitive at large. So, in the sense of becoming friends to
<BR>
reproduce... well, I probably don't need to remind you never to get friendly
<BR>
that way.
<BR>
<P><EM>&gt;  &gt; &quot;How far would AI extend its friendliness? Would it extend to you and me?&quot;
</EM><BR>
<EM>&gt;  &gt; Perhaps it would. The friendliness of religious fanatics definately does
</EM><BR>
not.
<BR>
<EM>&gt;
</EM><BR>
<EM>&gt; I wonder where you went now, I wish I could follow.
</EM><BR>
<P>No you don't. You only want to pretend that you don't understand that AI would
<BR>
have as much right as you or I to choose its friends. Religious fanatics are
<BR>
defective. Learn it, love it, live it.
<BR>
<P><EM>&gt;  &gt; Yes, we don't ever want to appear unfriendly to something more intelligent
</EM><BR>
than
<BR>
<EM>&gt;  &gt; ourselves. But why does friendliness come into it at all? I mean, have you
</EM><BR>
ever
<BR>
<EM>&gt;
</EM><BR>
<EM>&gt; We don't want to appear unfriendly to something powerful, yes. Because
</EM><BR>
<EM>&gt; then it will feel compelled to come and kick our butts. Perhaps it
</EM><BR>
<EM>&gt; won't do that if we just lie low.
</EM><BR>
<P>Fat chance. An actual  &gt;H AI (which would of course soon become even more &gt;H)
<BR>
would seek and destroy the lowest lying perps first.
<BR>
<P><EM>&gt;  &gt; thought that truth may have value greater than friendship? If our friends
</EM><BR>
all
<BR>
<EM>&gt;
</EM><BR>
<EM>&gt; Value in which particular value system?
</EM><BR>
<P>Value, n. The desirability or worth of a thing; intrinsic worth; utility.
<BR>
<P><EM>&gt;  &gt; become deranged as a result of some weird virus that makes them politicized
</EM><BR>
<EM>&gt;  &gt; zombies, perhaps we ought to place our trust in some artificial
</EM><BR>
intelligence
<BR>
<EM>&gt;
</EM><BR>
<EM>&gt; It is impossible to achieve quantitative infectivity on a genetically
</EM><BR>
<EM>&gt; diverse populace with a given pathogen without full knowledge about
</EM><BR>
<EM>&gt; the diff list.
</EM><BR>
<P>We can't know that it is impossible to achieve quantitative infectivity on a
<BR>
genetically diverse populace with a given pathogen without full knowledge about
<BR>
the diff list until it has been tried experimentally.
<BR>
<P><EM>&gt;  &gt; which remains impervious to such an attack, some AI which remains sane and
</EM><BR>
<EM>&gt;  &gt; balanced. Shall we trust the natural intelligence of Hitler and Stalin more
</EM><BR>
than
<BR>
<EM>&gt;
</EM><BR>
<EM>&gt; What makes you think an AI will remain sane and balanced? Clearly it can't.
</EM><BR>
<P>&quot;Clearly&quot;? This is a sane and balanced analysis? Hardly.
<BR>
Sane and balanced humans know that the majority of humans have displayed neither
<BR>
sanity nor balance in the last five thousand years of history.
<BR>
<P><EM>&gt;  &gt; the robot intelligence of our own device?
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt; I don't know what you're smoking, but I wish I had some of it, it
</EM><BR>
<EM>&gt; seems to be powerful stuff.
</EM><BR>
<P>Indeed, you wish, but &quot;if wishes were horses, beggars would ride.&quot; You obviously
<BR>
need some powerful psychotropic chemicals to jolt you out of your neurosis.
<BR>
<P><EM>&gt; 1) due to nature of these technologies sustainable relinquishment
</EM><BR>
<EM>&gt;    doesn't work, and some of the countermeasures make the original
</EM><BR>
<EM>&gt;    problem set pale in comparison
</EM><BR>
<P>You're definitely right about that. &quot;Relinquishment&quot; is a feeble attempt to make
<BR>
one's security blanket cover one's present anxieties.
<BR>
<P><EM>&gt; 2) these technologies are necessary to move on to the next levels of
</EM><BR>
<EM>&gt;    sophistication
</EM><BR>
<P>Perhaps these technologies are necessary to move humanity past its age old
<BR>
infantile predilection for mystery and myth. God is dead, supplanted by AI.
<BR>
<P><EM>&gt; 3) if don't do that, we're screwed on the long run, anyway
</EM><BR>
<P>We're screwed in the long run no matter what.
<BR>
---------------------
<BR>
&quot;A slipping gear could let your M203 grenade launcher fire when you
<BR>
least expect it.  That would make you quite unpopular in what's left
<BR>
of your unit.&quot; -- In the August 1993 issue, page 9, of PS magazine,
<BR>
the Army's magazine of preventive maintenance
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="6114.html">Dehede011@aol.com: "Re: Capitalists and concentration camps"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6112.html">Spudboy100@aol.com: "Re: Capitalists and concentration camps"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6046.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6113">[ date ]</A>
<A HREF="index.html#6113">[ thread ]</A>
<A HREF="subject.html#6113">[ subject ]</A>
<A HREF="author.html#6113">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:27 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
