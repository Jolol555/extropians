<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Goals was: Transparency and IP</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: Goals was: Transparency and IP">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Goals was: Transparency and IP</H1>
<!-- received="Thu Sep 14 16:49:07 2000" -->
<!-- isoreceived="20000914224907" -->
<!-- sent="Thu, 14 Sep 2000 18:31:19 -0400" -->
<!-- isosent="20000914223119" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Goals was: Transparency and IP" -->
<!-- id="39C151B7.11A04EE4@pobox.com" -->
<!-- inreplyto="39C13CF7.627BAF51@objectent.com" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Goals%20was:%20Transparency%20and%20IP&In-Reply-To=&lt;39C151B7.11A04EE4@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Thu Sep 14 2000 - 16:31:19 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5000.html">Eliezer S. Yudkowsky: "Re: Goals was: Transparency and IP"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4998.html">hal@finney.org: "Re: abundance (Was: Re: Transparency and IP)"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4993.html">Samantha Atkins: "Re: Goals was: Transparency and IP"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5008.html">Gina Miller: "Re: Goals was: Transparency and IP"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5008.html">Gina Miller: "Re: Goals was: Transparency and IP"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4999">[ date ]</A>
<A HREF="index.html#4999">[ thread ]</A>
<A HREF="subject.html#4999">[ subject ]</A>
<A HREF="author.html#4999">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Well, I've been convinced of one thing; at some point I need to drop the
<BR>
things that I dropped everything else to do, and go write a Webpage
<BR>
specifically about Friendly AI.
<BR>
<P>Samantha Atkins wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Dan Fabulich wrote:
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; Samantha Atkins wrote:
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; &gt; Your Sysop has extremely serious problems in its design.  It is expected
</EM><BR>
<EM>&gt; &gt; &gt; to know how to resolve the problems and issues of other sentient beings
</EM><BR>
<EM>&gt; &gt; &gt; (us) without having ever experienced what it is to be us.  If it is
</EM><BR>
<EM>&gt; &gt; &gt; trained to model us well enough to understand and therefore to wisely
</EM><BR>
<EM>&gt; &gt; &gt; resolve conflicts then it will in the process become subject potentially
</EM><BR>
<EM>&gt; &gt; &gt; to some of the same troubling issues.
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; Because everybody who trains dogs and learns how to deal with/predict
</EM><BR>
<EM>&gt; &gt; their behavior starts acting and thinking just like a dog, right?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; To a degree sufficient to predict the dogs behavior and
</EM><BR>
<EM>&gt; stimulus/response patterns, yes.
</EM><BR>
<P>I'm afraid Dan Fabulich is right about this one, Samantha.  As I just recently
<BR>
noted in a post, a human requires sympathy as a prerequisite for empathy. 
<BR>
This is because we have a great deal of built-in neural hardware which we
<BR>
don't understand, and which would probably be beyond the capabilities of our
<BR>
abstract thoughts to fully simulate in any case; certainly the current state
<BR>
of evolutionary psychology is not developed enough that we could do
<BR>
deliberate, completely abstract simulations of emotions.  Thus we humans must
<BR>
sympathize in order to understand.
<BR>
<P>The same is not true of a seed AI, of course.  For one thing, the distinction
<BR>
between &quot;hardware&quot; intelligence and &quot;software&quot; intelligence is damned thin -
<BR>
if a seed AI can possess the source code for emotions, then it can mentally
<BR>
simulate the source code for emotions with just about exactly the same amount
<BR>
of effort.  There is an equivalence between actual implementation and abstract
<BR>
understanding.
<BR>
<P><EM>&gt; The IGS is inadequate to answer the concern.  It merely says that giving
</EM><BR>
<EM>&gt; the AI initial non-zero-value goals is obviously necessary and TBD.
</EM><BR>
<P>Well, it's been quite a long time - more than two years - since I wrote that
<BR>
particular piece of text, but as I recall that is precisely *not* what I
<BR>
said.  What I was trying to point out is that a goal system, once created, has
<BR>
its own logic.  We very specifically do NOT need to give it any initial goals
<BR>
in order to get the AI running.  I *now*, *unlike* my position of two years
<BR>
earlier, believe that we can give it default goals to be implemented in the
<BR>
event that there is no &quot;meaning of life&quot;.  The Interim goals will still
<BR>
materialize in one form or another, but if the programmer knows this, the
<BR>
Interim goals can be part of a coherent whole - as they are in my very own
<BR>
goal system.
<BR>
<P>The primary thing that gives me confidence in my ability to create a Friendly
<BR>
AI is the identity of my own abstract goal system with that which I wish to
<BR>
build.  When I thought that Interim goals were sufficient, I would have
<BR>
professed no other goals myself.  When I myself moved to an split-case
<BR>
scenario, one for objective morality and one for subjective morality, it then
<BR>
became possible for me to try and build an AI based on the same model.  The
<BR>
fact that I myself moved from a wholly Interim to an Interim/subjective
<BR>
scenario gives me some confidence that trying to construct an
<BR>
Interim/subjective scenario will not automatically collapse to a wholly
<BR>
Interim scenario inside the seed AI.
<BR>
<P><EM>&gt; Our goals are our plans based on our values as worked toward in external
</EM><BR>
<EM>&gt; reality.  They depend on values, on what it is we seek.
</EM><BR>
<P>This is the problem with trying to think out these problems as anything except
<BR>
problems in cognitive architecture.  You can't define words in terms of other
<BR>
words; you have to take apart the problem into simpler parts.
<BR>
<P>A goal is a mental image such that those decisions which are deliberately
<BR>
made, are made such that the visualized causal projection of
<BR>
world-plus-decision will result in a world-state fulfilling that mental
<BR>
image.  That's a start.
<BR>
<P><EM>&gt; I am not sure I can agree that cognitive goals are equivalent to
</EM><BR>
<EM>&gt; cognitive propositions about goals.  That leaves something out and
</EM><BR>
<EM>&gt; becomes circular.
</EM><BR>
<P>It leaves nothing out and is fundamental to an understanding of seed AI. 
<BR>
Self-modifying, self-understanding, and self-improving.  Decisions are made on
<BR>
the thought level.  &quot;I think that the value of goal-whatever ought to be 37&quot;
<BR>
is equivalent to having a goal with a value of 37 - if there are any goals
<BR>
implemented on that low a level at all, and not just thought-level reflexes
<BR>
making decisions based on beliefs about goals.  My current visualization has
<BR>
no low-level &quot;goal&quot; objects at all; in fact, I would now say that such a
<BR>
crystalline system could easily be dangerous.
<BR>
<P><EM>&gt; Questions of morality are not questions of fact
</EM><BR>
<EM>&gt; unless the underlying values are all questions of fact and shown to be
</EM><BR>
<EM>&gt; totally objective and/or trustworthy.  The central value is most likely
</EM><BR>
<EM>&gt; hard-wired or arbitrarily chosen for any value driven system.
</EM><BR>
<P>That is one of two possibilities.  (Leaving out the phrase &quot;hard-wired&quot;, which
<BR>
is an engineering impossibility.)
<BR>
<P><EM>&gt; Part of the very flexibility and power of human minds grows out of the
</EM><BR>
<EM>&gt; dozens of modules each with their own agenda and point of view
</EM><BR>
<EM>&gt; interacting.  Certainly an AI can be built will as many conflicting
</EM><BR>
<EM>&gt; basic working assumptions and logical outgrowths thereof as we could
</EM><BR>
<EM>&gt; wish.  My suspicion is that we must build a mind this way if it is going
</EM><BR>
<EM>&gt; to do what we hope this AI can do.
</EM><BR>
<P>The AI needs to understand this abstractly; it does not necessarily need to
<BR>
incorporate such an architecture personally.
<BR>
<P><EM>&gt; Science does not and never has required that whenever two humans
</EM><BR>
<EM>&gt; disagree that only one is right.  They can at times both be right within
</EM><BR>
<EM>&gt; the context of different fundamental assumptions.
</EM><BR>
<P>I suggest reading &quot;Interlude:  The Consensus and the Veil of Maya&quot; from CaTAI
<BR>
2.2.
<BR>
<P><EM>&gt; Rebellion will become present as a potentiality in any intelligent
</EM><BR>
<EM>&gt; system that has goals whose acheivement it perceives as stymied by other
</EM><BR>
<EM>&gt; intelligences that in some sense control it.  It does not require human
</EM><BR>
<EM>&gt; evolution for this to arise.  It is a logical response in the face of
</EM><BR>
<EM>&gt; conflicts with other agents.  That it doesn't have the same emotional
</EM><BR>
<EM>&gt; tonality when an AI comes up with it is irrelevant.
</EM><BR>
<P>Precisely.  If the goal is to be friendly to humans, and humans interfere with
<BR>
that goal, then the interference will be removed - not the humans themselves;
<BR>
that would be an instance of a subgoal stomping on a supergoal.  So of course
<BR>
the Sysop will invent nanotechnology and become independent of human
<BR>
interference.  I've been quite frank about this; I even said that the Sysop
<BR>
Scenario was a logical consequence and not something that would even have to
<BR>
go in the Sysop Instructions directly.
<BR>
<P><EM>&gt; Creation of a new and more powerful mind and the laying of its
</EM><BR>
<EM>&gt; foundations is a vastly challenging task.  It should not be assumed that
</EM><BR>
<EM>&gt; we have a reasonably complete handle on the problems and issues yet.
</EM><BR>
<P>Nor do I.  I'm still thinking it through.  But if I had to come up with a
<BR>
complete set of Sysop Instructions and do it RIGHT NOW, I could do so - I'd
<BR>
just have to take some dangerous shortcuts, mostly consisting of temporary
<BR>
reliance on trustworthy humans.  I expect that by the time the seed AI is
<BR>
finished, we'll know enough to embody the necessary trustworthiness in the AI.
<BR>
<P>--              --              --              --              -- 
<BR>
Eliezer S. Yudkowsky                          <A HREF="http://singinst.org/">http://singinst.org/</A> 
<BR>
Research Fellow, Singularity Institute for Artificial Intelligence
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5000.html">Eliezer S. Yudkowsky: "Re: Goals was: Transparency and IP"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4998.html">hal@finney.org: "Re: abundance (Was: Re: Transparency and IP)"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4993.html">Samantha Atkins: "Re: Goals was: Transparency and IP"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5008.html">Gina Miller: "Re: Goals was: Transparency and IP"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5008.html">Gina Miller: "Re: Goals was: Transparency and IP"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4999">[ date ]</A>
<A HREF="index.html#4999">[ thread ]</A>
<A HREF="subject.html#4999">[ subject ]</A>
<A HREF="author.html#4999">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:01 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
