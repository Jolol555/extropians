<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Fwd: Earthweb from transadmin</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Fwd: Earthweb from transadmin">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Fwd: Earthweb from transadmin</H1>
<!-- received="Sun Sep 17 20:16:05 2000" -->
<!-- isoreceived="20000918021605" -->
<!-- sent="Sun, 17 Sep 2000 22:11:43 -0400" -->
<!-- isosent="20000918021143" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Fwd: Earthweb from transadmin" -->
<!-- id="39C579DF.1FC7334A@pobox.com" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Fwd:%20Earthweb%20from%20transadmin&In-Reply-To=&lt;39C579DF.1FC7334A@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Sun Sep 17 2000 - 20:11:43 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5155.html">Adrian Tymes: "Re: The Future of Work"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5153.html">Michael LaTorra: "Ye Are Gods (was: Re: just me)"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5175.html">Eugene Leitl: "Fwd: Earthweb from transadmin"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5175.html">Eugene Leitl: "Fwd: Earthweb from transadmin"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5154">[ date ]</A>
<A HREF="index.html#5154">[ thread ]</A>
<A HREF="subject.html#5154">[ subject ]</A>
<A HREF="author.html#5154">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
&nbsp;
<BR>

<BR><P><STRONG>attached mail follows:</STRONG><HR NOSHADE>
<BR><body>
<br>

<!-- |**|begin egp html banner|**| -->

<table border=0 cellspacing=0 cellpadding=2>
<tr bgcolor=#FFFFCC valign=middle>
<td width=77><a href="http://www.egroups.com/"><img border=0 src="http://www.egroups.com/img/logo/logo72.gif" width="72" height="32" alt="eGroups"></a></td>
<td width=388>
<font size="-1">
<a href="http://www.egroups.com/mygroups">My Groups</a> |
<a href="http://www.egroups.com/group/transadmin">transadmin Main Page</a>
| <!-- |@|begin eGroups banner|@| runid: 8150 crid: 4125 --><a target="_blank" href="http://click.egroups.com/1/8150/6/_/_/_/969133843/">Start a new group!</a><!-- |@|end eGroups banner|@| --></font>
</td>
</tr>
</table>
<br>

<!-- |**|end egp html banner|**| -->

<p>

<tt>
Alex Future Bokov wrote:<BR>
&gt; <BR>
&gt; PPS: Oh, one more thing. The 'forgotten' thing I wanted to ask was &quot;How<BR>
&gt; much intelligence is sufficient to meet the objectives? You<BR>
&gt; demonstrated that an &gt;AI would be smarter than an EarthWeb,<BR>
<BR>
(Background for non-attendees:&nbsp; My opinion was that the Earthweb is a<BR>
transhuman, but not a superintelligence.&nbsp; It can form sequences of thoughts<BR>
that are beyond human capability, but any individual thought still has to fit<BR>
inside a single human mind.&nbsp; The Earthweb can be faster than an individual<BR>
human, can have vastly more knowledge, and can even think in genuinely<BR>
transhuman sequences - in those cases where multiple, intersecting experts can<BR>
build upon each others' ideas.&nbsp; An individual human can have one flash of<BR>
genius, or a few flashes of genius; the Earthweb, in theory, can pile<BR>
thousands of flashes of genius one on top of the other, to form sequences<BR>
qualitatively different from those that any single human has ever come up with<BR>
during Earth's previous history.&nbsp; But any individual genius-flash still has to<BR>
come from a single human.&nbsp; Thus the ideal Earthweb is a transhuman but not a<BR>
superintelligence.)<BR>
<BR>
&gt; but would<BR>
&gt; an EarthWeb do just fine for the purposes, and with less likelihood (by<BR>
&gt; definition) of having priorities that conflict with those of humanity,<BR>
&gt; and requiring very little new technology?&nbsp; How does one even go about<BR>
&gt; estimating the level of intelligence needed to safeguard the world from<BR>
&gt; nanodisaster and bring about sentient matter?&quot;<BR>
<BR>
It seems to me that the problem is one of cooperation (or &quot;enforcement&quot;)<BR>
rather than raw intelligence.&nbsp; The transhumanists show up in the 1980s and<BR>
have all these great ideas about how to defend the world from grey goo, then<BR>
the Singularitarians show up in the 1990s and have this great idea about<BR>
bypassing the whole problem via superintelligence.&nbsp; Both of these are<BR>
instances of the exercise of smartness contributing to the safeguarding of the<BR>
world - but the problem is not having the bright idea; the problem is putting<BR>
enough backbone behind it.&nbsp; The Earthweb is great for coming up with ideas,<BR>
but says nothing about backing, or enforcement.<BR>
<BR>
In other words, it looks to me like the Earthweb would say:&nbsp; &quot;Hey, let's go<BR>
build an AI!&quot;&nbsp; Or perhaps the Earthweb is brighter than I am, and would see an<BR>
even simpler and faster way to do it - though I have difficulty imagining what<BR>
one would be.<BR>
<BR>
The neat part about superintelligence isn't just that an SI is really smart;<BR>
it's that an SI can very rapidly build new technologies and use them according<BR>
to a unified set of motives.&nbsp; Before an Earthweb could even begin to replace<BR>
superintelligence as a guardian, it would have to (a) come up with a smart<BR>
plan, (b) invent the technology to implement it, and (c) ensure that the<BR>
technology was used to implement (a).&nbsp; It looks to me like (c) would be the<BR>
major problem, since the Earthweb by its nature is public and distributed. <BR>
But perhaps the Earthweb could come up with a clever solution even to this...<BR>
<BR>
My feeling, though, is that even if the Earthweb does solve all these problems<BR>
and come up with a clever way to build a better world using only the limited<BR>
intelligence of the Earthweb, just going out and building a seed AI will still<BR>
look like an even more superior solution.<BR>
<BR>
In summary, though, the power of Earthweb, as with any transhuman, would lie<BR>
primarily in its smartness, not its brute intelligence.&nbsp; The Earthweb can act<BR>
as guardian if and only if the Earthweb itself comes up with some really<BR>
clever way to act as guardian.<BR>
<BR>
A practical problem is that all the Earthweb techniques I've seen, including<BR>
the idea-futures-on-steroids of Marc Stiegler's _Earthweb_, are still subject<BR>
to distortion by majority prejudice.&nbsp; If a majority of the betting money is in<BR>
the hands of folk with a blind prejudice against AIs, then only a very stable<BR>
culture - only a culture that has had the Earthweb for years or even decades -<BR>
will have the systemic structure whereby informed discussion can overcome<BR>
prejudices.&nbsp; In other words, the Earthweb proposals I've seen have no method<BR>
to distinguish between genius and stupidity except by using the minds of other<BR>
humans, and it will take a while before the system builds up enough internal<BR>
complexity to have a systemic method of distinguishing - independently of the<BR>
human components - the subtle structural differences between a human making a<BR>
good judgement and a human making a bad judgement.<BR>
<BR>
Idea futures are only the beginning of such a method.&nbsp; Idea futures mean<BR>
betting money on the judgements, so that the winners of each round have<BR>
greater weight in successive rounds.&nbsp; Idea futures are better than blind<BR>
majority votes, but they aren't perfect.&nbsp; How do you use idea futures to<BR>
resolve an issue like Friendly AI, even leaving out the payoff problem?&nbsp; It<BR>
would take several iterated rounds on issues of the same order, with similar<BR>
stakes and similar content and similar required knowledge and *resolvable<BR>
predictions*, before the capitalist efficiencies came into play and the bad<BR>
betters started dropping out.<BR>
<BR>
For the Earthweb to resolve a problem like that, it would need a systemic<BR>
structure that systematically resolved each idea into sub-issues, identifying<BR>
each assumption and deduction and sequitur, and discussing and betting on<BR>
these subcomponents separately.&nbsp; In other words, the Earthweb would have to<BR>
actually change the structure of thoughts and thinking and decision-making<BR>
processes, after which it's plausible that the outcome of the decision would<BR>
be better than the sum of the betting humans - if nothing else, because the<BR>
humans who made bets on the final outcome would have the granular resolution<BR>
of the discussion available for examination.<BR>
<BR>
Wanna take this to SL4?&nbsp;&nbsp;&nbsp; ( <a href="http://sysopmind.com/sing/SL4.html">http://sysopmind.com/sing/SL4.html</a> )<BR>
If not, can I forward this message there?&nbsp; And to Extropians, Robin Hanson,<BR>
and Marc Stiegler - I think they'd all be interested.<BR>
<BR>
&gt; PPPS: Anybody who might be wondering, what I mean by EarthWeb is a sort<BR>
&gt; of world-wide 'Slash meets eCommerce meets eBay meets email on<BR>
&gt; steroids' that evolves into an emergent entity in its own right. As for<BR>
&gt; &gt;AI, see SingInst's pages.<BR>
<BR>
And what I mean by &quot;Earthweb&quot; is the entity visualized by Marc Stiegler in the<BR>
book &quot;Earthweb&quot;.<BR>
<BR>
See also:<BR>
<BR>
<a href="http://www.the-earthweb.com">http://www.the-earthweb.com</a><BR>
<a href="http://www.baen.com/chapters/eweb_1.htm">http://www.baen.com/chapters/eweb_1.htm</a> (chapters 1 through 6 available)<BR>
<BR>
--&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- <BR>
Eliezer S. Yudkowsky&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="http://singinst.org/">http://singinst.org/</a> <BR>
Research Fellow, Singularity Institute for Artificial Intelligence<BR>
</tt>

<br>
<tt>
To unsubscribe from this group, send an email to:<BR>
transadmin-unsubscribe@egroups.com<BR>
<BR>
</tt>
<br>

</body>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5155.html">Adrian Tymes: "Re: The Future of Work"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5153.html">Michael LaTorra: "Ye Are Gods (was: Re: just me)"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5175.html">Eugene Leitl: "Fwd: Earthweb from transadmin"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5175.html">Eugene Leitl: "Fwd: Earthweb from transadmin"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5154">[ date ]</A>
<A HREF="index.html#5154">[ thread ]</A>
<A HREF="subject.html#5154">[ subject ]</A>
<A HREF="author.html#5154">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:19 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
