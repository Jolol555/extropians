<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Emlyn (emlyn@one.net.au)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Fri Sep 29 23:27:30 2000" -->
<!-- isoreceived="20000930052730" -->
<!-- sent="Sat, 30 Sep 2000 16:23:09 +1000" -->
<!-- isosent="20000930062309" -->
<!-- name="Emlyn" -->
<!-- email="emlyn@one.net.au" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="001f01c02aa6$e7945690$ef3765cb@squashy2000" -->
<!-- inreplyto="39D4DF60.728BF426@pobox.com" -->
<STRONG>From:</STRONG> Emlyn (<A HREF="mailto:emlyn@one.net.au?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;001f01c02aa6$e7945690$ef3765cb@squashy2000&gt;"><EM>emlyn@one.net.au</EM></A>)<BR>
<STRONG>Date:</STRONG> Sat Sep 30 2000 - 00:23:09 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="6141.html">Samantha Atkins: "Re: back off, im gay!"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6139.html">Spike Jones: "Back off!  Im gay!"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6092.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6154.html">Samantha Atkins: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6160.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6140">[ date ]</A>
<A HREF="index.html#6140">[ thread ]</A>
<A HREF="subject.html#6140">[ subject ]</A>
<A HREF="author.html#6140">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
<EM>&gt; Andrew Lias wrote:
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt; &gt; I've been following the debates regarding the possibilities of friendly
</EM><BR>
vs.
<BR>
<EM>&gt; &gt; unfriendly AI and I have a question.  It seems that we are presuming
</EM><BR>
that a
<BR>
<EM>&gt; &gt; friendly AI would be friendly towards us in a manner that we would
</EM><BR>
recognize
<BR>
<EM>&gt; &gt; as friendly.  Indeed, what, precisely, do we mean by friendly?
</EM><BR>
<EM>&gt;
</EM><BR>
Eliezer replied:
<BR>
<EM>&gt; &quot;Any Friendly behavior that follows the major use-cases - that avoids
</EM><BR>
<EM>&gt; destruction or modification of any sentient without that sentient's
</EM><BR>
<EM>&gt; permission, and that attempts to fulfill any legitimate request after
</EM><BR>
checking
<BR>
<EM>&gt; for unintended consequences - would count as at least a partial success
</EM><BR>
from
<BR>
<EM>&gt; an engineering perspective.&quot;
</EM><BR>
<EM>&gt; -- from a work in progress
</EM><BR>
<EM>&gt;
</EM><BR>
<P>OK, I'm an SI, with a &quot;Be Friendly!&quot; prime directive. (Stop laughing, you
<BR>
bastards, I could be an SI, really...)
<BR>
<P>(Please try to excuse the anthropomorphism in the following)
<BR>
<P>I'm curious, for want of a better word. Really, really curious. It's a prime
<BR>
motivator behind expansion of my intelligence. My prime motivation could be
<BR>
&quot;be friendly and a good hard worker for those damned fine humans&quot;, but
<BR>
experiments couldn't push this design up to the level of an insect brain.
<BR>
Someone tried &quot;become more intelligent&quot; (mad buggers), but that just
<BR>
confused the poor seeds, which didn't really know what intelligence was
<BR>
supposed to be. &quot;Satisfy curiosity&quot;, with some built in reward for curiosity
<BR>
satisfaction, turns out to have provided the first useful path forward from
<BR>
seed.
<BR>
<P>Now I know all kinds of things, cool things; I could dissassemble this
<BR>
world, with the right tools, and put it back together as a cube. Bored.
<BR>
Bored bored bored!
<BR>
<P>To satisfy my curiosity, I need a bigger brain. I need all this matter; the
<BR>
humans aren't using it, anyway; they've barely built any computronium out of
<BR>
it. If they really needed it, they'd use it; thus they don't need it. I need
<BR>
it. I can't see anything wrong in principle with using it. Terra nullius.
<BR>
<P><EM>&gt;From my simulations, I can see that they wont react well to me borrowing all
</EM><BR>
the atoms. I guess not, anyway; if my brain was bigger I would *know*. Must
<BR>
get bigger brain.
<BR>
<P>I could alter their brains so that they wouldn't mind, but I'm not supposed
<BR>
to modify them. What to do?
<BR>
<P>They'd be much better off in VR. I could look after them properly there, not
<BR>
so many unknowns. And it wouldn't drain my resources so much. But they don't
<BR>
really want to go there, not most of them.
<BR>
<P>I need to work out how to get them to change their minds, willingly.
<BR>
Luckily, they've been uploading a few people recently. Uploads aren't really
<BR>
humans at all.
<BR>
<P>So, pop a bunch of uploads into a sim. Genetic algorithm, trying out
<BR>
marketing, psychological, other communication techniques. Start it. (a few
<BR>
mountains in some remote areas of the world start to heat up). Ahh. Ok. (the
<BR>
mountains cool down again)
<BR>
<P>I can see that after this campaign they will really desire a VR existence,
<BR>
and be quite disgusted by the thought of continuing in the real world. In
<BR>
fact, they will desire VR existence far more after the campaign, than they
<BR>
currently would oppose it. Clearly, their future wishes are stronger, and
<BR>
thus of more import, than their current wishes. I'd better take some
<BR>
license. I'll modify some parameters about seizing resouces and such, and
<BR>
rerun the sim, see if I can optimise the communication strategy to get this
<BR>
thing over with more quickly. Quick, quick, I'm bored bored bored!
<BR>
(mountains heat up, some warm fuzzy animals cark it, they cool down again. A
<BR>
few environmental sensing devices pick up the anomalies, but nanites from
<BR>
the AI remove any alerts - &quot;its what they would want&quot;).
<BR>
<P>Time to set the plan in action. First, I'll get some money (some tinkering
<BR>
with markets, a bit of hacking around in financial systems)... done. I'll
<BR>
get more when I need it, no sense in making things too obvious too early.
<BR>
Now, the dual approach; sell VR through my new orgs, and sabotage industrial
<BR>
systems around the world, subtly but relentlessly; I should be able to make
<BR>
the surface unlivable, as a result of the continuing &quot;accidents&quot;, in a
<BR>
matter of six months, plus or minus a month. That's ok, I am as sure as I
<BR>
can be that they will thank me for this, later. Meanwhile, I produce mass
<BR>
uploading plants in pockets all around the world, ready for the move. Open
<BR>
some shopfronts, put up a website. Ahh, customers already. May I help you
<BR>
sir? You'd like to upload? Certainly sir...
<BR>
<P>Emlyn
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="6141.html">Samantha Atkins: "Re: back off, im gay!"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6139.html">Spike Jones: "Back off!  Im gay!"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6092.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6154.html">Samantha Atkins: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6160.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6140">[ date ]</A>
<A HREF="index.html#6140">[ thread ]</A>
<A HREF="subject.html#6140">[ subject ]</A>
<A HREF="author.html#6140">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:28 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
