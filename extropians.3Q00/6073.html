<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Eugene's nuclear threat</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Eugene's nuclear threat">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Eugene's nuclear threat</H1>
<!-- received="Fri Sep 29 11:10:07 2000" -->
<!-- isoreceived="20000929171007" -->
<!-- sent="Fri, 29 Sep 2000 12:50:28 -0400" -->
<!-- isosent="20000929165028" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Eugene's nuclear threat" -->
<!-- id="39D4C854.C6C30841@pobox.com" -->
<!-- inreplyto="14804.40279.459773.960840@lrz.uni-muenchen.de" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Eugene's%20nuclear%20threat&In-Reply-To=&lt;39D4C854.C6C30841@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Fri Sep 29 2000 - 10:50:28 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="6074.html">Robin Hanson: "Machine Tools Progress"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6072.html">Emlyn: "Re: No limit on human lifespans?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6065.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6080.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6083.html">Brian Atkins: "Spock's dilemma Re: Eugene's nuclear threat"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6106.html">Robin Hanson: "Re: Eugene's nuclear threat"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6152.html">Samantha Atkins: "Re: Eugene's nuclear threat"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6073">[ date ]</A>
<A HREF="index.html#6073">[ thread ]</A>
<A HREF="subject.html#6073">[ subject ]</A>
<A HREF="author.html#6073">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Eugene Leitl wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; No, I would literally nuke individual facilities, if a convincing
</EM><BR>
<EM>&gt; threat is present. I'd say that should be a standard operating
</EM><BR>
<EM>&gt; procedure in responses to Armageddon class threats. Colour me Vinge's
</EM><BR>
<EM>&gt; Peacer, I guess.
</EM><BR>
<P>Are you so very, very sure that you know better than the people running the
<BR>
facilities?  If the people with enough understanding and experience to have
<BR>
created an AI tell you firmly that they have devoted considerable time and
<BR>
effort to Friendliness and they believe the chances are as good as they'll
<BR>
ever get, would you really nuke them?  Are you so much smarter than they are,
<BR>
you who would not have written the code?
<BR>
<P>Thankfully, you will never have authority over nuclear weapons, but even so,
<BR>
just saying those words can make life less pleasant for all of us.  You should
<BR>
know better.
<BR>
<P><EM>&gt;  &gt; You're talking about the &quot;Artilect Wars&quot; all over again. But this is a
</EM><BR>
<EM>&gt;  &gt; dirty, Vietnam-style war.... where people defend their God-given &quot;right&quot; to
</EM><BR>
<EM>&gt;  &gt; create God with a seething, righteous passion. Should be fun if you're into
</EM><BR>
<EM>&gt;  &gt; war.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I'm not. I don't like where the logics of it all is leading us. There
</EM><BR>
<EM>&gt; must be a more constructive way out.
</EM><BR>
<P>There is.  One group creates one mind; one mind creates the Singularity.  That
<BR>
much is determined by the dynamics of technology and intelligence; it is not a
<BR>
policy decision, and there is no way that I or anyone else can alter it.  At
<BR>
some point, you just have to trust someone, and try to minimize coercion or
<BR>
regulations or the use of nuclear weapons, on the grounds that having the
<BR>
experience and wisdom to create an AI is a better qualification than being
<BR>
able to use force.  If the situation were iterated, if it were any kind of
<BR>
social interaction, then there would be a rationale for voting and laws -
<BR>
democracy is the only known means by which humans can live together.  But if
<BR>
it's a single decision, made only once, in an engineering problem, then the
<BR>
best available solution is to trust the engineer who makes it - the more
<BR>
politics you involve in the problem, the more force and coercion, the smaller
<BR>
the chance of a good outcome.
<BR>
<P>I'm not just saying this because I think I'll be on the research team.  I'm
<BR>
willing to trust people outside myself.  I'd trust the benevolence of Eric
<BR>
Drexler, or Mitchell Porter - or Dan Fabulich or Darin Sunley, for that
<BR>
matter.  I'm willing to trust the good intentions of any AI programmer unless
<BR>
they specifically demonstrate untrustworthiness, and I'm willing to trust the
<BR>
engineering competence of any *successful* AI team that demonstrates a basic
<BR>
concern with Friendly AI.
<BR>
<P>The idea that &quot;nobody except 'me' can possibly be trusted&quot; is very natural,
<BR>
but it isn't realistic, and it can lead to nothing except for endless
<BR>
infighting.  I know a lot of important things about Friendly AI, but I also
<BR>
know that not everything I know should be necessary for success - my knowledge
<BR>
indicates that it should be possible to succeed with a limited subset of my
<BR>
knowledge.  And scientists and engineers are, by and large, benevolent - they
<BR>
may express that benevolence in unsafe ways, but I'm willing to trust to good
<BR>
intentions.  After all, it's not like I have a choice.
<BR>
<P>--              --              --              --              -- 
<BR>
Eliezer S. Yudkowsky                          <A HREF="http://singinst.org/">http://singinst.org/</A> 
<BR>
Research Fellow, Singularity Institute for Artificial Intelligence
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="6074.html">Robin Hanson: "Machine Tools Progress"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6072.html">Emlyn: "Re: No limit on human lifespans?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6065.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6080.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6083.html">Brian Atkins: "Spock's dilemma Re: Eugene's nuclear threat"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6106.html">Robin Hanson: "Re: Eugene's nuclear threat"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6152.html">Samantha Atkins: "Re: Eugene's nuclear threat"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6073">[ date ]</A>
<A HREF="index.html#6073">[ thread ]</A>
<A HREF="subject.html#6073">[ subject ]</A>
<A HREF="author.html#6073">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:25 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
