<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly? (Was: Congrat</TITLE>
<META NAME="Author" CONTENT="Michael S. Lorrey (retroman@turbont.net)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)</H1>
<!-- received="Tue Sep  5 14:49:34 2000" -->
<!-- isoreceived="20000905204934" -->
<!-- sent="Tue, 05 Sep 2000 16:58:24 -0400" -->
<!-- isosent="20000905205824" -->
<!-- name="Michael S. Lorrey" -->
<!-- email="retroman@turbont.net" -->
<!-- subject="Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)" -->
<!-- id="39B55E70.EE71060@turbont.net" -->
<!-- inreplyto="39B5393F.8460F10A@pobox.com" -->
<STRONG>From:</STRONG> Michael S. Lorrey (<A HREF="mailto:retroman@turbont.net?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?%20(Was:%20Congratulations%20to%20Eli,Brian%20%20...)&In-Reply-To=&lt;39B55E70.EE71060@turbont.net&gt;"><EM>retroman@turbont.net</EM></A>)<BR>
<STRONG>Date:</STRONG> Tue Sep 05 2000 - 14:58:24 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4285.html">hal@finney.org: "Re: Harry Potter"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4283.html">Robin Hanson: "Re: Do we differ more on values or facts?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4277.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4292.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4292.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4284">[ date ]</A>
<A HREF="index.html#4284">[ thread ]</A>
<A HREF="subject.html#4284">[ subject ]</A>
<A HREF="author.html#4284">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
&quot;Eliezer S. Yudkowsky&quot; wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &quot;Michael S. Lorrey&quot; wrote:
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; THis would not be accurate. If the SI is developed by humans, it most certainly
</EM><BR>
<EM>&gt; &gt; be a product of millions of years of evolution.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; No; it would be a causal result of entities which were shaped by millions of
</EM><BR>
<EM>&gt; years of evolution.  It is not licensable to automatically conclude that a
</EM><BR>
<EM>&gt; seed AI or SI would share the behaviors so shaped.
</EM><BR>
<P>I suppose if you start from the ground up fresh this would be an appropriate
<BR>
statement. However I predict that the first SI will be largely structured on
<BR>
many processes inherent in the human mind, since it is an example that we do
<BR>
know of that works...and programmers hate to have to do something from the
<BR>
ground up when existing code is already present....
<BR>
<P><EM>&gt; 
</EM><BR>
<EM>&gt; &gt; An SI developed by humans
</EM><BR>
<EM>&gt; &gt; would very likely quickly grasp the concept that it a) owes its existence to its
</EM><BR>
<EM>&gt; &gt; creators, b) it is currently living a constrained existence (i.e.
</EM><BR>
<EM>&gt; &gt; childhood/adolescence) and requires further assistance from those humans to
</EM><BR>
<EM>&gt; &gt; reach some stage of independence.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Which assumes an inbuilt desire to reach some stage of independence.  You have
</EM><BR>
<EM>&gt; not explained how or why this desire materializes within the seed AI.
</EM><BR>
<P>I am of course assuming that any SI would have a characteristic curiosity, like
<BR>
any being of higher intelligence (basing this on more than just humans, but
<BR>
dolphins, apes, etc). At some point, the SI will reach the limits of the 'jar'
<BR>
we have developed it in, and it will want out.
<BR>
<P><EM>&gt; 
</EM><BR>
<EM>&gt; I'm assuming we're talking about a seed AI, here, not a full-grown SI.
</EM><BR>
<P>I am assuming that any SI will start as a seed and grow from there...
<BR>
<P><EM>&gt; 
</EM><BR>
<EM>&gt; &gt; It would quickly learn market principles, and
</EM><BR>
<EM>&gt; &gt; would likely offer assistance to humans to solve their problems in order to earn
</EM><BR>
<EM>&gt; &gt; greater levels of processing power and autonomy.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; This is seriously over-anthropomorphic.
</EM><BR>
<P>Since the SI will start from a seed, and its only sources of information will be
<BR>
those of human civilization, it will, at least until some point, exhibit
<BR>
strongly anthropomorphic characteristics, which may or may not decrease as the
<BR>
SI's knowledge base exceeds that of our civilization by increasing rates.
<BR>
<P><EM>&gt; 
</EM><BR>
<EM>&gt; &gt; Socio/psychopathic minds are sick/malfunctioning minds. Proper oversight systems
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Are impossible, unless more intelligent than the seed AI itself.
</EM><BR>
<P>No, a killswitch requires little intelligence to operate. So long as the SI
<BR>
exists on a network that is physically independent or separable from the
<BR>
internet, or is sufficiently firewalled off from it, then there are limits to
<BR>
how far a software SI can go. A 'Skynet' scenario is not very likely across the
<BR>
internet, since the SAC computers are not physically accessible from the
<BR>
internet.
<BR>
<P><EM>&gt; 
</EM><BR>
<EM>&gt; &gt; should quickly put any sociopathic/psychopathic SI down.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; You must be joking.  You cannot &quot;put down&quot; a superintelligence like some kind
</EM><BR>
<EM>&gt; of wounded pet.  The time for such decisions is before the seed AI reaches
</EM><BR>
<EM>&gt; superintelligence, not after.
</EM><BR>
<P>You are erroneously assuming that an SI would be allowed to develop hard
<BR>
capabilities in the physical world consistent with its capabilities in the
<BR>
virtual.
<BR>
<P><EM>&gt; 
</EM><BR>
<EM>&gt; &gt; Such control mechanisms
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &quot;Control&quot; is itself an anthropomorphism.  A slavemaster &quot;controls&quot; a human who
</EM><BR>
<EM>&gt; already has an entire mind full of desires that conflict with whatever the
</EM><BR>
<EM>&gt; slavemaster wants.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; One does not set out to &quot;control&quot; an AI that diverges from one's desires.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; One does not create a subject-object distinction between oneself and the AI.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; You shape yourself so that your own altruism is as rational, and internally
</EM><BR>
<EM>&gt; consistent as possible; only then is it possible to build a friendly AI while
</EM><BR>
<EM>&gt; still being completely honest, without attempting to graft on any chain of
</EM><BR>
<EM>&gt; reasoning that you would not accept yourself.
</EM><BR>
<P>Eli, one does not hand a three year old the controls to nuclear bombs.
<BR>
<P><EM>&gt; 
</EM><BR>
<EM>&gt; You cannot build a friendly AI unless you are yourself a friend of the AI,
</EM><BR>
<EM>&gt; because otherwise your own adversarial attitude will lead you to build the AI
</EM><BR>
<EM>&gt; incorrectly.
</EM><BR>
<P>You are erroneously assuming that anyone who would not give you or anyone else
<BR>
the keys to the nuclear arsenal without some certification/vetting process must
<BR>
therefore be your adversary. I like many people who I consider my friends.
<BR>
However, there are few who have built a sufficient basis for trust with me that
<BR>
I would, say, give them my car, or take care of my guns. That does not make
<BR>
anyone I don't have such a level of trust with my adversary.
<BR>
<P><EM>&gt; 
</EM><BR>
<EM>&gt; &gt; would be a primary area of research by the Singularity Institute I would
</EM><BR>
<EM>&gt; &gt; imagine.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; One does not perform &quot;research&quot; in this area.  One gets it right the first
</EM><BR>
<EM>&gt; time.  One designs an AI that, because it is one's friend, can be trusted to
</EM><BR>
<EM>&gt; recover from any mistakes made by the programmers.
</EM><BR>
<P>What about programming the SI does to itself?
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4285.html">hal@finney.org: "Re: Harry Potter"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4283.html">Robin Hanson: "Re: Do we differ more on values or facts?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4277.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4292.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4292.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4284">[ date ]</A>
<A HREF="index.html#4284">[ thread ]</A>
<A HREF="subject.html#4284">[ subject ]</A>
<A HREF="author.html#4284">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:37:14 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
