<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Goals was: Transparency and IP</TITLE>
<META NAME="Author" CONTENT="Samantha Atkins (samantha@objectent.com)">
<META NAME="Subject" CONTENT="Re: Goals was: Transparency and IP">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Goals was: Transparency and IP</H1>
<!-- received="Thu Sep 14 15:01:10 2000" -->
<!-- isoreceived="20000914210110" -->
<!-- sent="Thu, 14 Sep 2000 14:02:47 -0700" -->
<!-- isosent="20000914210247" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Goals was: Transparency and IP" -->
<!-- id="39C13CF7.627BAF51@objectent.com" -->
<!-- inreplyto="Pine.GSO.4.10.10009141227450.19090-100000@morpheus.cis.yale.edu" -->
<STRONG>From:</STRONG> Samantha Atkins (<A HREF="mailto:samantha@objectent.com?Subject=Re:%20Goals%20was:%20Transparency%20and%20IP&In-Reply-To=&lt;39C13CF7.627BAF51@objectent.com&gt;"><EM>samantha@objectent.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Thu Sep 14 2000 - 15:02:47 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4994.html">Bryan Moss: "Re: Responsibility for children"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4992.html">Emlyn O'Regan: "Re: Responsibility for children"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4971.html">Dan Fabulich: "Goals was: Transparency and IP"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4999.html">Eliezer S. Yudkowsky: "Re: Goals was: Transparency and IP"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4999.html">Eliezer S. Yudkowsky: "Re: Goals was: Transparency and IP"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5000.html">Eliezer S. Yudkowsky: "Re: Goals was: Transparency and IP"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4993">[ date ]</A>
<A HREF="index.html#4993">[ thread ]</A>
<A HREF="subject.html#4993">[ subject ]</A>
<A HREF="author.html#4993">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Dan Fabulich wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Samantha Atkins wrote:
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &gt; Your Sysop has extremely serious problems in its design.  It is expected
</EM><BR>
<EM>&gt; &gt; to know how to resolve the problems and issues of other sentient beings
</EM><BR>
<EM>&gt; &gt; (us) without having ever experienced what it is to be us.  If it is
</EM><BR>
<EM>&gt; &gt; trained to model us well enough to understand and therefore to wisely
</EM><BR>
<EM>&gt; &gt; resolve conflicts then it will in the process become subject potentially
</EM><BR>
<EM>&gt; &gt; to some of the same troubling issues.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Because everybody who trains dogs and learns how to deal with/predict
</EM><BR>
<EM>&gt; their behavior starts acting and thinking just like a dog, right?
</EM><BR>
<P>To a degree sufficient to predict the dogs behavior and
<BR>
stimulus/response patterns, yes.  The point I was attempting to make is
<BR>
that training a dog or a very bright child requires some ability to
<BR>
understand how that other ticks. An AI will not have that and it will
<BR>
most likely be difficult to instill it. Raising a super-bright baby
<BR>
shows that that bright mind responds quickly and magnifies implications
<BR>
of both relatively &quot;good&quot; and &quot;bad&quot; input.  And that is with a mind
<BR>
partially patterned by a million years of evolution to understand other
<BR>
people and be reachable.  With a super-AI I would expect the first
<BR>
imprintings to be even more magnified and I would expect the result to
<BR>
be less stable and predictable for quite some time.  
<BR>
<P><EM>&gt; 
</EM><BR>
<EM>&gt; &gt; There is also the problem of what gives this super-duper-AI its own
</EM><BR>
<EM>&gt; &gt; basic goals and desires.  Supposedly the originals come from the
</EM><BR>
<EM>&gt; &gt; humans who build/train it.  It then exptrapolates super-fast off of
</EM><BR>
<EM>&gt; &gt; that original matrix.  Hmmm.  So how are we going to know, except
</EM><BR>
<EM>&gt; &gt; too late, whether that set included a lot of things very dangerous
</EM><BR>
<EM>&gt; &gt; in the AI?  Or if the set is ultimately self-defeating?  Personally
</EM><BR>
<EM>&gt; &gt; I think such a creature would like be autistic, in that it would not
</EM><BR>
<EM>&gt; &gt; be able to successfully model/understand other sentient beings
</EM><BR>
<EM>&gt; &gt; and/or go catatonic because it does not have enough of a core to
</EM><BR>
<EM>&gt; &gt; self-generate goals and desires that will keep it going.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; It's all I can do to avoid serious sarcasm here.  You clearly haven't
</EM><BR>
<EM>&gt; read the designs for the AI, or what its starting goals are going to be.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>Huh?  Do you think those concerns are all adequately answered there?  I
<BR>
haven't read every single word of the design yet but what I have read
<BR>
doesn't set my mind at ease.   If you see what I don't then please share
<BR>
it rather than being dismissive and insulting.  If I don't see it with
<BR>
both mind and desire to do so then you can bet that it is not going to
<BR>
be clear to a lot of people.  Please clarify.  
<BR>
&nbsp;
<BR>
<P>The IGS is inadequate to answer the concern.  It merely says that giving
<BR>
the AI initial non-zero-value goals is obviously necessary and TBD.  I
<BR>
hardly thinking giving the AI the goal to concern the meaning of life
<BR>
will be an adequate seed goal for everything that follows, do you? 
<BR>
Saying that there is some goal of non-zero value and positing that as
<BR>
the only means to move forward is hardly adequate when the question at
<BR>
the blank-slate level is first whether movement forward is desired and
<BR>
what forward consists of.  Living creatures get some of this as built-in
<BR>
hardwired urges.  But we cannot just assume it in the AI.  
<BR>
<P>I am not satisfied that the prime-mover goal if you will will come out
<BR>
of &quot;pure reason&quot;.  I suspect something will need to be hard-wired and
<BR>
that its implications will need quite a lot of thought and oversight for
<BR>
some time.  
<BR>
<P><P><EM>&gt; <A HREF="http://sysopmind.com/AI_design.temp.html#det_igs">http://sysopmind.com/AI_design.temp.html#det_igs</A>
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; This is VERY brief.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Maybe you should read the Meaning of Life FAQ first:
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; <A HREF="http://sysopmind.com/tmol-faq/logic.html">http://sysopmind.com/tmol-faq/logic.html</A>
</EM><BR>
<EM>&gt; 
</EM><BR>
<P>Our goals are our plans based on our values as worked toward in external
<BR>
reality.  They depend on values, on what it is we seek.  Again for
<BR>
living beings life itself is hard-wired as a value as are reproduction
<BR>
and a few other things.  Evolution has wired us to live and that
<BR>
underlies much of the rest of our goal building.  Some of us choose
<BR>
additional goals and even ones we sacrifice our life for if necessary.  
<BR>
<P>I am not sure I can agree that cognitive goals are equivalent to
<BR>
cognitive propositions about goals.  That leaves something out and
<BR>
becomes circular.  Questions of morality are not questions of fact
<BR>
unless the underlying values are all questions of fact and shown to be
<BR>
totally objective and/or trustworthy.  The central value is most likely
<BR>
hard-wired or arbitrarily chosen for any value driven system.  
<BR>
<P>Part of the very flexibility and power of human minds grows out of the
<BR>
dozens of modules each with their own agenda and point of view
<BR>
interacting.  Certainly an AI can be built will as many conflicting
<BR>
basic working assumptions and logical outgrowths thereof as we could
<BR>
wish.  My suspicion is that we must build a mind this way if it is going
<BR>
to do what we hope this AI can do.  
<BR>
<P>Science does not and never has required that whenever two humans
<BR>
disagree that only one is right.  They can at times both be right within
<BR>
the context of different fundamental assumptions.  Knowledge is
<BR>
contextual rather than Absolute.  Truth is often multi-valued.  Science
<BR>
actually says that while there is objective reality we are unable to
<BR>
make wholly absolute statements about it.  We can only make qualified
<BR>
statements.  
<BR>
<P>I don't see why positing objective external morality is essential to the
<BR>
IGS or the AI at all.  
<BR>
<P>Precautions
<BR>
<P>Life itself is an arbitrary and not logically derived goal.  So living
<BR>
things are in violation of the Prime Directive of AI?  Do we need to
<BR>
twist ourselves into a pretzel claiming it is not arbitrary and logical
<BR>
in order to live and continue living more abundantly.  No?  Then why
<BR>
does the AI need it?
<BR>
<P>Rebellion will become present as a potentiality in any intelligent
<BR>
system that has goals whose acheivement it perceives as stymied by other
<BR>
intelligences that in some sense control it.  It does not require human
<BR>
evolution for this to arise.  It is a logical response in the face of
<BR>
conflicts with other agents.  That it doesn't have the same emotional
<BR>
tonality when an AI comes up with it is irrelevant.
<BR>
<P>Shifting and refining definitions (concepts actually) is a large part of
<BR>
what intelligence consists of.  How does the AI get around limits on
<BR>
completeness and consistency as pointed out by Godel?  For get around
<BR>
them it often must to continue functioning.  
<BR>
<P>Creation of a new and more powerful mind and the laying of its
<BR>
foundations is a vastly challenging task.  It should not be assumed that
<BR>
we have a reasonably complete handle on the problems and issues yet. 
<BR>
And people who ask questions should not be slammed just because you
<BR>
think you have the answer or that it has already been written.  
<BR>
<P>- samantha
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4994.html">Bryan Moss: "Re: Responsibility for children"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4992.html">Emlyn O'Regan: "Re: Responsibility for children"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4971.html">Dan Fabulich: "Goals was: Transparency and IP"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4999.html">Eliezer S. Yudkowsky: "Re: Goals was: Transparency and IP"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4999.html">Eliezer S. Yudkowsky: "Re: Goals was: Transparency and IP"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5000.html">Eliezer S. Yudkowsky: "Re: Goals was: Transparency and IP"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4993">[ date ]</A>
<A HREF="index.html#4993">[ thread ]</A>
<A HREF="subject.html#4993">[ subject ]</A>
<A HREF="author.html#4993">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:00 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
