<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Eugene Leitl (eugene.leitl@lrz.uni-muenchen.de)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Thu Sep  7 05:48:23 2000" -->
<!-- isoreceived="20000907114823" -->
<!-- sent="Thu, 7 Sep 2000 03:43:59 -0700 (PDT)" -->
<!-- isosent="20000907104359" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="14775.29039.845473.410374@lrz.uni-muenchen.de" -->
<!-- inreplyto="009301c0185b$5ae19e40$8fbc473f@jrmolloy" -->
<STRONG>From:</STRONG> Eugene Leitl (<A HREF="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;14775.29039.845473.410374@lrz.uni-muenchen.de&gt;"><EM>eugene.leitl@lrz.uni-muenchen.de</EM></A>)<BR>
<STRONG>Date:</STRONG> Thu Sep 07 2000 - 04:43:59 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4481.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4479.html">Eugene Leitl: "corn again"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4429.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="4502.html">Jason Joel Thompson: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4543.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4480">[ date ]</A>
<A HREF="index.html#4480">[ thread ]</A>
<A HREF="subject.html#4480">[ subject ]</A>
<A HREF="author.html#4480">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
J. R. Molloy writes:
<BR>
<P><EM> &gt; All the AIs don't have to run factories and do the markets. Sacrifice a few
</EM><BR>
<P>Any AI on the global network is an AI one step away from being
<BR>
entirely uncontained. Any really smart AI dealing with humans, whether
<BR>
online or offline is one or two steps away from being free. Read the
<BR>
archives, we've talked about this before.
<BR>
<P>How would you contain a prisoner who's working the salt mines if all
<BR>
what it takes to make him disappear and cause a major civil war
<BR>
devastating the world is a short contact with a single grain of salt?
<BR>
You can keep him away from salt, though that would be difficult, but
<BR>
then he can't work the mines. Why would you want to keep such a
<BR>
dangerous, useless prisoner? Or, what if you don't realize that salt
<BR>
has such a magical effect on him? That many prisons in the world are
<BR>
keeping such prisoners, most of them run by ignorant administrators?
<BR>
<P>Look, I'm sorry if my analogies are so skewed, but that's because we
<BR>
haven't been in a such poised regime before (with the exception of
<BR>
when life emerged here), about to punctuate the equilibrium and
<BR>
suddenly spill into a new local minimum. We don't have the imagery to
<BR>
talk about such things.
<BR>
<P><EM> &gt; million of them to supervise the others, i.e., give some of them the job of
</EM><BR>
<EM> &gt; police, some psychologists, a few executioners, [insert your favourite
</EM><BR>
<EM> &gt; enforcement agent here] so that these AIs can monitor the entire population.
</EM><BR>
<P>Anything nontranscendent and powerful standing under central control
<BR>
monitoring the entire population is a serious mistake. Allowing such
<BR>
situation to persist is like playing russian roulette every afternoon.
<BR>
<P><EM> &gt; Only the ones who perform above average as supervisors get to reproduce. The
</EM><BR>
<EM> &gt; very best line of supervisors get appointed as governors, executive officers,
</EM><BR>
<EM> &gt; and overseers of the AI meritocracy.
</EM><BR>
&nbsp;
<BR>
You're making strange noises, but I don't know what they mean. Sounds
<BR>
all like instant desaster to me.
<BR>
<P><EM> &gt; &gt; You never got bitten by a dog? A maltreated dog? A crazied, sick dog?
</EM><BR>
<EM> &gt; &gt; Never, ever?
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; I've never been bitten by a dog, but I've heard about other people getting
</EM><BR>
<EM> &gt; bitten. Mailmen get bitten from time to time, but it's almost never, ever fatal.
</EM><BR>
<P>The difference is in this case is, that when a single dog bites a
<BR>
single human in the world, all people die instantly, and the world
<BR>
becomes the domain of dogs. You could also distribute a million remote
<BR>
controls for an Armageddon machine among the populace, instructing
<BR>
them not to press the button. How many seconds will it take on the
<BR>
average, until the world (as we know it) ceases to exist?
<BR>
<P><EM> &gt; Dog-level AI wouldn't be all that dangerous. Whether it takes five years to go
</EM><BR>
<P>That's what I said, but teenagers sometimes kill people,
<BR>
too. Teenagers are thought to be more intelligent than dogs. We're
<BR>
talking about a god with his own agenda. Sounds lots more dangerous
<BR>
than a teenager with a gun. Or a nuke. Or the launch codes.
<BR>
<P><EM> &gt; from dog-level AI to human-level AI or 500 million years to go from dog-level
</EM><BR>
<EM> &gt; natural intelligence to human level natural intelligence, along the way to the
</EM><BR>
<EM> &gt; higher levels of intelligence, evolution provides for behavior modification
</EM><BR>
<EM> &gt; which can accommodate the higher levels of intelligence. (Although one might
</EM><BR>
<EM> &gt; doubt this when considering the 5, 000 wars humans have fought in the last 3,000
</EM><BR>
<EM> &gt; years.)
</EM><BR>
<P>What if it takes five minutes for the seed AI to achieve god status,
<BR>
taking over the global networks? The ursoup did simmer for a long
<BR>
while, before the first autocatalytic set emerged by chance. Once
<BR>
emerged, the thing burned through the colossale turine like
<BR>
wildfire. The organic chemicals never knew what hit them. Most of them
<BR>
were metabolized, a few were assimilated.
<BR>
<P>A broken-out god (an ecology of gods and other critters, actually)
<BR>
will be very fast. Even assuming that it won't discover some very
<BR>
interesting physics in those endless minutes and hours it will have at
<BR>
it's disposal.
<BR>
<P><EM> &gt; Speaking of wars, the first fully functional dog-level robots will
</EM><BR>
<EM> &gt; probably be dogs of war.
</EM><BR>
&nbsp;
<BR>
Autonomous weapon platforms with the intelligence of a common house
<BR>
fly will be very, very deadly. Something as smart as a house kitty, oh
<BR>
my god.
<BR>
<P><EM> &gt; &gt; How strange, I thought it was the other way round. Statistical
</EM><BR>
<EM> &gt; &gt; properties, emergent properties, all things which you don't have to
</EM><BR>
<EM> &gt; &gt; deal with when all you have is a single specimen.
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; I think it would be as foolish to try to bring a single specimen up to
</EM><BR>
<P>True, that's why it will never happen.
<BR>
<P><EM> &gt; If all of the millions of dog-level AIs fall into positive autofeedback loops,
</EM><BR>
<P>What is the probability that all the air molecules in the room
<BR>
suddenly decide to assemble into just one corner of it, leaving you
<BR>
gasping in a hard vacuum? The probability that it happens is very low, 
<BR>
but not zero.
<BR>
<P>Now, if you have a billion of agents, what is the probability that all
<BR>
of them enter the runaway regime in the same minute, if not second?
<BR>
The probability for it is a lot higher than in the suddenly airless
<BR>
room example, but it is negligeably low.
<BR>
<P><EM> &gt; (don't forget, some are guard dogs, some are drug sniffing dogs, some are
</EM><BR>
<EM> &gt; seeing-eye dogs for the blind, some are sheep dogs, none are merely pets) that
</EM><BR>
<EM> &gt; would be wonderfully beneficial because it would make it much easier to train
</EM><BR>
<EM> &gt; them. Nobody wants a really stupid dog. The most intelligent animals (including
</EM><BR>
<P>No one wants a really stupid dog, that's the problem.
<BR>
<P><EM> &gt; humans) are generally and usually the most cooperative, and easy to train. The
</EM><BR>
<EM> &gt; dangerous criminal element is almost always average or below average in
</EM><BR>
<EM> &gt; intelligence. If the dog-level AI instantly falls into a positive autofeedback
</EM><BR>
<EM> &gt; loop (about as likely as a human falling into a positive bio-feedback loop and
</EM><BR>
<EM> &gt; becoming a super genius), you simply ask your family guard dog (now an SI) to
</EM><BR>
<P>You accelerate me by a factor of a million, and give me a female (the
<BR>
AI needs no such thing, and the AI is facultatively immortal), plus
<BR>
full access to the world's information, including my fully annotated
<BR>
genome (the AI will have that, and whatever the AI can come up with,
<BR>
which will be a lot). Then you give me a day. Almost three thousand
<BR>
years pass in my time frame, and I'm starting with everything you
<BR>
already have. One day passes in yours. Quite a day. Give me your year,
<BR>
and a million year pass in my time frame.
<BR>
<P>Use your imagination.
<BR>
<P><EM> &gt; continue its line of duty with hugely expanded responsibilities and obligations
</EM><BR>
<EM> &gt; to protect the family against hackers, attackers, and intruders of every kind.
</EM><BR>
<EM> &gt; The first thing your faithful SI pooch does might surprise you with its
</EM><BR>
<EM> &gt; brilliance. If you have money in the stock market, the SI would instantly check
</EM><BR>
<EM> &gt; to see if your SI stock broker's dog has acted to safeguard your investments
</EM><BR>
<EM> &gt; from any rogue market manipulators. The beautiful part is you'd not have to roll
</EM><BR>
<EM> &gt; up a newspaper to chastise the SI doggie, because it would have more reason to
</EM><BR>
<EM> &gt; offer behavior modification to *you*. What used to be your canine security guard
</EM><BR>
<EM> &gt; has transformed into your own personal genie.
</EM><BR>
<EM> &gt; If you don't trust high IQ, then you'd better kick Eliezer off this list before
</EM><BR>
<EM> &gt; he scuttles the enterprise.
</EM><BR>
<P>I have no trouble with Eliezer's project, as long as he doesn't start
<BR>
stealing from biology, or will start using evolutionary
<BR>
algorithms. Since he says he'll never will, I'm sleeping quite safely.
<BR>
<P><EM> &gt; [Don't forget dog-level AI spelled backwards is IA level-god.]
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; &gt; Yeah, *right*.
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; Stupidity, rather than too much intelligence, is the cause of most of the
</EM><BR>
<EM> &gt; world's problems. Just between us, friends, I'd place a hell of a lot more trust
</EM><BR>
<EM> &gt; in an SI than in any of the current candidates for political office. It seems to
</EM><BR>
<P>How much trust do ants put into humans? And humans essentially leave
<BR>
them alone.
<BR>
<P><EM> &gt; me that fear of malevolent SIs is misplaced. Having grown up (instantly) in a
</EM><BR>
<EM> &gt; free market environment, the first thing the SIs would have to do is make some
</EM><BR>
<EM> &gt; money. The SIs would have to curry favor with the right people (and more
</EM><BR>
<P>See, many people think like you. They think it's safe. That's why I'm worried.
<BR>
<P><EM> &gt; importantly with the right SIs) to acquire power. With a sufficiently high
</EM><BR>
<EM> &gt; number of SIs, they'd instantly figure out how best to create a stable and
</EM><BR>
<EM> &gt; prosperous SI community.
</EM><BR>
&nbsp;
<BR>
But does this also mean a stable and prosperous human community? I
<BR>
think it means almost instanteous extinction for the whole of the
<BR>
biology, us included, as a side effect of them going about their
<BR>
business.
<BR>
<P><EM> &gt; Life is profoundly unreasonable, utterly devoid of basic common sense
</EM><BR>
<P>Well, then we're about to die. And nothing we can do about it. Excuse
<BR>
me, there is this party I have to attend to. Life is short. You know.
<BR>
<P><EM> &gt; expectations. Life doesn't want to breed an AI. Life wants to breed millions of
</EM><BR>
<EM> &gt; them simultaneously and instantly.
</EM><BR>
<P>Not in this universe. Nucleation processes are rare, but when they
<BR>
occur they restructure the landscape entirely. We seem to be at the
<BR>
threshold of such event, and we're smart enough to realize that
<BR>
something is coming. As a whole, we can't do anything about it,
<BR>
apparently.
<BR>
<P><EM> &gt; Life is really weird that way. For example, Sasha Chislenko had a much better
</EM><BR>
<EM> &gt; life than I have. Yet he apparently killed himself, and I'm still plugging along
</EM><BR>
<EM> &gt; optimistically hoping for things to get better. [perhaps he found life too time
</EM><BR>
<EM> &gt; consuming] Now, if a smart and savvy guy like him decides to chuck it in, while
</EM><BR>
<EM> &gt; underachieving jerks like me keep merrily cruisin' along, hey! maybe I'm missing
</EM><BR>
<EM> &gt; something; perhaps he knew something (or his depression revealed something to
</EM><BR>
<EM> &gt; him) that I don't know. Maybe I should join the billions of grateful dead folks.
</EM><BR>
<P>Neurochemistry is a fluke. Whether you win, or you lose is not
<BR>
something which you can influence a lot.
<BR>
<P><EM> &gt; Anyway, we all have to die sometime. Why not die bringing a higher life form
</EM><BR>
<EM> &gt; into existence? Look how many fools throw their lives away on idiotic notions
</EM><BR>
<P>Sorry, I'd rather become that higher life form than die by being
<BR>
stupid enough to make that life form before it's own due time.
<BR>
<P><EM> &gt; like patriotism and honor. I think Morovec was brilliant when he said (in Palo
</EM><BR>
<EM> &gt; Alto) that the most responsible thing we can do is to build artificial life,
</EM><BR>
<EM> &gt; especially if it surpasses us in cognitive capability. The riskiest thing you
</EM><BR>
<P>Moravec is brilliant, but he's also a raving psychotic looniac. You
<BR>
did hear what was (to his obvious discomfort) being cited from one of
<BR>
his less formal interviews? He can die merrily for what I care, this
<BR>
doesn't mean I (and people near to me) will have to die because Herr
<BR>
Moravec thinks that artifical life is cool. I also think that
<BR>
artificial life is very cool, but not cool enough to warrant our
<BR>
extinction. Especially is this extinction is potentially preventable,
<BR>
by tweaking the boundary conditions in the right fashion.
<BR>
<P><EM> &gt; can do is to not take any risks. The best insurance against some mad scientist
</EM><BR>
<EM> &gt; building an AI is to have thousands of responsible and reasonable scientists
</EM><BR>
<EM> &gt; building millions of AIs.
</EM><BR>
<P>If you take a thousand of obviously responsible and reasonable
<BR>
scientists, you have *at least* 10 raving looniacs among them, and
<BR>
even responsible and reasonable scientists do make mistakes,
<BR>
especially if you have a thousand of them working independently. Or a
<BR>
hundred thousand.
<BR>
<P><EM> &gt; Lots of people didn't want humans to build nuclear weapons. Too dangerous. But
</EM><BR>
<P>Nuclear weapons? Fiddlesticks. Who cares about a global nuclear war,
<BR>
it still won't kill humanity sustainably.
<BR>
<P><EM> &gt; if it weren't for nuclear weapons, I'd likely have died in my twenties fighting
</EM><BR>
<EM> &gt; a war with the USSR or some other maniacal regime. Nuclear weapons have been an
</EM><BR>
<P>During the Cuba crisis it was due to restraint of a single mere man
<BR>
against strong insistance of his advisors that my and your parents did
<BR>
not become grilled radioactive steak. It was a very, very close
<BR>
escape.
<BR>
<P>But as I said, who cares about nuclear weapons. Or even engineered
<BR>
biological weapons. All they can do is kill billions of people. But
<BR>
not every single one of them.
<BR>
<P><EM> &gt; effective deterrent to another world war. Multitudes of SIs will be perhaps the
</EM><BR>
<EM> &gt; only deterrent to fanatic religious retards (in the Middle East and dozens of
</EM><BR>
<EM> &gt; other hot spots) dragging the world into another global conflagration.
</EM><BR>
<P>You would be funny, if I didn't think you were serious.
<BR>
&nbsp;
<BR>
<EM> &gt; &gt; Socialize a god.
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; I have a better idea: Let a million gods socialize us.
</EM><BR>
<P>Then you're willing to commit your suicide, and homicide on countless
<BR>
people around the world.
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4481.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4479.html">Eugene Leitl: "corn again"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4429.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="4502.html">Jason Joel Thompson: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4543.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4480">[ date ]</A>
<A HREF="index.html#4480">[ thread ]</A>
<A HREF="subject.html#4480">[ subject ]</A>
<A HREF="author.html#4480">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:37:26 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
