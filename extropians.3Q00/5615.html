<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Samantha Atkins (samantha@objectent.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Sun Sep 24 18:45:31 2000" -->
<!-- isoreceived="20000925004531" -->
<!-- sent="Sun, 24 Sep 2000 17:47:28 -0700" -->
<!-- isosent="20000925004728" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="39CEA0A0.3717C0EB@objectent.com" -->
<!-- inreplyto="39CE29F3.17802BB@pobox.com" -->
<STRONG>From:</STRONG> Samantha Atkins (<A HREF="mailto:samantha@objectent.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;39CEA0A0.3717C0EB@objectent.com&gt;"><EM>samantha@objectent.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Sun Sep 24 2000 - 18:47:28 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5616.html">hal@finney.org: "Re: GUNS: Why here?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5614.html">Samantha Atkins: "Re: Mostly stuff about software (was Homeless + Jobs, Lots of stuff  about Software world)"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5575.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="5628.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5632.html">Franklin Wayne Poley: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5615">[ date ]</A>
<A HREF="index.html#5615">[ thread ]</A>
<A HREF="subject.html#5615">[ subject ]</A>
<A HREF="author.html#5615">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
&quot;Eliezer S. Yudkowsky&quot; wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Samantha Atkins wrote:
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; &quot;Eliezer S. Yudkowsky&quot; wrote:
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; &gt; Oh, why bother.  I really am starting to get a bit frustrated over here.  I've
</EM><BR>
<EM>&gt; &gt; &gt; been talking about this for weeks and it doesn't seem to have any effect
</EM><BR>
<EM>&gt; &gt; &gt; whatsoever.  Nobody is even bothering to distinguish between subgoals and
</EM><BR>
<EM>&gt; &gt; &gt; supergoals.  You're all just playing with words.
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; Hey!  Wait a second.  If you are going to be a leader in the greatest
</EM><BR>
<EM>&gt; &gt; project in human history (or in any project for that matter) you have to
</EM><BR>
<EM>&gt; &gt; learn and learn damn fast to be able to motivate and enroll people.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; No, actually I should expect that the seed AI project will have smaller total
</EM><BR>
<EM>&gt; expenditures, frcom start to finish, than a typical major corporation's Y2K
</EM><BR>
<EM>&gt; projects.  I used to think in terms of the greatest project in human history,
</EM><BR>
<EM>&gt; but I no longer think that'll be necessary, and a damn good thing, as I don't
</EM><BR>
<EM>&gt; think we're gonna *get* the largest budget in human history.
</EM><BR>
<P><P>Huh?  I wasn't using &quot;greatest&quot; in respect to budget or size of
<BR>
development team (although I think both will be greater than you
<BR>
think).  I was using it in terms of the criticality of this project. 
<BR>
You continuously tell us it is our only hope.  It is difficult to
<BR>
imagine a project much &quot;greater&quot; than that.
<BR>
<P><EM>&gt; 
</EM><BR>
<EM>&gt; &gt; You need other human
</EM><BR>
<EM>&gt; &gt; beings to understand enough to keep you from getting lynched or shut
</EM><BR>
<EM>&gt; &gt; down for trying such a thing.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Yes.  A finite and rather small number of human beings, most of whom will
</EM><BR>
<EM>&gt; almost certainly Get It on the first try.  If, hypothetically, I were a
</EM><BR>
<EM>&gt; pessimistic and cynical person, then I were start saying things like:  &quot;And if
</EM><BR>
<EM>&gt; I spend time talking to anyone else, then that just increases the probability
</EM><BR>
<EM>&gt; that I'll get lynched or shut down.  Foresight tried to play nicey-nice with
</EM><BR>
<EM>&gt; everyone, as a result of which they are now being screwed over by the National
</EM><BR>
<EM>&gt; Nanotechnology Research Initiative.&quot;
</EM><BR>
<EM>&gt;
</EM><BR>
<P>I think that is a highly questionable attitude.  I do not believe that
<BR>
even yourself &quot;got it&quot; on the first try.  Nor do I believe the evolution
<BR>
of the idea is complete and finished for anyone to get in one try. 
<BR>
Frankly I think this part of your thinking is dangerously immature.
<BR>
&nbsp;&nbsp;
<BR>
<EM>&gt; &gt; You are so brilliant is so many ways but I think you have a lot to learn
</EM><BR>
<EM>&gt; &gt; about reaching and working with people.  The success of the project
</EM><BR>
<EM>&gt; &gt; depends hugely on you learning that.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I wish I knew more about dealing with people, but I no longer give it as high
</EM><BR>
<EM>&gt; a priority as I once did.
</EM><BR>
<EM>&gt; 
</EM><BR>
<P>How can that be anything but a mistake when you require people, since
<BR>
they are the only intelligences to use in getting this thing off the
<BR>
ground, and their resources in order to produce the Seed?  Do you really
<BR>
believe that all of those you need will just automatically think enough
<BR>
like you or agree enough with your conclusions that little/no effort is
<BR>
necessary on your part to understand and deal with them further?   What
<BR>
kind of model leads you to this conclusion?
<BR>
<P>- samantha
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5616.html">hal@finney.org: "Re: GUNS: Why here?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5614.html">Samantha Atkins: "Re: Mostly stuff about software (was Homeless + Jobs, Lots of stuff  about Software world)"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5575.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="5628.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5632.html">Franklin Wayne Poley: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5615">[ date ]</A>
<A HREF="index.html#5615">[ thread ]</A>
<A HREF="subject.html#5615">[ subject ]</A>
<A HREF="author.html#5615">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:49 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
