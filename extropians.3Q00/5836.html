<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="J. R. Molloy (jr@shasta.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Tue Sep 26 12:46:52 2000" -->
<!-- isoreceived="20000926184652" -->
<!-- sent="Tue, 26 Sep 2000 10:30:00 -0700" -->
<!-- isosent="20000926173000" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="005c01c027ea$2208b380$8cbc473f@jrmolloy" -->
<!-- inreplyto="14800.42000.300652.193613@lrz.uni-muenchen.de" -->
<STRONG>From:</STRONG> J. R. Molloy (<A HREF="mailto:jr@shasta.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;005c01c027ea$2208b380$8cbc473f@jrmolloy&gt;"><EM>jr@shasta.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Tue Sep 26 2000 - 11:30:00 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5837.html">Michael S. Lorrey: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5835.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5814.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5836">[ date ]</A>
<A HREF="index.html#5836">[ thread ]</A>
<A HREF="subject.html#5836">[ subject ]</A>
<A HREF="author.html#5836">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Eugene Leitl writes,
<BR>
<P><EM>&gt; You can't reciprocate to a god in any meaningful way. Tell me why a
</EM><BR>
<EM>&gt; god should consider us anything else than a feature of the landscape?
</EM><BR>
<P>With our prosthetic limbs, titanium hips, and artificial eyes, we are already
<BR>
beginning to resemble our machines. A hybrid species of human and robot with
<BR>
intelligence vastly superior to that of purely biological mankind is what many
<BR>
extropy-oriented thinkers envision.
<BR>
<P>&quot;Nanobot technology will provide fully immersive, totally convincing virtual
<BR>
reality in the following way. The nanobots take up positions close to every
<BR>
interneuronal connection coming from all of our biological sensory receptors
<BR>
(e.g., eyes, ears, skin). We already have technology for electronic devices to
<BR>
communicate with neurons in both directions that requires no direct physical
<BR>
contact with the neurons. For example, scientists at the Max Planck Institute in
<BR>
Heidelberg, Germany, have developed &quot;neuron transistors&quot; that can detect the
<BR>
firing of a nearby neuron, or alternatively, can cause a nearby neuron to fire
<BR>
or suppress it from firing. This amounts to two-way communication between
<BR>
neurons and the electronic-based neuron transistors.&quot; --Ray Kurzweil
<BR>
<P>When I become a god (or when you do), why should I (or you) consider those who
<BR>
have not done so as anything else than a feature of the landscape? Because godly
<BR>
intelligence includes compassion. In fact, as Jiddu Krishnamurti has pointed
<BR>
out, fully mature intelligence does not exist without compassion.
<BR>
<P>A god must be sane. A synthetic god must have synthetic sanity. High-quality
<BR>
sanity would (in contrast to low-quality sanity) exhibit robust adaptability and
<BR>
resilience. In addition, it would tolerate exposure to various chemical and
<BR>
physiological insults as well as intense stimulation via sensory input (and
<BR>
sensory deprivation).
<BR>
<P>An AI would want to be friendly because that's the sane thing to do. So, what do
<BR>
we mean by sanity?
<BR>
<P>The need to define sanity indicates a desire to prove ourselves sane.
<BR>
<P>But the whole history of humanity proves that humanity lacks sanity as much
<BR>
as it lacks anything. Basically, humanity proves itself insane by fighting
<BR>
five thousand wars in the last three thousand years. The greed, jealousy,
<BR>
and selfishness that prevails throughout human social orders indicates the
<BR>
dominance of something other than sanity.
<BR>
<P>Sane people exult in doing what comes naturally, in satisfying a congenital
<BR>
appetite for the beauty of reason, the thrill of understanding, the
<BR>
excitement of knowing reality directly.
<BR>
<P>A wise man once defined sanity as &quot;coming out of the mind into the open,
<BR>
into the silence, where no thought, no desire disturbs you. In that pool of
<BR>
silence, with not a single ripple upon it, sanity abides.&quot;
<BR>
<P>Sanity means to secure your happiness, and by extension, the happiness of
<BR>
your neighbors, family, friends, and community, since one cannot really feel
<BR>
happy when surrounded by misery.
<BR>
<P>When your means fulfill your ends, if you can manage that -- the harmony
<BR>
between the means and the end -- then you've got sanity. When your means
<BR>
don't satisfy your end, and you continue to repeat the same means, never
<BR>
arriving at a gratifying end, then you've got a neurosis. &quot;Make every act of
<BR>
your life dedicated to love and sanity arises out of it,&quot; a romantic man has
<BR>
said.
<BR>
<P>When liberated people want to play the game of sanity, they live as sanely
<BR>
as anyone can live. Then they can outdo Aristotle with logic and reason.
<BR>
They can follow every rule and regulation. But if they want to play the
<BR>
lunatic game, then they can become as demagogic, theologic, and ideologic as
<BR>
any other maniac.
<BR>
<P>The perfectly sane brain thinks only when necessary. It reverts to ecstatic
<BR>
tranquility (i.e., it allows  thought-noise to dissipate, clearing the way
<BR>
for the emergence of superlative sentience or learning) whenever it can, for
<BR>
as long as it can. Conversely, the insane brain cannot stop chattering.
<BR>
<P>A psychologist asked a psychonomer, &quot;How do you deal with neurotics?&quot;
<BR>
<P>The psychonomer replied, &quot;We trap them?&quot;
<BR>
<P>&quot;Oh? And how do you do that?&quot; asked the psychologist.
<BR>
<P>&quot;By making it impossible for them to ask any more questions,&quot; answered the
<BR>
psychonomer.
<BR>
<P>Do you think it is sanity when half the human race is dying of starvation? You
<BR>
ask why a god should consider us anything else than a feature of the landscape.
<BR>
Yet *we* consider half the human race a feature of the landscape! What you're
<BR>
doing is projecting your own callous attitude onto the AI. Don't make the AI so
<BR>
cheap! Don't sell the AI short. With its greater intelligence will come greater
<BR>
compassion, and therefore greater sanity.
<BR>
<P>With the amplified intelligence of AI/SI comes liberating truth. Meditate on
<BR>
these things, and discover that creativity, compassion, consciousness, and
<BR>
especially sanity accompany vastly expanded intelligence. Otherwise, it is
<BR>
lunacy to even think of evolving an AI. From crazed and callous computer
<BR>
scientists comes crazed and callous AI.
<BR>
<P>The reason we ask why an AI would want to be friendly is that we are not sure
<BR>
why *we* would want to be friendly. As the friendliness of the extropy e-mail
<BR>
list increases, so too will our confidence increase in the friendliness of the
<BR>
AI.
<BR>
<P>--J. R.
<BR>
<P>&quot;It's haughty of us to think we're the end product of evolution. All of us are a
<BR>
part of producing whatever is coming next. We're at an exciting time. We're
<BR>
close to the singularity. Go back to that litany of chemistry leading to
<BR>
single-celled organisms, leading to intelligence. The first step took a billion
<BR>
years, the next step took a hundred million, and so on. We're at a stage where
<BR>
things change on the order of decades, and it seems to be speeding up.
<BR>
Technology has the autocatalytic effect of fast computers, which let us design
<BR>
better and faster computers faster. We're heading toward something which is
<BR>
going to happen very soon -- in our lifetimes -- and which is fundamentally
<BR>
different from anything that's happened in human history before.&quot;
<BR>
--W. Daniel Hillis
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5837.html">Michael S. Lorrey: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5835.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5814.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5836">[ date ]</A>
<A HREF="index.html#5836">[ thread ]</A>
<A HREF="subject.html#5836">[ subject ]</A>
<A HREF="author.html#5836">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:11 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
