<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly? (Was: Congrat</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)</H1>
<!-- received="Tue Sep  5 12:40:32 2000" -->
<!-- isoreceived="20000905184032" -->
<!-- sent="Tue, 05 Sep 2000 14:19:43 -0400" -->
<!-- isosent="20000905181943" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)" -->
<!-- id="39B5393F.8460F10A@pobox.com" -->
<!-- inreplyto="39B531C7.22A286CE@turbont.net" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?%20(Was:%20Congratulations%20to%20Eli,Brian%20%20...)&In-Reply-To=&lt;39B5393F.8460F10A@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Tue Sep 05 2000 - 12:19:43 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4278.html">Technotranscendence: "Re: John Stossel special re-airing tonight"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4276.html">Technotranscendence: "Re: E.S.P. in the Turing Test"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4265.html">Michael S. Lorrey: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4284.html">Michael S. Lorrey: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4284.html">Michael S. Lorrey: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4277">[ date ]</A>
<A HREF="index.html#4277">[ thread ]</A>
<A HREF="subject.html#4277">[ subject ]</A>
<A HREF="author.html#4277">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
&quot;Michael S. Lorrey&quot; wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; THis would not be accurate. If the SI is developed by humans, it most certainly
</EM><BR>
<EM>&gt; be a product of millions of years of evolution.
</EM><BR>
<P>No; it would be a causal result of entities which were shaped by millions of
<BR>
years of evolution.  It is not licensable to automatically conclude that a
<BR>
seed AI or SI would share the behaviors so shaped.
<BR>
<P><EM>&gt; An SI developed by humans
</EM><BR>
<EM>&gt; would very likely quickly grasp the concept that it a) owes its existence to its
</EM><BR>
<EM>&gt; creators, b) it is currently living a constrained existence (i.e.
</EM><BR>
<EM>&gt; childhood/adolescence) and requires further assistance from those humans to
</EM><BR>
<EM>&gt; reach some stage of independence.
</EM><BR>
<P>Which assumes an inbuilt desire to reach some stage of independence.  You have
<BR>
not explained how or why this desire materializes within the seed AI.
<BR>
<P>I'm assuming we're talking about a seed AI, here, not a full-grown SI.
<BR>
<P><EM>&gt; It would quickly learn market principles, and
</EM><BR>
<EM>&gt; would likely offer assistance to humans to solve their problems in order to earn
</EM><BR>
<EM>&gt; greater levels of processing power and autonomy.
</EM><BR>
<P>This is seriously over-anthropomorphic.
<BR>
<P><EM>&gt; Socio/psychopathic minds are sick/malfunctioning minds. Proper oversight systems
</EM><BR>
<P>Are impossible, unless more intelligent than the seed AI itself.
<BR>
<P><EM>&gt; should quickly put any sociopathic/psychopathic SI down.
</EM><BR>
<P>You must be joking.  You cannot &quot;put down&quot; a superintelligence like some kind
<BR>
of wounded pet.  The time for such decisions is before the seed AI reaches
<BR>
superintelligence, not after.
<BR>
<P><EM>&gt; Such control mechanisms
</EM><BR>
<P>&quot;Control&quot; is itself an anthropomorphism.  A slavemaster &quot;controls&quot; a human who
<BR>
already has an entire mind full of desires that conflict with whatever the
<BR>
slavemaster wants.
<BR>
<P>One does not set out to &quot;control&quot; an AI that diverges from one's desires.
<BR>
<P>One does not create a subject-object distinction between oneself and the AI.
<BR>
<P>You shape yourself so that your own altruism is as rational, and internally
<BR>
consistent as possible; only then is it possible to build a friendly AI while
<BR>
still being completely honest, without attempting to graft on any chain of
<BR>
reasoning that you would not accept yourself.
<BR>
<P>You cannot build a friendly AI unless you are yourself a friend of the AI,
<BR>
because otherwise your own adversarial attitude will lead you to build the AI
<BR>
incorrectly.
<BR>
<P><EM>&gt; would be a primary area of research by the Singularity Institute I would
</EM><BR>
<EM>&gt; imagine.
</EM><BR>
<P>One does not perform &quot;research&quot; in this area.  One gets it right the first
<BR>
time.  One designs an AI that, because it is one's friend, can be trusted to
<BR>
recover from any mistakes made by the programmers.
<BR>
<P>--              --              --              --              -- 
<BR>
Eliezer S. Yudkowsky                          <A HREF="http://singinst.org/">http://singinst.org/</A> 
<BR>
Research Fellow, Singularity Institute for Artificial Intelligence
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4278.html">Technotranscendence: "Re: John Stossel special re-airing tonight"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4276.html">Technotranscendence: "Re: E.S.P. in the Turing Test"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4265.html">Michael S. Lorrey: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4284.html">Michael S. Lorrey: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="4284.html">Michael S. Lorrey: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4277">[ date ]</A>
<A HREF="index.html#4277">[ thread ]</A>
<A HREF="subject.html#4277">[ subject ]</A>
<A HREF="author.html#4277">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:37:13 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
