<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Sun Sep 24 01:31:16 2000" -->
<!-- isoreceived="20000924073116" -->
<!-- sent="Sun, 24 Sep 2000 03:29:40 -0400" -->
<!-- isosent="20000924072940" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="39CDAD64.2123E6D4@pobox.com" -->
<!-- inreplyto="F146oCZVfUMZLrVvQh500003171@hotmail.com" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;39CDAD64.2123E6D4@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Sun Sep 24 2000 - 01:29:40 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5561.html">Randy Smith: "Re: Fear of Letting People Get Things They Want"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5559.html">Darin Sunley: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5557.html">Zero Powers: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="5563.html">Samantha Atkins: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5566.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5560">[ date ]</A>
<A HREF="index.html#5560">[ thread ]</A>
<A HREF="subject.html#5560">[ subject ]</A>
<A HREF="author.html#5560">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Zero Powers wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Based upon your belief, I presume, that AI will be so completely unlike
</EM><BR>
<EM>&gt; humanity that there is no way we can even begin to imagine the AI's
</EM><BR>
<EM>&gt; thoughts, values and motivations?
</EM><BR>
<P>You can't put yourself in the AI's shoes!  I'm not saying that you can't
<BR>
understand it at all; I'm saying that you cannot say &quot;Imagine what YOU would
<BR>
do in that situation&quot; when you're dealing with motivational effects.  *IF*
<BR>
you've identified *ALL* your assumptions than you *MAY* be able to get away
<BR>
with &quot;putting yourself in the AI's shoes&quot; if you're trying to decide whether a
<BR>
given chain of subgoals-given-a-supergoal is stupid or smart.  And when I say
<BR>
&quot;you&quot;, I mean &quot;me and Mitchell Porter and maybe three or four other people&quot;.
<BR>
<P>If you're wondering whether the Sysop will exterminate humanity because
<BR>
someone asked it to solve the Riemann Hypothesis, which it does because it's
<BR>
friendly, and then it needs the extra resource space so it erases humanity -
<BR>
then it turns out that yes, the put-yourself-in-the-AI's-shoes happens to work
<BR>
for this particular case - exterminating someone in order to serve a subgoal
<BR>
of being friendly is outright stupid.  But it's not intrinsically stupid - if
<BR>
the AI were directly programmed to solve the Riemann Hypothesis, then
<BR>
something along those lines would be a reasonable outcome.  Do you see what
<BR>
I'm saying here?  &quot;Put yourself in the AI's shoes&quot; only works for identifying
<BR>
subgoals-given-supergoals, not identifying the supergoals themselves.
<BR>
<P>Oh, why bother.  I really am starting to get a bit frustrated over here.  I've
<BR>
been talking about this for weeks and it doesn't seem to have any effect
<BR>
whatsoever.  Nobody is even bothering to distinguish between subgoals and
<BR>
supergoals.  You're all just playing with words.
<BR>
<P>--              --              --              --              -- 
<BR>
Eliezer S. Yudkowsky                          <A HREF="http://singinst.org/">http://singinst.org/</A> 
<BR>
Research Fellow, Singularity Institute for Artificial Intelligence
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5561.html">Randy Smith: "Re: Fear of Letting People Get Things They Want"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5559.html">Darin Sunley: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5557.html">Zero Powers: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="5563.html">Samantha Atkins: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5566.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5560">[ date ]</A>
<A HREF="index.html#5560">[ thread ]</A>
<A HREF="subject.html#5560">[ subject ]</A>
<A HREF="author.html#5560">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:47 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
