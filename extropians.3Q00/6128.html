<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Fwd: Earthweb from transadmin</TITLE>
<META NAME="Author" CONTENT="Samantha Atkins (samantha@objectent.com)">
<META NAME="Subject" CONTENT="Re: Fwd: Earthweb from transadmin">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Fwd: Earthweb from transadmin</H1>
<!-- received="Fri Sep 29 19:17:05 2000" -->
<!-- isoreceived="20000930011705" -->
<!-- sent="Fri, 29 Sep 2000 18:19:23 -0700" -->
<!-- isosent="20000930011923" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Fwd: Earthweb from transadmin" -->
<!-- id="39D53F9B.85786F4@objectent.com" -->
<!-- inreplyto="39CCD4D7.A5E36584@pobox.com" -->
<STRONG>From:</STRONG> Samantha Atkins (<A HREF="mailto:samantha@objectent.com?Subject=Re:%20Fwd:%20Earthweb%20from%20transadmin&In-Reply-To=&lt;39D53F9B.85786F4@objectent.com&gt;"><EM>samantha@objectent.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Fri Sep 29 2000 - 19:19:23 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="6129.html">Andy Toth: "Re: Robots, but philosophers (or, Hal-2001)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6127.html">Spike Jones: "Re: Asian Swamp Eels"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5508.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5258.html">Matt Gingell: "Re: Fwd: Earthweb from transadmin"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6128">[ date ]</A>
<A HREF="index.html#6128">[ thread ]</A>
<A HREF="subject.html#6128">[ subject ]</A>
<A HREF="author.html#6128">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
&quot;Eliezer S. Yudkowsky&quot; wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Samantha Atkins wrote:
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; You are right that human goals are not uniformly friendly to human
</EM><BR>
<EM>&gt; &gt; beings.  But I would tend to agree with the POV that an intelligence
</EM><BR>
<EM>&gt; &gt; build on or linking human intelligence and automating their interaction,
</EM><BR>
<EM>&gt; &gt; sharing of knowledge, finding each other and so on would be more likely
</EM><BR>
<EM>&gt; &gt; to at least majorly empathize with and understand human beings.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Samantha, you have some ideas about empathy that are flat wrong.  I really
</EM><BR>
<EM>&gt; don't know how else to say it.  You grew up on a human planet and you got some
</EM><BR>
<EM>&gt; weird ideas.
</EM><BR>
<P>Hey, so did you.  What makes you think that your supposedly different
<BR>
ideas about empathy are more correct when they have not even been tested
<BR>
in the laboratory of real entities interacting?  
<BR>
<P><EM>&gt; 
</EM><BR>
<EM>&gt; The *programmer* has to think like the AI to empathize with the AI.  The
</EM><BR>
<EM>&gt; converse isn't true.  Different cognitive architectures.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>There is no way to &quot;think like the AI&quot; as we in very large ways do not
<BR>
yet know much about how to build such an AI much less how it will
<BR>
think.  You have some theories about this but there is nothing
<BR>
preventing them from being quite far from how what you propose will
<BR>
actually think.
<BR>
<P>&nbsp;
<BR>
<EM>&gt; &gt; Why would an Earthweb use only humans as intelligent components?  The
</EM><BR>
<EM>&gt; &gt; web could have a lot of non-human agents and logic processors and other
</EM><BR>
<EM>&gt; &gt; specialized gear.  Some decisions, particularly high speed ones and ones
</EM><BR>
<EM>&gt; &gt; requiring major logic crunching, might increasingly not be made
</EM><BR>
<EM>&gt; &gt; explicitly by human beings.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Then either you have an independent superintelligence, or you have a process
</EM><BR>
<EM>&gt; built from human components.  No autonomic process, even one requiring &quot;major
</EM><BR>
<EM>&gt; logic crunching&quot;, qualifies as &quot;intelligence&quot; for these purposes.  A thought
</EM><BR>
<EM>&gt; requires a unified high-bandwidth brain in order to exist.  You cannot have a
</EM><BR>
<EM>&gt; thought spread across multiple brains, not if those brains are separated by
</EM><BR>
<EM>&gt; the barriers of speech.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>You have a process that includes human components for those things that
<BR>
humans are better at than the increasingly autonomous and powerful
<BR>
software agents and AIs.  I disagree that some of these AI and agent
<BR>
components do not qualify as intelligent.  Not fully human level or not
<BR>
self-conscious yes, but that is not essential to intelligence as a
<BR>
general term.  These non-human components of the process can have
<BR>
unified minds and thought-trains.  Just a bit more limited and not
<BR>
self-conscious.  Are you defining all thought as requiring
<BR>
self-awareness?  
<BR>
<P>If a group of humans cooperate on a major project do they have a unified
<BR>
mind although all aspects of the problem are not literally in some group
<BR>
overmind?
<BR>
Do they need this unification or overmind for thought to be present?
<BR>
<P><P><EM>&gt; Remember, the default rule for &quot;folk cognitive science&quot; is that you see only
</EM><BR>
<EM>&gt; thoughts and the interactions of thoughts.  We don't have built-in perceptions
</EM><BR>
<EM>&gt; for the neural source code, the sensory modalities, or the contents of the
</EM><BR>
<EM>&gt; concept level.  And if all you see of the thoughts is the verbal traceback,
</EM><BR>
<EM>&gt; then you might think that the Earthweb was thinking.  But three-quarters of
</EM><BR>
<EM>&gt; the complexity of thought is in the underlying substrate, and that substrate
</EM><BR>
<EM>&gt; can't emerge accidentally - it doesn't show up even in &quot;major logic
</EM><BR>
<EM>&gt; crunching&quot;.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>Actually we do have some ability to introspect at the concept level and
<BR>
have managed to ferret out some of how all of these levels work.  We
<BR>
can't see them directly at this  time.  But that can (and given enough
<BR>
time will) change.  I doubt the AI would choose to see these levels very
<BR>
often or even necessarily initially have or give itself this ability. 
<BR>
Some of the low-level processes simply do not require the substantial
<BR>
overhead imposed by self-awareness.  The AI can examine these things in
<BR>
more detail if so designed (or if it wishes to modify itself to do so). 
<BR>
Thinking does not require being self-aware within all these substrates. 
<BR>
Or do you wish to propose that humans cannot think?
<BR>
&nbsp;
<BR>
<EM>&gt; There is no &quot;full confidence&quot; here.  Period.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; That said, the Earthweb can't reach superintelligence.  If the Earthweb
</EM><BR>
<EM>&gt; *could* reach superintelligence than I would seriously have a harder time
</EM><BR>
<EM>&gt; visualizing the Earthweb-process than I would with seed AI.  Just because the
</EM><BR>
<EM>&gt; Earthweb has human components doesn't make the system behavior automatically
</EM><BR>
<EM>&gt; understandable.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>Agreed. 
<BR>
&nbsp;
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &gt; Frankly I don't know how you can with a straight face
</EM><BR>
<EM>&gt; &gt; say that a superintelligence is solid when all you have is a lot of
</EM><BR>
<EM>&gt; &gt; theory and your own basically good intentions and optimism.  That isn't
</EM><BR>
<EM>&gt; &gt; very solid in the development world I inhabit.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Wrong meaning of the term &quot;solid&quot;, sorry for using something ambiguous.  Not
</EM><BR>
<EM>&gt; &quot;solid&quot; as in &quot;easy to develop&quot;.  (We're assuming that it's been developed and
</EM><BR>
<EM>&gt; discussing the status afterwards.)  &quot;Solid&quot; as in &quot;internally stable&quot;.
</EM><BR>
<EM>&gt; Superintelligence is one of the solid attractors for a planetary technological
</EM><BR>
<EM>&gt; civilization.  The other solid attractor is completely destroying all the life
</EM><BR>
<EM>&gt; on the planet (if a single bacterium is left, it can evolve again, so that's
</EM><BR>
<EM>&gt; not a stable state).
</EM><BR>
<EM>&gt;
</EM><BR>
<P>So, you are claiming that only those two are solid attractors.  That is
<BR>
a claim but it is not well support and I disagree.  At the least I claim
<BR>
that the SI is not inherently less likely to destroy us than is not
<BR>
having an early SI.  
<BR>
&nbsp;
<BR>
<EM>&gt; Now, is the Earthweb solid?  In that million-year sense?
</EM><BR>
<EM>&gt;
</EM><BR>
<P>Your million year stability of the SI is a fantasy based on a lot of
<BR>
unproven assumptions.  Again, you claim it is utterly stable.  But
<BR>
without proof or at least much better reasoning/discussion the claim is
<BR>
worthless and not a motivation for rushing to build the SI.
<BR>
<P>&nbsp;
<BR>
<EM>&gt; &gt; I hear you but I'm not quite ready to give up on the human race at as a
</EM><BR>
<EM>&gt; &gt; bad job
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Do you understand that your sentimentality, backed up with sufficient power,
</EM><BR>
<EM>&gt; could easily kill you and could as easily kill off the human species?
</EM><BR>
<EM>&gt;
</EM><BR>
<P>So could assuming that the SI is our only hope and turning away from
<BR>
what can be done even without an SI.  So this also is not a real
<BR>
argument.
<BR>
<P><P><EM>&gt;
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Try everything, I agree.  I happen to think that the AI is the most important
</EM><BR>
<EM>&gt; thing and the most likely to win.  I'm not afraid to prioritize my eggs.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>Fair enough.  But I don't get the insistence that you are right and all
<BR>
other options are really more likely to get us all killed.
<BR>
<P>&nbsp;
<BR>
<EM>&gt; &gt; So instead you think a small huddle of really bright people will solve
</EM><BR>
<EM>&gt; &gt; or make all the problems of the ages moot by creating in effect a
</EM><BR>
<EM>&gt; &gt; Super-being, or at least its Seed.  How is this different from the old
</EM><BR>
<EM>&gt; &gt; &quot;my nerds are smarter than your nerds and we will win&quot; sort of
</EM><BR>
<EM>&gt; &gt; mentality?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; You're asking the wrong question.  The question is whether we can win.
</EM><BR>
<EM>&gt; Mentalities are ever-flexible and can be altered; if we need a particular
</EM><BR>
<EM>&gt; mentality to win, that issue isn't entirely decoupled from strategy but it
</EM><BR>
<EM>&gt; doesn't dictate strategy either.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>Yes.  I asked the worng question.  I meant to ask why you believe such
<BR>
an audacious and urgent goal is best served by having a small huddle of
<BR>
brilliant people work on the goal in relative isolation (versus open
<BR>
source, sharing of results, larger efforts, etc.)
<BR>
<P>&nbsp;
<BR>
<EM>&gt; &gt; By what right will you withhold behind close doors all the
</EM><BR>
<EM>&gt; &gt; tech leading up to your success that could be used in so many fruitful
</EM><BR>
<EM>&gt; &gt; ways on the outside?  If you at least stayed open you would enable the
</EM><BR>
<EM>&gt; &gt; tech to be used on other attempts to make the coming transitions
</EM><BR>
<EM>&gt; &gt; smoother.  What makes you think that you and your nerds are so good that
</EM><BR>
<EM>&gt; &gt; you will see everything and don't need the eyeballs of other nerds
</EM><BR>
<EM>&gt; &gt; outside your cabal at all?  This, imho, is much more arrogant than
</EM><BR>
<EM>&gt; &gt; creating the Seed itself.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; The first reason I turned away from open source is that I started thinking
</EM><BR>
<EM>&gt; about the ways that the interim development stages of seed AI could be
</EM><BR>
<EM>&gt; misused.  Not &quot;misused&quot;, actually, so much as the miscellaneous economic
</EM><BR>
<EM>&gt; fallout.  If we had to live on this planet for fifty years I'd say to heck
</EM><BR>
<EM>&gt; with it, humanity will adjust - but as it is, that whole scenario can be
</EM><BR>
<EM>&gt; avoided, especially since I'm no longer sure an AI industry would help that
</EM><BR>
<EM>&gt; much on building the nonindustrial AI.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>The economic fallout will occur in any case.  AI is being used in all
<BR>
sorts of ways today that will lead to that  inescapably.  I doubt pieces
<BR>
of the SI project will add much to that trend.  
<BR>
<P>Do you think that you are qualified to decide for all of us what
<BR>
technology should and should not be available?  If so please acquaint
<BR>
those of us who are understandably skeptical with your credentials. Do
<BR>
the people who work with you on this goal get a say or is it a conditon
<BR>
of being on the team that they let you decide these things?  
<BR>
<P>AI industry is a different beast from open sourcing results.  You will
<BR>
not have a monopoly on AI by any stretch of the imagination.  Opening
<BR>
the work will allow oversight and input and the creation other
<BR>
applications that might well be critical to both the viability of the
<BR>
project and to our long term well-being.  
<BR>
&nbsp;
<BR>
<EM>&gt; So far, there are few enough people who demonstrate that they have understood
</EM><BR>
<EM>&gt; the pattern of &quot;Coding a Transhuman AI&quot;.  I have yet to witness a single
</EM><BR>
<EM>&gt; person go on and extend the pattern to areas I have not yet covered.  I'm
</EM><BR>
<EM>&gt; sorry, and I dearly wish it was otherwise, but that's the way things are.
</EM><BR>
<P>The pattern of the AI seed is not the largest problem.  Reaching
<BR>
agreement with the full range of your surrounding opinions and positions
<BR>
is.  Your statement of sorrow and implied grading of people are both
<BR>
irrelevant and derailing of discussion.  Anything that remotely comes
<BR>
off like &quot;those who can understand already do and the rest are hopeless&quot;
<BR>
is a position you probably should avoid even the appearance of holding. 
<BR>
Understanding what you propose and agreeing with you are quite different
<BR>
things.
<BR>
<P>- samantha
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="6129.html">Andy Toth: "Re: Robots, but philosophers (or, Hal-2001)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6127.html">Spike Jones: "Re: Asian Swamp Eels"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5508.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5258.html">Matt Gingell: "Re: Fwd: Earthweb from transadmin"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6128">[ date ]</A>
<A HREF="index.html#6128">[ thread ]</A>
<A HREF="subject.html#6128">[ subject ]</A>
<A HREF="author.html#6128">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:27 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
