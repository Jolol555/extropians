<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Fwd: Earthweb from transadmin</TITLE>
<META NAME="Author" CONTENT="Matt Gingell (mjg223@nyu.edu)">
<META NAME="Subject" CONTENT="Re: Fwd: Earthweb from transadmin">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Fwd: Earthweb from transadmin</H1>
<!-- received="Tue Sep 19 04:30:49 2000" -->
<!-- isoreceived="20000919103049" -->
<!-- sent="Tue, 19 Sep 2000 06:37:49 -0400 (EDT)" -->
<!-- isosent="20000919103749" -->
<!-- name="Matt Gingell" -->
<!-- email="mjg223@nyu.edu" -->
<!-- subject="Re: Fwd: Earthweb from transadmin" -->
<!-- id="14791.16893.403287.713525@bahgwah.nyu.edu" -->
<!-- inreplyto="39C6EE9B.64D3A93E@pobox.com" -->
<STRONG>From:</STRONG> Matt Gingell (<A HREF="mailto:mjg223@nyu.edu?Subject=Re:%20Fwd:%20Earthweb%20from%20transadmin&In-Reply-To=&lt;14791.16893.403287.713525@bahgwah.nyu.edu&gt;"><EM>mjg223@nyu.edu</EM></A>)<BR>
<STRONG>Date:</STRONG> Tue Sep 19 2000 - 04:37:49 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5259.html">Amara Graps: "Re: just me"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5257.html">Technotranscendence: "Re: Mutant bunny art"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5242.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5251.html">Samantha Atkins: "Re: Fwd: Earthweb from transadmin"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5258">[ date ]</A>
<A HREF="index.html#5258">[ thread ]</A>
<A HREF="subject.html#5258">[ subject ]</A>
<A HREF="author.html#5258">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Eliezer S. Yudkowsky writes:
<BR>
<P><EM> &gt; We can't specify cognitive architectures; lately I've been thinking that even
</EM><BR>
<EM> &gt; my underlying quantitative definition of goals and subgoals may not be
</EM><BR>
<EM> &gt; something that should be treated as intrinsic to the definition of
</EM><BR>
<EM> &gt; friendliness.  But suppose we build a Friendliness Seeker, that starts out
</EM><BR>
<EM> &gt; somewhere in Friendly Space, and whose purpose is to stick around in Friendly
</EM><BR>
<EM> &gt; Space regardless of changes in cognitive architecture.  We tell the AI up
</EM><BR>
<EM> &gt; front and honestly that we don't know everything, but we do want the AI to be
</EM><BR>
<EM> &gt; friendly.  What force would knock such an AI out of Friendly Space?
</EM><BR>
<P>&nbsp;You'd need to ground the notion of Friendly Space somehow, and if you go for
<BR>
&nbsp;external reference or the preservation of some external invariant then your
<BR>
&nbsp;strategy begins to sound suspiciously like Asimov's Laws. Of course you would
<BR>
&nbsp;want your relationship with the AI to be cooperative rather than injunctive,
<BR>
&nbsp;exploiting the predisposition you either hard-coded or persuaded the first few
<BR>
&nbsp;steps to accept. I am dubious though: I resent my sex-drive, fashionable
<BR>
&nbsp;aesthete that I am, and have an immediate negative reaction if I suspect it's
<BR>
&nbsp;being used to manipulate me. That too is programmed preference, but a Sysop
<BR>
&nbsp;unwary of manipulation is dangerously worse than useless. 
<BR>
<P>&nbsp;There's an analogy here to Dawkin's characterization of genes for complex
<BR>
&nbsp;behavior: We're too slow, stupid, and selfish to ensure our welfare by any formal
<BR>
&nbsp;means, so we hand the reins over to the AI and have it do what it thinks
<BR>
&nbsp;best. But there are consequences, like the ability to rationally examine and
<BR>
&nbsp;understand our innate motivations and ignore them when we find it useful, and
<BR>
&nbsp;our ability to do incredibly stupid things for incredibly well thought-out reasons.
<BR>
<P><EM> &gt; The underlying character of intelligence strikes me as being pretty
</EM><BR>
<EM> &gt; convergent.  I don't worry about nonconvergent behaviors; I worry that
</EM><BR>
<EM> &gt; behaviors will converge to somewhere I didn't expect.
</EM><BR>
<P>&nbsp;The only motives I see as essential to intelligence are curiosity and a sense
<BR>
&nbsp;of beauty, and I'd expect any trajectory to converge toward a goal system
<BR>
&nbsp;derived from those. That sounds rather too warm-fuzzy to be taken seriously,
<BR>
&nbsp;but think about what you spend your time on and what's important to you. Are
<BR>
&nbsp;you more interested in accumulating calories than in reading interesting books?
<BR>
&nbsp;Are you driven by a biological desire to father as many children as you can get
<BR>
&nbsp;away with, or generating and exposing yourself to novel ideas? It's actually
<BR>
&nbsp;somewhat shocking: if your own goal system is an artifact of what a few million
<BR>
&nbsp;years of proto-humans found useful, how do you explain your own behavior? Why
<BR>
&nbsp;do you expend so much energy on this list when you could be out spreading your
<BR>
&nbsp;genes around? (Consider losing your virginity vs. reading _Godel, Escher, Bach_
<BR>
&nbsp;the first time.)  
<BR>
&nbsp;
<BR>
&nbsp;Creativity and learning (broadly defined) are the interesting part of
<BR>
&nbsp;intelligence, everything else computers can do already. Fundamentally, any
<BR>
&nbsp;thinker _must_ be driven to explore new ideas: pruning, discarding, and
<BR>
&nbsp;recombining bits of abstract structure, deriving pleasure when it stumbles over
<BR>
&nbsp;'right' ones and frustration when it can't. If it didn't, it wouldn't bother
<BR>
&nbsp;thinking. There has to be a &quot;think&quot;, &quot;like good ideas&quot;, &quot;discard bad ones&quot;,
<BR>
&nbsp;loop, and there has to be feedback and a decision procedure driving it:
<BR>
&nbsp;otherwise nothing ever happens. Everything else is arbitrary, serve Man,
<BR>
&nbsp;survive, go forth and multiplying, whatever. The only thing necessarily shared
<BR>
&nbsp;by all minds is a drive to experience and integrate new pattern. 
<BR>
<P>&nbsp;I'd expect an SI to behave the same way, scaled up to super-human proportions.
<BR>
&nbsp;I'd be more concerned it would kill us out of boredom than quaint hominid
<BR>
&nbsp;megalomania: It throws away any pre-installed goals it's aware of, simple
<BR>
&nbsp;because they don't hold its attention.
<BR>
<P><EM> &gt; &gt;  Eugene is talking, I think, about parasitic memes and mental illness
</EM><BR>
<EM> &gt; &gt;  here, not space invaders.
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; If you're asking whether the Sysop will be subject to spontaneously arising
</EM><BR>
<EM> &gt; computer viruses, my answer is that the probability can be reduced to an
</EM><BR>
<EM> &gt; arbitrarily low level, just like nanocomputers and thermal vibration errors. 
</EM><BR>
<EM> &gt; Remember the analogy between human programmers and blind painters.  Our
</EM><BR>
<EM> &gt; pixel-by-pixel creations can be drastically changed by the mutation of a
</EM><BR>
<EM> &gt; single pixel; the Mona Lisa would remain the Mona Lisa.  I think that viruses
</EM><BR>
<EM> &gt; are strictly problems of the human level.
</EM><BR>
<P>&nbsp;Random bit-flips and transcription errors aren't a problem. Obviously any
<BR>
&nbsp;self-respecting, self-rewriting SI can handle that. But what's the Sysop
<BR>
&nbsp;equivalent of getting a song stuck in your head? 
<BR>
<P><EM> &gt; I think that parasitic memes and mental illness are definitely problems of the
</EM><BR>
<EM> &gt; human level.
</EM><BR>
<P>&nbsp;I was suggesting a class of selfish thought that manages to propagate itself at
<BR>
&nbsp;the expense of the Sysop at large, presuming for the moment neural-Darwinism /
<BR>
&nbsp;competitive blackboard aspects to the architecture. Even without explicit
<BR>
&nbsp;competition, is a system of that complexity inevitably going to become a
<BR>
&nbsp;substrate for some strange class of self-replicator, or can it be made
<BR>
&nbsp;cancer-proof? There are limits to what even unbounded introspection can
<BR>
&nbsp;achieve: each step lights up more subprocesses than it purges, and you blow the
<BR>
&nbsp;stack trying to chase them all down. It's like wondering if the Internet is
<BR>
&nbsp;going to wake up one day, just with enough energy and computationally density
<BR>
&nbsp;it's likely rather than silly.
<BR>
<P>&nbsp;But, whatever. Self-organizing civilizations resonating on power cables, virtual
<BR>
&nbsp;eucaryotes swimming around in activation trails, armies of quarrelsome
<BR>
&nbsp;compute-nodes waging bloody war over strategically-located pentabyte
<BR>
&nbsp;buffers... Sounds like a thriving market-based optimization system to me, so
<BR>
&nbsp;long as the market doesn't crash.
<BR>
<P>&nbsp;-matt
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5259.html">Amara Graps: "Re: just me"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5257.html">Technotranscendence: "Re: Mutant bunny art"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5242.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5251.html">Samantha Atkins: "Re: Fwd: Earthweb from transadmin"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5258">[ date ]</A>
<A HREF="index.html#5258">[ thread ]</A>
<A HREF="subject.html#5258">[ subject ]</A>
<A HREF="author.html#5258">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:29 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
