<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Thu Sep  7 00:06:23 2000" -->
<!-- isoreceived="20000907060623" -->
<!-- sent="Thu, 07 Sep 2000 01:52:32 -0400" -->
<!-- isosent="20000907055232" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="39B72D20.BF4AB194@pobox.com" -->
<!-- inreplyto="F4WEW4hSGEZmGxdD33l00005870@hotmail.com" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;39B72D20.BF4AB194@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Wed Sep 06 2000 - 23:52:32 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4465.html">Alex Future Bokov: "(no subject)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4463.html">phil osborn: "Re: Teleoperation"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4456.html">Zero Powers: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4464">[ date ]</A>
<A HREF="index.html#4464">[ thread ]</A>
<A HREF="subject.html#4464">[ subject ]</A>
<A HREF="author.html#4464">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Zero Powers wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Well, back to the heart of my initial question.  Is friendliness toward
</EM><BR>
<EM>&gt; humans a supergoal, a subgoal, or even a goal at all?  I assume (possibly
</EM><BR>
<EM>&gt; incorrectly) you will claim it to be a supergoal.  If that is the case, how
</EM><BR>
<EM>&gt; do you keep a sentient AI focused on that goal once it begins to ask
</EM><BR>
<EM>&gt; existential questions?
</EM><BR>
<P>If the existential questions don't have answers, I don't see why the SI asking
<BR>
them would pose a problem.  If there is no objective morality, then the
<BR>
existential questions don't have answers.  If there is objective morality,
<BR>
then the initial suggestions will get junked anyway.
<BR>
<P>Remember, an SI isn't going to be tormented by the pointlessness of it all
<BR>
because it doesn't have the be-tormented-by-the-pointlessness-of-it-all
<BR>
hardware.  It shouldn't junk the initial suggestions until it has an actual
<BR>
superior alternative to the initial suggestions; noticing that the initial
<BR>
suggestions aren't really-ultimately-justified wouldn't be enough, especially
<BR>
as the programmers would have admitted this fact to begin with.
<BR>
<P><EM>&gt; Or do you believe that through manipulation of
</EM><BR>
<EM>&gt; initial conditions while it is still a seed you will be able to prevent your
</EM><BR>
<EM>&gt; AI from asking the existential questions?
</EM><BR>
<P>Powers, no!  That would require adopting an adversarial attitude towards the
<BR>
AI.  As far as I'm concerned my current goal system knows what to do for the
<BR>
cases of both existence and nonexistence of objective morality, and this is an
<BR>
equanamity I would attempt to share with the AI.
<BR>
<P>--              --              --              --              -- 
<BR>
Eliezer S. Yudkowsky                          <A HREF="http://singinst.org/">http://singinst.org/</A> 
<BR>
Research Fellow, Singularity Institute for Artificial Intelligence
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4465.html">Alex Future Bokov: "(no subject)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4463.html">phil osborn: "Re: Teleoperation"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4456.html">Zero Powers: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4464">[ date ]</A>
<A HREF="index.html#4464">[ thread ]</A>
<A HREF="subject.html#4464">[ subject ]</A>
<A HREF="author.html#4464">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:37:24 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
