<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="James Rogers (jamesr@best.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Wed Sep  6 13:36:15 2000" -->
<!-- isoreceived="20000906193615" -->
<!-- sent="Wed, 6 Sep 2000 05:45:38 -0700" -->
<!-- isosent="20000906124538" -->
<!-- name="James Rogers" -->
<!-- email="jamesr@best.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="00090607472700.00756@tachyon" -->
<!-- inreplyto="14774.22656.823650.713352@lrz.uni-muenchen.de" -->
<STRONG>From:</STRONG> James Rogers (<A HREF="mailto:jamesr@best.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;00090607472700.00756@tachyon&gt;"><EM>jamesr@best.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Wed Sep 06 2000 - 06:45:38 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4408.html">Lee Daniel Crocker: "Re: Bugs in free markets."</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4406.html">stdnt428@hampshire.edu: "Re: NLP (Neuro Linguistic Programming) as an Extropianistic Tool!"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4378.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="4412.html">James Rogers: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4407">[ date ]</A>
<A HREF="index.html#4407">[ thread ]</A>
<A HREF="subject.html#4407">[ subject ]</A>
<A HREF="author.html#4407">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
On Wed, 06 Sep 2000, Eugene Leitl wrote:
<BR>
<EM>&gt; James Rogers writes:
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt;  &gt; I don't think this is an accurate characterization.  The rules don't
</EM><BR>
<EM>&gt;  &gt; change at all; there is still fierce competition among intelligent,
</EM><BR>
<EM>&gt;  &gt; self-directed entities.  All that happens is that the competition moves
</EM><BR>
<EM>&gt;  &gt; out of the grossly physical domain for the most part.
</EM><BR>
<EM>&gt;  
</EM><BR>
<EM>&gt; We're still pretty far from an equilibrium, still being in a
</EM><BR>
<EM>&gt; spontaneous expansion process of civilisation into the wilderness. The
</EM><BR>
<EM>&gt; grossly physical part may well come back with a vengeance (see the
</EM><BR>
<EM>&gt; emergence of neoplagues and pests for a shade of things potentially to
</EM><BR>
<EM>&gt; come), when we're nearing the more sustainable/equilibrium part of the
</EM><BR>
<EM>&gt; development. Assuming the constraints of finite concentration of
</EM><BR>
<EM>&gt; matterenergy in spacetime must always hold, things must eventually
</EM><BR>
<EM>&gt; plateau. Before things might pass through a sequence of bottlenecks,
</EM><BR>
<EM>&gt; e.g. if we can't expand freely into space, after having covered the
</EM><BR>
<EM>&gt; planet with a thick crust of manmade artefacts and people.
</EM><BR>
<P><P>I agree that we aren't there yet, but in a hundred years we'll most likely
<BR>
either be there or be dead.  Without thinking about it too much, I would
<BR>
say that it is almost required that the intelligence curve needs to pick up
<BR>
soon, or we humans will end up brute-forcing ourselves into an unpleasant
<BR>
situation.
<BR>
<P>&nbsp;&nbsp;
<BR>
<EM>&gt;  &gt; The capability to project outcomes of actions as a result of increased
</EM><BR>
<EM>&gt;  &gt; intelligence and knowledge is most likely responsible for this shift. 
</EM><BR>
<EM>&gt;  &gt; The ability to accurately forecast the costs/benefits for a broad range of
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Of course, by expressing the planned behaviour, we're collectively
</EM><BR>
<EM>&gt; changing the state of the system, and hence introduce an additional
</EM><BR>
<EM>&gt; uncertainty. The others constitute a major part of the fitness
</EM><BR>
<EM>&gt; function, which has to move when their strategies move. For instance,
</EM><BR>
<EM>&gt; all the fat dotcom fishes in a tiny pond create a lot of ruckus, and
</EM><BR>
<EM>&gt; muddy the waters. Mutually making planning more difficult. Time for
</EM><BR>
<EM>&gt; some dynamite fishing ;)
</EM><BR>
<P><P>On an individual basis the system appears chaotic, but to me it seems much
<BR>
more regular on a macroscopic basis.  Those dotcom fish may muddy their
<BR>
pond, but the ocean looks as clear as it ever was.  It is easier to
<BR>
project outcomes for a system than for individual players in a system (in
<BR>
a fractal sort of way).  Also, the limits of resolution, at least for
<BR>
these types of things, has a lot to do with the extent of
<BR>
intelligence/information and computational power available.  It is
<BR>
probably possible to resolve the dotcom ruckus, but it would require
<BR>
resources that may not currently exist.  I would also agree that as the
<BR>
resolution increases, so does the uncertainty, although not necessarily at
<BR>
the same rate or in the same manner.
<BR>
<P>&nbsp;
<BR>
<EM>&gt;  &gt; potential actions would encourage more subtle and less costly
</EM><BR>
<EM>&gt;  &gt; manipulations than brute force to achieve the same effective results in a
</EM><BR>
<EM>&gt;  &gt; competitive environment.  It also allows one to recognize losing
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; All that assuming smart agents. They are not the only one on the
</EM><BR>
<EM>&gt; stage. Dumb agents still pretty much brute-force.
</EM><BR>
<P><P>True, and we are starting to see the beginning of a mixture of the two in
<BR>
the world on a more global scale.  However, it requires a command economy
<BR>
to even have a chance of brute-forcing a flexible, mobile economy with
<BR>
excellent intelligence assets.  And the window in which even this is
<BR>
possible is shrinking (already closed?).
<BR>
<P>Note that this is directly applicable to warfare (we can use it as a model)
<BR>
and can be seen in the evolution of armaments and battlefield tactics. 
<BR>
Heavy armored assets are only really valid in the context of insufficient
<BR>
battlefield intelligence and to a lesser extent, mobility.  Armor and
<BR>
strategies of attrition compensate for poor situational intelligence by
<BR>
providing a hedge against the possibility of unexpected/unknown
<BR>
weaponry in the former, and by bulldozing possible targets of unknown
<BR>
strength and character in the latter with excessive force.  The extra
<BR>
expense is that resources are expended on speculation of the mere
<BR>
probability that they may be needed, whether they actually are or not. In
<BR>
organizations that have good intelligence assets, mobility becomes much
<BR>
more valuable than hardness as you can find well-described targets quicker
<BR>
and avoid danger easier with better situational awareness, hence the
<BR>
migration away from armored ground assets towards air assets in modern
<BR>
militaries.  The best defense is a good offense, *if* you have good
<BR>
situational intelligence. A linear increase in battlefield intelligence
<BR>
allows the use of existing mobility in ways that exponentially increase
<BR>
the cost of aggressing with relatively poor intelligence assets.  Even in
<BR>
cases where weaponry is equivalent in capability, situational intelligence
<BR>
is an enormous force multiplier.  With sufficiently large deltas in
<BR>
situational intelligence, qualitative and quantitative differences in the
<BR>
armaments themselves become largely irrelevant (e.g. the Gulf War).
<BR>
<P>As intelligence technologies (both in the information gathering and
<BR>
computational sense) improve, the amount of brute force required to
<BR>
overcome increasingly intelligent and mobile adversaries will grow
<BR>
geometrically.  Right now, we are at the very beginning of this curve.
<BR>
Very soon it will be economically unfeasible consider brute force
<BR>
capability as a significant competitive asset.  In my opinion.
<BR>
<P>Of course, this also suggests the (rather obvious) consequences of
<BR>
unleashing an AI on the world...
<BR>
<P>-James Rogers
<BR>
&nbsp;<A HREF="mailto:jamesr@best.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;00090607472700.00756@tachyon&gt;">jamesr@best.com</A>
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4408.html">Lee Daniel Crocker: "Re: Bugs in free markets."</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4406.html">stdnt428@hampshire.edu: "Re: NLP (Neuro Linguistic Programming) as an Extropianistic Tool!"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4378.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="4412.html">James Rogers: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4407">[ date ]</A>
<A HREF="index.html#4407">[ thread ]</A>
<A HREF="subject.html#4407">[ subject ]</A>
<A HREF="author.html#4407">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:37:21 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
