<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Samantha Atkins (samantha@objectent.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Mon Sep 25 22:45:05 2000" -->
<!-- isoreceived="20000926044505" -->
<!-- sent="Mon, 25 Sep 2000 21:46:42 -0700" -->
<!-- isosent="20000926044642" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="39D02A32.5356B308@objectent.com" -->
<!-- inreplyto="020001c0274b$2dba7a00$9bbc473f@jrmolloy" -->
<STRONG>From:</STRONG> Samantha Atkins (<A HREF="mailto:samantha@objectent.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;39D02A32.5356B308@objectent.com&gt;"><EM>samantha@objectent.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon Sep 25 2000 - 22:46:42 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5788.html">Jason Joel Thompson: "Re: Some thoughts on Politics"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5786.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5753.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="5790.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5787">[ date ]</A>
<A HREF="index.html#5787">[ thread ]</A>
<A HREF="subject.html#5787">[ subject ]</A>
<A HREF="author.html#5787">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
&quot;J. R. Molloy&quot; wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Samantha Atkins writes,
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &gt; I do get this.  And yet it still seems that even if we can't convince
</EM><BR>
<EM>&gt; &gt; them its coming or get them to really understand what that means (though
</EM><BR>
<EM>&gt; &gt; who knows?) if they are convinced, that we owe it to them to do our best
</EM><BR>
<EM>&gt; &gt; to make sure they aren't run over and that they are actually benefitted
</EM><BR>
<EM>&gt; &gt; (as a minimum).
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &quot;we owe it to them&quot;? And what do they owe us? What have they done (lately) to
</EM><BR>
<EM>&gt; promote and extend the cognitive sciences? What have they done to advance and
</EM><BR>
<EM>&gt; enlarge the store of human knowledge? Never mind... Forget them... let's ask
</EM><BR>
<EM>&gt; ourselves what *we've* done to bring the world closer to TS.
</EM><BR>
<P>I would point out that some of those people raised us, fed us, educated
<BR>
us, on and on it goes.  How many generations of people, scientists,
<BR>
mundanes, and so on do you think this civilization and this level of
<BR>
science rest upon?  Do you think all of those human lives and human
<BR>
dreams whether they are like yours or as scientific or not are invalid
<BR>
and pointless as long as you get what you want?  Unless the TS answers
<BR>
and fulfills all those dreams (or better) it is a rip-off.  
<BR>
<P><P><EM>&gt; 
</EM><BR>
<EM>&gt; &gt; Well, the extremes are both unlikely.  I would suspect it is somewhere
</EM><BR>
<EM>&gt; &gt; in between, it will make its own mistakes.  But I find it very unlikely
</EM><BR>
<EM>&gt; &gt; that the masses will not be manipulated against those &quot;selfish,
</EM><BR>
<EM>&gt; &gt; egotistical scientists&quot; who let this thing loose in their midst.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Who will do the manipulating? ...and how will they manage to manipulate &quot;the
</EM><BR>
<EM>&gt; masses&quot; more effectively than the AI can do so?
</EM><BR>
<EM>&gt;
</EM><BR>
<P>Do you think the AI will spring full grown out of the mind of Eliezer
<BR>
(god help us)?  Do you think it will automagically know how to win
<BR>
friends and influence people?  Do you think there is zero time between
<BR>
serious work starting on the AI and it being together enough to fix
<BR>
everything?  Personally I think there is non-zero time when we &quot;mere&quot;
<BR>
humans will need to do our best to keep this species in more or less one
<BR>
piece.  And I think that doing that is every bit as important as
<BR>
building the Singularity.  I would not be surprised if doing that is
<BR>
crucial to the success of the entire project.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Sure.  But what of the humans who will be even more largely out of work
</EM><BR>
<EM>&gt; &gt; and feel/be even more redundant?  How will you organize society so that
</EM><BR>
<EM>&gt; &gt; these folks get taken care of so that they don't see this as a
</EM><BR>
<EM>&gt; &gt; tremendous threat and possibly the end of their own means of survival?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; It's called capitalism. Buggy whip makers had to adjust to the invention of the
</EM><BR>
<EM>&gt; automobile, and tomorrow's workers (especially knowledge workers) will need to
</EM><BR>
<EM>&gt; adjust to the invention of AI.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>We are not talking about any such civilized rate of change at all. 
<BR>
There is nowhere left to go for increasing levels of workers (largely
<BR>
following the IQ Bell Curve).  If you think it can't happen to me or to
<BR>
you then I don't think you are paying attention.  Capitalism and waving
<BR>
the hands is not an answer.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; If that is and continues to be *all* that makes the world go 'round then
</EM><BR>
<EM>&gt; &gt; we all end up on the trash-heap of history in the very short run.  A
</EM><BR>
<EM>&gt; &gt; great motivation to do the R&amp;D isn't it?
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Not the way I see it. Since money is what makes the world go 'round, best get to
</EM><BR>
<EM>&gt; making plenty of it. The very best motivation to do the R&amp;D that will enable you
</EM><BR>
<EM>&gt; to make the most of it. (Max More seems to have grok'd this long ago.)
</EM><BR>
<EM>&gt; 
</EM><BR>
<P>And the day after full nanotech or transhuman AI almost all of your
<BR>
money and your holdings are worthless.  I think you are missing the
<BR>
pressing need to have some idea of how to reform society and economics
<BR>
before we get to that day and even at various quite mundane points along
<BR>
the way.  Please tell me if I am wrong and convince me if you can.  I
<BR>
would very much like to believe the world is as simple as some seem wont
<BR>
to see it.
<BR>
<P>- samantha
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5788.html">Jason Joel Thompson: "Re: Some thoughts on Politics"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5786.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5753.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="5790.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5787">[ date ]</A>
<A HREF="index.html#5787">[ thread ]</A>
<A HREF="subject.html#5787">[ subject ]</A>
<A HREF="author.html#5787">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:06 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
