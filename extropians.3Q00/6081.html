<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Andrew Lias (andrew.lias@corp.usa.net)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Fri Sep 29 11:45:06 2000" -->
<!-- isoreceived="20000929174506" -->
<!-- sent="Fri, 29 Sep 2000 11:27:09 -0600" -->
<!-- isosent="20000929172709" -->
<!-- name="Andrew Lias" -->
<!-- email="andrew.lias@corp.usa.net" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="NEBBLDNPLKMJPLEGEOCKAEKPCIAA.andrew.lias@corp.usa.net" -->
<!-- inreplyto="Why would AI want to be friendly?" -->
<STRONG>From:</STRONG> Andrew Lias (<A HREF="mailto:andrew.lias@corp.usa.net?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;NEBBLDNPLKMJPLEGEOCKAEKPCIAA.andrew.lias@corp.usa.net&gt;"><EM>andrew.lias@corp.usa.net</EM></A>)<BR>
<STRONG>Date:</STRONG> Fri Sep 29 2000 - 11:27:09 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="6082.html">Barbara Lamar: "Re: Some thoughts on Politics"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6080.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="5641.html">J. R. Molloy: "Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6092.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6164.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6081">[ date ]</A>
<A HREF="index.html#6081">[ thread ]</A>
<A HREF="subject.html#6081">[ subject ]</A>
<A HREF="author.html#6081">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
[Non-member submission]
<BR>
<P>First things first: I'm new to the list -- be gentle. :-)
<BR>
<P>I've been following the debates regarding the possibilities of friendly vs.
<BR>
unfriendly AI and I have a question.  It seems that we are presuming that a
<BR>
friendly AI would be friendly towards us in a manner that we would recognize
<BR>
as friendly.  Indeed, what, precisely, do we mean by friendly?
<BR>
<P>Let us (to invoke a favored cliche) suppose that an AI is evolved such that
<BR>
it's understanding of being friendly towards humans is that it should try to
<BR>
insure the survival of humanity and that it should attempt to maximize our
<BR>
happiness.  What is to prevent it from deciding that the best way to
<BR>
accomplish those goals is to short circuit our manifestly self-destructive
<BR>
sense of intelligence and to re-wire our brains so that we are incapable of
<BR>
being anything but deleriously happy at all times? [1]
<BR>
<P>Now, I'm not suggesting that *this* is a plausible example (as noted, it's
<BR>
very much a science-fiction cliche), but I am concerned that any definition
<BR>
or set of parameters we develop for the evolution of friendly AI may include
<BR>
unforseen consequences in the definition that we simply can't anticipate at
<BR>
our level of intelligence -- and that's supposing that the SI will still
<BR>
want to be friendly.
<BR>
<P>What am I missing?
<BR>
<P><PRE>
--
Andrew Lias
<P>[1] Thinking about this a bit, making a mandate to preserve humanity might
be an exceptionally bad thing to try and program, from a transhumanist
perspective.  &quot;Success&quot; may be the development of an SI that will actively
prevent us from uploading/upgrading ourselves in order to stop us from
&quot;losing&quot; our humanity. 
</PRE>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="6082.html">Barbara Lamar: "Re: Some thoughts on Politics"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6080.html">Eliezer S. Yudkowsky: "Re: Eugene's nuclear threat"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="5641.html">J. R. Molloy: "Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6092.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6164.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6081">[ date ]</A>
<A HREF="index.html#6081">[ thread ]</A>
<A HREF="subject.html#6081">[ subject ]</A>
<A HREF="author.html#6081">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:25 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
