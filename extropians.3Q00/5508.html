<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Fwd: Earthweb from transadmin</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: Fwd: Earthweb from transadmin">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Fwd: Earthweb from transadmin</H1>
<!-- received="Sat Sep 23 10:05:55 2000" -->
<!-- isoreceived="20000923160555" -->
<!-- sent="Sat, 23 Sep 2000 12:05:43 -0400" -->
<!-- isosent="20000923160543" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Fwd: Earthweb from transadmin" -->
<!-- id="39CCD4D7.A5E36584@pobox.com" -->
<!-- inreplyto="39CC7443.D094F493@objectent.com" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Fwd:%20Earthweb%20from%20transadmin&In-Reply-To=&lt;39CCD4D7.A5E36584@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Sat Sep 23 2000 - 10:05:43 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5509.html">J. R. Molloy: "Doomsday scenarios"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5507.html">David Blenkinsop: "Re: Does the state-vector collapse?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5493.html">Samantha Atkins: "Re: Fwd: Earthweb from transadmin"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="6128.html">Samantha Atkins: "Re: Fwd: Earthweb from transadmin"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6128.html">Samantha Atkins: "Re: Fwd: Earthweb from transadmin"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5508">[ date ]</A>
<A HREF="index.html#5508">[ thread ]</A>
<A HREF="subject.html#5508">[ subject ]</A>
<A HREF="author.html#5508">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Samantha Atkins wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; You are right that human goals are not uniformly friendly to human
</EM><BR>
<EM>&gt; beings.  But I would tend to agree with the POV that an intelligence
</EM><BR>
<EM>&gt; build on or linking human intelligence and automating their interaction,
</EM><BR>
<EM>&gt; sharing of knowledge, finding each other and so on would be more likely
</EM><BR>
<EM>&gt; to at least majorly empathize with and understand human beings.
</EM><BR>
<P>Samantha, you have some ideas about empathy that are flat wrong.  I really
<BR>
don't know how else to say it.  You grew up on a human planet and you got some
<BR>
weird ideas.
<BR>
<P>The *programmer* has to think like the AI to empathize with the AI.  The
<BR>
converse isn't true.  Different cognitive architectures.
<BR>
<P><EM>&gt; Why would an Earthweb use only humans as intelligent components?  The
</EM><BR>
<EM>&gt; web could have a lot of non-human agents and logic processors and other
</EM><BR>
<EM>&gt; specialized gear.  Some decisions, particularly high speed ones and ones
</EM><BR>
<EM>&gt; requiring major logic crunching, might increasingly not be made
</EM><BR>
<EM>&gt; explicitly by human beings.
</EM><BR>
<P>Then either you have an independent superintelligence, or you have a process
<BR>
built from human components.  No autonomic process, even one requiring &quot;major
<BR>
logic crunching&quot;, qualifies as &quot;intelligence&quot; for these purposes.  A thought
<BR>
requires a unified high-bandwidth brain in order to exist.  You cannot have a
<BR>
thought spread across multiple brains, not if those brains are separated by
<BR>
the barriers of speech.
<BR>
<P>Remember, the default rule for &quot;folk cognitive science&quot; is that you see only
<BR>
thoughts and the interactions of thoughts.  We don't have built-in perceptions
<BR>
for the neural source code, the sensory modalities, or the contents of the
<BR>
concept level.  And if all you see of the thoughts is the verbal traceback,
<BR>
then you might think that the Earthweb was thinking.  But three-quarters of
<BR>
the complexity of thought is in the underlying substrate, and that substrate
<BR>
can't emerge accidentally - it doesn't show up even in &quot;major logic
<BR>
crunching&quot;.
<BR>
<P><EM>&gt; With the AI you only know what goes in at the beginning.  You have no
</EM><BR>
<EM>&gt; idea what things will develop from that core that you can have full
</EM><BR>
<EM>&gt; confidence in.
</EM><BR>
<P>There is no &quot;full confidence&quot; here.  Period.
<BR>
<P>That said, the Earthweb can't reach superintelligence.  If the Earthweb
<BR>
*could* reach superintelligence than I would seriously have a harder time
<BR>
visualizing the Earthweb-process than I would with seed AI.  Just because the
<BR>
Earthweb has human components doesn't make the system behavior automatically
<BR>
understandable.
<BR>
<P><EM>&gt; &gt; I shouldn't really be debating this.  You can't see, intuitively, that a
</EM><BR>
<EM>&gt; &gt; superintelligence is solid and the Earthweb is air and shadow by comparision?
</EM><BR>
<EM>&gt; &gt; The Earthweb is pretty hot by human standards, but only by human standards.
</EM><BR>
<EM>&gt; &gt; It has no greater stability than the human species itself.  It may arguably be
</EM><BR>
<EM>&gt; &gt; useful on the way to a solid attractor like the Sysop Scenario, but it can't
</EM><BR>
<EM>&gt; &gt; possibly be anything more than a way-station.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; If we could see it intuitively I would guess none of us would be
</EM><BR>
<EM>&gt; questioning it.
</EM><BR>
<P>Not true.  Many times, people perceive something intuitively without having a
<BR>
name for that perception, so they forget about it and start playing with
<BR>
words.  I was hoping that was the case here... apparently not.
<BR>
<P><EM>&gt; Frankly I don't know how you can with a straight face
</EM><BR>
<EM>&gt; say that a superintelligence is solid when all you have is a lot of
</EM><BR>
<EM>&gt; theory and your own basically good intentions and optimism.  That isn't
</EM><BR>
<EM>&gt; very solid in the development world I inhabit.
</EM><BR>
<P>Wrong meaning of the term &quot;solid&quot;, sorry for using something ambiguous.  Not
<BR>
&quot;solid&quot; as in &quot;easy to develop&quot;.  (We're assuming that it's been developed and
<BR>
discussing the status afterwards.)  &quot;Solid&quot; as in &quot;internally stable&quot;. 
<BR>
Superintelligence is one of the solid attractors for a planetary technological
<BR>
civilization.  The other solid attractor is completely destroying all the life
<BR>
on the planet (if a single bacterium is left, it can evolve again, so that's
<BR>
not a stable state).
<BR>
<P>Now, is the Earthweb solid?  In that million-year sense?
<BR>
<P><EM>&gt; I hear you but I'm not quite ready to give up on the human race at as a
</EM><BR>
<EM>&gt; bad job
</EM><BR>
<P>Do you understand that your sentimentality, backed up with sufficient power,
<BR>
could easily kill you and could as easily kill off the human species?
<BR>
<P><EM>&gt; Your own basic optimism would seem to indicate that as
</EM><BR>
<EM>&gt; intelligence goes up the ability to see moral/ethical things more
</EM><BR>
<EM>&gt; cleanly and come to &quot;friendlier&quot; decisions goes up.
</EM><BR>
<P>Yep.  The key word in that sentence was &quot;ability&quot;, by the way.  Not
<BR>
&quot;tendency&quot;, &quot;ability&quot;.  That was my mistake back in the early days (1996 or
<BR>
so).
<BR>
<P><EM>&gt; I think it behooves us to try all approaches we can think of that might
</EM><BR>
<EM>&gt; remotely help.  I'm not ready to put all my eggs in the AI basket.
</EM><BR>
<P>Try everything, I agree.  I happen to think that the AI is the most important
<BR>
thing and the most likely to win.  I'm not afraid to prioritize my eggs.
<BR>
<P><EM>&gt; So instead you think a small huddle of really bright people will solve
</EM><BR>
<EM>&gt; or make all the problems of the ages moot by creating in effect a
</EM><BR>
<EM>&gt; Super-being, or at least its Seed.  How is this different from the old
</EM><BR>
<EM>&gt; &quot;my nerds are smarter than your nerds and we will win&quot; sort of
</EM><BR>
<EM>&gt; mentality?
</EM><BR>
<P>You're asking the wrong question.  The question is whether we can win. 
<BR>
Mentalities are ever-flexible and can be altered; if we need a particular
<BR>
mentality to win, that issue isn't entirely decoupled from strategy but it
<BR>
doesn't dictate strategy either.
<BR>
<P><EM>&gt; By what right will you withhold behind close doors all the
</EM><BR>
<EM>&gt; tech leading up to your success that could be used in so many fruitful
</EM><BR>
<EM>&gt; ways on the outside?  If you at least stayed open you would enable the
</EM><BR>
<EM>&gt; tech to be used on other attempts to make the coming transitions
</EM><BR>
<EM>&gt; smoother.  What makes you think that you and your nerds are so good that
</EM><BR>
<EM>&gt; you will see everything and don't need the eyeballs of other nerds
</EM><BR>
<EM>&gt; outside your cabal at all?  This, imho, is much more arrogant than
</EM><BR>
<EM>&gt; creating the Seed itself.
</EM><BR>
<P>The first reason I turned away from open source is that I started thinking
<BR>
about the ways that the interim development stages of seed AI could be
<BR>
misused.  Not &quot;misused&quot;, actually, so much as the miscellaneous economic
<BR>
fallout.  If we had to live on this planet for fifty years I'd say to heck
<BR>
with it, humanity will adjust - but as it is, that whole scenario can be
<BR>
avoided, especially since I'm no longer sure an AI industry would help that
<BR>
much on building the nonindustrial AI.
<BR>
<P>So far, there are few enough people who demonstrate that they have understood
<BR>
the pattern of &quot;Coding a Transhuman AI&quot;.  I have yet to witness a single
<BR>
person go on and extend the pattern to areas I have not yet covered.  I'm
<BR>
sorry, and I dearly wish it was otherwise, but that's the way things are.
<BR>
<P>--              --              --              --              -- 
<BR>
Eliezer S. Yudkowsky                          <A HREF="http://singinst.org/">http://singinst.org/</A> 
<BR>
Research Fellow, Singularity Institute for Artificial Intelligence
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5509.html">J. R. Molloy: "Doomsday scenarios"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5507.html">David Blenkinsop: "Re: Does the state-vector collapse?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5493.html">Samantha Atkins: "Re: Fwd: Earthweb from transadmin"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="6128.html">Samantha Atkins: "Re: Fwd: Earthweb from transadmin"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6128.html">Samantha Atkins: "Re: Fwd: Earthweb from transadmin"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5508">[ date ]</A>
<A HREF="index.html#5508">[ thread ]</A>
<A HREF="subject.html#5508">[ subject ]</A>
<A HREF="author.html#5508">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:44 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
