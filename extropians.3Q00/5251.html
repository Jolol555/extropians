<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Fwd: Earthweb from transadmin</TITLE>
<META NAME="Author" CONTENT="Samantha Atkins (samantha@objectent.com)">
<META NAME="Subject" CONTENT="Re: Fwd: Earthweb from transadmin">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Fwd: Earthweb from transadmin</H1>
<!-- received="Tue Sep 19 00:27:16 2000" -->
<!-- isoreceived="20000919062716" -->
<!-- sent="Mon, 18 Sep 2000 23:29:33 -0700" -->
<!-- isosent="20000919062933" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Fwd: Earthweb from transadmin" -->
<!-- id="39C707CD.3DA6DBF7@objectent.com" -->
<!-- inreplyto="39C66667.A543DD6E@pobox.com" -->
<STRONG>From:</STRONG> Samantha Atkins (<A HREF="mailto:samantha@objectent.com?Subject=Re:%20Fwd:%20Earthweb%20from%20transadmin&In-Reply-To=&lt;39C707CD.3DA6DBF7@objectent.com&gt;"><EM>samantha@objectent.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Tue Sep 19 2000 - 00:29:33 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5252.html">KPJ: "Re: Raising children, spouses, and friends"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5250.html">Jason Joel Thompson: "Re: Fwd: Earthweb from transadmin"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5215.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5251">[ date ]</A>
<A HREF="index.html#5251">[ thread ]</A>
<A HREF="subject.html#5251">[ subject ]</A>
<A HREF="author.html#5251">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
&quot;Eliezer S. Yudkowsky&quot; wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Eugene Leitl wrote:
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; So, please tell me how you can predict the growth of the core
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I do not propose to predict the growth of the core.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Commonsense arguments are enough.  If you like, you can think of the
</EM><BR>
<EM>&gt; commonsense arguments as referring to fuzzily-bordered probability volumes in
</EM><BR>
<EM>&gt; the Hamiltonian space of possibilities, but I don't see how that would
</EM><BR>
<EM>&gt; contribute materially to intelligent thinking.
</EM><BR>
<P>I, and apparently many others, do not find the arguments made to date
<BR>
particularlly commensensical or fully convincing that the AI will be
<BR>
friendly.
<BR>
<P><EM>&gt; 
</EM><BR>
<EM>&gt; I can predict the behavior of the core in terms of ternary logic:  Either it's
</EM><BR>
<EM>&gt; friendly, or it's not friendly, or I have failed to understand What's Going
</EM><BR>
<EM>&gt; On.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; All else being equal, it should be friendly.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>All else being equal, any of the possibilities above are equally likely
<BR>
with a combination of either of the first two and the third being very
<BR>
likely.  :-)  Seriously, this is not remotedly a valid argument that the
<BR>
AI will be friendly.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Tell me how a piece of &quot;code&quot; during the bootstrap process and
</EM><BR>
<EM>&gt; &gt; afterwards can formally predict what another piece of &quot;code&quot;
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I do not propose to make formal predictions of any type.  Intelligence
</EM><BR>
<EM>&gt; exploits the regularities in reality; these regularities can be formalized as
</EM><BR>
<EM>&gt; fuzzily-bordered volumes of phase space - say, the space of possible minds
</EM><BR>
<EM>&gt; that can be described as &quot;friendly&quot; - but this formalization adds nothing.
</EM><BR>
<EM>&gt; Build an AI right smack in the middle of &quot;friendly space&quot; and it doesn't
</EM><BR>
<EM>&gt; matter how what kind of sophistries you can raise around the edges.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>But you have not yet shown that a) friendly space is well enough defined
<BR>
or b) that you can constrain your AI to it.  You have gone to some pains
<BR>
to clarify that &quot;constrain&quot; is precisely what cannot be done.  This
<BR>
leaves that the original design precludes the AI being not friendly. 
<BR>
But for this to be believed more widely requires more details of that
<BR>
design and why they preclude unfriendly outcomes. 
<BR>
&nbsp;
<BR>
<EM>&gt; I cannot formally predict the molecular behavior of a skyscraper; the
</EM><BR>
<EM>&gt; definition of skyscraper is not formally definable around the edges; I can
</EM><BR>
<EM>&gt; still tell the difference between a skyscraper and a hut.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>Irrelevant.
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Tell me how a team of human programmers is supposed to break through
</EM><BR>
<EM>&gt; &gt; the complexity bareer while building the seed AI without resorting to
</EM><BR>
<EM>&gt; &gt; evolutionary algorithms
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; We've been through this.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Evolution is the degenerate case of intelligent design in which intelligence
</EM><BR>
<EM>&gt; equals zero.  If I happen to have a seed AI lying around, why should it be
</EM><BR>
<EM>&gt; testing millions of unintelligent mutations when it could be testing millions
</EM><BR>
<EM>&gt; of intelligent mutations?
</EM><BR>
<EM>&gt;
</EM><BR>
<P>For tuning some of the fairly chaotic systems that will be part of its
<BR>
makeup.  Especially for tuning its sensory modalities.  Or did you think
<BR>
you and a company of master-hackers where going to program those
<BR>
capabilities in from scratch?  I suspect that hyper-fast GA driven
<BR>
desing and tuning will be essential to many of the subsystems of the AI
<BR>
and will also be employed by it directly for certain classes of
<BR>
problems.  
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; Tell me how a single distributed monode can arbitrate synchronous
</EM><BR>
<EM>&gt; &gt; events separated by light seconds, minutes, hours, years, megayears
</EM><BR>
<EM>&gt; &gt; distances without having to resort to relativistic signalling.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; You confuse computational architecture with cognitive coherence and
</EM><BR>
<EM>&gt; motivational coherence.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>Did that actually address the question fully?
<BR>
&nbsp;
<BR>
<EM>&gt; &gt; If it's not single, tell me what other nodes will do with a node's
</EM><BR>
<EM>&gt; &gt; decision they consider not kosher, and how they enforce it.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I do not expect motivational conflicts to arise due to distributed processing,
</EM><BR>
<EM>&gt; any more than I expect different nodes to come up with different laws of
</EM><BR>
<EM>&gt; arithmetic.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>Because you believe that you or perhaps the AI have solved or will solve
<BR>
the great problem of a universal objective morality and that too
<BR>
intelligent entities cannot disagree if they are both intelligent enough
<BR>
to understand that morality?  Do I need to point out that this is a
<BR>
philosophically shaky position?  Or do you believe something
<BR>
significantly different than this?
<BR>
<P>&nbsp;
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &gt; Tell me how many operations the thing will need to sample all possible
</EM><BR>
<EM>&gt; &gt; trajectories on the behaviour of the society as a whole (sounds
</EM><BR>
<EM>&gt; &gt; NP-complete to me), to pick the best of all possible worlds. (And will
</EM><BR>
<EM>&gt; &gt; it mean that all of us will have to till our virtual gardens?)
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I don't understand why you think I'm proposing such a thing.  I am not
</EM><BR>
<EM>&gt; proposing to instruct the Sysop to create the best of all possible worlds; I
</EM><BR>
<EM>&gt; am proposing that building a Sysop instructed to be friendly while preserving
</EM><BR>
<EM>&gt; individual rights is the best possible world *I* can attempt to create.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>How would you define individual rights?  Do I have the right to do
<BR>
something the Sysop may think is wacky?  Or does the Sysop override my
<BR>
decisions whenever it thinks it is significantly &quot;for my own good&quot;.  Is
<BR>
this actually &quot;being friendly&quot; to the type of creatures human beings
<BR>
are?
<BR>
For certain, you will find many human beings who will consider such the
<BR>
height of unbearable unfriendliness.  When they rebel (which some of
<BR>
them inevitably will) exactly how will the Sysop choose to be friendly?
<BR>
<P>&nbsp;
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &gt; There's more, but I'm finished for now. If you can argue all of above
</EM><BR>
<EM>&gt; &gt; points convincingly (no handwaving please), I might start to consider
</EM><BR>
<EM>&gt; &gt; that there's something more to your proposal than just hot air. So
</EM><BR>
<EM>&gt; &gt; show us the money, instead of constantly pelting the list with many
</EM><BR>
<EM>&gt; &gt; redundant descriptions of how wonderful the sysop will be. Frankly,
</EM><BR>
<EM>&gt; &gt; I'm getting sick of it.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Frankly, 'gene, I'm starting to get pretty sick of your attitude.  Who are you
</EM><BR>
<EM>&gt; to decide whether my proposal is hot air?  I can't see that it makes the least
</EM><BR>
<EM>&gt; bit of difference to the world what you think of my proposal, and frankly, you
</EM><BR>
<EM>&gt; have now managed to tick me off.  I may consider my AI work to be superior to
</EM><BR>
<EM>&gt; yours, but I don't propose that you have a responsibility to convince me of
</EM><BR>
<EM>&gt; one damn thing.  I expect to be extended the same courtesy.
</EM><BR>
<EM>&gt; 
</EM><BR>
<P><P>I for one, don't think the proposal is hot air at all.  I do think there
<BR>
is two much hand-waving about the implications of such a creation and
<BR>
about what the likelihood of it being friendly is.  
<BR>
<P>Your getting ticked off or other people being rude or not is not
<BR>
relevant to the real questions and concerns. You, of all people, know
<BR>
this.  So please take a deep breath.  I realize this is true but utterly
<BR>
irrelevant advice at the moment.  :-)
<BR>
<P>- samantha
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5252.html">KPJ: "Re: Raising children, spouses, and friends"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5250.html">Jason Joel Thompson: "Re: Fwd: Earthweb from transadmin"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5215.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5251">[ date ]</A>
<A HREF="index.html#5251">[ thread ]</A>
<A HREF="subject.html#5251">[ subject ]</A>
<A HREF="author.html#5251">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:29 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
