<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="hal@finney.org (hal@finney.org)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Sat Sep 30 13:18:10 2000" -->
<!-- isoreceived="20000930191810" -->
<!-- sent="Sat, 30 Sep 2000 12:15:09 -0700" -->
<!-- isosent="20000930191509" -->
<!-- name="hal@finney.org" -->
<!-- email="hal@finney.org" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="200009301915.MAA16940@finney.org" -->
<!-- inreplyto="Why would AI want to be friendly?" -->
<STRONG>From:</STRONG> <A HREF="mailto:hal@finney.org?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;200009301915.MAA16940@finney.org&gt;"><EM>hal@finney.org</EM></A><BR>
<STRONG>Date:</STRONG> Sat Sep 30 2000 - 13:15:09 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="6177.html">Samantha Atkins: "Re: Why wouldn't friendly AI leave fundies in the dust?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6175.html">Franklin Wayne Poley: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="5641.html">J. R. Molloy: "Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6176">[ date ]</A>
<A HREF="index.html#6176">[ thread ]</A>
<A HREF="subject.html#6176">[ subject ]</A>
<A HREF="author.html#6176">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Andrew Lias wrote:
<BR>
<EM>&gt; I've been following the debates regarding the possibilities of friendly vs.
</EM><BR>
<EM>&gt; unfriendly AI and I have a question.  It seems that we are presuming that a
</EM><BR>
<EM>&gt; friendly AI would be friendly towards us in a manner that we would recognize
</EM><BR>
<EM>&gt; as friendly.  Indeed, what, precisely, do we mean by friendly?
</EM><BR>
<P>I think this is a good point (and welcome to the list!).  It's not so
<BR>
much a problem in what we build into the AI, it's that &quot;friendly&quot; is
<BR>
a very loose concept.  It's so loose that I doubt that it is really
<BR>
meaningful as a top-level goal.  Such a goal would have to have a
<BR>
relatively precise definition, otherwise the AI will not be able to
<BR>
rigorously derive subgoals from it.
<BR>
<P>What do we really want an AI to do?  We need to clarify our own thoughts
<BR>
and ideas on the matter to have any hope of telling it what we want.
<BR>
<P>One problem is that there is going to be a tradeoff between challenge
<BR>
and happiness.  The AI can make people happy by putting nanobots in their
<BR>
heads.  He can give them challenges that they are rigged always to win,
<BR>
but that cheats them out of any meaningful success.  He can give them true
<BR>
challenges where they may fail, but then some people will be unhappy.
<BR>
<P>The word &quot;friendly&quot; fails to give any insight to the AI about where he
<BR>
should aim for on this scale (and on other similar tradeoffs).  Humans
<BR>
will disagree about what a &quot;friendly&quot; AI should do.  Satisfying some
<BR>
people will disappoint others.  Even a single person may have mixed
<BR>
feelings about what is best.
<BR>
<P>I don't believe there is an answer to the question of what the AI should
<BR>
do for humans.  It's not that we're not smart enough to see the answer,
<BR>
it's that the question itself is not well posed.  That word &quot;should&quot;
<BR>
keeps creeping in.
<BR>
<P>Ultimately we're back to Absolute Morality again.  We want the AI to be
<BR>
God, to do what is Good, what is Best.  Capital letters are everywhere.
<BR>
This concept just doesn't wash.  There are no absolutes when it comes
<BR>
to these issues, IMO.
<BR>
<P>Hal
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="6177.html">Samantha Atkins: "Re: Why wouldn't friendly AI leave fundies in the dust?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6175.html">Franklin Wayne Poley: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="5641.html">J. R. Molloy: "Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6176">[ date ]</A>
<A HREF="index.html#6176">[ thread ]</A>
<A HREF="subject.html#6176">[ subject ]</A>
<A HREF="author.html#6176">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:30 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
