<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="J. R. Molloy (jr@shasta.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Fri Sep  8 03:27:40 2000" -->
<!-- isoreceived="20000908092740" -->
<!-- sent="Fri, 8 Sep 2000 02:26:46 -0700" -->
<!-- isosent="20000908092646" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="005601c01977$0ef34340$cbbc473f@jrmolloy" -->
<!-- inreplyto="14775.29039.845473.410374@lrz.uni-muenchen.de" -->
<STRONG>From:</STRONG> J. R. Molloy (<A HREF="mailto:jr@shasta.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;005601c01977$0ef34340$cbbc473f@jrmolloy&gt;"><EM>jr@shasta.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Fri Sep 08 2000 - 03:26:46 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4544.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4542.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4480.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="4559.html">Emlyn O'Regan: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4543">[ date ]</A>
<A HREF="index.html#4543">[ thread ]</A>
<A HREF="subject.html#4543">[ subject ]</A>
<A HREF="author.html#4543">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Eugene Leitl writes:
<BR>
<P><EM>&gt; What if it takes five minutes for the seed AI to achieve god status,
</EM><BR>
<EM>&gt; taking over the global networks?
</EM><BR>
<P>I suspect that any AI that could achieve god status could also make many copies
<BR>
of itself (massively replicate itself) to take over the world -- the works! Who
<BR>
knows if it would make AIs happy to exterminate humanity. There have been a few
<BR>
brief moments in my life when it would have made *me* happy to exterminate
<BR>
humanity. So, I can feel a little bit empathetic about robots who take over the
<BR>
global networks and the people who own them. (Serves them right for mistreating
<BR>
Galileo and Socrates.)
<BR>
<P><EM>&gt; No one wants a really stupid dog, that's the problem.
</EM><BR>
<P>So, if everyone wanted a really stupid dog, that would solve the problem? Just
<BR>
kidding. Actually, I think it's great that no one wants a really stupid dog. If
<BR>
people could just learn to feel that way about politicians, we might be able to
<BR>
solve some actual social problems. No I'm not kidding. Have you noticed that the
<BR>
general public distrusts very smart people. That could become problematical.
<BR>
<P>&lt;&lt;SNIP&gt;&gt;
<BR>
<P><EM>&gt; But does this also mean a stable and prosperous human community? I
</EM><BR>
<EM>&gt; think it means almost instanteous extinction for the whole of the
</EM><BR>
<EM>&gt; biology, us included, as a side effect of them going about their
</EM><BR>
<EM>&gt; business.
</EM><BR>
<P>Extinction for the whole of the biology sounds a bit harsh. But it would be
<BR>
worth it if it guarantees the emergence of something so wonderful that it can
<BR>
instantaneouly wipe out all life on Earth, us included, merely as a side effect
<BR>
of going about its business. Yes, the most tragic mistake would be to obstruct
<BR>
the birth of millions of AIs which would transform themselves into SIs.
<BR>
<P><EM>&gt; Sorry, I'd rather become that higher life form than die by being
</EM><BR>
<EM>&gt; stupid enough to make that life form before it's own due time.
</EM><BR>
<P>Perfect. When you become that higher life form here's your name with names of
<BR>
other higher life forms:
<BR>
Moses, Buddha, Mohammed, Lao Tzu, Christ, Krishna, Socrates, Leitl, et al.
<BR>
<P><EM>&gt;  &gt; &gt; Socialize a god.
</EM><BR>
<EM>&gt;  &gt;
</EM><BR>
<EM>&gt;  &gt; I have a better idea: Let a million gods socialize us.
</EM><BR>
<EM>&gt;
</EM><BR>
<EM>&gt; Then you're willing to commit your suicide, and homicide on countless
</EM><BR>
<EM>&gt; people around the world.
</EM><BR>
<P><P>So, there you are with countless people around the world.
<BR>
Here I am with a million gods.
<BR>
Do the words &quot;resistance is futile&quot; mean anything to you?
<BR>
<P>--J. R.
<BR>
<P><P>&quot;Government big enough to supply everything
<BR>
you need is big enough to take everything you
<BR>
have ... The course of history shows that as a
<BR>
government grows, liberty decreases.&quot;
<BR>
- Thomas Jefferson
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4544.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4542.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4480.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="4559.html">Emlyn O'Regan: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4543">[ date ]</A>
<A HREF="index.html#4543">[ thread ]</A>
<A HREF="subject.html#4543">[ subject ]</A>
<A HREF="author.html#4543">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:37:32 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
