<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Fwd: Earthweb from transadmin</TITLE>
<META NAME="Author" CONTENT="Matt Gingell (mjg223@nyu.edu)">
<META NAME="Subject" CONTENT="Re: Fwd: Earthweb from transadmin">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Fwd: Earthweb from transadmin</H1>
<!-- received="Mon Sep 18 21:50:20 2000" -->
<!-- isoreceived="20000919035020" -->
<!-- sent="Mon, 18 Sep 2000 23:57:14 -0400 (EDT)" -->
<!-- isosent="20000919035714" -->
<!-- name="Matt Gingell" -->
<!-- email="mjg223@nyu.edu" -->
<!-- subject="Re: Fwd: Earthweb from transadmin" -->
<!-- id="14790.58394.758038.73971@bahgwah.nyu.edu" -->
<!-- inreplyto="39C66667.A543DD6E@pobox.com" -->
<STRONG>From:</STRONG> Matt Gingell (<A HREF="mailto:mjg223@nyu.edu?Subject=Re:%20Fwd:%20Earthweb%20from%20transadmin&In-Reply-To=&lt;14790.58394.758038.73971@bahgwah.nyu.edu&gt;"><EM>mjg223@nyu.edu</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon Sep 18 2000 - 21:57:14 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5238.html">Zero Powers: "Re: OpenMind Project at MIT"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5236.html">Zero Powers: "Re: responsibility for children"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5215.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5242.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5242.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5237">[ date ]</A>
<A HREF="index.html#5237">[ thread ]</A>
<A HREF="subject.html#5237">[ subject ]</A>
<A HREF="author.html#5237">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Eliezer S. Yudkowsky writes:
<BR>
<EM> &gt; Eugene Leitl wrote:
</EM><BR>
<EM> &gt; &gt; 
</EM><BR>
<EM> &gt; &gt; So, please tell me how you can predict the growth of the core
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; I do not propose to predict the growth of the core.
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; Commonsense arguments are enough.  If you like, you can think of the
</EM><BR>
<EM> &gt; commonsense arguments as referring to fuzzily-bordered probability volumes in
</EM><BR>
<EM> &gt; the Hamiltonian space of possibilities, but I don't see how that would
</EM><BR>
<EM> &gt; contribute materially to intelligent thinking.
</EM><BR>
<P>&nbsp;[snip]
<BR>
<P><EM> &gt; &gt; Tell me how a piece of &quot;code&quot; during the bootstrap process and
</EM><BR>
<EM> &gt; &gt; afterwards can formally predict what another piece of &quot;code&quot;
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; I do not propose to make formal predictions of any type.  Intelligence
</EM><BR>
<EM> &gt; exploits the regularities in reality; these regularities can be formalized as
</EM><BR>
<EM> &gt; fuzzily-bordered volumes of phase space - say, the space of possible minds
</EM><BR>
<EM> &gt; that can be described as &quot;friendly&quot; - but this formalization adds nothing. 
</EM><BR>
<EM> &gt; Build an AI right smack in the middle of &quot;friendly space&quot; and it doesn't
</EM><BR>
<EM> &gt; matter how what kind of sophistries you can raise around the edges.
</EM><BR>
<P>&nbsp;Actually, it's quite useful, or at least it makes clear what's being
<BR>
&nbsp;argued about: Posit a space of brains, each paired with a coordinate
<BR>
&nbsp;specifying the successor that brain will design. Call it a trajectory
<BR>
&nbsp;set or a phase space or a manifold or whatever else you feel like
<BR>
&nbsp;calling it.
<BR>
<P>&nbsp;You seem to think there's a dense, more-or-less coherent, domain of
<BR>
&nbsp;attraction with a happily-ever-after stable point in the middle. A
<BR>
&nbsp;blotch of white paint with some grey bleed around the edges. If we
<BR>
&nbsp;start close to the center, our AI can orbit around as much as it
<BR>
&nbsp;likes but never escape into not-nice space. The boundaries are fuzzy,
<BR>
&nbsp;but that's unimportant: I can know the Empire State Building is a
<BR>
&nbsp;skyscraper without knowing exactly how many floors it has, or having
<BR>
&nbsp;any good reason for believing a twelve story apartment building
<BR>
&nbsp;isn't. So long as we're careful to start somewhere unambiguous, we
<BR>
&nbsp;don't have to worry about formal definitions or prove nothing nasty
<BR>
&nbsp;is going to happen.
<BR>
<P>&nbsp;Eugene, on the other hand, seems to think this is not the case.
<BR>
&nbsp;Rather, the friendly and unfriendly subspaces are embedded in each
<BR>
&nbsp;other, and a single chaotic step in the wrong direction shoots a
<BR>
&nbsp;thread off into unpredictability. More like intertwined balls of
<BR>
&nbsp;turbulent Cantor-String, knotted together at infinitely many
<BR>
&nbsp;meta-stable catastrophe points, than like the comforting whirlpool of
<BR>
&nbsp;nice brains designing ever nicer ones. Your final destination is
<BR>
&nbsp;still a function of where you start, but it's so sensitive to initial
<BR>
&nbsp;conditions it depends more on the floating point rounding model you
<BR>
&nbsp;used on your first build than it does on anything you actually though
<BR>
&nbsp;about. 
<BR>
<P>&nbsp;The later seems more plausible, my being a pessimist, the trajectory
<BR>
&nbsp;to hell being paved with locally-good intentions, etc. But who knows?
<BR>
&nbsp;That any prediction we're capable of making is necessarily wrong
<BR>
&nbsp;seems like a reasonable rule of thumb. You're the last person I'd
<BR>
&nbsp;expect to see make an appeal to common sense...
<BR>
<P><EM> &gt; Evolution is the degenerate case of intelligent design in which intelligence
</EM><BR>
<EM> &gt; equals zero.  If I happen to have a seed AI lying around, why should it be
</EM><BR>
<EM> &gt; testing millions of unintelligent mutations when it could be testing millions
</EM><BR>
<EM> &gt; of intelligent mutations?
</EM><BR>
<P>&nbsp;Intelligent design without intelligence is exactly what makes
<BR>
&nbsp;evolution such an interesting bootstrap: It doesn't beg the question
<BR>
&nbsp;of how to build an intelligent machine by assuming you happen to have
<BR>
&nbsp;one lying around already.
<BR>
<P><EM> &gt; &gt; Tell me how the thing is guarded against spontaneous emergence of
</EM><BR>
<EM> &gt; &gt; autoreplicators in its very fabric, and from invasion of alien
</EM><BR>
<EM> &gt; &gt; autoreplicators from the outside.
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; Solar Defense is the Sysop's problem; I fail to see why this problem is
</EM><BR>
<EM> &gt; particularly more urgent for the Sysop Scenario then in any of the other
</EM><BR>
<EM> &gt; possible futures.
</EM><BR>
<P>&nbsp;Eugene is talking, I think, about parasitic memes and mental illness
<BR>
&nbsp;here, not space invaders.
<BR>
<P><EM> &gt; &gt; Tell me how many operations the thing will need to sample all possible
</EM><BR>
<EM> &gt; &gt; trajectories on the behaviour of the society as a whole (sounds
</EM><BR>
<EM> &gt; &gt; NP-complete to me), to pick the best of all possible worlds. (And will
</EM><BR>
<EM> &gt; &gt; it mean that all of us will have to till our virtual gardens?)
</EM><BR>
<EM> &gt;
</EM><BR>
<EM> &gt; I don't understand why you think I'm proposing such a thing.  I am not
</EM><BR>
<EM> &gt; proposing to instruct the Sysop to create the best of all possible worlds; I
</EM><BR>
<EM> &gt; am proposing that building a Sysop instructed to be friendly while preserving
</EM><BR>
<EM> &gt; individual rights is the best possible world *I* can attempt to create.
</EM><BR>
<P>&nbsp;The best bad option is better than the rest, I suppose, and if seed
<BR>
&nbsp;AI ends up being the way that works then it certainly can't hurt to
<BR>
&nbsp;program some affection into it. But it's rather like thinking a
<BR>
&nbsp;friendly, symbiotic, strain of bacteria is likely to eventually
<BR>
&nbsp;evolve into friendly people. The first few steps might preserve
<BR>
&nbsp;something of our initial intent, but none of the well-meaning
<BR>
&nbsp;intermediates is going to have any more luck anticipating the
<BR>
&nbsp;behavior of a qualitatively better brain than you or I. 
<BR>
<P>&nbsp;-matt
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5238.html">Zero Powers: "Re: OpenMind Project at MIT"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5236.html">Zero Powers: "Re: responsibility for children"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5215.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5242.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5242.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5237">[ date ]</A>
<A HREF="index.html#5237">[ thread ]</A>
<A HREF="subject.html#5237">[ subject ]</A>
<A HREF="author.html#5237">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:28 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
