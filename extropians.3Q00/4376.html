<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly? (Was: Congrat</TITLE>
<META NAME="Author" CONTENT="Eugene Leitl (eugene.leitl@lrz.uni-muenchen.de)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)</H1>
<!-- received="Wed Sep  6 10:32:18 2000" -->
<!-- isoreceived="20000906163218" -->
<!-- sent="Wed, 6 Sep 2000 08:14:18 -0700 (PDT)" -->
<!-- isosent="20000906151418" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)" -->
<!-- id="14774.24394.447997.612700@lrz.uni-muenchen.de" -->
<!-- inreplyto="20000905.182435.-641377.2.shabrika@juno.com" -->
<STRONG>From:</STRONG> Eugene Leitl (<A HREF="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?%20(Was:%20Congratulations%20to%20Eli,Brian%20...)&In-Reply-To=&lt;14774.24394.447997.612700@lrz.uni-muenchen.de&gt;"><EM>eugene.leitl@lrz.uni-muenchen.de</EM></A>)<BR>
<STRONG>Date:</STRONG> Wed Sep 06 2000 - 09:14:18 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4377.html">[ Robert-Coyote ]: "Re: exercise with mild vascular occlusion stimulates growth hormone and muscular strength"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4375.html">hal@finney.org: "Re: META: Why I'm boycotting Extropy(TM)."</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4299.html">Barbara Lamar: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4300.html">Zero Powers: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4376">[ date ]</A>
<A HREF="index.html#4376">[ thread ]</A>
<A HREF="subject.html#4376">[ subject ]</A>
<A HREF="author.html#4376">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Barbara Lamar writes:
<BR>
<EM> &gt; In an earlier email in this thread, Eliezer wrote the following:
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; &gt;&gt;If, as seems to be the default scenario, all supergoals are ultimately
</EM><BR>
<EM> &gt; arbitrary, then the superintelligence should do what we ask it to, for
</EM><BR>
<EM> &gt; lack of
</EM><BR>
<EM> &gt; anything better to do.&lt;&lt;
</EM><BR>
<P>Sorry, I don't believe in Santa Claus. This assumes a single spatially
<BR>
distributed thing which is brittle as hell. I do not see a viable
<BR>
trajectory towards such a world state, and I do not see how such state
<BR>
can be sustainable.
<BR>
&nbsp;
<BR>
The whole of evolutionary biology and a lot of computational physics
<BR>
would go out of the window if that thing will become reality. Since I
<BR>
have a lot of faith in evolutionary biology and computational physics,
<BR>
I'd rather assume that thing is a yet another red herring, one of far
<BR>
too many.
<BR>
<P><EM> &gt; In another email, Eliezer wrote this:
</EM><BR>
<EM> &gt; &gt; If it's smart enough, then it won't make mistakes.  If it's not 
</EM><BR>
<P>&quot;If it's smart enough, then it won't make mistakes&quot;. 
<BR>
<P>Sounds rather transcendent. Can anything in the physical world be that
<BR>
smart, especially since the world contains that supersmart thing?
<BR>
Wouldn't I have to have full knowledge of my state at t, while I'm
<BR>
myself at t, and have not enough bits to represent myself and the
<BR>
state of the world I'm supposed to be in full knowledge of?
<BR>
<P><EM> &gt; &gt; smart enough,
</EM><BR>
<EM> &gt; &gt; then it should be able to appreciate this fact, and help us add 
</EM><BR>
<EM> &gt; &gt; safeguards to
</EM><BR>
<EM> &gt; &gt; prevent itself from making mistakes that would interfere with its 
</EM><BR>
<EM> &gt; &gt; own goals.
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; I'm having a difficult time reconciling these 2 statements.  If the SI
</EM><BR>
<EM> &gt; has no preferences and would do whatever asked for lack of anything
</EM><BR>
<EM> &gt; better to do, then how could it be said to have its own goals?  
</EM><BR>
<P>We're not smart. But we can make something much smarter than us in a
<BR>
fully controlled fashion, and can predict all properties of this
<BR>
hypothetical future entitity reliably before we even have attempted to
<BR>
build it. Yeah, right.
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4377.html">[ Robert-Coyote ]: "Re: exercise with mild vascular occlusion stimulates growth hormone and muscular strength"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4375.html">hal@finney.org: "Re: META: Why I'm boycotting Extropy(TM)."</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4299.html">Barbara Lamar: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4300.html">Zero Powers: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4376">[ date ]</A>
<A HREF="index.html#4376">[ thread ]</A>
<A HREF="subject.html#4376">[ subject ]</A>
<A HREF="author.html#4376">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:37:20 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
