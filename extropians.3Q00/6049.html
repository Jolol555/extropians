<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Eugene Leitl (eugene.leitl@lrz.uni-muenchen.de)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Fri Sep 29 04:42:53 2000" -->
<!-- isoreceived="20000929104253" -->
<!-- sent="Fri, 29 Sep 2000 01:52:46 -0700 (PDT)" -->
<!-- isosent="20000929085246" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="14804.22622.313357.881993@lrz.uni-muenchen.de" -->
<!-- inreplyto="01bd01c029e9$99ba0e00$c5bc473f@jrmolloy" -->
<STRONG>From:</STRONG> Eugene Leitl (<A HREF="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;14804.22622.313357.881993@lrz.uni-muenchen.de&gt;"><EM>eugene.leitl@lrz.uni-muenchen.de</EM></A>)<BR>
<STRONG>Date:</STRONG> Fri Sep 29 2000 - 02:52:46 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="6050.html">Eugene Leitl: "Re: Boeing's HAL-2001"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6048.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6035.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6146.html">Samantha Atkins: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6049">[ date ]</A>
<A HREF="index.html#6049">[ thread ]</A>
<A HREF="subject.html#6049">[ subject ]</A>
<A HREF="author.html#6049">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
J. R. Molloy writes:
<BR>
<P><EM> &gt; Smart but poor people go to work for rich dumb people all the time. Henry Ford
</EM><BR>
<EM> &gt; (or was it Edison?) reportedly said that the key to his success was hiring
</EM><BR>
<EM> &gt; people who were smarter than him.
</EM><BR>
&nbsp;
<BR>
Do you work for rabbits? For daisies? What can you give a &gt;H AI what
<BR>
the &gt;H AIs can't get for themselves?
<BR>
<P>And why do I have to repeat that argument? Isn't it obvious?
<BR>
&nbsp;
<BR>
<EM> &gt; Are we both projecting inner feelings onto our fantasy creatures?
</EM><BR>
&nbsp;
<BR>
No. I want them to be friendly just as you. However, in my
<BR>
extrapolations I attempt to preserve a trace of rigour, relying on a
<BR>
few assumptions: limits to computational physics and persistance of
<BR>
evolution (imperfect reproduction in face of limited resources).
<BR>
<P>I don't know on what assumptions you base your extrapolations, because
<BR>
you have never stated them explicitly. All I see is that you pick out
<BR>
facts suiting you and ignoring others, shifting positions on a post to
<BR>
post basis. In case you're wondering, this does not exactly lend
<BR>
credibility to your arguments. This is also generating a lot of noise
<BR>
on the list, so I will not continue this indefinitely.
<BR>
&nbsp;
<BR>
<EM> &gt; &gt; Unfortunately, the method which makes the AI powerful automatically
</EM><BR>
<EM> &gt; &gt; makes it less than friendly. If it's friendly, it's less than useful.
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; I don't see that at all, perhaps because I have no use for unfriendly genius.
</EM><BR>
<P>Of which relevance is this to the issue at hand? I don't have use for
<BR>
idiots nor haemorrhagic viruses either, nevertheless there are
<BR>
sufficient amounts of them out there. Evolution must love them, 'cause
<BR>
it made so many of them.
<BR>
<P><EM> &gt; Then again, the perfect intelligent robot would be friendly to its owner and
</EM><BR>
<EM> &gt; unfriendly toward others, rather like a guard dog.
</EM><BR>
<P>Then again, why not a bandersnatch. 
<BR>
<P>Everybody knows a unicorn's horn can scratch steel, and it can be made
<BR>
docile when it will lay its head in a virgin maiden's lap. So let's go
<BR>
hunt some unicorns, there must be some out there in the foothills.
<BR>
&nbsp;
<BR>
<EM> &gt; So, what is your position? You think roboticists should be forbidden to make
</EM><BR>
<EM> &gt; robots that are too smart?
</EM><BR>
<P>I think codes directly mutating machine instructions (including
<BR>
virtual machine instructions) using evolutionary algorithms should be
<BR>
considered dangerous, and hence regulated. Enforcement should receive
<BR>
progressively higher priority as resources in single installations go
<BR>
up thanks to Moore. AI@home type of projects using above technologies
<BR>
should be outlawed. Research facilities using above techniques should
<BR>
be permanently reviewed on how they handle data carrier (regardless
<BR>
what is on that carrier) and strict offline quarantine. Data carrier
<BR>
traffic must be one-way: from the outside to the inside only. This
<BR>
means the cluster you do research with must be located in a physically
<BR>
secure permanently offline facility. Decomissioned components must be
<BR>
destroyed onsite (e.g. hard drives and flash memories using
<BR>
thermite). Etc. etc. etc.
<BR>
<P>Notice that above is not designed to contain AI, just to contain
<BR>
infectious code you generate during AI research. You definitely do not 
<BR>
want that roaming the networks of near future without artificial
<BR>
immune systems.
<BR>
<P>I don't think physical solutions for fragmenting the Net are globally
<BR>
enforcible (and also are likely to be misused), and hence I'm not
<BR>
suggesting this here.
<BR>
<P>Research into engineered biological pathogens and free-environment
<BR>
(this includes space) capable molecular autoreplicators should also be
<BR>
similiarly regulated. I would personally think best locations for
<BR>
these would way be outside of Earth's gravity well, in a really really 
<BR>
good containment. Something which you could rapidly deorbit into the
<BR>
Sun would seem a good place. (A nuke would be probably not safe).
<BR>
<P>Whether it will be any good is open to question, but at least you'll
<BR>
be plugging some holes and buying precious time. Reducing the amount
<BR>
of potential nucleation sites reduces the probability of nucleation
<BR>
event over a given period of time, as long as it doesn't bring people
<BR>
on stupid ideas. (Many teenagers would be thrilled to do classified
<BR>
research from their bedrooms).
<BR>
<P>If I knew someone is about to succeed in building a &gt;H AI or a gray
<BR>
goo autoreplicator before we're ready for it, despite an universal
<BR>
moratorium I would nuke him without a second thought.
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="6050.html">Eugene Leitl: "Re: Boeing's HAL-2001"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6048.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6035.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6146.html">Samantha Atkins: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6049">[ date ]</A>
<A HREF="index.html#6049">[ thread ]</A>
<A HREF="subject.html#6049">[ subject ]</A>
<A HREF="author.html#6049">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:23 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
