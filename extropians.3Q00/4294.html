<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly? (Was: Congrat</TITLE>
<META NAME="Author" CONTENT="Barbara Lamar (shabrika@juno.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)</H1>
<!-- received="Tue Sep  5 16:50:46 2000" -->
<!-- isoreceived="20000905225046" -->
<!-- sent="Tue, 5 Sep 2000 16:11:32 -0500" -->
<!-- isosent="20000905211132" -->
<!-- name="Barbara Lamar" -->
<!-- email="shabrika@juno.com" -->
<!-- subject="Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)" -->
<!-- id="20000905.161830.-620315.5.shabrika@juno.com" -->
<!-- inreplyto="Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)" -->
<STRONG>From:</STRONG> Barbara Lamar (<A HREF="mailto:shabrika@juno.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?%20(Was:%20Congratulations%20to%20Eli,Brian%20...)&In-Reply-To=&lt;20000905.161830.-620315.5.shabrika@juno.com&gt;"><EM>shabrika@juno.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Tue Sep 05 2000 - 15:11:32 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4295.html">Technotranscendence: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4293.html">Stirling Westrup: "Re: Do we differ more on values or facts?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4296.html">Barbara Lamar: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4294">[ date ]</A>
<A HREF="index.html#4294">[ thread ]</A>
<A HREF="subject.html#4294">[ subject ]</A>
<A HREF="author.html#4294">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Note:  my questions below are intended as sincere questions, not as
<BR>
attempts to start an argument.  I've followed this thread with great
<BR>
interest, having myself asked this question and variations of it.
<BR>
<P><P>On Tue, 05 Sep 2000 14:19:43 -0400 &quot;Eliezer S. Yudkowsky&quot;
<BR>
&lt;<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?%20(Was:%20Congratulations%20to%20Eli,Brian%20...)&In-Reply-To=&lt;20000905.161830.-620315.5.shabrika@juno.com&gt;">sentience@pobox.com</A>&gt; writes:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Which assumes an inbuilt desire to reach some stage of independence. 
</EM><BR>
<EM>&gt;  You have
</EM><BR>
<EM>&gt; not explained how or why this desire materializes within the seed 
</EM><BR>
<EM>&gt; AI.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I'm assuming we're talking about a seed AI, here, not a full-grown 
</EM><BR>
<EM>&gt; SI.
</EM><BR>
<P>What if you don't make this assumption?  Would a full-grown SI
<BR>
necessarily have an inbuilt desire to reach some stage of independence?  
<BR>
&nbsp;Independence from what or whom?   It seems as though this question must
<BR>
be answered before one can meaningfully discuss whether or not such
<BR>
independence would be a necessary condition of SuperIntelligence.
<BR>
<P><EM>&gt; You must be joking.  You cannot &quot;put down&quot; a superintelligence like 
</EM><BR>
<EM>&gt; some kind
</EM><BR>
<EM>&gt; of wounded pet. 
</EM><BR>
<P>No, not like a wounded pet.  But I can  imagine wanting to destroy a SI
<BR>
that I perceive as a threat to me. 
<BR>
<P>&nbsp;.The time for such decisions is before the seed AI 
<BR>
<EM>&gt; reaches
</EM><BR>
<EM>&gt; superintelligence, not after.
</EM><BR>
<P>Is this because once the AI reaches the SI stage it would be hopeless for
<BR>
the less intelligent humans to try to destroy it?  Or because of moral
<BR>
concerns?  Or some combination of both? Or is it logically necessary
<BR>
given the properties of humans &amp; SI's?
<BR>
<P><P><EM>&gt; One does not set out to &quot;control&quot; an AI that diverges from one's 
</EM><BR>
<EM>&gt; desires.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; One does not create a subject-object distinction between oneself and 
</EM><BR>
<EM>&gt; the AI.
</EM><BR>
<P>Would the pronoun &quot;one&quot; in the sentences above refer only to the
<BR>
creator(s) of the AI?  Or to some larger group of humans?  Would these
<BR>
sentences be valid if the pronoun &quot;one&quot; could also refer to an SI in
<BR>
relation to an AI?
<BR>
&nbsp;
<BR>
<EM>&gt; You shape yourself so that your own altruism is as rational, and 
</EM><BR>
<EM>&gt; internally
</EM><BR>
<EM>&gt; consistent as possible; only then is it possible to build a friendly 
</EM><BR>
<EM>&gt; AI while
</EM><BR>
<EM>&gt; still being completely honest, without attempting to graft on any 
</EM><BR>
<EM>&gt; chain of
</EM><BR>
<EM>&gt; reasoning that you would not accept yourself.
</EM><BR>
<P>You're not implying that the AI will necessarily take on the personality
<BR>
of its creator, though?  Why would honesty on the part of the creator be
<BR>
necessary?  Honesty with respect to what?
<BR>
<P><EM>&gt; You cannot build a friendly AI unless you are yourself a friend of 
</EM><BR>
<EM>&gt; the AI,
</EM><BR>
<EM>&gt; because otherwise your own adversarial attitude will lead you to 
</EM><BR>
<EM>&gt; build the AI
</EM><BR>
<EM>&gt; incorrectly.
</EM><BR>
<P>The above makes sense--but the fact that the creator is a friend of the
<BR>
AI doesn't imply that all humans are friends of the AI. It seems as
<BR>
though the inverse of this statement would also be true--that the Ai
<BR>
would not be a friends of all humans.   It seems unlikely that the
<BR>
creator could be friendly towards all of humanity except at an abstract
<BR>
level.  But maybe friendliness at the abstract level is all that's
<BR>
required? 
<BR>
<P>What about when a human or group of humans has interests that conflict
<BR>
with another human's or group of humans'?   What sort of instructions
<BR>
would the AI have for dealing with such situations?
<BR>
<P>Barbara
<BR>
________________________________________________________________
<BR>
YOU'RE PAYING TOO MUCH FOR THE INTERNET!
<BR>
Juno now offers FREE Internet Access!
<BR>
Try it today - there's no risk!  For your FREE software, visit:
<BR>
<A HREF="http://dl.www.juno.com/get/tagj">http://dl.www.juno.com/get/tagj</A>.
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4295.html">Technotranscendence: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4293.html">Stirling Westrup: "Re: Do we differ more on values or facts?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="4296.html">Barbara Lamar: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian ...)"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4294">[ date ]</A>
<A HREF="index.html#4294">[ thread ]</A>
<A HREF="subject.html#4294">[ subject ]</A>
<A HREF="author.html#4294">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:37:14 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
