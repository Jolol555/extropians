<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Sat Sep 30 19:40:57 2000" -->
<!-- isoreceived="20001001014057" -->
<!-- sent="Sat, 30 Sep 2000 21:39:55 -0400" -->
<!-- isosent="20001001013955" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="39D695EB.AC725FD1@pobox.com" -->
<!-- inreplyto="000901c02b4e$5942fe20$99630c3d@squashy2000" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;39D695EB.AC725FD1@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Sat Sep 30 2000 - 19:39:55 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="6185.html">Doug Jones: "Re: Back off!  Im gay!"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6183.html">Emlyn: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6183.html">Emlyn: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6184">[ date ]</A>
<A HREF="index.html#6184">[ thread ]</A>
<A HREF="subject.html#6184">[ subject ]</A>
<A HREF="author.html#6184">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Emlyn wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Eliezer wrote:
</EM><BR>
<EM>&gt; &gt; As for that odd scenario you posted earlier, curiosity - however necessary or
</EM><BR>
<EM>&gt; &gt; unnecessary to a functioning mind - is a perfectly reasonable subgoal of
</EM><BR>
<EM>&gt; &gt; Friendliness, and therefore doesn't *need* to have independent motive force.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I'm not sure I understand how curiosity can be a subgoal for a seed ai; I'd
</EM><BR>
<EM>&gt; love some more on that.
</EM><BR>
<P>You need curiosity in order to think, learn, and discover, and you need to
<BR>
think, learn, and discover in order to be more generally efficient at
<BR>
manipulating reality, and being more generally efficient at manipulating
<BR>
reality means you can be more efficiently Friendly.
<BR>
<P><EM>&gt; I read catai, and most of catai 2.0 (do you still call it that)?
</EM><BR>
<P>It's up to 2.2 these days.
<BR>
<P><EM>&gt; But I can't
</EM><BR>
<EM>&gt; remember some crucial things you said about goals/subgoals. Specifically, do
</EM><BR>
<EM>&gt; you expect them to be strictly hierarchical, or is it a more general
</EM><BR>
<EM>&gt; network, where if x is a (partial) subgoal of y, y can also be a (partial)
</EM><BR>
<EM>&gt; subgoal of x?
</EM><BR>
<P>&quot;I'm not rightly sure what kind of thinking could lead to this confusion.&quot; 
<BR>
Goals and subgoals are thoughts, not source code.  Subgoals, if they exist at
<BR>
all, exist as regularities in plans - that is, certain states of the Universe
<BR>
are reliably useful in achieving the real, desired states.  Since &quot;subgoals&quot;
<BR>
have no independent desirability - they are only useful way-stations along the
<BR>
road to the final state - they can be hierarchical, or networked, or strange
<BR>
loops, or arranged in whatever order you like; or rather, whatever order best
<BR>
mirrors reality.
<BR>
<P><EM>&gt; Certainly, it strikes me that there ought to be multiple &quot;top
</EM><BR>
<EM>&gt; level&quot; goals,
</EM><BR>
<P>BLEAH!
<BR>
<P><EM>&gt; and they ought to come into conflict;
</EM><BR>
<P>Quintuple bonus BLEAH!
<BR>
<P><EM>&gt; I don't think that one top level goal do the job.
</EM><BR>
<P>Why on Earth not?
<BR>
<P>--              --              --              --              -- 
<BR>
Eliezer S. Yudkowsky                          <A HREF="http://singinst.org/">http://singinst.org/</A> 
<BR>
Research Fellow, Singularity Institute for Artificial Intelligence
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="6185.html">Doug Jones: "Re: Back off!  Im gay!"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6183.html">Emlyn: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6183.html">Emlyn: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6184">[ date ]</A>
<A HREF="index.html#6184">[ thread ]</A>
<A HREF="subject.html#6184">[ subject ]</A>
<A HREF="author.html#6184">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:30 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
