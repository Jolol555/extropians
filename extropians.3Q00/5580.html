<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Eliezer S. Yudkowsky (sentience@pobox.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Sun Sep 24 11:31:30 2000" -->
<!-- isoreceived="20000924173130" -->
<!-- sent="Sun, 24 Sep 2000 13:20:10 -0400" -->
<!-- isosent="20000924172010" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="39CE37CA.63D8A798@pobox.com" -->
<!-- inreplyto="000e01c025f9$b3b45c00$0201a8c0@dcs" -->
<STRONG>From:</STRONG> Eliezer S. Yudkowsky (<A HREF="mailto:sentience@pobox.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;39CE37CA.63D8A798@pobox.com&gt;"><EM>sentience@pobox.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Sun Sep 24 2000 - 11:20:10 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5581.html">Pvthur@aol.com: "Perceptual Worlds (Was: Why friendly?)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5579.html">James J. Hughes: "Capitalists and concentration camps"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5559.html">Darin Sunley: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5580">[ date ]</A>
<A HREF="index.html#5580">[ thread ]</A>
<A HREF="subject.html#5580">[ subject ]</A>
<A HREF="author.html#5580">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Darin Sunley wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; It's so seldom I get a chance to contribute meaningfully to the conversation
</EM><BR>
<EM>&gt; here, but let me jump in...
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; You cannot win a negotiation against something that quite rightly views you
</EM><BR>
<EM>&gt; as a deterministic process.
</EM><BR>
<P>Exactly.  Humans sometimes look down on people they can consider easy to
<BR>
predict, so I should emphasize that &quot;looking down&quot; is an attitude and not an
<BR>
inevitable consequence - but yes, a mortal human is probably pretty much a
<BR>
deterministic and manipulable process to a transhuman, never mind a
<BR>
superintelligence.
<BR>
<P><EM>&gt; I think the relevant image here was of an AI carrying on an optimal
</EM><BR>
<EM>&gt; conversation with a human being by testing it's repsonses against a trillion
</EM><BR>
<EM>&gt; high fidelity simulations of that human, during the time it took the human
</EM><BR>
<EM>&gt; to draw a breath.
</EM><BR>
<P>This is the usual image, but even that level of superintelligence shouldn't be
<BR>
necessary.  As I once said, Deep Blue and Kasparov were evenly matched (more
<BR>
or less) despite the disparity in moves-per-second because Deep Blue was
<BR>
playing chess, while Kasparov was playing the regularities in the game of
<BR>
chess.  Very different search trees!
<BR>
<P>Running a trillion high-fidelity simulations is the Deep Blue path to
<BR>
irresistable persuasiveness, but that shouldn't be necessary.  There are
<BR>
regularities in the Game of Us.  I cannot view a human being on that level,
<BR>
but I can see enough to know that level exists.  There's a limited number of
<BR>
emotional tones and intuitions in the human mind.  There are a finite number
<BR>
of pieces and a finite number of moves, and the &quot;pieces&quot; and &quot;moves&quot; are
<BR>
regularities far, far above the neural level.
<BR>
<P>A human being can't model another human being this way; first of all, nobody
<BR>
knows what all the pieces are - have you ever seen a complete list of the
<BR>
emotional tones?  And even if someone did know it all, completely - a level of
<BR>
knowledge considerably in excess of that needed to build an AI - the size of
<BR>
the search tree would probably overflow the limits of human abstract thought.
<BR>
<P>The knowledge and intelligence needed to see a human being as a manipulable
<BR>
process lie considerably above the level of human intelligence.  But it
<BR>
doesn't require superintelligence, just fairly mild transhumanity.  You have
<BR>
to know all the pieces and be able to keep track of the most likely
<BR>
possibilities; that's around it.  I'll never be able to do it, or even come
<BR>
close, but I can see enough to know it can be done.
<BR>
<P>--              --              --              --              -- 
<BR>
Eliezer S. Yudkowsky                          <A HREF="http://singinst.org/">http://singinst.org/</A> 
<BR>
Research Fellow, Singularity Institute for Artificial Intelligence
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5581.html">Pvthur@aol.com: "Perceptual Worlds (Was: Why friendly?)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5579.html">James J. Hughes: "Capitalists and concentration camps"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5559.html">Darin Sunley: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5580">[ date ]</A>
<A HREF="index.html#5580">[ thread ]</A>
<A HREF="subject.html#5580">[ subject ]</A>
<A HREF="author.html#5580">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:48 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
