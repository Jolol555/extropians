<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Franklin Wayne Poley (culturex@vcn.bc.ca)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Mon Sep 25 15:04:10 2000" -->
<!-- isoreceived="20000925210410" -->
<!-- sent="Mon, 25 Sep 2000 14:03:51 -0700 (PDT)" -->
<!-- isosent="20000925210351" -->
<!-- name="Franklin Wayne Poley" -->
<!-- email="culturex@vcn.bc.ca" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="Pine.GSO.4.21.0009251400120.4694-100000@vcn.bc.ca" -->
<!-- inreplyto="39CFA4DD.DB8FE09C@objectent.com" -->
<STRONG>From:</STRONG> Franklin Wayne Poley (<A HREF="mailto:culturex@vcn.bc.ca?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;Pine.GSO.4.21.0009251400120.4694-100000@vcn.bc.ca&gt;"><EM>culturex@vcn.bc.ca</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon Sep 25 2000 - 15:03:51 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5730.html">Bryan Moss: "Re: GENE/SOC:  AAAS on human genengineering"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5728.html">Robin Hanson: "Re: Fear of Letting People Get Things They Want"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5718.html">Samantha Atkins: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="5735.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5737.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5729">[ date ]</A>
<A HREF="index.html#5729">[ thread ]</A>
<A HREF="subject.html#5729">[ subject ]</A>
<A HREF="author.html#5729">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
On Mon, 25 Sep 2000, Samantha Atkins wrote:
<BR>
<P><EM>&gt; &quot;J. R. Molloy&quot; wrote:
</EM><BR>
<EM>&gt; &gt; 
</EM><BR>
<EM>&gt; &gt; &gt; No, because this is not the Singularity Seed but simply successful
</EM><BR>
<EM>&gt; &gt; &gt; genetic programming projects.  You seem to be answering a very different
</EM><BR>
<EM>&gt; &gt; &gt; question than I was asking.
</EM><BR>
<EM>&gt; &gt; &gt;
</EM><BR>
<EM>&gt; &gt; &gt; - samantha
</EM><BR>
<EM>&gt; &gt; 
</EM><BR>
<EM>&gt; &gt; Right. You had asked, &quot;In what way is this good for or even compatible with the
</EM><BR>
<EM>&gt; &gt; nature of human beings?&quot;
</EM><BR>
<EM>&gt; &gt; 
</EM><BR>
<EM>&gt; &gt; But I maintain that what is good for or compatible with human nature is not
</EM><BR>
<EM>&gt; &gt; necessarily the most extropic path to higher levels of self-organized complexity
</EM><BR>
<EM>&gt; &gt; or more sentient life forms.
</EM><BR>
<EM>&gt; &gt; What is good for human beings may, after all, be bad for transhuman beings.
</EM><BR>
<EM>&gt; &gt; If AI is not friendly toward humans, that doesn't mean it will be unfriendly
</EM><BR>
<EM>&gt; &gt; toward transhumans.
</EM><BR>
<P><EM>&gt; I assume that a
</EM><BR>
<EM>&gt; transhuman is an evolved/augmented form of a human in many respects, and
</EM><BR>
<EM>&gt; thus shares many goals with humans, especially in the early near-human
</EM><BR>
<EM>&gt; stages.  So my question is as applicable to them as to &quot;mere&quot; humans.  I
</EM><BR>
<EM>&gt; am worried when I hear about a proposed AI that is so much smarter than
</EM><BR>
<EM>&gt; all humans (and transhumans for that matter) that it can and should make
</EM><BR>
<EM>&gt; decisions for all of us that we should (will be forced to?) obey without
</EM><BR>
<EM>&gt; questioning especially since our puny brains/processing units cannot
</EM><BR>
<EM>&gt; match Its Intelligence. 
</EM><BR>
<P><P>I have given hundreds of IQ tests over the course of my career and
<BR>
participated in the development of one of them (Cattell's CAB). If I were
<BR>
to measure transhuman-machine intelligence and human intelligence; and
<BR>
compare the profiles, how would they differ?
<BR>
FWP
<BR>
<P>-------------------------------------------------------------------------------
<BR>
Machine Psychology:
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;<A HREF="http://users.uniserve.com/~culturex/Machine-Psychology.htm">http://users.uniserve.com/~culturex/Machine-Psychology.htm</A>&gt;
<BR>
-------------------------------------------------------------------------------
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5730.html">Bryan Moss: "Re: GENE/SOC:  AAAS on human genengineering"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5728.html">Robin Hanson: "Re: Fear of Letting People Get Things They Want"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5718.html">Samantha Atkins: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="5735.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5737.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5729">[ date ]</A>
<A HREF="index.html#5729">[ thread ]</A>
<A HREF="subject.html#5729">[ subject ]</A>
<A HREF="author.html#5729">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:58 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
