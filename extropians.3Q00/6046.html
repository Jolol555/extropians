<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Eugene Leitl (eugene.leitl@lrz.uni-muenchen.de)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Fri Sep 29 02:48:36 2000" -->
<!-- isoreceived="20000929084836" -->
<!-- sent="Fri, 29 Sep 2000 00:45:27 -0700 (PDT)" -->
<!-- isosent="20000929074527" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="14804.18583.690077.538245@lrz.uni-muenchen.de" -->
<!-- inreplyto="007701c029a5$bd7555a0$c5bc473f@jrmolloy" -->
<STRONG>From:</STRONG> Eugene Leitl (<A HREF="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;14804.18583.690077.538245@lrz.uni-muenchen.de&gt;"><EM>eugene.leitl@lrz.uni-muenchen.de</EM></A>)<BR>
<STRONG>Date:</STRONG> Fri Sep 29 2000 - 01:45:27 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="6047.html">Eugene Leitl: "Re: Interview With Minsky"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6045.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6017.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6113.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6046">[ date ]</A>
<A HREF="index.html#6046">[ thread ]</A>
<A HREF="subject.html#6046">[ subject ]</A>
<A HREF="author.html#6046">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
J. R. Molloy writes:
<BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; I feel more affinity toward AI (which we should remember does not actually
</EM><BR>
<EM> &gt; exist) than I have ever been able to relate to the nine tenths of humanity which
</EM><BR>
<EM> &gt; remain immured in belief systems.
</EM><BR>
&nbsp;
<BR>
Now that is pretty harsh. Particularly considering the fact that
<BR>
everybody capable of thought is immured in a belief system. Say, how
<BR>
about a nice cask of amontillado?
<BR>
&nbsp;
<BR>
<EM> &gt; &gt; The first AI which has nucleated in the
</EM><BR>
<EM> &gt; &gt; network will copy (sexually) mutated copies of itself all over the
</EM><BR>
<EM> &gt; &gt; place in a wormlike fashion.
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; Now who's anthropomorphizing? How do you know what AIs will do? It seems as
</EM><BR>
<P>Is that your understanding of humanity? I was actually thinking about
<BR>
an inoculated Petri dish when I was writing this. The lowest common
<BR>
denominator of life: make mutated copies of yourself.
<BR>
<P><EM> &gt; likely as not to me, that AIs will steer clear of sexuality for the very
</EM><BR>
<EM> &gt; sensible reason that it clouds ratiocination and obscures common sense.
</EM><BR>
&nbsp;
<BR>
Groan. We're on very different buses here, again. Let's see, we have a
<BR>
cracker (or a psychotic researcher) writing an evolving Net worm, or a
<BR>
piece of Net infrastructure spontaneusly getting frisky and zooming
<BR>
off, because it has fallen into an undesigned state, suffered a bit
<BR>
mutation, or is being operated in an unforeseen context.
<BR>
<P>Assuming this, a system which doesn't propagate gets selected out very
<BR>
early. If it will start mutating itself on a single copy it is going
<BR>
to screw itself up sooner or later if it doesn't copy itself all over
<BR>
the place. Because you can't predict what impact a given change will
<BR>
have. Nonviable mutation frees the previously occupied slot by virtue
<BR>
of reduced fitness of the occupant relatively to its surrounding, to
<BR>
be reseeded from the population of viables. A population of critters
<BR>
in a limited resource theater is solving an optimization task. And
<BR>
hence doing a lot more than an isolated critter, or even a population
<BR>
of isolated critters. A lot of mutation is driven by recombination
<BR>
alone, so you will see fitness increasing even if you switch off
<BR>
mutation yet allow pairs of individua to recombine their genome (this
<BR>
is more than just a poor man's PRNG). This is what that sex thing is
<BR>
about.
<BR>
<P>Rationalization and common sense? These are extremely high-level
<BR>
concepts, which are not immediately translatable to AI. I think
<BR>
&quot;rationalization&quot; assumes a flawed, since dualized self model. You're
<BR>
mostly talking about artifacts of introspection, instead of the real
<BR>
thing. Like talking about the surface of the lake as the antithesis of
<BR>
the rest of the lake.
<BR>
<P>As to common sense, I presume this means street smarts. Ability to
<BR>
make the right decisions rapidly in face of incomplete and/or
<BR>
conflicting data. Darwinian systems are known to be able to handle
<BR>
that very nicely, why, they've grown up in the street.
<BR>
<P>The notion that AIs are going to be crystal clear citadels of pure
<BR>
thought must appear ludicrous. Because that notion does not make any
<BR>
sense in an evolutionary theatre.
<BR>
<P><EM> &gt; &gt; Because the copy process is much faster
</EM><BR>
<EM> &gt; &gt; than adding new nodes (even if you have nanotechnology) you have
</EM><BR>
<EM> &gt; &gt; instant resource scarcity and hence competition for limited
</EM><BR>
<EM> &gt; &gt; resources. Those individua with lesser fitness will have to go to the
</EM><BR>
<EM> &gt; &gt; great bit bucket in the sky.
</EM><BR>
<EM> &gt; 
</EM><BR>
<EM> &gt; So AI individua will be *very* friendly toward each other. The question then is,
</EM><BR>
<P>Huh? Your logic seems to be working on very different principles from
<BR>
mine. I just told you that AIs will have to compete and die just as we
<BR>
do, and you say &quot;they will be very friendly to each other&quot;. Remind me
<BR>
to never become your friend, will you?
<BR>
<P><EM> &gt; &quot;How far would AI extend its friendliness? Would it extend to you and me?&quot;
</EM><BR>
<EM> &gt; Perhaps it would. The friendliness of religious fanatics definately does not.
</EM><BR>
&nbsp;
<BR>
I wonder where you went now, I wish I could follow.
<BR>
<P><EM> &gt; Yes, we don't ever want to appear unfriendly to something more intelligent than
</EM><BR>
<EM> &gt; ourselves. But why does friendliness come into it at all? I mean, have you ever
</EM><BR>
<P>We don't want to appear unfriendly to something powerful, yes. Because
<BR>
then it will feel compelled to come and kick our butts. Perhaps it
<BR>
won't do that if we just lie low.
<BR>
<P><EM> &gt; thought that truth may have value greater than friendship? If our friends all
</EM><BR>
<P>Value in which particular value system?
<BR>
<P><EM> &gt; become deranged as a result of some weird virus that makes them politicized
</EM><BR>
<EM> &gt; zombies, perhaps we ought to place our trust in some artificial intelligence
</EM><BR>
<P>It is impossible to achieve quantitative infectivity on a genetically
<BR>
diverse populace with a given pathogen without full knowledge about
<BR>
the diff list.
<BR>
<P><EM> &gt; which remains impervious to such an attack, some AI which remains sane and
</EM><BR>
<EM> &gt; balanced. Shall we trust the natural intelligence of Hitler and Stalin more than
</EM><BR>
<P>What makes you think an AI will remain sane and balanced? Clearly it can't.
<BR>
<P><EM> &gt; the robot intelligence of our own device?
</EM><BR>
&nbsp;
<BR>
I don't know what you're smoking, but I wish I had some of it, it
<BR>
seems to be powerful stuff.
<BR>
<P><EM> &gt; &quot;These technologies are just too much for a species at our level of
</EM><BR>
<EM> &gt; sophistication.&quot;
</EM><BR>
<EM> &gt; --Bill Joy
</EM><BR>
<P>Gee, what a deep statement. Thanks, Billie-Joy. Nevermind that:
<BR>
<P>1) due to nature of these technologies sustainable relinquishment
<BR>
&nbsp;&nbsp;&nbsp;doesn't work, and some of the countermeasures make the original
<BR>
&nbsp;&nbsp;&nbsp;problem set pale in comparison
<BR>
<P>2) these technologies are necessary to move on to the next levels of
<BR>
&nbsp;&nbsp;&nbsp;sophistication 
<BR>
<P>3) if don't do that, we're screwed on the long run, anyway
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="6047.html">Eugene Leitl: "Re: Interview With Minsky"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6045.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="6017.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6113.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6046">[ date ]</A>
<A HREF="index.html#6046">[ thread ]</A>
<A HREF="subject.html#6046">[ subject ]</A>
<A HREF="author.html#6046">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:22 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
