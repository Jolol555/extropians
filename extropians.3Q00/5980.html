<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Frontiers of Friendly AI</TITLE>
<META NAME="Author" CONTENT="Dan Fabulich (dan@darkforge.cc.yale.edu)">
<META NAME="Subject" CONTENT="Re: Frontiers of Friendly AI">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Frontiers of Friendly AI</H1>
<!-- received="Thu Sep 28 13:10:35 2000" -->
<!-- isoreceived="20000928191035" -->
<!-- sent="Thu, 28 Sep 2000 13:59:48 -0400 (EDT)" -->
<!-- isosent="20000928175948" -->
<!-- name="Dan Fabulich" -->
<!-- email="dan@darkforge.cc.yale.edu" -->
<!-- subject="Re: Frontiers of Friendly AI" -->
<!-- id="Pine.LNX.4.10.10009281334490.19177-100000@localhost.localdomain" -->
<!-- inreplyto="39D3678C.39389E09@pobox.com" -->
<STRONG>From:</STRONG> Dan Fabulich (<A HREF="mailto:dan@darkforge.cc.yale.edu?Subject=Re:%20Frontiers%20of%20Friendly%20AI&In-Reply-To=&lt;Pine.LNX.4.10.10009281334490.19177-100000@localhost.localdomain&gt;"><EM>dan@darkforge.cc.yale.edu</EM></A>)<BR>
<STRONG>Date:</STRONG> Thu Sep 28 2000 - 11:59:48 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5981.html">Samantha Atkins: "Re: Some thoughts on Politics"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5979.html">Ross A. Finlayson: "Re: robodog is ready to talk"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5971.html">Eliezer S. Yudkowsky: "Frontiers of Friendly AI"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="6039.html">J. R. Molloy: "Re: Frontiers of Friendly AI"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6039.html">J. R. Molloy: "Re: Frontiers of Friendly AI"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5980">[ date ]</A>
<A HREF="index.html#5980">[ thread ]</A>
<A HREF="subject.html#5980">[ subject ]</A>
<A HREF="author.html#5980">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
[Non-member submission]
<BR>
<P>Eliezer S. Yudkowsky wrote:
<BR>
<P><EM> &gt;      Possible problem: On a moment-to-moment basis, the vast
</EM><BR>
<EM> &gt; majority of tasks are not materially affected by the fact that the
</EM><BR>
<EM> &gt; supergoal is Friendliness.  The optimal strategy for playing chess
</EM><BR>
<EM> &gt; is not obviously affected by whether the supergoal is Friendliness
</EM><BR>
<EM> &gt; or hostility.  Therefore, the system will tend to accumulate learned
</EM><BR>
<EM> &gt; complexity for the subgoals, but will not accumulate complexity for
</EM><BR>
<EM> &gt; the top of the goal chain - Friendliness, and any standard links
</EM><BR>
<EM> &gt; from Friendliness to the necessity for self-enhancement or some
</EM><BR>
<EM> &gt; other standard subgoal, will remain crystalline and brittle.  If
</EM><BR>
<EM> &gt; most of the de facto complexity rests with the subgoals, then is it
</EM><BR>
<EM> &gt; likely that future superintelligent versions of the AI's self will
</EM><BR>
<EM> &gt; grant priority to the subgoals?
</EM><BR>
<P>Yes, it is; this is a feature, not a bug.  We WANT it to start
<BR>
figuring goals out for itself; if the AI reasons that the meaning of
<BR>
life is X, then, somehow or other, X has got to be able to overwrite
<BR>
the IGS or whatever else we put up at the top.  Friendliness, in and
<BR>
of itself, isn't a design goal for the SI.  Friendliness is a goal we
<BR>
hope that it keeps in mind as it pursues its TRUE goal: the truth.
<BR>
<P><EM> &gt;      Stage:  The earliest, most primitive versions of the seed AI.
</EM><BR>
<EM> &gt;
</EM><BR>
<EM> &gt;      Possible problem: There is an error in the goal system, or the
</EM><BR>
<EM> &gt; goal system is incomplete.  Either you had to simplify at first
</EM><BR>
<EM> &gt; because the primitive AI couldn't understand all the referents, or
</EM><BR>
<EM> &gt; you (the programmer) changed your mind about something.  How do you
</EM><BR>
<EM> &gt; get the AI to let you change the goal system?  Obviously, changing
</EM><BR>
<EM> &gt; the goal system is an action that would tend to interfere with
</EM><BR>
<EM> &gt; whatever goals the AI currently possesses.
</EM><BR>
<P>It's hard to imagine when exactly this would be a problem.  If the AI
<BR>
is THAT primitive, and we regard it to be Really Important that we
<BR>
change our minds in a rather radical way, the option remains to shut
<BR>
the thing down and start it up again with the new architecture.
<BR>
<P>Alternately, you hopefully remembered to build the goal system so that
<BR>
it was not supposed to pursue (sub)goals X, Y and Z but to pursue
<BR>
(sub)goal A: &quot;try to do what we ask you to do, even if we change our
<BR>
minds.&quot;  So, at the primitive stages, doing what you ask shouldn't be
<BR>
a problem, presuming it's at least complex enough to understand what
<BR>
you're asking for in the first place.
<BR>
<P>However, if the AI has reached a stage where shutting it down is
<BR>
unfeasible, you'll just have to try to explain to it why it should
<BR>
follow your new goal system.  Hopefully, it'll see the light.
<BR>
<P><EM> &gt;      Bonus problem:  Suppose that you screw up badly enough that the AI not
</EM><BR>
<EM> &gt; only attempts to preserve the original goals, but also realizes that it (the
</EM><BR>
<EM> &gt; AI) must do so surreptitiously in order to succeed.  Can you think of any
</EM><BR>
<EM> &gt; methods that might help identify such a problem?
</EM><BR>
<P>No.  At the primitive stages, we might hope to be smart enough to be
<BR>
able to look at its inner workings, detect whether it's deceiving us,
<BR>
and react accordingly, but that's going to become unfeasible very
<BR>
early on.  Beyond that point, there's no obvious way to identify the
<BR>
problem besides watching it very closely and asking it questions about
<BR>
what it does, in much the same way you'd check to see whether a human
<BR>
believed your goal architecture or not.
<BR>
<P>-Dan
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-unless you love someone-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-nothing else makes any sense-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e.e. cummings
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5981.html">Samantha Atkins: "Re: Some thoughts on Politics"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5979.html">Ross A. Finlayson: "Re: robodog is ready to talk"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5971.html">Eliezer S. Yudkowsky: "Frontiers of Friendly AI"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="6039.html">J. R. Molloy: "Re: Frontiers of Friendly AI"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6039.html">J. R. Molloy: "Re: Frontiers of Friendly AI"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5980">[ date ]</A>
<A HREF="index.html#5980">[ thread ]</A>
<A HREF="subject.html#5980">[ subject ]</A>
<A HREF="author.html#5980">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:19 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
