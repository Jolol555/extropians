<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Eugene Leitl (eugene.leitl@lrz.uni-muenchen.de)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Wed Sep  6 04:03:50 2000" -->
<!-- isoreceived="20000906100350" -->
<!-- sent="Wed, 6 Sep 2000 01:22:12 -0700 (PDT)" -->
<!-- isosent="20000906082212" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="14773.65204.352861.599680@lrz.uni-muenchen.de" -->
<!-- inreplyto="000001c017b6$3c5eb3e0$a0bc473f@jrmolloy" -->
<STRONG>From:</STRONG> Eugene Leitl (<A HREF="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;14773.65204.352861.599680@lrz.uni-muenchen.de&gt;"><EM>eugene.leitl@lrz.uni-muenchen.de</EM></A>)<BR>
<STRONG>Date:</STRONG> Wed Sep 06 2000 - 02:22:12 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4347.html">Eugene Leitl: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4345.html">John  M Grigg: "Re: META: Why I'm boycotting Extropy(TM)."</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4330.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="4429.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4346">[ date ]</A>
<A HREF="index.html#4346">[ thread ]</A>
<A HREF="subject.html#4346">[ subject ]</A>
<A HREF="author.html#4346">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
J. R. Molloy writes:
<BR>
<EM> &gt; Why would AIs want to be friendly?
</EM><BR>
<P>&quot;Friendliness&quot; (prevalence of cooperative strategies over defecting)
<BR>
emerges in iterated (playing multiple rounds) interactions of agents,
<BR>
provided they can measurably profit from such interactions (the total
<BR>
is greater than the sum of it's parts = i.e. the iterated interaction
<BR>
is not a zero sum game) and can identify the agents they've dealt in
<BR>
the past. That's the stage.
<BR>
<P>Iterated, ok. Identification, ok. Non-zero sum game, ok. Now notice
<BR>
that above &quot;cooperative&quot; does not involve humans scared out of their
<BR>
wits, scrabbling to contain a global evolving worm effortlessly
<BR>
permeating their firewalls, nor dealing with a SI, because the latter
<BR>
does not profit from interactions with humankind measurably,
<BR>
similiarly to us not interacting profitably with an ant colony under a
<BR>
tree three streets away from our villa. So, we're either hostile, or
<BR>
insignificant. These are not good odds.
<BR>
<P>Primitive (say, doglike) intelligences, particularly primitive
<BR>
intelligences far away from the computational substrate they're
<BR>
running on, are probably containable. Advanced (usable) intelligences
<BR>
are principally uncontainable, because their very unpredictability
<BR>
(orelse a simple algorithm could do their work) and potential
<BR>
open-endedness makes them so useful.
<BR>
<P>I apologize for telling the same thing over and over again, but
<BR>
apparently a few people have not yet heard these old beaten-up
<BR>
arguments.
<BR>
<P><EM> &gt; Because if cognitive scientists can make one AI, they can make millions
</EM><BR>
<EM> &gt; (billions) of them, simply by copying them. When developers have sufficient
</EM><BR>
<P>Right, and they will, because the darn things are so useful. An AI
<BR>
good enough to do the markets would be worth a fortune on the free
<BR>
markets, even not mentioning something which could run a factory, or
<BR>
drive a car safely [insert your favourite product here]
<BR>
<P><EM> &gt; numbers of intelligent agents, they simply let the IAs compete for the right to
</EM><BR>
<EM> &gt; reproduce. These evolvable agents then do their own genetic programming. The
</EM><BR>
<P>Nothing new, that's how you've made the darn things in the first place.
<BR>
<P><EM> &gt; friendliest AIs get to reproduce, the rest get terminated. The socialization of
</EM><BR>
<P>So you've got a billion AIs gyring and gimbling in their wabe, and how 
<BR>
exactly do you intend to supervise them, and guarantee that their
<BR>
behaviour will be certifyable not hostile in all possible situations? 
<BR>
<P>If you know how to do it, please tell me, because I have not a ghost
<BR>
of an idea how to do it practically.
<BR>
<P><EM> &gt; AIs would be a snap compared to the socialization of human children.
</EM><BR>
<EM> &gt; By the time the AIs evolve to above-human-intelligence, they would be far more
</EM><BR>
<EM> &gt; trustworthy than any human, due to many generations of culling the herd of AIs.
</EM><BR>
<P>You never got bitten by a dog? A maltreated dog? A crazied, sick dog?
<BR>
Never, ever?
<BR>
<P><EM> &gt; Think of AI as a huge population of intelligent agents rather than as a single
</EM><BR>
<EM> &gt; entity, and the problem of making them friendly disappears. All you have to do
</EM><BR>
<P>How strange, I thought it was the other way round. Statistical
<BR>
properties, emergent properties, all things which you don't have to
<BR>
deal with when all you have is a single specimen.
<BR>
<P>Of course, there is no such thing as a single specimen, unless the
<BR>
thing instantly falls into a positive autofeedback loop -- same thing
<BR>
happened when the first autocatalytic set nucleated in the prebiotic
<BR>
ursoup. But then you're dealing with a jack-in-the-box SI, and all is
<BR>
moot, anyway.
<BR>
<P><EM> &gt; is discard any artificially intelligent individuals which show symptoms of
</EM><BR>
<EM> &gt; unfriendliness, and you end up with very friendly, docile, and helpful agents
</EM><BR>
<EM> &gt; all very intent on breeding themselves into friendly, docile, and helpful SIs.
</EM><BR>
&nbsp;
<BR>
Yeah, *right*.
<BR>
<P><EM> &gt; AFAIK, Asimov never considered the possibility of genetic programming and
</EM><BR>
<EM> &gt; evolvable machines which could compete against each other to reach higher levels
</EM><BR>
<EM> &gt; of IQ. With thousands (or millions and billions) of artificially intelligent
</EM><BR>
<P>Asimov was full of shit. Read Vinge, and always remember that he's a
<BR>
single human science fiction author, not a biblical prophet.
<BR>
<P><EM> &gt; agents battling each other to reproduce, all humans would need to do is to cull
</EM><BR>
<EM> &gt; the herd, so to speak. Any unfriendly AI agents (unlike human children) could
</EM><BR>
<EM> &gt; simply be terminated. This would result in a population of AIs with docility and
</EM><BR>
<EM> &gt; compliance as part of their genetic code. Moravec's Mind Children could
</EM><BR>
<P>It is exactly prevalence of such profoundly unreasonable, utterly
<BR>
devoid from basic common sense expectations that makes me consider
<BR>
breeding an AI a highly dangerous business. Because there is such a
<BR>
demand for the things, and because so many teams are working on them,
<BR>
someone will finally succeed. What will happen then is essentially
<BR>
unknowable, and most likely irreversible, and that's why we should be
<BR>
very very careful about when, how and the context we do it in.
<BR>
<P><EM> &gt; obviously number in the millions from the start, because as soon as one is
</EM><BR>
<EM> &gt; developed, it could be duplicated ad infinitum. With an unlimited supply of
</EM><BR>
<EM> &gt; genetically programmed AIs, their evolution could be guided and directed as
</EM><BR>
<EM> &gt; experimenters see fit. The socialization of AI would consequently be far easier
</EM><BR>
<EM> &gt; than the socialization of humans.
</EM><BR>
<P>Socialize a god.
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4347.html">Eugene Leitl: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4345.html">John  M Grigg: "Re: META: Why I'm boycotting Extropy(TM)."</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4330.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="4429.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4346">[ date ]</A>
<A HREF="index.html#4346">[ thread ]</A>
<A HREF="subject.html#4346">[ subject ]</A>
<A HREF="author.html#4346">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:37:17 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
