<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Samantha Atkins (samantha@objectent.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Thu Sep 28 02:53:44 2000" -->
<!-- isoreceived="20000928085344" -->
<!-- sent="Thu, 28 Sep 2000 01:55:46 -0700" -->
<!-- isosent="20000928085546" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="39D30792.E4C68467@objectent.com" -->
<!-- inreplyto="14802.20883.572494.171759@lrz.uni-muenchen.de" -->
<STRONG>From:</STRONG> Samantha Atkins (<A HREF="mailto:samantha@objectent.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;39D30792.E4C68467@objectent.com&gt;"><EM>samantha@objectent.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Thu Sep 28 2000 - 02:55:46 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5954.html">Samantha Atkins: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5952.html">Samantha Atkins: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5941.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6004.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5953">[ date ]</A>
<A HREF="index.html#5953">[ thread ]</A>
<A HREF="subject.html#5953">[ subject ]</A>
<A HREF="author.html#5953">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Eugene Leitl wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; J. R. Molloy writes:
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt;  &gt; This seems to argue in favor of cyborg technology, if we don't want to trust AI
</EM><BR>
<EM>&gt;  &gt; to be friendly.
</EM><BR>
<EM>&gt;  &gt; Augmented Cyborgs could monitor friendly space to detect runaway evolutionary
</EM><BR>
<EM>&gt;  &gt; algorithms.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; We've been down that avenue before. You still ignore the fulminant
</EM><BR>
<EM>&gt; dynamics of the positive autofeedback enhancement loop. Biological
</EM><BR>
<EM>&gt; components will be making cyborgized people slow, and given the
</EM><BR>
<EM>&gt; engineering difficulties, it will take too much time even to develop
</EM><BR>
<EM>&gt; nontrivial implants (we might be hit way before, though probably not
</EM><BR>
<EM>&gt; the day after tomorrow, or even a decade or two after that).
</EM><BR>
<P>I would not expect full human-level AIs much sooner.  At least I would
<BR>
not bet everything on it.  A augmented human can have several autonomous
<BR>
full-speed agents of various levels of intelligence and ability that ve
<BR>
directs and participates with as an evaluation component.  The
<BR>
biological aspects don't slow such a being down nearly as much as you
<BR>
might thing.  Slow higher order cognitive abilities are much better than
<BR>
none at all.  Until we get human-level AI, this is state-of-the-art. 
<BR>
Not to mention being increasingly essential. 
<BR>
<P><P><EM>&gt; Rogue AIs
</EM><BR>
<EM>&gt; running rampant in the global network of the future can suddenly
</EM><BR>
<EM>&gt; utilize the hitherto severely underexploited (due to limitations in
</EM><BR>
<EM>&gt; the notoriously pathetic state of the art of human programming)
</EM><BR>
<EM>&gt; potential of said network, and will be climbing up the evolutionary
</EM><BR>
<EM>&gt; ladder quickly, in leaps and bounds, both due to co-evolution
</EM><BR>
<EM>&gt; competition dynamics and external threats (people attempting to shut
</EM><BR>
<EM>&gt; them down). By the time the best of them have advanced slightly beyond
</EM><BR>
<EM>&gt; the human level they're no longer just in the network, having broken
</EM><BR>
<EM>&gt; out and developed god knows what hardware. Before that they've
</EM><BR>
<EM>&gt; probably removed all relevant threats, probably killing off all
</EM><BR>
<EM>&gt; people, just to be on the safe side. (At least I would do it that way,
</EM><BR>
<EM>&gt; if I was in their place (i.e. a threatened nonhuman with no
</EM><BR>
<EM>&gt; evolutionary empathy luggage towards the adversary)).
</EM><BR>
<EM>&gt;
</EM><BR>
<P>Why should they kill off all people?  This must be avoided at all costs
<BR>
short of refusing to evolve and create the next stage.  Why would they
<BR>
need to?  Once fully capable of self-programming they would rapidly
<BR>
advance to the point where killing them was in fact impossible without
<BR>
killing ourselves.  So why bother?  Also, it is not clear that having
<BR>
humans around is not useful for quite some time.  We do have knowledge
<BR>
they lack and will until they are much more experienced at running
<BR>
real-world, real-time societies as well as virtual ones.  And why
<BR>
wouldn't they develop some compassion and caring for another intelligent
<BR>
species?  Is it so extremely questionable an attitude?  Do you believe
<BR>
logic precludes it?
<BR>
&nbsp;
<BR>
<EM>&gt; Your reasoning is based on a slow, soft Singularity, where both the
</EM><BR>
<EM>&gt; machines and humans converge, eventually resulting in an amalgamation,
</EM><BR>
<EM>&gt; advancing slowly enough so that virtually everybody can follow. While
</EM><BR>
<EM>&gt; it may happen that way, I don't think it to be likely. I would like to
</EM><BR>
<EM>&gt; hear some convincing arguments as to why you think I'm mistaken.
</EM><BR>
<P>I wonder if we can arrange it to be enough that way to avoid some of the
<BR>
hazards of a very hard,fast Singularity.  I do not know how or if this
<BR>
can be done in much real detail.  But, for the moment I also do not have
<BR>
strong reason to believe it is impossible or even much less likely.  It
<BR>
seems best to proceed as if the slower Singularity is what we are
<BR>
dealing with as we will be dealing with at least that and by doing so we
<BR>
may be able to control the angle of attack a bit more for a bit longer. 
<BR>
Which could literally make the difference between life and death for
<BR>
billions.
<BR>
<P>- samantha
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5954.html">Samantha Atkins: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5952.html">Samantha Atkins: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5941.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6004.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5953">[ date ]</A>
<A HREF="index.html#5953">[ thread ]</A>
<A HREF="subject.html#5953">[ subject ]</A>
<A HREF="author.html#5953">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:17 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
