<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Fwd: Earthweb from transadmin</TITLE>
<META NAME="Author" CONTENT="Samantha Atkins (samantha@objectent.com)">
<META NAME="Subject" CONTENT="Re: Fwd: Earthweb from transadmin">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Fwd: Earthweb from transadmin</H1>
<!-- received="Sat Sep 23 03:12:02 2000" -->
<!-- isoreceived="20000923091202" -->
<!-- sent="Sat, 23 Sep 2000 02:13:39 -0700" -->
<!-- isosent="20000923091339" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Fwd: Earthweb from transadmin" -->
<!-- id="39CC7443.D094F493@objectent.com" -->
<!-- inreplyto="39C7ABC9.95B2A061@pobox.com" -->
<STRONG>From:</STRONG> Samantha Atkins (<A HREF="mailto:samantha@objectent.com?Subject=Re:%20Fwd:%20Earthweb%20from%20transadmin&In-Reply-To=&lt;39CC7443.D094F493@objectent.com&gt;"><EM>samantha@objectent.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Sat Sep 23 2000 - 03:13:39 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5494.html">Samantha Atkins: "Re: GUNS: Why here?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5492.html">Jason Joel Thompson: "Re: Fear of Letting People Get Things They Want"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5285.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5508.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5508.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5493">[ date ]</A>
<A HREF="index.html#5493">[ thread ]</A>
<A HREF="subject.html#5493">[ subject ]</A>
<A HREF="author.html#5493">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
&quot;Eliezer S. Yudkowsky&quot; wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Alex Future Bokov wrote:
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; Um, yes, that is the original debate, and the reason this thread has
</EM><BR>
<EM>&gt; &gt; EarthWeb in its name. To recap, Eli and I agree that the capabilities
</EM><BR>
<EM>&gt; &gt; of an AI as he envisions it would be a superset of the capabilities of
</EM><BR>
<EM>&gt; &gt; an EarthWeb like entity as I envision it. However, I'm arguing that an
</EM><BR>
<EM>&gt; &gt; EarthWeb is more easily acchievable, by definition friendly to human
</EM><BR>
<EM>&gt; &gt; goals, and possibly sufficient for preventing runaway techno-disaster.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; On the contrary; the Earthweb is only as friendly as it is smart.  I really
</EM><BR>
<EM>&gt; don't see why the Earthweb would be benevolent.  Benevolent some of the time,
</EM><BR>
<EM>&gt; yes, but all of the time?  The Earthweb is perfectly capable of making
</EM><BR>
<EM>&gt; mistakes that are outright stupid, especially if it's an emotionally charged
</EM><BR>
<EM>&gt; issue being considered for the first time.
</EM><BR>
<EM>&gt; 
</EM><BR>
<P>You are right that human goals are not uniformly friendly to human
<BR>
beings.  But I would tend to agree with the POV that an intelligence
<BR>
build on or linking human intelligence and automating their interaction,
<BR>
sharing of knowledge, finding each other and so on would be more likely
<BR>
to at least majorly empathize with and understand human beings.  
<BR>
<P><P><EM>&gt; I have no confidence in any decision-making process that uses humans as the
</EM><BR>
<EM>&gt; sole intelligent components.  Period.  I know what goes into humans, and it
</EM><BR>
<EM>&gt; isn't sugar and spice and everything nice.  I can have some confidence that a
</EM><BR>
<EM>&gt; well-built Sysop is far too smart to make certain mistakes; I have no such
</EM><BR>
<EM>&gt; confidence in humans.  Because we have no choice - because even inaction is
</EM><BR>
<EM>&gt; still a decision - we must rely on human intelligence during the run-up to
</EM><BR>
<EM>&gt; Singularity.  But I will not bet the planet on humans being able to
</EM><BR>
<EM>&gt; intelligently wield the powers that become available beyond that point.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>Why would an Earthweb use only humans as intelligent components?  The
<BR>
web could have a lot of non-human agents and logic processors and other
<BR>
specialized gear.  Some decisions, particularly high speed ones and ones
<BR>
requiring major logic crunching, might increasingly not be made
<BR>
explicitly by human beings.
<BR>
<P>With the AI you only know what goes in at the beginning.  You have no
<BR>
idea what things will develop from that core that you can have full
<BR>
confidence in.  The things that make up the core, the seed, are initial
<BR>
values but we frankly do not know what types of equations we are dealing
<BR>
with or how chaotic they are or  much about what attractors are
<BR>
likely.   
<BR>
<P>&nbsp;
<BR>
<EM>&gt; I shouldn't really be debating this.  You can't see, intuitively, that a
</EM><BR>
<EM>&gt; superintelligence is solid and the Earthweb is air and shadow by comparision?
</EM><BR>
<EM>&gt; The Earthweb is pretty hot by human standards, but only by human standards.
</EM><BR>
<EM>&gt; It has no greater stability than the human species itself.  It may arguably be
</EM><BR>
<EM>&gt; useful on the way to a solid attractor like the Sysop Scenario, but it can't
</EM><BR>
<EM>&gt; possibly be anything more than a way-station.
</EM><BR>
<EM>&gt; 
</EM><BR>
<P>If we could see it intuitively I would guess none of us would be
<BR>
questioning it.  Frankly I don't know how you can with a straight face
<BR>
say that a superintelligence is solid when all you have is a lot of
<BR>
theory and your own basically good intentions and optimism.  That isn't
<BR>
very solid in the development world I inhabit.  You could be largely
<BR>
correct.  But it isn't obvious that you are.
<BR>
<P><EM>&gt; An arguable argument would be that the Earthweb would be better capable of
</EM><BR>
<EM>&gt; handling the run-up to a superintelligence scenario, perhaps by holding the
</EM><BR>
<EM>&gt; planet together long enough to get genuine enhanced humans into play.  I think
</EM><BR>
<EM>&gt; you'd still be wrong.  I think that realistically speaking, the construction
</EM><BR>
<EM>&gt; of the first versions of the Earthweb would be followed by six billion people
</EM><BR>
<EM>&gt; shouting their prejudices at top volume.  And yes, I've read _Earthweb_ and
</EM><BR>
<EM>&gt; Robin Hanson's paper on idea futures and I still think so.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>I hear you but I'm not quite ready to give up on the human race at as a
<BR>
bad job and just go for building an AI that will do everything right so
<BR>
our basically hopeless brokeness doesn't end up killing us all. 
<BR>
EarthWeb is one vision along the way that is a bit more human and has
<BR>
steps a bit closer to where we are and might, just conceivably, provide
<BR>
a framework for beginning to get beyond just shouting.  
<BR>
<P>Personally it seems far more likely to me that humans begin to
<BR>
self-enhance and learn a bit more about living together peaceably and
<BR>
more productively and that that brings us through current and looming
<BR>
crises. Your own basic optimism would seem to indicate that as
<BR>
intelligence goes up the ability to see moral/ethical things more
<BR>
cleanly and come to &quot;friendlier&quot; decisions goes up.  Or do you believe
<BR>
that the very evolution that enabled our intelligence also embedded such
<BR>
deep and intractable tendencies that we can never learn this and act on
<BR>
it enough?  Do you believe we can of ourselves do nothing but fight and
<BR>
squabble even in the midst of great abundance and opportunity?  
<BR>
<P>If so then exactly how will the AI being present as the &quot;Sysop&quot; make our
<BR>
miserableness any less miserable and onery?  It might keep us from
<BR>
killing each other but your scenario does not show me we would get any
<BR>
wiser or saner.  In fact the presence of the Protector might take away a
<BR>
considerable amount of impetus to improve ourselves.  It would simply
<BR>
matter a lot less.
<BR>
&nbsp;
<BR>
<EM>&gt; .  At the end of the day, the memetic baggage of a
</EM><BR>
<EM>&gt; planet just has too much inertia to be altered in such a short amount of
</EM><BR>
<EM>&gt; time.  Back when I expected a Singularity in 2030, I was a lot more interested
</EM><BR>
<EM>&gt; in Singularity memetics; as it is...
</EM><BR>
<EM>&gt; 
</EM><BR>
<P>I think it behooves us to try all approaches we can think of that might
<BR>
remotely help.  I'm not ready to put all my eggs in the AI basket. 
<BR>
<P><EM>&gt; This isn't a game; we have to win as fast as possible, as simply as possible,
</EM><BR>
<EM>&gt; and not be distracted by big complex strategies.  That's why I ditched the
</EM><BR>
<EM>&gt; entire running-distributed-over-the-Internet plan and then ditched the
</EM><BR>
<EM>&gt; open-source-AI-industry strategy; too many ways it could all go wrong.  The
</EM><BR>
<EM>&gt; Earthweb scenario, if you try to visualize all the details, is even worse.
</EM><BR>
<EM>&gt;
</EM><BR>
<P>So instead you think a small huddle of really bright people will solve
<BR>
or make all the problems of the ages moot by creating in effect a
<BR>
Super-being, or at least its Seed.  How is this different from the old
<BR>
&quot;my nerds are smarter than your nerds and we will win&quot; sort of
<BR>
mentality?  By what right will you withhold behind close doors all the
<BR>
tech leading up to your success that could be used in so many fruitful
<BR>
ways on the outside?  If you at least stayed open you would enable the
<BR>
tech to be used on other attempts to make the coming transitions
<BR>
smoother.  What makes you think that you and your nerds are so good that
<BR>
you will see everything and don't need the eyeballs of other nerds
<BR>
outside your cabal at all?  This, imho, is much more arrogant than
<BR>
creating the Seed itself. 
<BR>
<P>- samantha
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5494.html">Samantha Atkins: "Re: GUNS: Why here?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5492.html">Jason Joel Thompson: "Re: Fear of Letting People Get Things They Want"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5285.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="5508.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5508.html">Eliezer S. Yudkowsky: "Re: Fwd: Earthweb from transadmin"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5493">[ date ]</A>
<A HREF="index.html#5493">[ thread ]</A>
<A HREF="subject.html#5493">[ subject ]</A>
<A HREF="author.html#5493">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:43 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
