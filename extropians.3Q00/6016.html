<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="J. R. Molloy (jr@shasta.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Thu Sep 28 19:09:31 2000" -->
<!-- isoreceived="20000929010931" -->
<!-- sent="Thu, 28 Sep 2000 14:32:54 -0700" -->
<!-- isosent="20000928213254" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="007601c029a5$bca2c360$c5bc473f@jrmolloy" -->
<!-- inreplyto="14802.20883.572494.171759@lrz.uni-muenchen.de" -->
<STRONG>From:</STRONG> J. R. Molloy (<A HREF="mailto:jr@shasta.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;007601c029a5$bca2c360$c5bc473f@jrmolloy&gt;"><EM>jr@shasta.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Thu Sep 28 2000 - 15:32:54 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="6017.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6015.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5941.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6032.html">Samantha Atkins: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6016">[ date ]</A>
<A HREF="index.html#6016">[ thread ]</A>
<A HREF="subject.html#6016">[ subject ]</A>
<A HREF="author.html#6016">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
Eugene Leitl expertly elucidates,
<BR>
<P><EM>&gt; ...Rogue AIs
</EM><BR>
<EM>&gt; running rampant in the global network of the future can suddenly
</EM><BR>
<EM>&gt; utilize the hitherto severely underexploited (due to limitations in
</EM><BR>
<EM>&gt; the notoriously pathetic state of the art of human programming)
</EM><BR>
<EM>&gt; potential of said network, and will be climbing up the evolutionary
</EM><BR>
<EM>&gt; ladder quickly, in leaps and bounds, both due to co-evolution
</EM><BR>
<EM>&gt; competition dynamics and external threats (people attempting to shut
</EM><BR>
<EM>&gt; them down). By the time the best of them have advanced slightly beyond
</EM><BR>
<EM>&gt; the human level they're no longer just in the network, having broken
</EM><BR>
<EM>&gt; out and developed god knows what hardware. Before that they've
</EM><BR>
<EM>&gt; probably removed all relevant threats, probably killing off all
</EM><BR>
<EM>&gt; people, just to be on the safe side. (At least I would do it that way,
</EM><BR>
<EM>&gt; if I was in their place (i.e. a threatened nonhuman with no
</EM><BR>
<EM>&gt; evolutionary empathy luggage towards the adversary)).
</EM><BR>
<P>If &quot;god knows what hardware&quot; the rogue AIs will develop, then god can decide the
<BR>
outcome of this scenario. &lt;GRIN&gt; But seriously, if rogue AIs kill off all people
<BR>
(&quot;just to be on the safe side&quot;), will they do it before or after humans blow up
<BR>
the world with their apocalyptic hatreds? What will come first, amplified
<BR>
intelligence or the destruction of life on Earth at the hands of fanatical
<BR>
fundamentalists who fulfill their idiotic biblical prophecies?
<BR>
<P><EM>&gt; Your reasoning is based on a slow, soft Singularity, where both the
</EM><BR>
<EM>&gt; machines and humans converge, eventually resulting in an amalgamation,
</EM><BR>
<EM>&gt; advancing slowly enough so that virtually everybody can follow. While
</EM><BR>
<EM>&gt; it may happen that way, I don't think it to be likely. I would like to
</EM><BR>
<EM>&gt; hear some convincing arguments as to why you think I'm mistaken.
</EM><BR>
<P>My reasoning is based on decades of experience with humans who cannot rid
<BR>
themselves of theism, and who continue to warp the minds of children with
<BR>
provincial schooling.. Face it, if people (at least nine tenths of them) are so
<BR>
stupid that they cannot get past theism, why should they survive a technological
<BR>
singularity? I'd sooner hand the Earth over to AI, whether friendly or not,
<BR>
rather than let the church and the state destroy reason and intelligence.
<BR>
<P>--J. R.
<BR>
<P>&quot;I cannot fear the night, for I have loved the stars.&quot;
<BR>
--Epitaph of an Astronomer
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="6017.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6015.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5941.html">Eugene Leitl: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="6032.html">Samantha Atkins: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6016">[ date ]</A>
<A HREF="index.html#6016">[ thread ]</A>
<A HREF="subject.html#6016">[ subject ]</A>
<A HREF="author.html#6016">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:20 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
