<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="xgl (xli03@emory.edu)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Tue Sep  5 20:28:12 2000" -->
<!-- isoreceived="20000906022812" -->
<!-- sent="Tue, 5 Sep 2000 22:29:50 -0400 (EDT)" -->
<!-- isosent="20000906022950" -->
<!-- name="xgl" -->
<!-- email="xli03@emory.edu" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="Pine.GSO.4.05.10009052207050.20413-100000@jet.cc.emory.edu" -->
<!-- inreplyto="F142JwnzsLqIfQdmEB4000099b9@hotmail.com" -->
<STRONG>From:</STRONG> xgl (<A HREF="mailto:xli03@emory.edu?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;Pine.GSO.4.05.10009052207050.20413-100000@jet.cc.emory.edu&gt;"><EM>xli03@emory.edu</EM></A>)<BR>
<STRONG>Date:</STRONG> Tue Sep 05 2000 - 20:29:50 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="4325.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4323.html">Ziana Astralos: "Re: offering alternative"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4308.html">Zero Powers: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4324">[ date ]</A>
<A HREF="index.html#4324">[ thread ]</A>
<A HREF="subject.html#4324">[ subject ]</A>
<A HREF="author.html#4324">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
this disclaimer shouldn't be necessary: i speak for myself only.
<BR>
<P>On Tue, 5 Sep 2000, Zero Powers wrote:
<BR>
<P><EM>&gt; 
</EM><BR>
<EM>&gt; Right.  But once it reaches and surpasses human levels, won't it be 
</EM><BR>
<EM>&gt; sentient?  Won't it begin to ask existential questions like &quot;Who am I?&quot; &quot;Why 
</EM><BR>
<EM>&gt; am I here?&quot; &quot;Is it fair for these ignorant humans to tell me what I can and 
</EM><BR>
<EM>&gt; cannot do?&quot;  Won't it read Rand?  Will it become an objectivist?  It is not 
</EM><BR>
<EM>&gt; very likely to be religious or humanist.  Won't it begin to wonder what 
</EM><BR>
<EM>&gt; activities are in its own best interest, as opposed to what is in the best 
</EM><BR>
<EM>&gt; interests of us?
</EM><BR>
<EM>&gt;
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self interest is an evolved adaptation, and fairness is a mostly
<BR>
human concept. i doubt that an SI would even harbor any special attachment
<BR>
to the &quot;self&quot; -- all that matters is the goal. as to existential
<BR>
questions, it's perhaps ironic that the hardest questions for us humans
<BR>
would probably be trivial for an SI. to the best of my knowledge, 
<BR>
eliezer's design _begins_ with the meaning of life -- crudely speaking, do
<BR>
the right thing.
<BR>
<P><EM>&gt; Sure you can program in initial decision making procedures, but once it 
</EM><BR>
<EM>&gt; reaches sentience (and that *is* the goal, isn't it?) aren't all bets off?
</EM><BR>
<EM>&gt; 
</EM><BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;interesting. our idea of sentience derives mostly from the only
<BR>
specimen we have found so far -- human beings. but human beings are messy
<BR>
hacks. what is sentience like without self interest? without emotion? what
<BR>
is a pure mind (if such a thing can exist)? eliezer's minimalist design is
<BR>
pretty much as pure a mind as one can get (and that's why i prefer to call
<BR>
it a transcendent mind) -- and thus as far from my intuition as it can get
<BR>
as well. perhaps all bets _are_ off ... but not in the way we think.
<BR>
<P>-x
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="4325.html">Eliezer S. Yudkowsky: "Re: Why would AI want to be friendly? (Was: Congratulations to Eli,Brian  ...)"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="4323.html">Ziana Astralos: "Re: offering alternative"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="4308.html">Zero Powers: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#4324">[ date ]</A>
<A HREF="index.html#4324">[ thread ]</A>
<A HREF="subject.html#4324">[ subject ]</A>
<A HREF="author.html#4324">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:37:16 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
