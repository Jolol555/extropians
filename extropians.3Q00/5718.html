<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Re: Why would AI want to be friendly?</TITLE>
<META NAME="Author" CONTENT="Samantha Atkins (samantha@objectent.com)">
<META NAME="Subject" CONTENT="Re: Why would AI want to be friendly?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Re: Why would AI want to be friendly?</H1>
<!-- received="Mon Sep 25 13:16:17 2000" -->
<!-- isoreceived="20000925191617" -->
<!-- sent="Mon, 25 Sep 2000 12:17:49 -0700" -->
<!-- isosent="20000925191749" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Why would AI want to be friendly?" -->
<!-- id="39CFA4DD.DB8FE09C@objectent.com" -->
<!-- inreplyto="002f01c02711$46631600$9bbc473f@jrmolloy" -->
<STRONG>From:</STRONG> Samantha Atkins (<A HREF="mailto:samantha@objectent.com?Subject=Re:%20Why%20would%20AI%20want%20to%20be%20friendly?&In-Reply-To=&lt;39CFA4DD.DB8FE09C@objectent.com&gt;"><EM>samantha@objectent.com</EM></A>)<BR>
<STRONG>Date:</STRONG> Mon Sep 25 2000 - 13:17:49 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="5719.html">Franklin Wayne Poley: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5717.html">Michael S. Lorrey: "Re: Capitalists and concentration camps"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5695.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="5722.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5729.html">Franklin Wayne Poley: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5718">[ date ]</A>
<A HREF="index.html#5718">[ thread ]</A>
<A HREF="subject.html#5718">[ subject ]</A>
<A HREF="author.html#5718">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
&quot;J. R. Molloy&quot; wrote:
<BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; &gt; No, because this is not the Singularity Seed but simply successful
</EM><BR>
<EM>&gt; &gt; genetic programming projects.  You seem to be answering a very different
</EM><BR>
<EM>&gt; &gt; question than I was asking.
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt; - samantha
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; Right. You had asked, &quot;In what way is this good for or even compatible with the
</EM><BR>
<EM>&gt; nature of human beings?&quot;
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; But I maintain that what is good for or compatible with human nature is not
</EM><BR>
<EM>&gt; necessarily the most extropic path to higher levels of self-organized complexity
</EM><BR>
<EM>&gt; or more sentient life forms.
</EM><BR>
<EM>&gt; What is good for human beings may, after all, be bad for transhuman beings.
</EM><BR>
<EM>&gt; If AI is not friendly toward humans, that doesn't mean it will be unfriendly
</EM><BR>
<EM>&gt; toward transhumans.
</EM><BR>
<EM>&gt; 
</EM><BR>
<P>So now we have at least three camps that may have conflicting
<BR>
interests/goals: humans, transhumans and AIs?  I assume that a
<BR>
transhuman is an evolved/augmented form of a human in many respects, and
<BR>
thus shares many goals with humans, especially in the early near-human
<BR>
stages.  So my question is as applicable to them as to &quot;mere&quot; humans.  I
<BR>
am worried when I hear about a proposed AI that is so much smarter than
<BR>
all humans (and transhumans for that matter) that it can and should make
<BR>
decisions for all of us that we should (will be forced to?) obey without
<BR>
questioning especially since our puny brains/processing units cannot
<BR>
match Its Intelligence.  This is not a very palatable goal or state for
<BR>
humans or for transhumans or for any other category of being that
<BR>
treasure its independent ability to make its own mistakes and to
<BR>
evolve/grow over time.  It is a capping, an utter topping, an asymptote
<BR>
reached,  that makes other forms relatively irrelevant and insignificant
<BR>
and would lead them imho to be rather dis-spirited unless the super AI
<BR>
very carefully left trans/humans freedom and room to develop.  
<BR>
<P>I also have a problem with opposing transhumans to humans. It is the
<BR>
classic problem of the elite.  There is a large problem of whether and
<BR>
how space is left for those who do not wish to follow the transhuman
<BR>
path.
<BR>
<P>- samantha
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="5719.html">Franklin Wayne Poley: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="5717.html">Michael S. Lorrey: "Re: Capitalists and concentration camps"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="5695.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Reply:</STRONG> <A HREF="5722.html">J. R. Molloy: "Re: Why would AI want to be friendly?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="5729.html">Franklin Wayne Poley: "Re: Why would AI want to be friendly?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#5718">[ date ]</A>
<A HREF="index.html#5718">[ thread ]</A>
<A HREF="subject.html#5718">[ subject ]</A>
<A HREF="author.html#5718">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:38:57 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
