<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>extropians: Lanier's losing his edge?</TITLE>
<META NAME="Author" CONTENT="Eugene Leitl (eugene.leitl@lrz.uni-muenchen.de)">
<META NAME="Subject" CONTENT="Lanier's losing his edge?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1>Lanier's losing his edge?</H1>
<!-- received="Fri Sep 29 14:48:29 2000" -->
<!-- isoreceived="20000929204829" -->
<!-- sent="Fri, 29 Sep 2000 08:01:27 -0700 (PDT)" -->
<!-- isosent="20000929150127" -->
<!-- name="Eugene Leitl" -->
<!-- email="eugene.leitl@lrz.uni-muenchen.de" -->
<!-- subject="Lanier's losing his edge?" -->
<!-- id="14804.44743.114732.888911@lrz.uni-muenchen.de" -->
<STRONG>From:</STRONG> Eugene Leitl (<A HREF="mailto:eugene.leitl@lrz.uni-muenchen.de?Subject=Re:%20Lanier's%20losing%20his%20edge?&In-Reply-To=&lt;14804.44743.114732.888911@lrz.uni-muenchen.de&gt;"><EM>eugene.leitl@lrz.uni-muenchen.de</EM></A>)<BR>
<STRONG>Date:</STRONG> Fri Sep 29 2000 - 09:01:27 MDT
<P>
<!-- next="start" -->
<UL>
<LI><STRONG>Next message:</STRONG> <A HREF="6105.html">Brian D Williams: "Re: Asian swamp eels"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6103.html">CYMM: "Re: Symbiotic Evolution (was Why would AI want to be friendly?)"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="6124.html">Dan Fabulich: "Re: Lanier's losing his edge?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6124.html">Dan Fabulich: "Re: Lanier's losing his edge?"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="6130.html">Randy Smith: "Re: Lanier's losing his edge?"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="6131.html">Randy Smith: "Re: Lanier's losing his edge?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6104">[ date ]</A>
<A HREF="index.html#6104">[ thread ]</A>
<A HREF="subject.html#6104">[ subject ]</A>
<A HREF="author.html#6104">[ author ]</A>
</UL>
<HR NOSHADE><P>
<!-- body="start" -->
<P>
(((I've ran into Lanier's article on Edge.org posted on extropians@ on
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;FoRK, and decided to read it. Since the Edge folks screw up
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;the HTML, and also broke the text into 14 (fourteen) individual
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;pages, I cut &amp; pasted the thing for your edification and comments.
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;Lanier is not stupid, and makes several points, but I disagree
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;with the general tone of the article, and have already spotted
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;several non sequiturs, which I will address when I have time.
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;Meanwhile, please deconstruct at leisure.
<BR>
)))
<BR>
<P><P><A HREF="http://www.edge.org/3rd_culture/lanier/lanier_index.html">http://www.edge.org/3rd_culture/lanier/lanier_index.html</A>
<BR>
<P>Introduction
<BR>
<P>Jaron Lanier, a pioneer in virtual reality, musician, and currently the
<BR>
lead scientist for the National Tele-Immersion Initiative, worries about
<BR>
the future of human culture more than the gadgets. In his &quot;Half a
<BR>
Manifesto&quot; he takes on those he terms the &quot;cybernetic totalists&quot; who
<BR>
do not seem &quot;to not have been educated in the tradition of scientific
<BR>
skepticism. I understand why they are intoxicated. There IS a
<BR>
compelling simple logic behind their thinking and elegance in thought is
<BR>
infectious.&quot;
<BR>
<P>&quot;There is a real chance that evolutionary psychology, artificial
<BR>
intelligence, Moore's Law fetishizing, and the rest of the package, will
<BR>
catch on in a big way, as big as Freud or Marx did in their times. Or
<BR>
bigger, since these ideas might end up essentially built into the software
<BR>
that runs our society and our lives. If that happens, the ideology of
<BR>
cybernetic totalist intellectuals will be amplified from novelty into a
<BR>
force that could cause suffering for millions of people.
<BR>
<P>&quot;The greatest crime of Marxism wasn't simply that much of what it
<BR>
claimed was false, but that it claimed to be the sole and utterly
<BR>
complete path to understanding life and reality. Cybernetic
<BR>
eschatology shares with some of history's worst ideologies a doctrine
<BR>
of historical predestination. There is nothing more gray, stultifying,
<BR>
or dreary than a life lived inside the confines of a theory. Let us
<BR>
hope that the cybernetic totalists learn humility before their day in
<BR>
the sun arrives.&quot;
<BR>
<P>Read on.....
<BR>
<P>JARON LANIER, a computer scientist and musician, is a pioneer of
<BR>
virtual reality, and founder and former CEO of VPL. He is currently
<BR>
the lead scientist for the National Tele-Immersion Initiative.
<BR>
<P>Join the Edge public forum at
<BR>
<P>ONE HALF OF A MANIFESTO
<BR>
By Jaron Lanier
<BR>
<P>For the last twenty years, I have found myself on the inside of a
<BR>
revolution, but on the outside of its resplendent dogma. Now that the
<BR>
revolution has not only hit the mainstream, but bludgeoned it into
<BR>
submission by taking over the economy, it's probably time for me to cry
<BR>
out my dissent more loudly than I have before.
<BR>
<P>And so I'll here share my thoughts with the respondents of edge.org,
<BR>
many of whom are, as much as anyone, responsible for this revolution,
<BR>
one which champions the assent of cybernetic technology as culture.
<BR>
<P>The dogma I object to is composed of a set of interlocking beliefs and
<BR>
doesn't have a generally accepted overarching name as yet, though I
<BR>
sometimes call it &quot;cybernetic totalism&quot;. It has the potential to
<BR>
transform human experience more powerfully than any prior ideology,
<BR>
religion, or political system ever has, partly because it can be so
<BR>
pleasing to the mind, at least initially, but mostly because it gets a
<BR>
free ride on the overwhelmingly powerful technologies that happen to
<BR>
be created by people who are, to a large degree, true believers.
<BR>
<P>Edge readers might be surprised by my use of the word &quot;cybernetic&quot;. I
<BR>
find the word problematic, so I'd like to explain why I chose it. I
<BR>
searched for a term that united the diverse ideas I was exploring, and
<BR>
also connected current thinking and culture with earlier generations
<BR>
of thinkers who touched on similar topics. The original usage of
<BR>
&quot;cybernetic&quot;, as by Norbert Weiner, was certainly not restricted to
<BR>
digital computers. It was originally meant to suggest a metaphor
<BR>
between marine navigation and a feedback device that governs a
<BR>
mechanical system, such as a thermostat. Weiner certainly recognized
<BR>
and humanely explored the extraordinary reach of this metaphor, one of
<BR>
the most powerful ever expressed.
<BR>
<P>I hope no one will think I'm equating Cybernetics and what I'm calling
<BR>
Cybernetic Totalism. The distance between recognizing a great metaphor
<BR>
and treating it as the only metaphor is the same as the distance
<BR>
between humble science and dogmatic religion.
<BR>
<P>Here is a partial roster of the component beliefs of cybernetic totalism:
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) That cybernetic patterns of information provide the ultimate and
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;best way to understand reality.
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) That people are no more than cybernetic patterns.
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3) That subjective experience either doesn't exist, or is unimportant
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;because it is some sort of ambient or peripheral effect.
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4) That what Darwin described in biology, or something like it, is in
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fact also the singular, superior description of all creativity and
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;culture.
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5) That qualitative as well as quantitative aspects of
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;information systems will be accelerated by Moore's Law.
<BR>
<P>And finally, the most dramatic:
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6) That biology and physics will merge with computer science
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(becoming biotechnology and nanotechnology), resulting in life and
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the physical universe becoming mercurial; achieving the supposed
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nature of computer software. Furthermore, all of this will happen
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;very soon! Since computers are improving so quickly, they will
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;overwhelm all the other cybernetic processes, like people, and will
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fundamentally change the nature of what's going on in the familiar
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;neighborhood of Earth at some moment when a new &quot;criticality&quot; is
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;achieved- maybe in about the year 2020. To be a human after that
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;moment will be either impossible or something very different than we
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;now can know.
<BR>
<P>During the last twenty years a stream of books has gradually informed
<BR>
the larger public about the belief structure of the inner circle of
<BR>
Digerati, starting softly, for instance with Godel, Escher, Bach, and
<BR>
growing more harsh with recent entries such as The Age of Spiritual
<BR>
Machines by Ray Kurtzweil.
<BR>
<P>Recently, public attention has finally been drawn to #6, the
<BR>
astonishing belief in an eschatological cataclysm in our lifetimes,
<BR>
brought about when computers become the ultra-intelligent masters of
<BR>
physical matter and life. So far as I can tell, a large number of my
<BR>
friends and colleagues believe in some version of this immanent doom.
<BR>
<P>I am quite curious who, among the eminent thinkers who largely accept
<BR>
some version of the first five points, are also comfortable with the
<BR>
sixth idea, the eschatology. In general, I find that technologists,
<BR>
rather than natural scientists, have tended to be vocal about the
<BR>
possibility of a near-term criticality. I have no idea, however, what
<BR>
figures like Richard Dawkins or Daniel Dennett make of it. Somehow I
<BR>
can't imagine these elegant theorists speculating about whether
<BR>
nanorobots might take over the planet in twenty years. It seems
<BR>
beneath their dignity. And yet, the eschatologies of Kurtzweil,
<BR>
Moravec, and Drexler follow directly and, it would seem, inevitably,
<BR>
from an understanding of the world that has been most sharply
<BR>
articulated by none other than Dawkins and Dennett. Do Dawkins,
<BR>
Dennett, and others in their camp see some flaw in logic that
<BR>
insulates their thinking from the eschatological implications? The
<BR>
primary candidate for such a flaw as I see it is that
<BR>
cyber-armageddonists have confused ideal computers with real
<BR>
computers, which behave differently. My position on this point can be
<BR>
evaluated separately from my admittedly provocative positions on the
<BR>
first five points, and I hope it will be.
<BR>
<P>Why this is only &quot;one half of a manifesto&quot;: I hope that readers will
<BR>
not think that I've sunk into some sort of glum rejection of digital
<BR>
technology. In fact, I'm more delighted than ever to be working in
<BR>
computer science and I find that it's rather easy to adopt a
<BR>
humanistic framework for designing digital tools. There is a lovely
<BR>
global flowering of computer culture already in place, arising for the
<BR>
most independently of the technological elites, which implicitly
<BR>
rejects the ideas I am attacking here. A full manifesto would attempt
<BR>
to describe and promote this positive culture.
<BR>
<P>I will now examine the five beliefs that must precede acceptance of
<BR>
the new eschatology, and then consider the eschatology itself.
<BR>
<P>Here we go:
<BR>
<P>Cybernetic Totalist Belief #1: That cybernetic patterns of information
<BR>
provide the ultimate and best way to understand reality.
<BR>
<P>There is an undeniable rush of excitement experienced by those who
<BR>
first are able to perceive a phenomenon cybernetically. For example,
<BR>
while I believe I can imagine what a thrill it must have been to use
<BR>
early photographic equipment in the 19th century, I can't imagine that
<BR>
any outsider could comprehend the sensation of being around early
<BR>
computer graphics technology in the nineteen-seventies. For here was
<BR>
not merely a way to make and show images, but a metaframework that
<BR>
subsumed all possible images. Once you can understand something in a
<BR>
way that you can shove it into a computer, you have cracked its code,
<BR>
transcended any particularity it might have at a given time. It was as
<BR>
if we had become the Gods of vision and had effectively created all
<BR>
possible images, for they would merely be reshufflings of the bits in
<BR>
the computers we had before us, completely under our command.
<BR>
<P>The cybernetic impulse is initially driven by ego (though, as we shall
<BR>
see, in its end game, which has not yet arrived, it will become the
<BR>
enemy of ego). For instance, Cybernetic Totalists look at culture and
<BR>
see &quot;memes&quot;, or autonomous mental tropes that compete for brain space
<BR>
in humans somewhat like viruses. In doing so they not only accomplish
<BR>
a triumph of &quot;campus imperialism&quot;, placing themselves in an imagined
<BR>
position of superior understanding vs. the whole of the humanities,
<BR>
but they also avoid having to pay much attention to the particulars of
<BR>
culture in a given time and place. Once you have subsumed something
<BR>
into its cybernetic reduction, any particular reshuffling of its bits
<BR>
seems unimportant.
<BR>
<P>Belief #1 appeared on the stage almost immediately with the first
<BR>
computers. It was articulated by the first generation of computer
<BR>
scientists; Weiner, Shannon, Turing. It is so fundamental that it
<BR>
isn't even stated anymore within the inner circle. It is so well
<BR>
rooted that it is difficult for me to remove myself from my
<BR>
all-encompassing intellectual environment long enough to articulate an
<BR>
alternative to it.
<BR>
<P>An alternative might be this: A cybernetic model of a phenomenon can
<BR>
never be the sole favored model, because we can't even build computers
<BR>
that conform to such models. Real computers are completely different
<BR>
from the ideal computers of theory. They break for reasons that are
<BR>
not always analyzable, and they seem to intrinsically resist many of
<BR>
our endeavors to improve them, in large part due to legacy and
<BR>
lock-in, among other problems. We imagine &quot;pure&quot; cybernetic systems
<BR>
but we can only prove we know how to build fairly dysfunctional
<BR>
ones. We kid ourselves when we think we understand something, even a
<BR>
computer, merely because we can model or digitize it.
<BR>
<P>There is also an epistemological problem that bothers me, even though
<BR>
my colleagues by and large are willing to ignore it. I don't think you
<BR>
can measure the function or even the existence of a computer without a
<BR>
cultural context for it. I don't think Martians would necessarily be
<BR>
able to distinguish a Macintosh from a space heater.
<BR>
<P>The above disputes ultimately turn on a combination of technical
<BR>
arguments about information theory and philosophical positions that
<BR>
largely arise from taste and faith.
<BR>
<P>So I try to augment my positions with pragmatic considerations, and
<BR>
some of these will begin to appear in my thoughts on...
<BR>
<P>Belief #2: That people are no more than cybernetic patterns
<BR>
<P>Every cybernetic totalist fantasy relies on artificial
<BR>
intelligence. It might not immediately be apparent why such fantasies
<BR>
are essential to those who have them. If computers are to become smart
<BR>
enough to design their own successors, initiating a process that will
<BR>
lead to God-like omniscience after a number of ever swifter passages
<BR>
from one generation of computers to the next, someone is going to have
<BR>
to write the software that gets the process going, and humans have
<BR>
given absolutely no evidence of being able to write such software. So
<BR>
the idea is that the computers will somehow become smart on their own
<BR>
and write their own software
<BR>
<P>My primary objection to this way of thinking is pragmatic: It results
<BR>
in the creation of poor quality real world software in the
<BR>
present. Cybernetic Totalists live with their heads in the future and
<BR>
are willing to accept obvious flaws in present software in support of
<BR>
a fantasy world that might never appear.
<BR>
<P>The whole enterprise of Artificial Intelligence is based on an
<BR>
intellectual mistake, and continues to expensively turn out poorly
<BR>
designed software as it is re-marketed under a new name for every new
<BR>
generation of programmers. Lately it has been called &quot;intelligent
<BR>
agents&quot;.  Last time around it was called &quot;expert systems&quot;.
<BR>
<P>Let's start at the beginning, when the idea first appeared. In
<BR>
Turing's famous thought experiment, a human judge is asked to
<BR>
determine which of two correspondents is human, and which is
<BR>
machine. If the judge cannot tell, Turing asserts that the computer
<BR>
should be treated as having essentially achieved the moral and
<BR>
intellectual status of personhood.
<BR>
<P>Turing's mistake was that he assumed that the only explanation for a
<BR>
successful computer entrant would be that the computer had become
<BR>
elevated in some way; by becoming smarter, more human. There is
<BR>
another, equally valid explanation of a winning computer, however,
<BR>
which is that the human had become less intelligent, less human-like.
<BR>
<P>An official Turing Test is held every year, and while the substantial
<BR>
cash prize has not been claimed by a program as yet, it will certainly
<BR>
be won sometime in the coming years. My view is that this event is
<BR>
distracting everyone from the real Turing Tests that are already being
<BR>
won.  Real, though miniature, Turing Tests are happening all the time,
<BR>
every day, whenever a person puts up with stupid computer software.
<BR>
<P>For instance, in the United States, we organize our financial lives in
<BR>
order to look good to the pathetically simplistic computer programs
<BR>
that determine our credit ratings. We borrow money when we don't need
<BR>
to, for example, to feed the type of data to the programs that we know
<BR>
they are programmed to respond to favorably.
<BR>
<P>In doing this, we make ourselves stupid in order to make the computer
<BR>
software seem smart. In fact we continue to trust the credit rating
<BR>
software even though there has been an epidemic of personal
<BR>
bankruptcies during a time of very low unemployment and great
<BR>
prosperity.
<BR>
<P>We have caused the Turing test to be passed. There is no
<BR>
epistemological difference between artificial intelligence and the
<BR>
acceptance of badly designed computer software.
<BR>
<P>My argument can be taken as an attack against the belief in eventual
<BR>
computer sentience, but a more sophisticated reading would be that it
<BR>
argues for a pragmatic advantage to holding an anti-AI belief (because
<BR>
those who believe in AI are more likely to put up with bad software).
<BR>
More importantly, I'm hoping the reader can see that Artificial
<BR>
Intelligence is better understood as a belief system instead of a
<BR>
technology.
<BR>
<P>The AI belief system is a direct explanation for a lot of bad software
<BR>
in the world, such as the annoying features in Microsoft Word and
<BR>
PowerPoint that guess at what the user really wanted to type. Almost
<BR>
every person I have asked has hated these features, and I have never
<BR>
met an engineer at Microsoft who could successfully turn the features
<BR>
completely off on my computer (running Mac Office '98), even though
<BR>
that is supposed to be possible.
<BR>
<P>Belief #3: That subjective experience either doesn't exist, or is
<BR>
unimportant because it is some sort of ambient or peripheral effect.
<BR>
<P>There is a new moral struggle taking shape over the question of when
<BR>
&quot;souls&quot; should be attributed to perceived patterns in the world.
<BR>
<P>Computers, genes, and the economy are some of the entities which
<BR>
appear to Cybernetic Totalists to populate reality today, along with
<BR>
human beings. It is certainly true that we are confronted with
<BR>
non-human and meta-human actors in our lives on a constant basis and
<BR>
these players sometimes appear to be more powerful than us.
<BR>
<P>So, the new moral question is: Do we make decisions solely on the
<BR>
basis of the needs and wants of &quot;traditional&quot; biological humans, or
<BR>
are any of these other players deserving of consideration?
<BR>
<P>I propose to make use of a simple image to consider the alternative
<BR>
points of view. This image is of an imaginary circle that each person
<BR>
draws around him/herself. We shall call this &quot;the circle of
<BR>
empathy&quot;. On the inside of the circle are those things that are
<BR>
considered deserving of empathy, and the corresponding respect,
<BR>
rights, and practical treatment as approximate equals.  On the outside
<BR>
of the circle are those things that are considered less important,
<BR>
less alive, less deserving of rights. (This image is only a tool for
<BR>
thought, and should certainly not be taken as my complete model for
<BR>
human psychology or moral dilemmas.) Roughly speaking, liberals hope
<BR>
to expand the circle, while conservatives wish to contract it.
<BR>
<P>Should computers, perhaps at some point in the future, be placed
<BR>
inside the &quot;circle of empathy&quot;?  The idea that they should is held
<BR>
close to the heart by the Cybernetic Totalists, who populate the elite
<BR>
technological academies and the businesses of the &quot;new economy&quot;.
<BR>
<P>There has often been a tender, but unintended humor in the
<BR>
argumentative writing by advocates of eventual computer sentience. The
<BR>
quest to rationally prove the possibility of sentience in a computer
<BR>
(or perhaps in the internet), is the modern version of proving God's
<BR>
existence. As is the case with the history of God, a great many great
<BR>
minds have spent excesses of energy on this quest, and eventually a
<BR>
cybernetically-minded 21st century version of Kant will appear in
<BR>
order to present a tedious &quot;proof&quot; that such adventures are futile. I
<BR>
simply don't have the patience to be that person.
<BR>
<P><P>As it happens, in the last five years or so arguments about computer
<BR>
sentience have started to subside. The idea is assumed to be true by
<BR>
most of my colleagues; for them, the argument is over. It is not over
<BR>
for me.
<BR>
<P>I must report that back when the arguments were still white hot, it
<BR>
was the oddest feeling to debate someone like Cybernetic Totalist
<BR>
philosopher Daniel Dennett. He would state that humans were simply
<BR>
specialized computers, and that imposing some fundamental ontological
<BR>
distinction between humans and computers was a sentimental waste of
<BR>
time.
<BR>
<P>&quot;But don't you experience your life? Isn't experience something apart
<BR>
from what you could measure in a computer?&quot;, I would say. My debating
<BR>
opponent would typically say something like &quot;Experience is just an
<BR>
illusion created because there is one part of a machine (you) that
<BR>
needs to create a model of the function of the rest of the machine-
<BR>
that part is your experiential center.&quot;
<BR>
<P>I would retort that experience is the only thing that isn't reduced by
<BR>
illusion. That even illusion is itself experience. A correlate, alas,
<BR>
is that experience is the very thing that can only be
<BR>
experienced. This lead me into the odd position of publicly wondering
<BR>
if some of my opponents simply lacked internal experience. (I once
<BR>
suggested that among all humanity, one could only definitively prove a
<BR>
lack of internal experience in certain professional philosophers.)
<BR>
<P>In truth, I think my perennial antagonists do have internal experience
<BR>
but choose not to admit it in public for a variety of reasons, most
<BR>
often because they enjoy annoying others.
<BR>
<P>Another motivation might be the &quot;Campus Imperialism&quot; I invoked
<BR>
earlier. Representatives of each academic discipline occasionally
<BR>
assert that they possess a most privileged viewpoint that somehow
<BR>
contains or subsumes the viewpoints of their rivals. Physicists were
<BR>
the alpha-academics for much of the twentieth century, though in
<BR>
recent decades &quot;postmodern&quot; humanities thinkers managed to stage
<BR>
something of a comeback, at least in their own minds. But
<BR>
technologists are the inevitable winners of this game, as they change
<BR>
the very components of our lives out from under us. It is tempting to
<BR>
many of them, apparently, to leverage this power to suggest that they
<BR>
also possess an ultimate understanding of reality, which is something
<BR>
quite apart from having tremendous influence on it.
<BR>
<P>Another avenue of explanation might be neo-Freudian, considering that
<BR>
the primary inventor of the idea of machine sentience, Alan Turing,
<BR>
was such a tortured soul. Turing died in an apparent suicide brought
<BR>
on by his having developed breasts as a result of enduring a hormonal
<BR>
regimen intended to reverse his homosexuality. It was during this
<BR>
tragic final period of his life that he argued passionately for
<BR>
machine sentience, and I have wondered whether he was engaging in a
<BR>
highly original new form of psychological escape and denial; running
<BR>
away from sexuality and mortality by becoming a computer.
<BR>
<P>At any rate, what is peculiar and revealing is that my cybernetic
<BR>
totalist friends confuse the viability of a perspective with its
<BR>
triumphant superiority. It is perfectly true that one can think of a
<BR>
person as a gene's way of propagating itself, as per Dawkins, or as a
<BR>
sexual organ used by machines to make more machines, as per McLuhan
<BR>
(as quoted in the masthead of every issue of Wired Magazine), and
<BR>
indeed it can even be beautiful to think from these perspectives from
<BR>
time to time. As the anthropologist Steve Barnett pointed out,
<BR>
however, it would be just as reasonable to assert that &quot;A person is
<BR>
shit's way of making more shit.&quot;
<BR>
<P>So let us pretend that the new Kant has already appeared and done
<BR>
his/her inevitable work. We can then say: The placement of one's
<BR>
circle of empathy is ultimately a matter of faith. We must accept the
<BR>
fact that we are forced to place the circle somewhere, and yet we
<BR>
cannot exclude extra-rational faith from our choice of where to place
<BR>
it.
<BR>
<P>My personal choice is to not place computers inside the circle. In
<BR>
this article I am stating some of my pragmatic, esthetic, and
<BR>
political reasons for this, though ultimately my decision rests on my
<BR>
particular faith. My position is unpopular and even resented in my
<BR>
professional and social environment.
<BR>
<P>Belief #4: That what Darwin described in biology, or something like
<BR>
it, is in fact also the singular, superior description of all possible
<BR>
creativity and culture.
<BR>
<P>Cybernetic totalists are obsessed with Darwin, for he described the
<BR>
closest thing we have to an algorithm for creativity. Darwin answers
<BR>
what would otherwise be a big hole in the Dogma: How will cybernetic
<BR>
systems be smart and creative enough to invent a post-human world? In
<BR>
order to embrace an eschatology in which the computers become smart as
<BR>
they become fast, some kind of Deus ex Machina must be invoked, and it
<BR>
has a beard.
<BR>
<P>Unfortunately, in the current climate I must take a moment to state
<BR>
that I am not a creationist.  I am in this essay criticizing what I
<BR>
perceive to be intellectual laziness; a retreat from trying to
<BR>
understand problems and instead hope for software that evolves
<BR>
itself. I am not suggesting that Nature required some extra element
<BR>
beyond natural evolution to create people.
<BR>
<P>I also don't meant to imply that there is a completely unified block
<BR>
of people opposing me, all of whom think exactly the same
<BR>
thoughts. There are in fact numerous variations of Darwinian
<BR>
eschatology. Some of the most dramatic renditions have not come from
<BR>
scientists or engineers, but from writers such as Kevin Kelly and
<BR>
Robert Wright, who have become entranced with broadened
<BR>
interpretations of Darwin. In their works, reality is perceived as a
<BR>
big computer program running the Darwin algorithm, perhaps headed
<BR>
towards some sort of Destiny.
<BR>
<P>Many of my technical colleagues also see at least some form of a
<BR>
causal arrow in evolution pointing to an ever greater degree of a
<BR>
hard-to-characterize something as time passes. The words used to
<BR>
describe that something are themselves hard to define; It is said to
<BR>
include increased complexity, organization, and representation. To
<BR>
computer scientist Danny Hillis, people seem to have more of such a
<BR>
thing than, say, single cell organisms, and it is natural to wonder if
<BR>
perhaps there will someday be some new creatures with even more of it
<BR>
than is found in people. (And of course the future birth of the new
<BR>
&quot;more so&quot; species is usually said to be related to computers.)
<BR>
Contrast this perspective with that of Stephen Jay Gould who argues in
<BR>
Full House that if there's an arrow in evolution, it's towards greater
<BR>
diversity over time, and we unlikely creatures known as humans, having
<BR>
arisen as one tiny manifestation of a massive, blind exploration of
<BR>
possible creatures, only imagine that the whole process was designed
<BR>
to lead to us.
<BR>
<P>There is no harder idea to test than an anthropic one, or its
<BR>
refutation. I'll admit that I tend to side with Gould on this one, but
<BR>
it is more important to point out an epistemological conundrum that
<BR>
should be considered by Darwinian eshatologists. If mankind is the
<BR>
measure of evolution thus far, then we will also be the measure of
<BR>
successor species that might be purported to be &quot;more evolved&quot; than
<BR>
us. We'll have to anthropomorphize in order to perceive this &quot;greater
<BR>
than human&quot; form of life, especially if it exists inside an
<BR>
information space such as the internet.
<BR>
<P>In other words, we'll be as reliable in assessing the status of the
<BR>
new super-beings as we are in assessing the traits of pet dogs in the
<BR>
present. We aren't up to the task. Before you tell me that it will be
<BR>
overwhelmingly obvious when the superintelligent new cyber-species
<BR>
arrives, visit a dog show. Or a gathering of people who believe they
<BR>
have been abducted by aliens in UFOs.  People are demonstrably insane
<BR>
when it comes to assessing non-human sentience.
<BR>
<P>There is, however, no question that the movement to interpret Darwin
<BR>
more broadly, and in particular to bring him into psychology and the
<BR>
humanities has offered some luminous insights that will someday be
<BR>
part of an improved understanding of nature, including human nature. I
<BR>
enjoy this stream of thought on various levels. It's also, let's admit
<BR>
it, impossible for a computer scientist not to be flattered by works
<BR>
which place what is essentially a form of algorithmic computation at
<BR>
the center of reality, and these thinkers tend to be confident and
<BR>
crisp and to occasionally have new and good ideas.
<BR>
<P>And yet I think cybernetic totalist Darwinians are often brazenly
<BR>
incompetent at public discourse and may be in part responsible,
<BR>
however unintentionally, for inciting a resurgence of fundamentalist
<BR>
religious reaction against rational biology. They seem to come up with
<BR>
takes on Darwin that are calculated to not only antagonize, but
<BR>
alienate those who don't share their views. Declarations from the
<BR>
&quot;nerdiest&quot; of the evolutionary psychologists can be particularly
<BR>
irritating.
<BR>
<P>One example that comes to mind is the recent book, The Natural History
<BR>
of Rape by Randy Thornhill and Craig T. Palmer, declaring that rape is
<BR>
a &quot;natural&quot; way to spread genes around.  We have seen all sorts of
<BR>
propositions tied to Darwin with a veneer of rationality. In fact you
<BR>
can argue almost any position using a Darwinian strategy.
<BR>
<P>For instance, Thornhill and Palmer go so far as to suggest that those
<BR>
who disagree with them are victims of evolutionary programming for the
<BR>
need to believe in a fictitious altruism in human nature. The authors
<BR>
say it is altruistic-seeming to not believe in evolutionary
<BR>
psychology, because such skepticism makes a public display of one's
<BR>
belief in brotherly love.  Displays of altruism are said to be
<BR>
attractive, and therefore to improve one's ability to lure mates. By
<BR>
this logic, evolutionary psychologists should soon breed themselves
<BR>
out of the population. Unless they resort to rape.
<BR>
<P>At any rate, Darwin's idea of evolution was of a different order than
<BR>
scientific theories that had come before, for at least two
<BR>
reasons. The most obvious and explosive reason was that the subject
<BR>
matter was so close to home. It was a shock to the 19th century mind
<BR>
to think of animals as blood relatives, and that shock continues to
<BR>
this day.
<BR>
<P>The second reason is less often recognized. Darwin created a style of
<BR>
reduction that was based on emergent principles instead of underlying
<BR>
laws (though some recent speculative physics theories can have a
<BR>
Darwinian flavor). There isn't any evolutionary &quot;force&quot; analogous to,
<BR>
say, electromagnetism. Evolution is a principle that can be discerned
<BR>
as emerging in events, but it cannot be described precisely as a force
<BR>
that directs events. This is a subtle distinction. The story of each
<BR>
photon is the same, in a way that the story of each animal and plant
<BR>
is different.  (Of course there are wonderful examples of precise,
<BR>
quantitative statements Darwinian theory and corresponding
<BR>
experiments, but these don't take place at anywhere close to the level
<BR>
of human experience, which is whole organisms that have complex
<BR>
behaviors in environments.)  &quot;Story&quot; is the operative
<BR>
word. Evolutionary thought has almost always been applied to specific
<BR>
situations through stories.
<BR>
<P>A story, unlike a theory, invites embroidery and variation, and indeed
<BR>
stories gain their communicative power by resonance with more primal
<BR>
stories. It is possible to learn physics without inventing a narrative
<BR>
in one's head to give meaning to photons and black holes. But it seems
<BR>
that it is impossible to learn Darwinian evolution without also
<BR>
developing an internal narrative to relate it to other stories one
<BR>
knows. At least no public thinker on the subject seems to have
<BR>
confronted Darwin without building a bridge to personal value systems.
<BR>
<P>But beyond the question of subjective flavoring, there remains the
<BR>
problem of whether Darwin has explained enough. Is it not possible
<BR>
that there remains an as-yet unarticulated idea that explains aspects
<BR>
of achievement and creativity that Darwin does not?
<BR>
<P>For instance, is Darwinian-styled explanation sufficient to understand
<BR>
the process of rational thought? There are a plethora of recent
<BR>
theories in which the brain is said to produce random distributions of
<BR>
subconscious ideas that compete with one another until only the best
<BR>
one has survived, but do these theories really fit with what people
<BR>
do?
<BR>
<P>In nature, evolution appears to be brilliant at optimizing, but stupid
<BR>
at strategizing. (The mathematical image that expresses this idea is
<BR>
that &quot;blind&quot; evolution has enourmous trouble getting unstuck from a
<BR>
local minima in an energy landscape.) The classic question would be:
<BR>
How could evolution have made such marvelous feet, claws, fins, and
<BR>
paws, but have missed the wheel? There are plenty of environments in
<BR>
which creatures would benefit from wheels, so why haven't any
<BR>
appeared? Not even once? (A great long term art project for some
<BR>
rebellious kid in school now: Genetically engineer an animal with
<BR>
wheels! See if DNA can be made to do it.)
<BR>
<P>People came up with the wheel and numerous other useful inventions
<BR>
that seem to have eluded evolution. It is possible that the
<BR>
explanation is simply that hands had access to a different set of
<BR>
inventions than DNA, even though both were guided by similar
<BR>
processes. But it seems to me premature to treat such an
<BR>
interpretation as a certainty. Is it not possible that in rational
<BR>
thought the brain does some as yet unarticulated thing that might have
<BR>
originated in a Darwinian process, but that cannot be explained by it?
<BR>
<P>The first two or three generations of artificial intelligence
<BR>
researchers took it as a given that blind evolution in itself couldn't
<BR>
be the whole of the story, and assumed that there were elements that
<BR>
distinguished human mentation from other Earthly processes. For
<BR>
instance, humans were thought by many to build abstract
<BR>
representations of the world in their minds, while the process of
<BR>
evolution needn't do that. Furthermore, these representations seemed
<BR>
to possess extraordinary qualities like the fearsome and perpetually
<BR>
elusive &quot;common sense&quot;.  After decades of failed attempts to build
<BR>
similar abstractions in computers, the field of AI gave up, but
<BR>
without admitting it. Surrender was couched as merely a series of
<BR>
tactical retreats.  AI these days is often conceived as more of a
<BR>
craft than a branch of science or engineering. A great many
<BR>
practitioners I've spoken with lately hope to see software evolve that
<BR>
does various things but seem to have sunk to an almost &quot;post-modern&quot;,
<BR>
or cynical lack of concern with understanding how these gizmos might
<BR>
actually work.
<BR>
<P>It is important to remember that craft-based cultures can come up with
<BR>
plenty of useful technologies, and that the motivation for our
<BR>
predecessors to embrace the Enlightenment and the ascent of
<BR>
rationality was not just to make more technologies more quickly. There
<BR>
was also the idea of Humanism, and a belief in the goodness of
<BR>
rational thinking and understanding. Are we really ready to abandon
<BR>
that?
<BR>
<P>Finally, there is an empirical point to be made: There has now been
<BR>
over a decade of work worldwide in Darwinian approaches to generating
<BR>
software, and while there have been some fascinating and impressive
<BR>
isolated results, and indeed I enjoy participating in such research,
<BR>
nothing has arisen from the work that would make software in general
<BR>
any better- as I'll describe in the next section.
<BR>
<P>So, while I love Darwin, I won't count on him to write code.
<BR>
<P>Belief #5: That qualitative as well as quantitative aspects of
<BR>
information systems will be accelerated by Moore's Law.
<BR>
<P>The hardware side of computers keeps on getting better and cheaper at
<BR>
an exponential rate known by the moniker &quot;Moore's Law&quot;. Every year and
<BR>
a half or so computation gets roughly twice as fast for a given
<BR>
cost. The implications of this are dizzying and so profound that they
<BR>
induce vertigo on first apprehension. What could a computer that was a
<BR>
million times faster than the one I am writing this text on be able to
<BR>
do? Would such a computer really be incapable of doing whatever it is
<BR>
my human brain does? The quantity of a &quot;million&quot; is not only too large
<BR>
to grasp intuitively, it is not even accessible experimentally for
<BR>
present purposes, so speculation is not irrational. What is stunning
<BR>
is to realize that many of us will find out the answer in our
<BR>
lifetimes, for such a computer might be a cheap consumer product in
<BR>
about, say 30 years.
<BR>
<P>This breathtaking vista must be starkly contrasted with the Great
<BR>
Shame of computer science, which is that we don't seem to be able to
<BR>
write software much better as computers get much faster. Computer
<BR>
software continues to disappoint. How I hated UNIX back in the
<BR>
seventies - that devilish accumulator of data trash, obscurer of
<BR>
function, enemy of the user! If anyone had told me back then that
<BR>
getting back to embarrassingly primitive UNIX would be the great hope
<BR>
and investment obsession of the year 2000, merely because it's name
<BR>
was changed to LINUX and its source code was opened up again, I never
<BR>
would have had the stomach or the heart to continue in computer
<BR>
science.
<BR>
<P>If anything, there's a reverse Moore's Law observable in software: As
<BR>
processors become faster and memory becomes cheaper, software becomes
<BR>
correspondingly slower and more bloated, using up all available
<BR>
resources. Now I know I'm not being entirely fair here. We have better
<BR>
speech recognition and language translation than we used to, for
<BR>
example, and we are learning to run larger data bases and
<BR>
networks. But our core techniques and technologies for software simply
<BR>
haven't kept up with hardware. (Just as some newborn race of
<BR>
superintelligent robots are about to consume all humanity, our dear
<BR>
old species will likely be saved by a Windows crash. The poor robots
<BR>
will linger pathetically, begging us to reboot them, even though
<BR>
they'll know it would do no good.)
<BR>
<P>There are various reasons that software tends to be unwieldly, but a
<BR>
primary one is what I like to call &quot;brittleness&quot;. Software breaks
<BR>
before it bends, so it demands perfection in a universe that prefers
<BR>
statistics. This in turn leads to all the pain of legacy/lock in, and
<BR>
other perversions.  The distance between the ideal computers we
<BR>
imagine in our thought experiments and the real computers we know how
<BR>
to unleash on the world could not be more bitter.
<BR>
<P>It is the fetishizing of Moore's Law that seduces researchers into
<BR>
complacency. If you have an exponential force on your side, surely it
<BR>
will ace all challenges. Who cares about rational understanding when
<BR>
you can instead really on an exponential extra-human fetish? But
<BR>
processing power isn't the only thing that scales impressively; so do
<BR>
the problems that processors have to solve.
<BR>
<P>Here's an example I offer to non-technical people to illustrate this
<BR>
point. Ten years ago I had a laptop with an indexing program that let
<BR>
me search for files by content. In order to respond quickly enough
<BR>
when I performed a search, it went through all the files in advance
<BR>
and indexed them, just as search engines like Google index the
<BR>
internet today. The indexing process took about an hour.
<BR>
<P>Today I have a laptop that is hugely more capacious and faster in
<BR>
every dimension, as predicted by Moore's Law. However, I now have to
<BR>
let my indexing program run overnight to do its job.  There are many
<BR>
other examples of computers seeming to get slower even though central
<BR>
processors are getting faster. Computer user interfaces tend to
<BR>
respond more slowly to user interface events, such as a keypress, than
<BR>
they did fifteen years ago, for instance. What's gone wrong?
<BR>
<P>The answer is complicated.
<BR>
<P>One part of the answer is fundamental. It turns out that when programs
<BR>
and datasets get bigger (and increasing storage and transmission
<BR>
capacities are driven by the same processes that drive Moore's
<BR>
exponential speedup), internal computational overhead often increases
<BR>
at a worse-than-linear rate. This is because of some nasty
<BR>
mathematical facts of life regarding algorithms. Making a problem
<BR>
twice as large usually makes it take a lot more than twice as long to
<BR>
solve. Some algorithms are worse in this way than others, and one
<BR>
aspect of getting a solid undergraduate education in computer science
<BR>
is learning about them. Plenty of problems have overheads that scale
<BR>
even more steeply than Moore's Law. Surprisingly few of the most
<BR>
essential algorithms have overheads that scale at a merely linear
<BR>
rate.
<BR>
<P>But that's only the beginning of the story. It's also true that if
<BR>
different parts of a system scale at different rates, and that's
<BR>
usually the case, one part might be overwhelmed by the other. In the
<BR>
case of my indexing program, the size of hard disks actually grew
<BR>
faster than the speed of interfaces to them. Overhead costs can be
<BR>
amplified by such examples of &quot;messy&quot; scaling, in which one part of a
<BR>
system cannot keep up with another. A bottleneck then appears, rather
<BR>
like girdlock in a poorly designed roadway. And the backup that
<BR>
results is just as bad as a morning commute on a typically inadequate
<BR>
roadway system. And just as tricky and expensive to plan for and
<BR>
prevent. (Trips on Manhattan streets were faster a hundred years ago
<BR>
than they are today. Horses are faster than cars.)
<BR>
<P>And then we come to our old antagonist, brittleness. The larger a
<BR>
piece of computer software gets, the more it is likely to be dominated
<BR>
by some form of legacy code, and the more brutal becomes the overhead
<BR>
of addressing the endless examples of subtle incompatibility that
<BR>
inevitably arise between chunks of software originally created in
<BR>
different contexts.
<BR>
<P>And even beyond these effects, there are failings of human character
<BR>
that worsen the state of software, and many of these are systemic and
<BR>
might arise even if non-human agents were writing the code. For
<BR>
instance, it is very time-consuming and expensive to plan ahead to
<BR>
make the tasks of future programmers easier, so each programmer tends
<BR>
to choose strategies that worsen the effects of brittleness. The time
<BR>
crunch faced by programmers is driven by none other than Moore's Law,
<BR>
which motivates an ever-faster turnaround of software revisions to get
<BR>
at least some form of mileage out of increasing processor speeds. So
<BR>
the result is often software that gets less efficient in some ways
<BR>
even as processors become faster.
<BR>
<P><P>I see no evidence that Moore's Law is steep enough to outrun all these
<BR>
problems without additional unforeseen intellectual achievements.
<BR>
<P>A fundamental statement of the question I'm examining here is: Does
<BR>
software tend to be unwieldly only because on human error, or is the
<BR>
difficulty intrinsic to the nature of software itself. If there is any
<BR>
credibility at all to the eschatological scenarios of Kurtzweil,
<BR>
Drexler, Moravec, et al, then this is the single most important
<BR>
question related to the future of mankind.
<BR>
<P>There is at least some metaphorical support for the possibility that
<BR>
software unwieldliness is intrinsic. In order to examine this
<BR>
possibility I'll have to break my own rule and be a cybernetic
<BR>
totalist for a moment.
<BR>
<P>Nature might seem to be less brittle than digital software, but if
<BR>
species are thought of as &quot;programs&quot;, then it looks like nature also
<BR>
has a software crisis. Evolution itself has evolved, introducing sex,
<BR>
for instance, but evolution has never found a way to be any speed but
<BR>
very slow. This might be at least in part because it takes a long time
<BR>
to explore the space of possible variations of an exceedingly vast and
<BR>
complex causal system to find new configurations that are
<BR>
viable. Natural evolution's slowness as a medium of transformation is
<BR>
apparently systemic, rather than esulting from some inherent
<BR>
sluggishness in its component parts. On the contrary, adaptation is
<BR>
capable of achieving thrilling speed, in select circumstances. An
<BR>
example of fast change is the adaptation of germs to our efforts to
<BR>
eradicate them. Resistance to antibiotics is a notorious contemporary
<BR>
example of biological speed.
<BR>
<P>Both human-created software and natural selection seem to accrue
<BR>
hierarchies of layers that vary in their potential for speedy
<BR>
change. Slow-changing layers protect local theaters within which there
<BR>
is a potential for faster change. In computers, this is the divide
<BR>
between operating systems and applications, or between browsers and
<BR>
web pages. In biology, it might be seen, for example, in the divide
<BR>
between nature- and nurture-dominated dynamics in the human mind.  But
<BR>
the lugubrious layers seem to usually define the overall character and
<BR>
potential of a system.
<BR>
<P>In the minds of some of my colleagues, all you have to do is identify
<BR>
one layer in a cybernetic system that's capable of fast change and
<BR>
then wait for Moore's Law to work it's magic. For instance, even if
<BR>
you're stuck with LINUX, you might implement a neural net program in
<BR>
it that eventually grows huge and fast enough (because of Moore's Law)
<BR>
to achieve a moment of insight and rewrite its own operating
<BR>
system. The problem is that in every example we know, a layer that can
<BR>
change fast also can't change very much. Germs can adopt to new drugs
<BR>
quickly, but would still take a very long time to evolve into
<BR>
Owls. This might be an inherent trade-off.  For an example in the
<BR>
digital world, you can write a new JAVA applet pretty quickly, but it
<BR>
won't look very different from other quickly written applets- take a
<BR>
look at what's been done with applets and you'll see that this is
<BR>
true.
<BR>
<P>Now we finally come to...
<BR>
<P>Belief #6, the coming cybernetic cataclysm.
<BR>
<P>When a thoughtful person marvels at Moore's Law, there might be awe
<BR>
and there might be terror. One version of the terror was expressed
<BR>
recently by Bill Joy, in a cover story for Wired Magazine. Bill
<BR>
accepts the pronouncements of Ray Kurtzweil and others, who believe
<BR>
that Moore's Law will lead to autonomous machines, perhaps by the year
<BR>
2020. That is the when computers will become, according to some
<BR>
estimates, about as powerful as human brains. (Not that anyone knows
<BR>
enough to really measure brains against computers yet. But for the
<BR>
sake of argument, let's suppose that the comparison is meaningful.)
<BR>
According to this scenario of the Terror, computers won't be stuck in
<BR>
boxes. They'll be more like robots, all connected together on the net,
<BR>
and they'll have a quite bag of tricks.
<BR>
<P>They'll be able to perform nano-manufacturing, for one thing. They'll
<BR>
quickly learn to reproduce and improve themselves. One fine day
<BR>
without warning, the new supermachines will brush humanity aside as
<BR>
casually as humans clear a forest for a new development. Or perhaps
<BR>
the machines will keep humans around to suffer the sort of indignity
<BR>
portrayed in the movie &quot;The Matrix&quot;.
<BR>
<P>Even if the machines would otherwise choose to preserve their human
<BR>
progenitors, evil humans will be able to manipulate the machines to do
<BR>
vast harm to the rest of us. This is a different scenario that Bill
<BR>
also explores. Biotechnology will have advanced to the point that
<BR>
computer programs will be able to manipulate DNA as if it were
<BR>
Javascript. If computers can calculate the effects of drugs, genetic
<BR>
modifications, and other biological trickery, and if the tools to
<BR>
realize such tricks are cheap, then all it takes is a one madman to,
<BR>
say, create an epidemic targeted at a single race. Biotechnology
<BR>
without a strong, cheap information technology component would not be
<BR>
sufficiently potent to bring about this scenario. Rather, it is the
<BR>
ability of software running on fabulously fast computers to cheaply
<BR>
model and guide the manipulation of biology that is at the root of
<BR>
this variant of the Terror. I haven't been able to fully convey Bill's
<BR>
concerns in this brief account, but you get the idea.
<BR>
<P>My version of the Terror is different. We can already see how the
<BR>
biotechnology industry is setting itself up for decades of expensive
<BR>
software trouble. While there are all sorts of useful databases and
<BR>
modeling packages being developed by biotech firms and labs, they all
<BR>
exist in isolated developmental bubbles. Each such tool expects the
<BR>
world to conform to its requirements. Since the tools are so valuable,
<BR>
the world will do exactly that, but we should expect to see vast
<BR>
resources applied to the problem of getting data from bubble into
<BR>
another.  There is no giant monolithic electronic brain being created
<BR>
with biological knowledge. There is instead a fractured mess of data
<BR>
and modeling fiefdoms. The medium for biological data transfer will
<BR>
continue to be sleep-deprived individual human researchers until some
<BR>
fabled future time when we know how to make software that is good at
<BR>
bridging bubbles on its own.
<BR>
<P>What is a long term future scenario like in which hardware keeps
<BR>
getting better and software remains mediocre? The great thing about
<BR>
crummy software is the amount of employment it generates. If Moore's
<BR>
Law is upheld for another twenty or thirty years, there will not only
<BR>
be a vast amount of computation going on Planet Earth, but also the
<BR>
maintenance of that computation will consume the efforts of almost
<BR>
every living person. We're talking about a planet of helpdesks.
<BR>
<P>I have argued elsewhere that this future would be a great thing,
<BR>
realizing the socialist dream of full employment by capitalist
<BR>
means. But let's consider the dark side.
<BR>
<P>Among the many processes that information systems make more efficient
<BR>
is the process of capitalism itself. A nearly friction-free economic
<BR>
environment allows fortunes to be accumulated in a few months instead
<BR>
of a few decades, but the individuals doing the accumulating are still
<BR>
living as long as they used to; longer, in fact. So those individuals
<BR>
who are good at getting rich have a chance to get richer before they
<BR>
die than their equally talented forebears.
<BR>
<P>There are two dangers in this. The smaller, more immediate danger is
<BR>
that young people acclimatized to a deliriously receptive economic
<BR>
environment might be emotionally wounded by what the rest of us would
<BR>
consider brief returns to normalcy. I do sometimes wonder if some of
<BR>
the students I work with who have gone on to dot com riches would be
<BR>
able to handle any financial frustration that lasted more than a few
<BR>
days without going into some sort of destructive depression or rage.
<BR>
<P>The greater danger is that the gulf between the richest and the rest
<BR>
could become transcendently grave. That is, even if we agree that a
<BR>
rising tide raises all ships, if the rate of the rising of the highest
<BR>
ships is greater than that of the lowest, they will become ever more
<BR>
separated. (And indeed, concentrations of wealth and poverty have
<BR>
increased during the Internet boom years in America.)
<BR>
<P>If Moore's Law or something like it is running the show, the scale of
<BR>
the separation could become astonishing. This is where my Terror
<BR>
resides, in considering the ultimate outcome of the increasing divide
<BR>
between the ultra-rich and the merely better off.
<BR>
<P>With the technologies that exist today, the wealthy and the rest
<BR>
aren't all that different; both bleed when pricked, for the classic
<BR>
example. But with the technology of the next twenty or thirty years
<BR>
they might become quite different indeed. Will the ultra-rich and the
<BR>
rest even be recognizable as the same species by the middle of the new
<BR>
century?
<BR>
<P>The possibilities that they will become essentially different species
<BR>
are so obvious and so terrifying that there is almost a banality in
<BR>
stating them. The rich could have their children made genetically more
<BR>
intelligent, beautiful, and joyous. Perhaps they could even be
<BR>
genetically disposed to have a superior capacity for empathy, but only
<BR>
to other people who meet some narrow range of criteria. Even stating
<BR>
these things seems beneath me, as if I were writing pulp science
<BR>
fiction, and yet the logic of the possibility is inescapable.
<BR>
<P>Let's explore just one possibility, for the sake of argument. One day
<BR>
the richest among us could turn nearly immortal, becoming virtual Gods
<BR>
to the rest of us. (An apparent lack of aging in both cell cultures
<BR>
and in whole organisms has been demonstrated in the laboratory.)
<BR>
<P>Let's not focus here on the fundamental questions of near immortality:
<BR>
whether it is moral or even desirable, or where one would find room if
<BR>
immortals insisted on continuing to have children. Let's instead focus
<BR>
on the question of whether immortality is likely to be expensive.
<BR>
<P>My guess is that immortality will be cheap if information technology
<BR>
gets much better, and expensive if software remains as crummy as it
<BR>
is.
<BR>
<P>I suspect that the hardware/software dichotomy will reappear in
<BR>
biotechnology, and indeed in other 21st century technologies. You can
<BR>
think of biotechnology as an attempt to make flesh into a computer, in
<BR>
the sense that biotechnology hopes to manage the processes of biology
<BR>
in ever greater detail, leading at some far horizon to perfect
<BR>
control. Likewise, nanotechnology hopes to do the same thing for
<BR>
materials science. If the body, and the material world at large become
<BR>
more manipulatable, more like a computer's memory, then the limiting
<BR>
factor will be the quality of the software that governs the
<BR>
manipulation.
<BR>
<P>Even though it's possible to program a computer to do virtually
<BR>
anything, we all know that's really not a sufficient description of
<BR>
computers. As I argued above: Getting computers to perform specific
<BR>
tasks of significant complexity in a reliable but modifiable way,
<BR>
without crashes or security breaches, is essentially impossible. We
<BR>
can only approximate this goal, and only at great expense.
<BR>
<P>Likewise, one can hypothetically program DNA to make virtually any
<BR>
modification in a living thing, and yet designing a particular
<BR>
modification and vetting it thoroughly will likely remain immensely
<BR>
difficult. (And, as I argued above, that might be one reason why
<BR>
biological evolution has never found a way to be anything speed other
<BR>
than very slow.) Similarly, one can hypothetically use nanotechnology
<BR>
to make matter do almost anything conceivable, but it will probably
<BR>
turn out to be much harder than we now imagine to get it do any
<BR>
particular thing of complexity without disturbing side
<BR>
effects. Scenarios that predict that biotechnology and nanotechnology
<BR>
will be able to quickly and cheaply create startling new things under
<BR>
the sun also must imagine that computers will become semi-autonomous,
<BR>
superintelligent, virtuoso engineers. But computers will do no such
<BR>
thing if the last half century of progress in software can serve as a
<BR>
predictor of the next half century.
<BR>
<P>In other words, bad software will make biological hacks like
<BR>
near-immortality expensive instead of cheap in the future. Even if
<BR>
everything else gets cheaper, the information technology side of the
<BR>
effort will get more expensive.
<BR>
<P>Cheap near-immortality for everyone is a self-limiting
<BR>
proposition. There isn't enough room to accommodate such an
<BR>
adventure. Also, roughly speaking, if immortality was to become cheap,
<BR>
so would the horrific biological weapons of Bill's scenario. On the
<BR>
other hand, expensive near immortality is something the world could
<BR>
absorb, at least for a good long while, because there would be fewer
<BR>
people involved. Maybe they could even keep the effort quiet.
<BR>
<P>So, here is the irony. The very features of computers which drive us
<BR>
crazy today, and keep so many of us gainfully employed, are the best
<BR>
insurance our species has for long term survival as we explore the far
<BR>
reaches of technological possibility. On the other hand, those same
<BR>
annoying qualities are what could make the 21st century into a
<BR>
madhouse scripted by the fantasies and desperate aspirations of the
<BR>
super-rich.
<BR>
<P><P>Conclusion
<BR>
<P>I share the belief of my cybernetic totalist colleagues that there
<BR>
will be huge and sudden changes in the near future brought about by
<BR>
technology. The difference is that I believe that whatever happens
<BR>
will be the responsibility of individual people who do specific
<BR>
things. I think that treating technology as if it were autonomous is
<BR>
the ultimate self-fulfilling prophecy. There is no difference between
<BR>
machine autonomy and the abdication of human responsibility.
<BR>
<P>Let's take the &quot;nanobots take over&quot; scenario. It seems to me that the
<BR>
most likely scenarios involve either:
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a) Super-nanobots everywhere that run old software- linux, say.
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This might be interesting. Good video games will be available,
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;anyway.
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b) Super-nanobots that evolve as fast as natural nanobots-
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;so don't do much for millions of years.
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;c) Super-nanobots that do new things soon, but are dependent
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on humans. In all these cases humans will be in control,
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for better or for worse.
<BR>
<P>So, therefore, I'll worry about the future of human culture more than
<BR>
I'll worry about the gadgets. And what worries me about the &quot;Young
<BR>
Turk&quot; cultural temperament seen in cybernetic totalists is that they
<BR>
seem to not have been educated in the tradition of scientific
<BR>
skepticism. I understand why they are intoxicated. There IS a
<BR>
compelling simple logic behind their thinking and elegance in thought
<BR>
is infectious.
<BR>
<P>There is a real chance that evolutionary psychology, artificial
<BR>
intelligence, Moore's Law fetishizing, and the rest of the package,
<BR>
will catch on in a big way, as big as Freud or Marx did in their
<BR>
times. Or bigger, since these ideas might end up essentially built
<BR>
into the software that runs our society and our lives. If that
<BR>
happens, the ideology of cybernetic totalist intellectuals will be
<BR>
amplified from novelty into a force that could cause suffering for
<BR>
millions of people.
<BR>
<P>The greatest crime of Marxism wasn't simply that much of what it
<BR>
claimed was false, but that it claimed to be the sole and utterly
<BR>
complete path to understanding life and reality. Cybernetic
<BR>
eschatology shares with some of history's worst ideologies a doctrine
<BR>
of historical predestination. There is nothing more gray, stultifying,
<BR>
or dreary than a life lived inside the confines of a theory. Let us
<BR>
hope that the cybernetic totalists learn humility before their day in
<BR>
the sun arrives.
<BR>
<P><P>(*Parts of this manifesto draw on material from two earlier
<BR>
essays. One appeared in CIO Magazine in English, and the other in
<BR>
Frankfurter Allgemeine Zeitung in German, as part of that newspaper's
<BR>
ongoing coverage of the Edge community.) 
<BR>
<P><!-- body="end" -->
<HR NOSHADE>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="6105.html">Brian D Williams: "Re: Asian swamp eels"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="6103.html">CYMM: "Re: Symbiotic Evolution (was Why would AI want to be friendly?)"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="6124.html">Dan Fabulich: "Re: Lanier's losing his edge?"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="6124.html">Dan Fabulich: "Re: Lanier's losing his edge?"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="6130.html">Randy Smith: "Re: Lanier's losing his edge?"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="6131.html">Randy Smith: "Re: Lanier's losing his edge?"</A>
<!-- reply="end" -->
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#6104">[ date ]</A>
<A HREF="index.html#6104">[ thread ]</A>
<A HREF="subject.html#6104">[ subject ]</A>
<A HREF="author.html#6104">[ author ]</A>
</UL>
<!-- trailer="footer" -->
<HR NOSHADE>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.hypermail.org/">hypermail 2b29</A> 
: <EM>Mon Oct 02 2000 - 17:39:26 MDT</EM>
</EM>
</SMALL>
</BODY>
</HTML>
