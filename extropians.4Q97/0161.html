<!-- received="Sun Oct  5 19:11:06 1997 MDT" -->
<!-- sent="Sun, 5 Oct 1997 16:20:59 -0700" -->
<!-- name="Hal Finney" -->
<!-- email="hal@rain.org" -->
<!-- subject="Re: Genius dogs" -->
<!-- id="199710052320.QAA01793@s20.term1.sb.rain.org" -->
<!-- inreplyto="Genius dogs" -->
<title>extropians: Re: Genius dogs</title>
<h1>Re: Genius dogs</h1>
Hal Finney (<i>hal@rain.org</i>)<br>
<i>Sun, 5 Oct 1997 16:20:59 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#161">[ date ]</a><a href="index.html#161">[ thread ]</a><a href="subject.html#161">[ subject ]</a><a href="author.html#161">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0162.html">Brian Atkins: "Re: The Spike, nanotech, and a future scenario"</a>
<li> <b>Previous message:</b> <a href="0160.html">GBurch1@aol.com: "Re: The Spike, nanotech, and a future scenario"</a>
<li> <b>Maybe in reply to:</b> <a href="0157.html">Nicholas Bostrom: "Genius dogs"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0168.html">Eliezer S. Yudkowsky: "Re: Genius dogs"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Nicholas Bostrom writes:<br>
<p>
<i>&gt; If we take a human brain and simply speed it up enough, will it be a</i><br>
<i>&gt; superintelligence? Would a dog brain be?</i><br>
<p>
We had some debate on this issue before, in June, 1996, in the more<br>
conventional terms of whether the insights of a genius could ever be<br>
achieved by a normal (or somewhat subnormal) person, given enough time.<br>
I argued that they could not, that no matter how long or how hard an<br>
average person thought about the problem, they would not come up with<br>
the theories of general relativity or quantum mechanics.<br>
<p>
A possible test for this I proposed was to take some hard problems from<br>
the Mensa tests and give an average guy unlimited time to try to solve<br>
them.  Might be hard to prevent cheating, though.<br>
<p>
<i>&gt; Any human of normal intelligence could function as a universal </i><br>
<i>&gt; Turing machine, if augmented with enough scrap paper, time and </i><br>
<i>&gt; patience.</i><br>
<i>&gt; [...]</i><br>
<i>&gt;  But whether or not it could be made so concise,</i><br>
<i>&gt;  by using some scrap paper input, I think that even dogs could be made</i><br>
<i>&gt;  to perform on a genius level. Here's how:</i><br>
<p>
This is not a very interesting answer to the question, IMO.<br>
By postulating the existence of a super-intelligent computer program,<br>
you are in effect assuming that super-intelligences already exist (since<br>
the program could presumably be run much more effectively on a computer<br>
than on the clumsy simulation a human or dog pack could provide).  The<br>
interesting question is how difficult it will be to create the program.<br>
<p>
Also, as opponents of Searle's Chinese Room paradox argue, if you do use<br>
humans or dogs to run the Turing machine, the super-intelligence is not<br>
that of the humans or dogs.  Rather, another entity comes into being,<br>
the intelligence being simulated.  So while you can *create* a super-<br>
intelligence using fast humans or dogs, they will not *be* the super-<br>
intelligence, as you originally asked above.<br>
<p>
The question I wonder about is, if the genii we talked about a few days<br>
ago granted the (misguided?) wish to speed up mentality a million-fold,<br>
would the resulting person be a super-intelligence simply in terms of<br>
applying his own native reasoning powers to the problems he faced.<br>
<p>
Hal<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0162.html">Brian Atkins: "Re: The Spike, nanotech, and a future scenario"</a>
<li> <b>Previous message:</b> <a href="0160.html">GBurch1@aol.com: "Re: The Spike, nanotech, and a future scenario"</a>
<li> <b>Maybe in reply to:</b> <a href="0157.html">Nicholas Bostrom: "Genius dogs"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0168.html">Eliezer S. Yudkowsky: "Re: Genius dogs"</a>
<!-- reply="end" -->
</ul>
