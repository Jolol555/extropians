<!-- received="Tue Oct 21 07:37:22 1997 MDT" -->
<!-- sent="Tue, 21 Oct 1997 23:06:59 +1000 (EST)" -->
<!-- name="Mitchell Porter" -->
<!-- email="mitch@thehub.com.au" -->
<!-- subject="Re: ETHICS: value-sets and value-systems" -->
<!-- id="199710211306.XAA18306@smople.thehub.com.au" -->
<!-- inreplyto="ETHICS: value-sets and value-systems" -->
<title>extropians: Re: ETHICS: value-sets and value-systems</title>
<h1>Re: ETHICS: value-sets and value-systems</h1>
Mitchell Porter (<i>mitch@thehub.com.au</i>)<br>
<i>Tue, 21 Oct 1997 23:06:59 +1000 (EST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#665">[ date ]</a><a href="index.html#665">[ thread ]</a><a href="subject.html#665">[ subject ]</a><a href="author.html#665">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0666.html">Brian D Williams: "Re: AW: An unusual level of intelligence"</a>
<li> <b>Previous message:</b> <a href="0664.html">Nicholas Bostrom: "Re: many minds interpretation of probability"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Felix Ungman wrote<br>
<p>
   Mitchell Porter:<br>
<i>   &gt;but it does imply that there are no facts of the form</i><br>
<i>   &gt;"Morality A is better than morality B", only facts of the</i><br>
<i>   &gt;form "Morality A is better than morality B, according to</i><br>
<i>   &gt;morality C".</i><br>
<p>
   Not true, I think. If a number of individuals share the same,<br>
   or at least similar, value-system, that morality implies<br>
   behavioural dynamics. Morality A may have game theoretically<br>
   stable points at "better" value-sets than Morality B. So even<br>
   though the value-sets may be higly subjective, the value-systems<br>
   are not.<br>
<p>
   The most obvious way a morality A can be better than morality B<br>
   is the amount of synergy it promotes.<br>
<p>
To say "A is better than B because it promotes greater synergy",<br>
you have to say synergy is *good*, and I claim that that statement <br>
makes no sense except in the context of a morality or "value-system".<br>
It may be objectively true that, say, A will outcompete B, but<br>
again (I claim) this has no intrinsic moral significance, any more<br>
than the fact that, say, the statement of A's principles appears <br>
earlier in the binary expansion of pi than the statement of B's <br>
principles; because there is no such thing as "intrinsic moral<br>
significance", only significance in the context of a particular<br>
value-set or value-system. I am not at all sure that the vague<br>
notions of "value-set" and "value-system" are the right way to <br>
analyse the phenomenon of choice - I would prefer concepts that<br>
were grounded in empirical neuroscience, and not just my<br>
personal version of folk psychology - but I'll stand by the<br>
general argument, that "absolute good" is probably as unreal as <br>
"absolute simultaneity".<br>
<p>
Nonetheless, the sort of objective ranking of value-systems<br>
you describe is highly relevant to anyone who's interested<br>
in choosing a value system, or even in altering their <br>
value set, and who is a "rational valuer", in Max More's sense.<br>
But to get the most out of it, one should probably first<br>
figure out, as completely as possible, one's "value set" -<br>
the "primary values", the things that are ends in themselves.<br>
Is survival, or freedom, or novelty an end in itself?<br>
If happiness is an end in itself, does it have more than <br>
one form? Do some forms matter (again, to YOU) more than others?<br>
If you can't rank one form of happiness above the other, perhaps <br>
you should be indifferent when faced with a choice between one <br>
form and another form (knowing this could save you time one day,<br>
since you'll know you don't need to spend time choosing). But <br>
what about a choice between having just one, and having both <br>
at once? If you prefer to have both forms of happiness at once,<br>
does that mean that "having as many forms of happiness<br>
simultaneously as possible" is a more important value for<br>
you than either particular form? And so on. :)<br>
<p>
I'm intrigued by Eric's idea that "the summum bonum is <br>
the generalizably extrinsic". I don't think it's any more <br>
defensible as a proposed Absolute Good than any other candidate, <br>
so I suppose I find it of interest as a psychological hypothesis <br>
("this is what we're really after") and as a hedonic heuristic <br>
("this is the thing to seek"). In fact, maybe so much emphasis<br>
is placed on nano and AI in extropian/transhuman circles <br>
because they promise to make so much else possible, and are<br>
therefore further examples of powerful extrinsic goods.<br>
<p>
-mitch<br>
<a href="http://www.thehub.com.au/~mitch">http://www.thehub.com.au/~mitch</a><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0666.html">Brian D Williams: "Re: AW: An unusual level of intelligence"</a>
<li> <b>Previous message:</b> <a href="0664.html">Nicholas Bostrom: "Re: many minds interpretation of probability"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
