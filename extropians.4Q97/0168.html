<!-- received="Mon Oct  6 00:06:28 1997 MDT" -->
<!-- sent="Sun, 05 Oct 1997 23:45:09 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Genius dogs" -->
<!-- id="199710052320.QAA01793@s20.term1.sb.rain.org" -->
<!-- inreplyto="Genius dogs" -->
<title>extropians: Re: Genius dogs</title>
<h1>Re: Genius dogs</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sun, 05 Oct 1997 23:45:09 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#168">[ date ]</a><a href="index.html#168">[ thread ]</a><a href="subject.html#168">[ subject ]</a><a href="author.html#168">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0169.html">Lee Daniel Crocker: "Re: Intellectual Property? was Bill Gates"</a>
<li> <b>Previous message:</b> <a href="0167.html">Eliezer S. Yudkowsky: "Re: The Spike, nanotech, and a future scenario"</a>
<li> <b>Maybe in reply to:</b> <a href="0157.html">Nicholas Bostrom: "Genius dogs"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0172.html">Anders Sandberg: "Re: Genius dogs"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Nicholas Bostrom wrote:<br>
<i>&gt; </i><br>
<i>&gt; If we take a human brain and simply speed it up enough, will it be a</i><br>
<i>&gt; superintelligence? Would a dog brain be?</i><br>
<p>
That depends on how you define "intelligence".  If you define it the way I<br>
defined "smartness" in _Staring Into The Singularity_, then the answer is no. <br>
If you define it in terms of exaflops, the answer is yes.<br>
<p>
Check out <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a><br>
<p>
It has the answers to all, yes, *all* of your questions.<br>
<p>
<i>&gt; Any human of normal intelligence could function as a universal</i><br>
<i>&gt; Turing machine, if augmented with enough scrap paper, time and</i><br>
<i>&gt; patience. According to Church's thesis, Turing computability equals</i><br>
<i>&gt; mechanical computability, so what the brain does is Turing</i><br>
<i>&gt; computable. (I assume that Penrose's argument to the contrary is</i><br>
<i>&gt; wrong, and we disregard possible exceptions that have to do with the</i><br>
<i>&gt; feasibility of supertasks or the unavailability of enough matter in</i><br>
<i>&gt; the universe.) What a (finite) superintelligence does would also be</i><br>
<i>&gt; Turing computable, so a human scrap paper, time &amp; patience), if</i><br>
<i>&gt; speeded up, could be a superintelligence, provided she run an</i><br>
<i>&gt; appropriate program. We don't know that this program could be made</i><br>
<i>&gt; short enough to be stored in human long-term memory, (else it would</i><br>
<i>&gt; have to be provided as an input on the scrap paper), but I believe it</i><br>
<i>&gt; could. My guess is that a universal intelligence algorithm could be</i><br>
<i>&gt; made so simple that it would be an easy task for a human to memorize</i><br>
<i>&gt; it. Perhaps it would suffice to specify some simple architectural</i><br>
<i>&gt;  properties of a neural network, together with some fairly simple</i><br>
<i>&gt;  learning rule -- something like combination of Backprop, Infomax and</i><br>
<i>&gt;  Hebb's rule perhaps. But whether or not it could be made so concise,</i><br>
<i>&gt;  by using some scrap paper input, I think that even dogs could be made</i><br>
<i>&gt;  to perform on a genius level. Here's how:</i><br>
<p>
You are entirely incorrect, for much the same reason as Searle.  It isn't the<br>
human that's superintelligent, it's the program.  It doesn't make a difference<br>
whether a human, dog, robot, or computer does the Turing operations, because<br>
it's the program that's intelligent or not.  All of this, by the way, is on<br>
the same semantic level as that old conumdrum:  "If a tree falls, and nobody<br>
hears it, does it make a sound?"<br>
<p>
<i>&gt; Recipe for Cani da Vinci, genius dogs</i><br>
[Entirely pointless method of turning dogs into Turing machines deleted.]<br>
<p>
Have you ever heard of the "Chinese Restaurant Problem"?<br>
<p>
<i>&gt; -- I anticipate that Anders will suggest that superintelligences will</i><br>
<i>&gt; make entertainment out of shaping human organizations into</i><br>
<i>&gt; entities that perform intelligent tasks, just as we have fun by</i><br>
<i>&gt; watching circus animals behave. :-)</i><br>
<p>
Um, I doubt it, but he'd be wrong if he did.<br>
Few things would be more pointless or less ethical.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0169.html">Lee Daniel Crocker: "Re: Intellectual Property? was Bill Gates"</a>
<li> <b>Previous message:</b> <a href="0167.html">Eliezer S. Yudkowsky: "Re: The Spike, nanotech, and a future scenario"</a>
<li> <b>Maybe in reply to:</b> <a href="0157.html">Nicholas Bostrom: "Genius dogs"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0172.html">Anders Sandberg: "Re: Genius dogs"</a>
<!-- reply="end" -->
</ul>
