<!-- received="Mon Dec 22 17:48:11 1997 MDT" -->
<!-- sent="Mon, 22 Dec 1997 16:48:03 -0800 (PST)" -->
<!-- name="Lee Daniel Crocker" -->
<!-- email="lcrocker@mercury.colossus.net" -->
<!-- subject="Future Technologies of Death" -->
<!-- id="199712230048.QAA06703@mercury.colossus.net" -->
<!-- inreplyto="199712222301.PAA20700@igc3.igc.apc.org" -->
<title>extropians: Future Technologies of Death</title>
<h1>Future Technologies of Death</h1>
Lee Daniel Crocker (<i>lcrocker@mercury.colossus.net</i>)<br>
<i>Mon, 22 Dec 1997 16:48:03 -0800 (PST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2894">[ date ]</a><a href="index.html#2894">[ thread ]</a><a href="subject.html#2894">[ subject ]</a><a href="author.html#2894">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2895.html">Wayne Hayes: "Constitutions (was: Look out! long hair gun loon!)"</a>
<li> <b>Previous message:</b> <a href="2893.html">Damien Broderick: "Re: Margaret Mead"</a>
<li> <b>In reply to:</b> <a href="2893.html">Damien Broderick: "Re: Margaret Mead"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2897.html">Kathryn Aegis: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="2897.html">Kathryn Aegis: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="2934.html">Hara Ra: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="2983.html">Tony Hollick: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="2995.html">Kathryn Aegis: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="2997.html">Damien Broderick: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3003.html">Hara Ra: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3054.html">Nick Bostrom: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3067.html">Hal Finney: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3091.html">Kathryn Aegis: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3106.html">Michael Lorrey: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3111.html">Adam Foust: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3112.html">Kathryn Aegis: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3113.html">agfoust@farthest.com: "Re: Future Technologies of Death"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Putting aside for the moment the realities of the day<br>
(something I am loathe to do, but can be coaxed into<br>
on occasion), and what technological/political solutions<br>
work in the present, it probably does make sense to<br>
think seriously about whether technologies designed to<br>
end life are a necessary part of /any/ extropic future,<br>
or whether they should properly be seen as a stopgap<br>
measure that can be done away with under the proper<br>
conditions in the future.<br>
<p>
It is, after all, our goal to encourage development of<br>
life, is it not?  Is it really possible that we might be<br>
able to evolve into beings/societies that don't find<br>
violence as necessary as we do today?  Is evolutionary<br>
progress possible wihtout using death as a method of<br>
selection?<br>
<p>
I think this last question is the salient point: if we<br>
don't have death as a means of selection, what will take<br>
its place?  Speed of expansion?  Control of resources?<br>
Is perfect passive security even theoretically possible?<br>
<p>
What is a resource?  Ultimately, if mass-energy is finite,<br>
and "life" must be implemented as patterns of mass-energy<br>
that can be destroyed and re-used by other patterns with<br>
volition (whatever that is), how can our patterns prevent<br>
themselves from being destroyed by sufficiently energetic<br>
predators?  Can we prevent the creation of such predators<br>
to begin with?  After all, since our evolution is memetic<br>
now instead of genetic, can't we just produce anti-death<br>
memes exclusively?<br>
<p>
I think that last question is a no: I can't imagine that<br>
sufficient diversity for growth is possible without fatal<br>
risk.  Discoveries cannot be planned or controlled: they<br>
are made at random, and risky ones are often made before<br>
the safe ones, and failure to exploit the risky ones will<br>
cause us to fall behind those who do.<br>
<p>
Can we manipulate the variables of the evolutionary<br>
equation to maximize growth and minimize death?  Again,<br>
I don't see how.  Maximizing one variable is likely to<br>
strictly determine all the others.  Can we maximize<br>
under the constraint of non-violence?  Would the winner<br>
of such a scenario be the non-violent one or the one who<br>
took more risks?<br>
<p>
I don't have any answers here--as fits a subject about<br>
the possibilities of the future instead of the details<br>
of the present--but even the questions make me doubt<br>
whether non-violence is really possible, even in the<br>
future.  I'd like to be proven wrong.<br>
<p>
<pre>
--
Lee Daniel Crocker &lt;lee@piclab.com&gt; &lt;<a href="http://www.piclab.com/lcrocker.html">http://www.piclab.com/lcrocker.html</a>&gt;
"All inventions or works of authorship original to me, herein and past,
are placed irrevocably in the public domain, and may be used or modified
for any purpose, without permission, attribution, or notification."--LDC
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2895.html">Wayne Hayes: "Constitutions (was: Look out! long hair gun loon!)"</a>
<li> <b>Previous message:</b> <a href="2893.html">Damien Broderick: "Re: Margaret Mead"</a>
<li> <b>In reply to:</b> <a href="2893.html">Damien Broderick: "Re: Margaret Mead"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2897.html">Kathryn Aegis: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="2897.html">Kathryn Aegis: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="2934.html">Hara Ra: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="2983.html">Tony Hollick: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="2995.html">Kathryn Aegis: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="2997.html">Damien Broderick: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3003.html">Hara Ra: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3054.html">Nick Bostrom: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3067.html">Hal Finney: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3091.html">Kathryn Aegis: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3106.html">Michael Lorrey: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3111.html">Adam Foust: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3112.html">Kathryn Aegis: "Re: Future Technologies of Death"</a>
<li> <b>Maybe reply:</b> <a href="3113.html">agfoust@farthest.com: "Re: Future Technologies of Death"</a>
<!-- reply="end" -->
</ul>
