<!-- received="Fri Nov 21 16:59:33 1997 MDT" -->
<!-- sent="Fri, 21 Nov 1997 18:59:19 -0500 (EST)" -->
<!-- name="Twink" -->
<!-- email="neptune@mars.superlink.net" -->
<!-- subject="Risk Avoidance" -->
<!-- id="199711212359.SAA07549@mars.superlink.net" -->
<!-- inreplyto="" -->
<title>extropians: Risk Avoidance</title>
<h1>Risk Avoidance</h1>
Twink (<i>neptune@mars.superlink.net</i>)<br>
<i>Fri, 21 Nov 1997 18:59:19 -0500 (EST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1684">[ date ]</a><a href="index.html#1684">[ thread ]</a><a href="subject.html#1684">[ subject ]</a><a href="author.html#1684">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1685.html">Twink: "Why UPLift"</a>
<li> <b>Previous message:</b> <a href="1683.html">Twink: "UPLifting Octopi Again"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
At 11:59 PM 11/20/97 +0100, Anders Sandberg &lt;asa@nada.kth.se&gt; wrote:<br>
<i>&gt;I disagree with that it is beyond our comprehension, but caution is</i><br>
<i>&gt;certainly adviced when doing climatic engineering - always have a</i><br>
<i>&gt;backup ecosphere. Raising global temperature to the last warm age</i><br>
<i>&gt;sounds nice, but has to be done with geological speed, otherwise the</i><br>
<i>&gt;ecosystems won't have the time to migrate as they should (OK, I'm</i><br>
<i>&gt;biased: I live in Sweden and it is November - I really would like to</i><br>
<i>&gt;get back to the bronze age climate here!).</i><br>
<p>
I agree about caution BUT let's not go overboard with it.  If we had to<br>
investigate all the potential problems with using fire before actually<br>
using, humans would probably still be an important source of food for<br>
leopards.:)<br>
<p>
<i>&gt;&gt; I guess we need big AI's to help us understand the cause-effect</i><br>
<i>&gt;&gt; relations in the global climate system, untill then: tread carefully.</i><br>
<i>&gt;</i><br>
<i>&gt;I think you overestimate the climate. It is complex, chaotic and we</i><br>
<i>&gt;know far too little yet, but it is not something we need big AI for,</i><br>
<i>&gt;rather very good simulators (taking ecology, astronomy and geology</i><br>
<i>&gt;into account). Sometimes we transhumanists are a bit too reverent</i><br>
<i>&gt;about superintelligence - it cannot solve every problem, and is not</i><br>
<i>&gt;the solution to every problem either.</i><br>
<p>
I do agree that amongst people who are into transhumanism and<br>
related ideas, there is a tendency to answer problems with "as<br>
soon as we have [insert your favorite technologic fantasy here]<br>
all problems will be solved.  This is much like the people in the<br>
UFO crowd who believe that once the saucers land all will be<br>
saved.  They have no need to really try to solve problems.<br>
Everything can be left on hold until the Great Day.  I'm not<br>
pointing this out as an ad hominem to anyone on this list, but as<br>
a word of warning.  Technoprogress is not guaranteed and new<br>
tech will bring new problems -- most likely, more exciting and<br>
better ones -- as well as benefits.<br>
<p>
In this context, macro-engineering is bound to have large scale<br>
effects.  I believe that is the point behind doing it.  Most projects<br>
we want -- uplifting, uploading, augmenting, space colonization,<br>
AI, etc. -- to see happen will have large scale effects.  The first<br>
fires humans used probably seemed tame, but look at where<br>
that went.  I would hope we don't burn down a forest just to<br>
light a campfire, but there is no risk free existence.  There are<br>
merely different paths we can take and their associated risks.<br>
To hope AI will be invented and suddenly all the risks will<br>
dissappear is a nice pipe dream.<br>
<p>
Daniel Ust<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1685.html">Twink: "Why UPLift"</a>
<li> <b>Previous message:</b> <a href="1683.html">Twink: "UPLifting Octopi Again"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
