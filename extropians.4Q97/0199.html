<!-- received="Mon Oct  6 19:05:57 1997 MDT" -->
<!-- sent="Mon, 6 Oct 1997 22:00:33 +0000" -->
<!-- name="Nicholas Bostrom" -->
<!-- email="bostrom@mail.ndirect.co.uk" -->
<!-- subject="Re: Genius dogs" -->
<!-- id="199710062102.WAA24612@andromeda.ndirect.co.uk" -->
<!-- inreplyto="Genius dogs" -->
<title>extropians: Re: Genius dogs</title>
<h1>Re: Genius dogs</h1>
Nicholas Bostrom (<i>bostrom@mail.ndirect.co.uk</i>)<br>
<i>Mon, 6 Oct 1997 22:00:33 +0000</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#199">[ date ]</a><a href="index.html#199">[ thread ]</a><a href="subject.html#199">[ subject ]</a><a href="author.html#199">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0200.html">Lee Daniel Crocker: "Re: Phonetic alphabet[wasRe: Bill Gates]"</a>
<li> <b>Previous message:</b> <a href="0198.html">YakWaxx@AOL.COM: "Re: The Upload Economy"</a>
<li> <b>Maybe in reply to:</b> <a href="0157.html">Nicholas Bostrom: "Genius dogs"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0326.html">Leevi Marttila: "Re: Genius dogs"</a>
<li> <b>Reply:</b> <a href="0326.html">Leevi Marttila: "Re: Genius dogs"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Let me first say that I agree that it would be an entirely useless <br>
enterprise to implement the dog Turing machine. I also agree that, as <br>
in Searle's Chinese room experiment, it would only be the whole <br>
system, not the individual dogs, that would be superintelligent. I <br>
furthermore agree that the whole message was rather silly, but I <br>
posted it because I thought it would elicit some comments that might <br>
spark an interesting discussion.<br>
<p>
Hal Finney wrote:<br>
<i>&gt; Nicholas Bostrom writes:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; If we take a human brain and simply speed it up enough, will it be a</i><br>
<i>&gt; &gt; superintelligence? Would a dog brain be?</i><br>
<i>&gt; </i><br>
<i>&gt; We had some debate on this issue before, in June, 1996, in the more</i><br>
<i>&gt; conventional terms of whether the insights of a genius could ever be</i><br>
<i>&gt; achieved by a normal (or somewhat subnormal) person, given enough time.</i><br>
<i>&gt; I argued that they could not, that no matter how long or how hard an</i><br>
<i>&gt; average person thought about the problem, they would not come up with</i><br>
<i>&gt; the theories of general relativity or quantum mechanics.</i><br>
<i>&gt; </i><br>
<i>&gt; A possible test for this I proposed was to take some hard problems from</i><br>
<i>&gt; the Mensa tests and give an average guy unlimited time to try to solve</i><br>
<i>&gt; them.  Might be hard to prevent cheating, though.</i><br>
<p>
Interesting. What is your opinion on the following hypothesis?<br>
<p>
(H) Take a person X of normal intelligence who knows the basics of <br>
some standard programming language. Give him an arbitrarily powerful <br>
computer, complete with camera eyes, microphones, robot arms etc. <br>
Then it is possible to educate X in less than a week in such a way <br>
that he will be able to program his computer to achieve <br>
superintelligence.<br>
<p>
The idea is that what one may call a universal intelligence algorithm <br>
might well be very simple. For example, if one knew enough about the <br>
initial conditions on the earth 4 billion years ago, one might be <br>
able to simulate evolution and thereby at least human genius level <br>
intelligence. The same could probably be done with a much leaner <br>
information base. It doesn't even seem implausible to me that some <br>
genetic algorithm or neural network architecture/learning rule that <br>
might be so simple that it could be written on the back on an <br>
envelope could achive superintelligence, given enough hardware and <br>
unlimited interaction with the external world. I can say something <br>
even stronger: I think it quite possible that a universal <br>
intelligence algorithm could be fairly easily discovered. Perhaps it <br>
could be done in a few months by a smart guy, perhaps in an <br>
afternoon. The reason nobody (as far as I know) has yet done this <br>
discovery is that the algorithm would be extremely inefficient and <br>
therefore practically useless. But it might still be fun from a <br>
theoretical point of view. Perhaps I will have a go at it myself.<br>
<p>
<i>&gt; The question I wonder about is, if the genii we talked about a few days</i><br>
<i>&gt; ago granted the (misguided?) wish to speed up mentality a million-fold,</i><br>
<i>&gt; would the resulting person be a super-intelligence simply in terms of</i><br>
<i>&gt; applying his own native reasoning powers to the problems he faced.</i><br>
<p>
Given sufficient motivation, disregarding the problem of sensory <br>
interaction (that would probably cause the person to go made within a <br>
second), and assuming he had been taught one universal intelligence <br>
algorithm, then the answer would be Yes.<br>
<p>
Nicholas Bostrom<br>
<a href="http://www.hedweb.com/nickb">http://www.hedweb.com/nickb</a><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0200.html">Lee Daniel Crocker: "Re: Phonetic alphabet[wasRe: Bill Gates]"</a>
<li> <b>Previous message:</b> <a href="0198.html">YakWaxx@AOL.COM: "Re: The Upload Economy"</a>
<li> <b>Maybe in reply to:</b> <a href="0157.html">Nicholas Bostrom: "Genius dogs"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0326.html">Leevi Marttila: "Re: Genius dogs"</a>
<li> <b>Reply:</b> <a href="0326.html">Leevi Marttila: "Re: Genius dogs"</a>
<!-- reply="end" -->
</ul>
