<!-- received="Sun Nov 23 15:04:32 1997 MDT" -->
<!-- sent="Sun, 23 Nov 1997 17:04:24 -0500 (EST)" -->
<!-- name="Twink" -->
<!-- email="neptune@mars.superlink.net" -->
<!-- subject="UPL: Dangers?" -->
<!-- id="199711232204.RAA10370@mars.superlink.net" -->
<!-- inreplyto="" -->
<title>extropians: UPL: Dangers?</title>
<h1>UPL: Dangers?</h1>
Twink (<i>neptune@mars.superlink.net</i>)<br>
<i>Sun, 23 Nov 1997 17:04:24 -0500 (EST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1744">[ date ]</a><a href="index.html#1744">[ thread ]</a><a href="subject.html#1744">[ subject ]</a><a href="author.html#1744">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1745.html">Michael Lorrey: "Re: Daedalus: (was Re: Think of a name and win $100!)"</a>
<li> <b>Previous message:</b> <a href="1743.html">Twink: "UPL: Fear and Trembling"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
At 01:06 PM 11/23/97 -0500, Keith Elis &lt;hagbard@ix.netcom.com&gt; wrote:<br>
<i>&gt;&gt; Because we are dealing with a multicellular animal.  Much easier to</i><br>
<i>&gt;&gt; control than current microbes.  Also, the octopus is a marine</i><br>
<i>&gt;&gt; animal.  Despite its escape artist skills, it will be much easier to</i><br>
<i>&gt;&gt; control such an organism, which is less likely to, say, escape to</i><br>
<i>&gt;&gt; the oceans from my apartment than, say, an uplifted rabbit or dog.</i><br>
<i>&gt;</i><br>
<i>&gt;With uplifiting, we face some of the same problems we face in dealing with</i><br>
<i>&gt;AI's. One of these similarities is our inability to actually know *WHEN* the</i><br>
<i>&gt;uplift has reached the level of sentience. (I.e., when do we *STOP* the</i><br>
<i>&gt;uplift?) Given that the uplift is likely to be a gradual process, we will</i><br>
<i>&gt;certainly have to find a method of estimating intelligence based on something</i><br>
<i>&gt;other than observed behavior or observed problem-solving ability. I suppose a</i><br>
<i>&gt;marked increase in brain activity/waves may be a clue, but this seems to be a</i><br>
<i>&gt;relatively inelegant, and in the end, inconclusive means of estimating our</i><br>
<i>&gt;success or failure.</i><br>
<p>
We woudl have to come up with some sort of independent way to measure<br>
sentience -- of which I cannot fathom right now.  There does seem to be<br>
correlations in humans between the speed with which neurons fire/react<br>
and intelligence.  However, this correlation is not that clearcut.  Perhaps<br>
Sandberg or others could add to this.<br>
<p>
<i>&gt;One concern that would stem from this is that the</i><br>
<i>&gt;newly-created intelligence may not *want* us to know it is intelligent. In such</i><br>
<i>&gt;a case, it is possible that we would have discovered the means to uplift</i><br>
<i>&gt;ourselves, but instead of doing so, we continue iterating our uplift procedures</i><br>
<i>&gt;on the test subjects, and before we realize it, we have uplifted them to be</i><br>
<i>&gt;smarter than we are.</i><br>
<p>
The same problem would happen for augmenting humans.  If you posit anti-<br>
human motives for the upliftee/augmentee (uplifting is augmenting a non-<br>
human, in a way), then, of course, this sort of thing could happen.  It's<br>
likelihood is another matter.<br>
<p>
I kind of doubt on the first experiments octopodes are going to come out of<br>
the tank with genius level IQs, a bad attitude, and deceiving us into thinking<br>
they are relatively stupid and benign.  More likely, we will be lucky to wind up<br>
with something as smart as a chimp on the first tries.  Along the way, we will<br>
learn a lot about how brains/intelligence works.<br>
<p>
<i>&gt;Maybe this is not probable, but I think finding some way to answer the question</i><br>
<i>&gt;of when we stop the uplift is necessary before we can even begin.</i><br>
<p>
When we achieve sentience.  After that the upliftees will have to decide whether<br>
they want to go further.<br>
<p>
Apply this to humans.  If you fear uplifted octopodes (or chimps, etc.) what<br>
about posthumans?  Should we allow posthumans, if such are produced,<br>
only to be so smart, etc.?<br>
<p>
Daniel Ust<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1745.html">Michael Lorrey: "Re: Daedalus: (was Re: Think of a name and win $100!)"</a>
<li> <b>Previous message:</b> <a href="1743.html">Twink: "UPL: Fear and Trembling"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
