<!-- received="Thu Nov 13 08:48:44 1997 MDT" -->
<!-- sent="Thu, 13 Nov 1997 08:48:34 -0700" -->
<!-- name="Brent Allsop" -->
<!-- email="allsop@swttools.fc.hp.com" -->
<!-- subject="RE: The copy paradox" -->
<!-- id="199711131548.AA022446114@raptor.fc.hp.com" -->
<!-- inreplyto="" -->
<title>extropians: RE: The copy paradox</title>
<h1>RE: The copy paradox</h1>
Brent Allsop (<i>allsop@swttools.fc.hp.com</i>)<br>
<i>Thu, 13 Nov 1997 08:48:34 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1392">[ date ]</a><a href="index.html#1392">[ thread ]</a><a href="subject.html#1392">[ subject ]</a><a href="author.html#1392">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1393.html">Wayne Hayes: "Re: AI and Logic: Induction, Deduction, Abduction (was Re: Penrose)"</a>
<li> <b>Previous message:</b> <a href="1391.html">GBurch1@aol.com: "Re: HUMOR: Culture-like shipnames"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Leevi Marttila &lt;lm+extropians@sip.fi&gt; responded:<br>
<p>
<i>&gt; You described something like how person would experience salty if he</i><br>
<i>&gt; is only told about salty experience.</i><br>
<p>
<i>&gt; You forgot corresponding 'look up table' containing how it relates</i><br>
<i>&gt; *internally* to other tastes, what emotions person experienced when</i><br>
<i>&gt; tasting, etc...</i><br>
<p>
<i>&gt; Actually look up table is too simplistic expression. How would you</i><br>
<i>&gt; describe trained artificial neural net semantically?</i><br>
<p>
	The same way abstract computer simulations describe or model<br>
them.  There is no more phenomenal information in any abstract<br>
computer model than there is in what we can abstractly say to each<br>
other.  As I said, you can abstractly model everything with most<br>
anything including speech and internal computer models, but any<br>
abstract model like speech, still, is only abstractly like it.  It<br>
isn't fundamentally like a real feeling or phenomenal sensation.<br>
<p>
	Also, neurons in our eyes and optical nerve learn how to<br>
abstractly represent and forward modeling information to the visual<br>
cortex.  But, we are not phenomenally conscious of this information<br>
until it arrives at the visual cortex where the phenomenal conscious<br>
representations are produced.  There is something more going on in the<br>
neurons of the primary visual cortex than the abstract stuff that is<br>
going on in the subconsciousness like the retina neurons.  I would bet<br>
that any neural net of today is still, like the neurons of the retina,<br>
not yet producing phenomenal sensations.  We haven't yet discovered<br>
precisely what this phenomenal process is and how it is formed into a<br>
unified and emotional awareness and why it is like what it is like.<br>
<p>
		Brent Allsop<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1393.html">Wayne Hayes: "Re: AI and Logic: Induction, Deduction, Abduction (was Re: Penrose)"</a>
<li> <b>Previous message:</b> <a href="1391.html">GBurch1@aol.com: "Re: HUMOR: Culture-like shipnames"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
