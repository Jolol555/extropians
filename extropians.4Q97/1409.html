<!-- received="Thu Nov 13 13:18:38 1997 MDT" -->
<!-- sent="Thu, 13 Nov 1997 13:18:26 -0700" -->
<!-- name="Brent Allsop" -->
<!-- email="allsop@swttools.fc.hp.com" -->
<!-- subject="RE: The copy paradox" -->
<!-- id="199711132018.AA024832306@raptor.fc.hp.com" -->
<!-- inreplyto="" -->
<title>extropians: RE: The copy paradox</title>
<h1>RE: The copy paradox</h1>
Brent Allsop (<i>allsop@swttools.fc.hp.com</i>)<br>
<i>Thu, 13 Nov 1997 13:18:26 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1409">[ date ]</a><a href="index.html#1409">[ thread ]</a><a href="subject.html#1409">[ subject ]</a><a href="author.html#1409">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1410.html">wolfkin@ldl.net: "Re: The Copy Paradox"</a>
<li> <b>Previous message:</b> <a href="1408.html">Max More: "Re: Clade ships"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Hal Finney &lt;hal@rain.org&gt; asked:<br>
<p>
<i>&gt; The question is, what constitutes an "abstract" model versus a real</i><br>
<i>&gt; one?  Is it a matter of whether the underlying substrate is silicon</i><br>
<i>&gt; versus protein?  Or is it a matter of the internal form and</i><br>
<i>&gt; structure of the model?</i><br>
<p>
	Silicon is fundamentally real silicon and protein is<br>
fundamentally real protein.  The two can computationally or abstractly<br>
model each other, producing identical computational output or<br>
behavior, but at the fundamental layer they are not anything like each<br>
other.  When one is formed in a way to model the behavior of the other<br>
it is a mere "abstract" behavior model and not fundamentally like the<br>
real thing.<br>
<p>
<i>&gt; I believe Leevi was taking the latter position, and arguing that a</i><br>
<i>&gt; sufficiently complicated model, even if running on silicon, would no</i><br>
<i>&gt; longer be abstract.</i><br>
<p>
	Sure you can model any behavior given sufficient computational<br>
complexity but at what point would the fundamental nature suddenly<br>
become the same as the model?  Fundamentally, silicone is still<br>
silicone.  It can never magically suddenly cross a sufficiently<br>
complex boundary and become fundamentally really like protein even<br>
though the model can precisely model, abstractly, that quality.  At<br>
the fundamental level they are different.<br>
<p>
<i>&gt; Things would acquire meaning not because of arbitrary assignments</i><br>
<i>&gt; (this register holds saltiness, that register holds blueness) but</i><br>
<i>&gt; due to the immense complexity of the interactions among the various</i><br>
<i>&gt; representations.</i><br>
<p>
	You can't chase the fundamental problem away just by adding<br>
enough complexity.  Sure the "meaning" or abstract behavior can be<br>
anything you want.  A very simple and trivial silicone color detecting<br>
machine has plenty of complexity to tell me what color something is<br>
even better than I can tell what color something is.  But, the<br>
representations of color in this simple machine, though far more<br>
complex, are fundamentally very different than the phenomenal<br>
representations I use to represent color.  The particular fundamental<br>
nature of the color detecting machine representations aren't relevant.<br>
They can be states of a transistor, voltages on a wire, or flux<br>
orientation on magnetic media...  The fundamental nature is not<br>
relevant to the computation in this simple yet more accurate and<br>
complex than me machine.  But for me, the fundamental nature of green,<br>
and how it is phenomenally different from red, is what enables me to<br>
produce the different words green and red.  To me, the fundamental<br>
nature of the representation and what it is like is all important.<br>
<p>
<i>&gt; Redness and blueness are so complex, so interrelated with other</i><br>
<i>&gt; concepts, that it would be impossible to disentangle them from each</i><br>
<i>&gt; other.</i><br>
<p>
	I disagree.  Red is fundamentally and simply red and blue is<br>
fundamentally and simply blue.  The two are not like each other and<br>
nothing else is subjectively the same as them.  There is no tangling<br>
at all.  Just because you can have complex relations between them and<br>
many other things doesn't change their non varying simple and<br>
fundamentally different nature.<br>
<p>
	You described the relevance of neural processing in the retina<br>
and argued that it all might gradually become conscious.  Again, I<br>
disagree.<br>
<p>
	True, all the subconscious preprocessing is very essential to<br>
extracting the information so that the required 3D information can be<br>
extracted from the stereo 2D images which enables our 3D awareness to<br>
be built out of qualia in the conscious space of our visual cortex.<br>
The green tree you are aware of and think is beyond your eyes, is<br>
really in your visual cortex and only beyond your phenomenal conscious<br>
model of your eyes which is also, merely a model of your eyes in your<br>
brain.  Everything you are aware of is simply a conscious model of the<br>
reality which is beyond your senses.  There is no gradual about it.<br>
One is the source of the stimuli and the other is the final result of<br>
much complex sensing and processing.  They are both at the opposite<br>
ends of the very complex cause and effect process.  One is in your<br>
brain and the other is beyond your senses.  One is conscious, the<br>
other is not.<br>
<p>
	There is no color, smell, sound, warmth... or pain beyond our<br>
senses; only the electromagnetic radiation, chemical content,<br>
acoustical vibrations, kinetic energy of molecules... and bodily<br>
damage our brains merely arbitrarily represent with such phenomenon.<br>
Each of these things can very successfully model the other, but they<br>
are not really the other and are only abstract models.  Red really is<br>
nothing like 700nm electromagnetic radiation, but it abstractly models<br>
it well in the conscious world of our awareness inside our brain.<br>
<p>
	When we peal away the layers or separate the parts we see a<br>
complex machine composed of very fundamentally real and distinct parts<br>
at the fundamental layer.  Some of these parts abstractly work at<br>
extracting the 3D info contained in the abstract subconscious 2D<br>
stereo images contained on the retina.  This causal process eventually<br>
constructs a glorious 3D model of reality out of phenomenal qualia.<br>
This model built out of qualia is our conscious visual awareness and<br>
is in our brain not beyond our eye.  The 2 2d images on the retina are<br>
clearly not conscious at all.  But the resulting 3d awareness clearly<br>
is and is obviously built out of something.  What this something is<br>
like is fundamentally the quality of consciousness.  Nothing abstract<br>
can be quite fundamentally like it.<br>
<p>
		Brent Allsop<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1410.html">wolfkin@ldl.net: "Re: The Copy Paradox"</a>
<li> <b>Previous message:</b> <a href="1408.html">Max More: "Re: Clade ships"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
