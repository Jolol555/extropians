<!-- received="Mon Nov 24 16:09:26 1997 MDT" -->
<!-- sent="24 Nov 1997 08:45:21 +0200" -->
<!-- name="Leevi Marttila" -->
<!-- email="lm+extropians@sip.fi" -->
<!-- subject="Re: The copy paradox" -->
<!-- id="199711242223.OAA19348@mercury.colossus.net" -->
<!-- inreplyto="The copy paradox" -->
<title>extropians: Re: The copy paradox</title>
<h1>Re: The copy paradox</h1>
Leevi Marttila (<i>lm+extropians@sip.fi</i>)<br>
<i>24 Nov 1997 08:45:21 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1781">[ date ]</a><a href="index.html#1781">[ thread ]</a><a href="subject.html#1781">[ subject ]</a><a href="author.html#1781">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1782.html">Brent Allsop: "Re: Uploading, that's needed !!  -Reply -Reply -Reply"</a>
<li> <b>Previous message:</b> <a href="1780.html">Anders Sandberg: "Re: Uploading, that's needed !!  -Reply -Reply -Reply"</a>
<li> <b>Maybe in reply to:</b> <a href="1087.html">Clemens Pittino: "The copy paradox"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1798.html">Bradley Graham Weslake: "Re: Crackpots"</a>
<li> <b>Reply:</b> <a href="1798.html">Bradley Graham Weslake: "Re: Crackpots"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Brent Allsop &lt;allsop@swttools.fc.hp.com&gt; writes:<br>
<p>
<i>&gt; Hal Finney &lt;hal@rain.org&gt; asked:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; The question is, what constitutes an "abstract" model versus a real</i><br>
<i>&gt; &gt; one?  Is it a matter of whether the underlying substrate is silicon</i><br>
<i>&gt; &gt; versus protein?  Or is it a matter of the internal form and</i><br>
<i>&gt; &gt; structure of the model?</i><br>
<i>&gt; </i><br>
<i>&gt; 	Silicon is fundamentally real silicon and protein is</i><br>
<i>&gt; fundamentally real protein.  The two can computationally or abstractly</i><br>
<i>&gt; model each other, producing identical computational output or</i><br>
<i>&gt; behavior, but at the fundamental layer they are not anything like each</i><br>
<i>&gt; other.  When one is formed in a way to model the behavior of the other</i><br>
<i>&gt; it is a mere "abstract" behavior model and not fundamentally like the</i><br>
<i>&gt; real thing.</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; I believe Leevi was taking the latter position, and arguing that a</i><br>
<i>&gt; &gt; sufficiently complicated model, even if running on silicon, would no</i><br>
<i>&gt; &gt; longer be abstract.</i><br>
<i>&gt; </i><br>
<i>&gt; 	Sure you can model any behavior given sufficient computational</i><br>
<i>&gt; complexity but at what point would the fundamental nature suddenly</i><br>
<i>&gt; become the same as the model?  Fundamentally, silicone is still</i><br>
<i>&gt; silicone.  It can never magically suddenly cross a sufficiently</i><br>
<i>&gt; complex boundary and become fundamentally really like protein even</i><br>
<i>&gt; though the model can precisely model, abstractly, that quality.  At</i><br>
<i>&gt; the fundamental level they are different.</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; Things would acquire meaning not because of arbitrary assignments</i><br>
<i>&gt; &gt; (this register holds saltiness, that register holds blueness) but</i><br>
<i>&gt; &gt; due to the immense complexity of the interactions among the various</i><br>
<i>&gt; &gt; representations.</i><br>
<i>&gt; </i><br>
<i>&gt; 	You can't chase the fundamental problem away just by adding</i><br>
<i>&gt; enough complexity.  Sure the "meaning" or abstract behavior can be</i><br>
<i>&gt; anything you want.  A very simple and trivial silicone color detecting</i><br>
<i>&gt; machine has plenty of complexity to tell me what color something is</i><br>
<i>&gt; even better than I can tell what color something is.  But, the</i><br>
<i>&gt; representations of color in this simple machine, though far more</i><br>
<i>&gt; complex, are fundamentally very different than the phenomenal</i><br>
<i>&gt; representations I use to represent color.  The particular fundamental</i><br>
<i>&gt; nature of the color detecting machine representations aren't relevant.</i><br>
<i>&gt; They can be states of a transistor, voltages on a wire, or flux</i><br>
<i>&gt; orientation on magnetic media...  The fundamental nature is not</i><br>
<i>&gt; relevant to the computation in this simple yet more accurate and</i><br>
<i>&gt; complex than me machine.  But for me, the fundamental nature of green,</i><br>
<i>&gt; and how it is phenomenally different from red, is what enables me to</i><br>
<i>&gt; produce the different words green and red.  To me, the fundamental</i><br>
<i>&gt; nature of the representation and what it is like is all important.</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; Redness and blueness are so complex, so interrelated with other</i><br>
<i>&gt; &gt; concepts, that it would be impossible to disentangle them from each</i><br>
<i>&gt; &gt; other.</i><br>
<i>&gt; </i><br>
<i>&gt; 	I disagree.  Red is fundamentally and simply red and blue is</i><br>
<i>&gt; fundamentally and simply blue.  The two are not like each other and</i><br>
<i>&gt; nothing else is subjectively the same as them.  There is no tangling</i><br>
<i>&gt; at all.  Just because you can have complex relations between them and</i><br>
<i>&gt; many other things doesn't change their non varying simple and</i><br>
<i>&gt; fundamentally different nature.</i><br>
<i>&gt; </i><br>
<i>&gt; 	You described the relevance of neural processing in the retina</i><br>
<i>&gt; and argued that it all might gradually become conscious.  Again, I</i><br>
<i>&gt; disagree.</i><br>
<i>&gt; </i><br>
<i>&gt; 	True, all the subconscious preprocessing is very essential to</i><br>
<i>&gt; extracting the information so that the required 3D information can be</i><br>
<i>&gt; extracted from the stereo 2D images which enables our 3D awareness to</i><br>
<i>&gt; be built out of qualia in the conscious space of our visual cortex.</i><br>
<i>&gt; The green tree you are aware of and think is beyond your eyes, is</i><br>
<i>&gt; really in your visual cortex and only beyond your phenomenal conscious</i><br>
<i>&gt; model of your eyes which is also, merely a model of your eyes in your</i><br>
<i>&gt; brain.  Everything you are aware of is simply a conscious model of the</i><br>
<i>&gt; reality which is beyond your senses.  There is no gradual about it.</i><br>
<i>&gt; One is the source of the stimuli and the other is the final result of</i><br>
<i>&gt; much complex sensing and processing.  They are both at the opposite</i><br>
<i>&gt; ends of the very complex cause and effect process.  One is in your</i><br>
<i>&gt; brain and the other is beyond your senses.  One is conscious, the</i><br>
<i>&gt; other is not.</i><br>
<i>&gt; </i><br>
<i>&gt; 	There is no color, smell, sound, warmth... or pain beyond our</i><br>
<i>&gt; senses; only the electromagnetic radiation, chemical content,</i><br>
<i>&gt; acoustical vibrations, kinetic energy of molecules... and bodily</i><br>
<i>&gt; damage our brains merely arbitrarily represent with such phenomenon.</i><br>
<i>&gt; Each of these things can very successfully model the other, but they</i><br>
<i>&gt; are not really the other and are only abstract models.  Red really is</i><br>
<i>&gt; nothing like 700nm electromagnetic radiation, but it abstractly models</i><br>
<i>&gt; it well in the conscious world of our awareness inside our brain.</i><br>
<i>&gt; </i><br>
<i>&gt; 	When we peal away the layers or separate the parts we see a</i><br>
<i>&gt; complex machine composed of very fundamentally real and distinct parts</i><br>
<i>&gt; at the fundamental layer.  Some of these parts abstractly work at</i><br>
<i>&gt; extracting the 3D info contained in the abstract subconscious 2D</i><br>
<i>&gt; stereo images contained on the retina.  This causal process eventually</i><br>
<i>&gt; constructs a glorious 3D model of reality out of phenomenal qualia.</i><br>
<i>&gt; This model built out of qualia is our conscious visual awareness and</i><br>
<i>&gt; is in our brain not beyond our eye.  The 2 2d images on the retina are</i><br>
<i>&gt; clearly not conscious at all.  But the resulting 3d awareness clearly</i><br>
<i>&gt; is and is obviously built out of something.  What this something is</i><br>
<i>&gt; like is fundamentally the quality of consciousness.  Nothing abstract</i><br>
<i>&gt; can be quite fundamentally like it.</i><br>
<i>&gt; </i><br>
<i>&gt; 		Brent Allsop</i><br>
<p>
What if some advanced civilization replaced one of your neurons with<br>
artificial one that behaved from the viewpoint of other brain like<br>
original. What if they replaced all neurons one by one with artificial<br>
one. Would your consciousness change? Would your phenomenon change?<br>
<p>
<pre>
-- 
LM lm+signature@sip.fi
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1782.html">Brent Allsop: "Re: Uploading, that's needed !!  -Reply -Reply -Reply"</a>
<li> <b>Previous message:</b> <a href="1780.html">Anders Sandberg: "Re: Uploading, that's needed !!  -Reply -Reply -Reply"</a>
<li> <b>Maybe in reply to:</b> <a href="1087.html">Clemens Pittino: "The copy paradox"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1798.html">Bradley Graham Weslake: "Re: Crackpots"</a>
<li> <b>Reply:</b> <a href="1798.html">Bradley Graham Weslake: "Re: Crackpots"</a>
<!-- reply="end" -->
</ul>
