<!-- received="Wed Nov 12 09:28:35 1997 MDT" -->
<!-- sent="Wed, 12 Nov 1997 09:28:30 -0700" -->
<!-- name="Brent Allsop" -->
<!-- email="allsop@swttools.fc.hp.com" -->
<!-- subject="RE: The copy paradox" -->
<!-- id="199711121628.AA010432110@raptor.fc.hp.com" -->
<!-- inreplyto="" -->
<title>extropians: RE: The copy paradox</title>
<h1>RE: The copy paradox</h1>
Brent Allsop (<i>allsop@swttools.fc.hp.com</i>)<br>
<i>Wed, 12 Nov 1997 09:28:30 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1365">[ date ]</a><a href="index.html#1365">[ thread ]</a><a href="subject.html#1365">[ subject ]</a><a href="author.html#1365">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1366.html">Bradley Felton: "Re: QUOTE: Bey on extropians"</a>
<li> <b>Previous message:</b> <a href="1364.html">Wayne Hayes: "AI and Logic: Induction, Deduction, Abduction (was Re: Penrose)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Ramez Naam (Exchange) &lt;ramezn@EXCHANGE.MICROSOFT.com&gt; responded:<br>
<p>
<i>&gt; The subjective experience of "Red" is a consequence of the</i><br>
<i>&gt; neurophysiological state of seeing (or imagining) light of a certain</i><br>
<i>&gt; frequency.</i><br>
<p>
	Yes, but imagining red and actual red from real 700nm light<br>
are different experiences, each undoubtedly the consequence of<br>
different though possibly similar neurophysiological states and/or<br>
systems.<br>
<p>
<i>&gt; We can verify analogous neurophysiological states for various</i><br>
<i>&gt; sounds, bodily sensations, smells, tastes, etc..</i><br>
<p>
<i>&gt; Given this, can we not reasonably suppose that the subjective</i><br>
<i>&gt; experience is a consequence of the neurophysiological state</i><br>
<i>&gt; triggered by the stimuli?</i><br>
<p>
	Yes of course.<br>
<p>
<i>&gt; If this is the case, can we not learn to distinguish those</i><br>
<i>&gt; "neuroqualia" which are essentially the same among all humans from</i><br>
<i>&gt; those neuroqualia which vary among individuals?</i><br>
<p>
	Once we understand what "neuroqualia" are yes.  I can't<br>
imagine any reason that would prevent such.<br>
<p>
<i>&gt; If so, would not the first set represent a set of "experiences"</i><br>
<i>&gt; which we could then share with another individual by instantiating</i><br>
<i>&gt; the same neurophysiological state in that individual?</i><br>
<p>
<i>&gt; Indeed, among individuals of different species, or between (for</i><br>
<i>&gt; example) a human and an AI, could we not construct a mapping table</i><br>
<i>&gt; that allowed us to translate these experiences to the appropriate</i><br>
<i>&gt; internal representation?</i><br>
<p>
	I described an "effing" process earlier in this thread in<br>
which a sensing machine observes the neurophysiological firings of all<br>
relevant "neural correlates" to a particular sensation.  Then this<br>
information is communicated to another brain which is augmented with a<br>
cortex that is able to produce generic sensations in the consciousness<br>
of that person.  Upon receiving the information from the sensor<br>
observing sensation process in the other brain, it would produce in<br>
the consciousness of the augmented brain an identical sensation.  It<br>
may be that the person doing the effing might say: "That isn't what<br>
salt tastes like to me".<br>
<p>
	I can imagine being able to "eff" sensation in other animals,<br>
like say a bat.  We would then know what it is like to be a bat.  I<br>
also imagine machines or computers being endowed with such generic<br>
qualia experiencing devices so that they too could experience and know<br>
what salty is.  I would imagine that computers with such phenomenal<br>
representation abilities would be many times more intelligent and have<br>
much more common sense than any computer that uses mere abstract<br>
representations.<br>
<p>
	The important thing is is that salty is salty.  Sure, you can<br>
abstractly represent any and everything you might want to represent<br>
about salty and instantiate this abstract information any way you<br>
want, but salty is still salty.  Anything that is fundamentally<br>
different than salty is not salty even though it may abstractly<br>
represent salty.  Only salty is like salty everything else is a mere<br>
and fundamentally different representation.<br>
<p>
<i>&gt; 1) This thought experiment may in fact be impossible.  Much like me</i><br>
<i>&gt; imagining (If I could go 10x the speed of light..)</i><br>
<p>
	All you would have to do is something like create a city of<br>
streets where red equals go and green equals stop.  The driving would<br>
all be the same, but the subjective experience of the drivers would be<br>
different.<br>
<p>
<i>&gt; 2) Research has demonstrated a psychological effect of color on</i><br>
<i>&gt; humans.  Different colors produce different levels of stress,</i><br>
<i>&gt; relaxation, attention, arousal, etc.. in fairly predictable and</i><br>
<i>&gt; consistent ways.  So I ask you, if I switch red and green, and all</i><br>
<i>&gt; fires are now green, haven't I actually made some difference in the</i><br>
<i>&gt; meaning of fire to the individual?  In the sense that the fire will</i><br>
<i>&gt; now evoke a different response than when it was red?</i><br>
<p>
	I would say yes to these questions.  The various levels of<br>
"stress, relaxation, attention, arousal, etc.." is a very complex<br>
process.  Sure it could be related to particular qualia or to various<br>
neural responses prior to and/or beyond the particular ones that<br>
produce particular colors because of memory associations and<br>
everything else.<br>
<p>
<i>&gt; You assume a gulf between sensation and realization that I do not</i><br>
<i>&gt; believe exists.  Imagine that sensation is a consequence of the</i><br>
<i>&gt; physical system that instantiates the information that is</i><br>
<i>&gt; represented in consciousness.  Or, put another way, sensation /is/</i><br>
<i>&gt; the stuff of consciousness.  A being that represents salty</i><br>
<i>&gt; internally in a sufficiently rich manner necessarily has an</i><br>
<i>&gt; "experience" of salty.</i><br>
<p>
	Yes, any old abstract representation can be sufficiently rich<br>
to represent or model salty.  But only my salty is precisely like my<br>
salty.<br>
<p>
<i>&gt; Hmm.  You posit an incredibly simple machine and then grant it</i><br>
<i>&gt; intelligence, which immediately strikes me as contradictory.</i><br>
<p>
	Sorry for the confusion.  I'm talking about the kinds of<br>
abstract machines people produce these days which use abstract look up<br>
tables and such to try to carry on conversations like humans do.  I'm<br>
saying that a poor programmer of intelligence would try to make his AI<br>
be a liar and try to have it spout verbiage a human might spout, via a<br>
lookup table or whatever, when asked what salty is.  A truly<br>
intelligent AI would logically recognize that it has no real<br>
subjective experience of salty or whatever and would try to explain<br>
this in any conversation on the topic, unless of course it was really<br>
trying to deceive someone that it was not a computer and intentionally<br>
being a liar.<br>
<p>
<i>&gt; So, long and short: if you had a significantly different neural</i><br>
<i>&gt; structure from me, I would expect the experience of tasting</i><br>
<i>&gt; something salty to be quite different for you from me.</i><br>
<p>
<i>&gt; Given our species-born similarities (I'm just guessing here that</i><br>
<i>&gt; you're of my species. :) ) I presume that our experiences are quite</i><br>
<i>&gt; similar.</i><br>
<p>
	Yes, of course.  But until we discover what salty really is<br>
and can do things like eff such sensations, produce the same in<br>
artificial machines and so on, we just don't yet know for sure.  But<br>
we do know what our sensations are like and that they are phenomenally<br>
real.  Red is most definitely not anything like salty.  We know such<br>
things more than we know anything else for indeed everything we<br>
consciously know is represented by such.<br>
<p>
		Brent Allsop<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1366.html">Bradley Felton: "Re: QUOTE: Bey on extropians"</a>
<li> <b>Previous message:</b> <a href="1364.html">Wayne Hayes: "AI and Logic: Induction, Deduction, Abduction (was Re: Penrose)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
