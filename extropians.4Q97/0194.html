<!-- received="Mon Oct  6 18:42:10 1997 MDT" -->
<!-- sent="Mon, 06 Oct 1997 11:17:16 -0500" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Genius dogs" -->
<!-- id="199710061614.JAA22607@s20.term1.sb.rain.org" -->
<!-- inreplyto="Genius dogs" -->
<title>extropians: Re: Genius dogs</title>
<h1>Re: Genius dogs</h1>
Eliezer S. Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Mon, 06 Oct 1997 11:17:16 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#194">[ date ]</a><a href="index.html#194">[ thread ]</a><a href="subject.html#194">[ subject ]</a><a href="author.html#194">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0195.html">Brian Atkins: "Re: The Spike, nanotech, and a future scenario"</a>
<li> <b>Previous message:</b> <a href="0193.html">Hal Finney: "Re: The Spike, nanotech, and a future scenario"</a>
<li> <b>Maybe in reply to:</b> <a href="0157.html">Nicholas Bostrom: "Genius dogs"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0199.html">Nicholas Bostrom: "Re: Genius dogs"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Anders Sandberg wrote:<br>
<i>&gt; </i><br>
<i>&gt; &gt; -- I anticipate that Anders will suggest that superintelligences will</i><br>
<i>&gt; &gt; make entertainment out of shaping human organizations into</i><br>
<i>&gt; &gt; entities that perform intelligent tasks, just as we have fun by</i><br>
<i>&gt; &gt; watching circus animals behave. :-)</i><br>
<i>&gt; </i><br>
<i>&gt; It seems like you have a quite good simulation of my thought processes.</i><br>
<i>&gt; </i><br>
<i>&gt; About Eliezer's view that this would be immoral: I think it can be</i><br>
<i>&gt; ethical to do, as long as the humans voluntarily agree to form the</i><br>
<i>&gt; organisation. "Wow! Look at this: that PostAnders entity has a really</i><br>
<i>&gt; great idea about a selforganized democracy. Let's try it!"</i><br>
<p>
Well, of course it's moral with informed consent.  *Anything* is moral<br>
with informed consent.  That's not the same as the "circus" theory.<br>
<p>
I still maintain that it would be pointless to form human organizations into<br>
Turing machines.  Besides, this doesn't take a posthuman entity at all.  I bet<br>
Bill Gates could make Microsoft simulate a small Turing machine with a few<br>
simple mandatory self-propagating memos.  Maybe he's doing it already.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0195.html">Brian Atkins: "Re: The Spike, nanotech, and a future scenario"</a>
<li> <b>Previous message:</b> <a href="0193.html">Hal Finney: "Re: The Spike, nanotech, and a future scenario"</a>
<li> <b>Maybe in reply to:</b> <a href="0157.html">Nicholas Bostrom: "Genius dogs"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="0199.html">Nicholas Bostrom: "Re: Genius dogs"</a>
<!-- reply="end" -->
</ul>
