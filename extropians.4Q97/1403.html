<!-- received="Thu Nov 13 11:13:17 1997 MDT" -->
<!-- sent="Thu, 13 Nov 1997 10:12:16 -0800" -->
<!-- name="Hal Finney" -->
<!-- email="hal@rain.org" -->
<!-- subject="Re: The Copy Paradox" -->
<!-- id="199711131812.KAA09834@s20.term1.sb.rain.org" -->
<!-- inreplyto="The Copy Paradox" -->
<title>extropians: Re: The Copy Paradox</title>
<h1>Re: The Copy Paradox</h1>
Hal Finney (<i>hal@rain.org</i>)<br>
<i>Thu, 13 Nov 1997 10:12:16 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1403">[ date ]</a><a href="index.html#1403">[ thread ]</a><a href="subject.html#1403">[ subject ]</a><a href="author.html#1403">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1404.html">Michael Butler: "Re: HUMOR: Culture-like shipnames"</a>
<li> <b>Previous message:</b> <a href="1402.html">Hal Finney: "RE: The copy paradox"</a>
<li> <b>Maybe in reply to:</b> <a href="1535.html">Brent Allsop: "The Copy Paradox"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1405.html">wolfkin@ldl.net: "Re: The Copy Paradox"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Harvey Newstrom, &lt;harv@gate.net&gt;, writes:<br>
<p>
<i>&gt; Even if my twin could be modified by a plastic surgeon to look exactly</i><br>
<i>&gt; like me, I still wouldn't want to die.  Even if you recorded my exact</i><br>
<i>&gt; brain state, and reset his brain to that exact configuration, I would</i><br>
<i>&gt; still not want to die.</i><br>
<p>
Certainly a physical copy would not be enough.  But things are different<br>
if you are talking about an exact mental copy.  It can be argued that<br>
identity is fundamentally a matter of patterns of processing.  Reproduce<br>
the pattern, and you reproduce the identity.<br>
<p>
<i>&gt; Note that I am not saying that he is not me, or that he is not another</i><br>
<i>&gt; instantiation of "me".  These are semantic word games which don't really</i><br>
<i>&gt; matter.  I just still desire to continue experiencing life.  Knowledge</i><br>
<i>&gt; that another person very similar to me or exactly like me will continue</i><br>
<i>&gt; to experience life does not change my opinion.  My twin has not directly</i><br>
<i>&gt; affected my subjective experience of life up till now.  Even modifying</i><br>
<i>&gt; him to be exactly like me does not change my subjective experience of</i><br>
<i>&gt; life.  In fact, you could modify him without my knowledge. I still would</i><br>
<i>&gt; oppose my personal death.  I don't know how making my twin brother more</i><br>
<i>&gt; like me makes it any more acceptable for me to die.  If you told me that</i><br>
<i>&gt; you had modified my brother to be exactly like me, I still don't see why</i><br>
<i>&gt; I would change my mind and let you shoot me.  Even if I really believed</i><br>
<i>&gt; that you actually accomplished what you claimed, I still wouldn't change</i><br>
<i>&gt; my mind.</i><br>
<i>&gt;</i><br>
<i>&gt; Does anyone know what is different between my understanding and that of</i><br>
<i>&gt; those who would be willing to die if there was a copy made of them?  I</i><br>
<i>&gt; am curious to find out why I am not thinking the way other people are. </i><br>
<i>&gt; Maybe it is my own experience with my own copy (twin) that makes me</i><br>
<i>&gt; skeptical of this approach?</i><br>
<p>
It's possible that your twin is making you focus too closely on genetic<br>
and physical similarity, which is really not the issue.  There is an<br>
enormous difference between someone who looks something like you and<br>
someone whose brain is running exactly the same program as yours.<br>
<p>
Here are two arguments which make the pattern theory of identity more<br>
plausible.<br>
<p>
The first is to imagine that you are an upload, a computer program which<br>
was originally a copy of some human mind.  You live within the computer,<br>
your mind is a program.  Now what kinds of transformations will you<br>
allow to occur to your program?<br>
<p>
It seems plausible, based on our experience with programs, that it should<br>
be OK to suspend your program for a while, and restart it.  You will<br>
of course not experience any passage of time during the suspension.<br>
If you object to this, consider for example that you may be running<br>
on a timesharing system where this happens all the time anyway, or you<br>
could imagine running on progressively slower computers until you were<br>
in effect suspended.  Neither of these would seem to give you reason to<br>
believe that you have died.<br>
<p>
It is also plausible that you would be willing to run on a computer network<br>
of some kind, with your processing spread over multiple computers.  (It may<br>
well be that this is the only practical way to run such a complex program.)<br>
In this situation, it could even be the case that the network is dynamic,<br>
with different processors being available from time to time, and the various<br>
parts of your program jumping from processor to processor over the course<br>
of your run.  You would not be subjectively aware of any of this, and your<br>
mental functioning would seem perfectly normal.<br>
<p>
Putting these together, you could imagine stopping your run on one computer,<br>
and later resuming it on a different computer.  You would not perceive any<br>
break in your train of thought or have any reason to believe that you had<br>
died.<br>
<p>
Now, this is effectively the same as starting up a copy of your mind while<br>
destroying the original.  It appears that this actually does preserve<br>
identity, at least in the case of uploads, and there is no reason why it<br>
should not do so for organic brains as well - it's just more difficult to<br>
arrange in that case.<br>
<p>
Another argument is to consider the case of a world where copying like<br>
you describe is easy and commonly used as a means of transportation.<br>
Someone has copies "on ice" on many different planets, and when they<br>
want to travel elsewhere they transmit a copy of their brain state, get<br>
that programmed into the brain of the copy, which then wakes up as them.<br>
(This is similar to what is done in Linda Nagata's "The Bohr Maker".)<br>
<p>
Imagine yourself someone who has participated in this form of<br>
transportation many times over the course of his life.  He has memories<br>
of having lived in each of these various bodies for a time, then<br>
transferring his consciousness to a different one, while the original<br>
body was destroyed (or at least its memory erased for future use).<br>
<p>
<i>&gt;From his point of view, these transformations preserved his consciousness</i><br>
and sense of self just as much as going to sleep at night and waking<br>
up in the morning.  He felt just the same, behaved the same, remembered<br>
everything he had done before the transfer.  There is no reason for him<br>
to view it as death.<br>
<p>
This would be the natural point of view of anyone who had tried it a<br>
few times.  So it would seem that actually experiencing the kind of<br>
transfer that you are afraid of leads to reassurance that it does preserve<br>
your sense of self, and there is no reason to fear it as death.<br>
<p>
Hal<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1404.html">Michael Butler: "Re: HUMOR: Culture-like shipnames"</a>
<li> <b>Previous message:</b> <a href="1402.html">Hal Finney: "RE: The copy paradox"</a>
<li> <b>Maybe in reply to:</b> <a href="1535.html">Brent Allsop: "The Copy Paradox"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1405.html">wolfkin@ldl.net: "Re: The Copy Paradox"</a>
<!-- reply="end" -->
</ul>
