<!-- received="Mon Oct 13 02:01:12 1997 MDT" -->
<!-- sent="Mon, 13 Oct 1997 00:16:44 -0700" -->
<!-- name="Max More" -->
<!-- email="maxmore@primenet.com" -->
<!-- subject="Re: "Morality?" - Composite Reply" -->
<!-- id="3.0.32.19971012235126.00690f08@mailhost.primenet.com" -->
<!-- inreplyto=""Morality?" - Composite Reply" -->
<title>extropians: Re: "Morality?" - Composite Reply</title>
<h1>Re: "Morality?" - Composite Reply</h1>
Max More (<i>maxmore@primenet.com</i>)<br>
<i>Mon, 13 Oct 1997 00:16:44 -0700</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#485">[ date ]</a><a href="index.html#485">[ thread ]</a><a href="subject.html#485">[ subject ]</a><a href="author.html#485">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0486.html">Michael Lorrey: "Re: Constraints on the "singularity""</a>
<li> <b>Previous message:</b> <a href="0484.html">Kennita Watson: "Re: Crop circles again?!! (was: Re: Bill Gates)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
At 11:55 PM 10/12/97 +0000, Nicholas Bostrom wrote:<br>
<i>&gt;Max More wrote:</i><br>
<i>&gt;</i><br>
<i>&gt;&gt; If there really were</i><br>
<i>&gt;&gt; vampires, and there survival really did depend on drinking the blood of</i><br>
<i>&gt;&gt; humans (in such a way that it killed them and there were no alternatives</i><br>
<i>&gt;&gt; like blood banks) then it would be *right* for vampires to attack humans.</i><br>
<i>&gt;&gt; It would also be *right* for humans to defend themselves. Differences in</i><br>
<i>&gt;&gt; nature produce different behaviors that are good for the beings that do</i><br>
<i>&gt;&gt; them. Given that humans have essentially the same nature, such moral</i><br>
<i>&gt;&gt; divergence is unlikely.</i><br>
<i>&gt;</i><br>
<i>&gt;Just to make sure I understand you correctly, is the following a </i><br>
<i>&gt;correct interpretation of the above passage?</i><br>
<i>&gt;</i><br>
<i>&gt;Even if all humans were identical, it could still be the case that </i><br>
<i>&gt;one human's deepest value is irrelevant to another. For example, if </i><br>
<i>&gt;each is perfectly egoistic, then Mr. X's deepest value might be the </i><br>
<i>&gt;flourishing of Mr X; whereas Mr. Y only cares about Mr. Y etc. The </i><br>
<i>&gt;obvious sense in which you could say that these egosists have a </i><br>
<i>&gt;common moral is that they could produce a set of norms that would </i><br>
<i>&gt;apply to them all, such as "Don't steal! (Because if you do, the </i><br>
<i>&gt;police will get you.)". I presume the point with the vampire example </i><br>
<i>&gt;is to give an example of how there could be creatures with </i><br>
<i>&gt;sufficiently different goals or abilities to make human morality </i><br>
<i>&gt;irrelevant to them.</i><br>
<p>
Right. Although your last statement does not represent my position quite<br>
correctly. The point is that the vampires would have no reason (in the<br>
situation as I've set it up) to refrain from killing humans. It would be<br>
pointless for humans to tell vampires that they were being immoral. What<br>
was moral for vampires might be immoral for humans to do to each other. But<br>
saying that human morality is irrevelant to them is too strong. Many moral<br>
principles and virtues that apply to humans might apply to vampires.<br>
Courage, for example. Benevolence and mutual aid might even be rational<br>
between vampires.<br>
<p>
<i>&gt;If this is right then what distinguishes moral knowledge from </i><br>
<i>&gt;other knowledge? Is it just that moral knowledge typically concerns </i><br>
<i>&gt;life strategies or codes for interacting with other humans? Would </i><br>
<i>&gt;"Take out an insurance!" or "Con thy neighbor subtly!" count as a </i><br>
<i>&gt;moral imperative for humans, supposing that it would be good advise </i><br>
<i>&gt;for most people (i.e. that each would better obtain her own values if </i><br>
<i>&gt;she follows it than if she doesn't)?</i><br>
<p>
I don't see moral knowledge as differing fundamentally from other<br>
knowledge. I see moral knowledge as being more difficult to come by and<br>
more difficult to test than scientific knowledge. But that's also true of<br>
economic, sociological, and historical knowledge. Reaching firm conclusions<br>
in ethics involves complicated reasoning about human psychology (and how<br>
much is inborn, how much acquired, and how much alterable), and the effects<br>
of various types of actions. (Game theory may sometimes help with the<br>
latter. I like David Gauthier's discussion in Morals By Agreement, but I<br>
find it to be only part of the answer.)<br>
<p>
Approaches to ethics that treat it in isolation from other areas of<br>
knowledge seem to me doomed to produce ethical systems that cannot<br>
adequately answer the ancient and excellent question "Why be moral?" I find<br>
the most promising approach to be that of virtue ethics with its integral<br>
concern for moral psychology. A fairly good collection on this is Flanagan<br>
and Rorty's Identity, Character, and Morality.<br>
<p>
To specifically address your proferred moral imperatives: "Take out life<br>
insurance" (if good advice for most people) could be taken as a moral<br>
imperative. However, I think of ethics primarily in terms of the virtues of<br>
character for a successful life and accompanying principles. "Take out life<br>
insurance" looks like a specific, highly context-dependent application of<br>
virtues and principles such as personal responsibility, rationality, and<br>
foresightness. "Con thy neighbor subtly" again seems highly<br>
context-dependent. I see ethics as involving a flexible hierarchy of<br>
virtues and principles. Some are pretty secure and very broadly applicable.<br>
Purported imperatives like "con your neighbor subtly" seems to be much<br>
further down the list of derivations. That is, they are many many<br>
circumstances that could make that a bad bit of advice.<br>
<p>
As another example: I would argue that the virtue of self-ownership (which<br>
includes personal responsibility, rationality, indepedent thinking, and<br>
self-direction) is a more basic value than truthfulness. While I believe<br>
truthfulness to generally be virtuous it depends far more on circumstances<br>
than does self-ownership. If I were living in Stalinist Russia, I might<br>
find it moral to lie in many situations.<br>
<p>
Again, all rational moral discussion depends on shared basic values.<br>
Perhaps we can go further, but I'm not sure at this stage whether I can<br>
rationally persuade someone fundamentally bent on self- and<br>
other-destruction and who explicitly rejects survival, happiness, and<br>
flourishing as goals. I'd like to think that I could, in principle, show<br>
their attitudes to be based on flawed factual beliefs and poor reasoning. I<br>
suspect this *is* possible -- and so rational ethics can become even more<br>
close to universal, but I will not claim this at present.<br>
<p>
Max<br>
<p>
<p>
Max More, Ph.D.<br>
more@extropy.org<br>
<a href="http://www.primenet.com/~maxmore">http://www.primenet.com/~maxmore</a><br>
President, Extropy Institute: exi-info@extropy.org, <a href="http://www.extropy.org">http://www.extropy.org</a><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0486.html">Michael Lorrey: "Re: Constraints on the "singularity""</a>
<li> <b>Previous message:</b> <a href="0484.html">Kennita Watson: "Re: Crop circles again?!! (was: Re: Bill Gates)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
