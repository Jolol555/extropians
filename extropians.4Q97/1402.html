<!-- received="Thu Nov 13 11:13:03 1997 MDT" -->
<!-- sent="Thu, 13 Nov 1997 09:45:29 -0800" -->
<!-- name="Hal Finney" -->
<!-- email="hal@rain.org" -->
<!-- subject="RE: The copy paradox" -->
<!-- id="199711131745.JAA09802@s20.term1.sb.rain.org" -->
<!-- inreplyto="" -->
<title>extropians: RE: The copy paradox</title>
<h1>RE: The copy paradox</h1>
Hal Finney (<i>hal@rain.org</i>)<br>
<i>Thu, 13 Nov 1997 09:45:29 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1402">[ date ]</a><a href="index.html#1402">[ thread ]</a><a href="subject.html#1402">[ subject ]</a><a href="author.html#1402">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1403.html">Hal Finney: "Re: The Copy Paradox"</a>
<li> <b>Previous message:</b> <a href="1401.html">Henri Kluytmans: "Re: QUOTE: Bey on extropians"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Brent Allsop, &lt;allsop@swttools.fc.hp.com&gt;, writes:<br>
<i>&gt; Leevi Marttila &lt;lm+extropians@sip.fi&gt; responded:</i><br>
<i>&gt; &gt; Actually look up table is too simplistic expression. How would you</i><br>
<i>&gt; &gt; describe trained artificial neural net semantically?</i><br>
<i>&gt;</i><br>
<i>&gt; 	The same way abstract computer simulations describe or model</i><br>
<i>&gt; them.  There is no more phenomenal information in any abstract</i><br>
<i>&gt; computer model than there is in what we can abstractly say to each</i><br>
<i>&gt; other.  As I said, you can abstractly model everything with most</i><br>
<i>&gt; anything including speech and internal computer models, but any</i><br>
<i>&gt; abstract model like speech, still, is only abstractly like it.  It</i><br>
<i>&gt; isn't fundamentally like a real feeling or phenomenal sensation.</i><br>
<p>
The question is, what constitutes an "abstract" model versus a real one?<br>
Is it a matter of whether the underlying substrate is silicon versus<br>
protein?  Or is it a matter of the internal form and structure of the<br>
model?<br>
<p>
I believe Leevi was taking the latter position, and arguing that a<br>
sufficiently complicated model, even if running on silicon, would no<br>
longer be abstract.  Things would acquire meaning not because of arbitrary<br>
assignments (this register holds saltiness, that register holds blueness)<br>
but due to the immense complexity of the interactions among the various<br>
representations.<br>
<p>
Saltiness could only be described by a complex relationship among<br>
millions or billions of simulated neurons with various activation<br>
levels, and likewise for other qualia.<br>
<p>
In such a system there is no possibility to change redness into blueness<br>
without affecting anything else.  Redness and blueness are so complex,<br>
so interrelated with other concepts, that it would be impossible to<br>
disentangle them from each other.<br>
<p>
<i>&gt; 	Also, neurons in our eyes and optical nerve learn how to</i><br>
<i>&gt; abstractly represent and forward modeling information to the visual</i><br>
<i>&gt; cortex.  But, we are not phenomenally conscious of this information</i><br>
<i>&gt; until it arrives at the visual cortex where the phenomenal conscious</i><br>
<i>&gt; representations are produced.  There is something more going on in the</i><br>
<i>&gt; neurons of the primary visual cortex than the abstract stuff that is</i><br>
<i>&gt; going on in the subconsciousness like the retina neurons.  I would bet</i><br>
<i>&gt; that any neural net of today is still, like the neurons of the retina,</i><br>
<i>&gt; not yet producing phenomenal sensations.  We haven't yet discovered</i><br>
<i>&gt; precisely what this phenomenal process is and how it is formed into a</i><br>
<i>&gt; unified and emotional awareness and why it is like what it is like.</i><br>
<p>
It is indeed an interesting question at what point in the neural<br>
processing our neurons begin to affect consciousness.  It's conceivable<br>
that there is no boundary, that it is a gradual transition.<br>
<p>
You think the retina is purely abstract, just pre-processing and<br>
conditioning the data before it is presented to the _real_ conscious<br>
neural network somewhere in the cortex.  But even there the cortext just<br>
does more processing.  The initial layers are known to look for certain<br>
features in the input, like lines and edges.  Certain neurons will fire<br>
when they "see" lines with specific orientations.  Other neurons look<br>
for simple patterns of movement, like vertically moving edges.<br>
<p>
We can easily imagine that this kind of feature analysis would be useful<br>
in beginning to understand the structure of an image.  But this is still<br>
very mechanical and, as you would say, abstract.  Presumably it is just<br>
a preliminary stage as well, to be passed to a still deeper level of<br>
processing, and perhaps that is where consciousness will begin.<br>
<p>
But the alternative is that we're peeling an onion, and when we get down<br>
to the center, there's nothing there.  All those layers we discarded as<br>
doing purely abstract calculations were collectively creating the very<br>
consciousness that we were looking for.  By this point of view, even the<br>
mechanical calculations of the retina can be said to be a part of our<br>
conscious experience of vision.  It plays a structural and causal role in<br>
the image processing which we call a sense of sight.<br>
<p>
Hal<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1403.html">Hal Finney: "Re: The Copy Paradox"</a>
<li> <b>Previous message:</b> <a href="1401.html">Henri Kluytmans: "Re: QUOTE: Bey on extropians"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
