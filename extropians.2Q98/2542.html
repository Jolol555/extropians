<!-- received="Tue Jun 23 05:16:01 1998 MDT" -->
<!-- sent="Tue, 23 Jun 1998 13:13:07 +0200" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)" -->
<!-- id="3.0.3.32.19980623041154.0309e594@208.198.32.3" -->
<!-- inreplyto="The AI revolution (Was: Re: &gt;H ART: The Truman Show)" -->
<title>extropians: Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)</title>
<h1>Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Tue, 23 Jun 1998 13:13:07 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2542">[ date ]</a><a href="index.html#2542">[ thread ]</a><a href="subject.html#2542">[ subject ]</a><a href="author.html#2542">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2543.html">Anders Sandberg: "Re: &gt;H ART: The Truman Show"</a>
<li> <b>Previous message:</b> <a href="2541.html">Anders Sandberg: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Maybe in reply to:</b> <a href="2521.html">den Otter: "The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2541.html">Anders Sandberg: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Reply:</b> <a href="2541.html">Anders Sandberg: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Daniel Fabulich wrote:<br>
<i>&gt; </i><br>
<i>&gt; On Tue, 23 Jun 1998, den Otter wrote:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; Emotions are important for us because they help us to survive. AIs</i><br>
<i>&gt; &gt; don't need to fend for themselves in a difficult environment; they</i><br>
<i>&gt; &gt; get all the energy, protection &amp; imput they need from humans and</i><br>
<i>&gt; &gt; other machines. All they have to do is solve puzzles (of biology,</i><br>
<i>&gt; &gt; programming etc). If you program it with an "urge" to solve puzzles</i><br>
<i>&gt; &gt; (just like your PC has an "urge" to execute your typed orders), it</i><br>
<i>&gt; &gt; will work just fine. No (other) "emotions" are needed, imo. It's</i><br>
<i>&gt; &gt; like with the birds and planes: both can fly, only their methods</i><br>
<i>&gt; &gt; are different, and both have their specialties.</i><br>
<i>&gt; </i><br>
<i>&gt; I think you're approaching this question from a very different direction</i><br>
<i>&gt; from many of those who subscribe to this list.  </i><br>
<p>
The majority isn't always right ;-)<br>
<p>
<i>&gt; Many of us suspect that if</i><br>
<i>&gt; and when we create an AI, it will not be any easier to "program" than</i><br>
<i>&gt; people are; that on some level the architecture behind AI will</i><br>
<i>&gt; sufficiently similiar to that within our own brains that we will not be</i><br>
<i>&gt; able to selectively remove anything fundamental from this mix, including</i><br>
<i>&gt; emotions, survival instincts, irrationalities and quirks.</i><br>
<p>
That's just one way to develop AI. There might very well be others. In<br>
any case, it wouldn't be very smart to just make an upgraded human<br>
(with an attitude). Genie AIs is what we want, mindless servants.<br>
 <br>
<i>&gt; Unlike flight, intelligence may well require something as complicated as a</i><br>
<i>&gt; bird or a person in order to work.  If so, there's no reason to assume</i><br>
<i>&gt; that we could strip emotions out of intelligence like we could take the</i><br>
<i>&gt; feathers out of flight.</i><br>
<p>
There is also no reason to assume that you *can't* strip (most)<br>
emotions out of intelligence, and still have something useful.<br>
At this point, we're all just guessing of course, but I strongly<br>
suspect that the idea that you can't have emotionless AI, <br>
intelligence without will, is mainly based on a mystification<br>
of the human mind. Our emotions aren't *that* special, they're<br>
just tools for a certain environment, which is mostly different<br>
from that of the AI's. The goals of AI are not survival and<br>
procreation, at least they never should be, but solving<br>
problems for humans. Fight/flight/love/hate/distrust/hunger<br>
for power/sadness etc. would only get in the way of performance, <br>
and make the AI very unreliable and potentially dangerous.<br>
<p>
When people (like scientists) deal with a difficult problem,<br>
they tend to focus fully on it, blocking out cluttering<br>
emotions. That's what you want the AI to be: a focussed<br>
thinker, not disturbed by the urge to mate or whatever.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2543.html">Anders Sandberg: "Re: &gt;H ART: The Truman Show"</a>
<li> <b>Previous message:</b> <a href="2541.html">Anders Sandberg: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Maybe in reply to:</b> <a href="2521.html">den Otter: "The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2541.html">Anders Sandberg: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Reply:</b> <a href="2541.html">Anders Sandberg: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<!-- reply="end" -->
</ul>
