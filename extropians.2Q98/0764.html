<!-- received="Fri Apr 24 10:12:52 1998 MDT" -->
<!-- sent="24 Apr 1998 18:12:46 +0200" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: Ethics of being a Creator" -->
<!-- id="3.0.1.32.19980424170818.00687278@popserver.stack.nl" -->
<!-- inreplyto="Fri, 24 Apr 1998 17:08:18 +0200" -->
<title>extropians: Re: Ethics of being a Creator</title>
<h1>Re: Ethics of being a Creator</h1>
Anders Sandberg (<i>asa@nada.kth.se</i>)<br>
<i>24 Apr 1998 18:12:46 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#764">[ date ]</a><a href="index.html#764">[ thread ]</a><a href="subject.html#764">[ subject ]</a><a href="author.html#764">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0765.html">den Otter: "Re: future pets"</a>
<li> <b>Previous message:</b> <a href="0763.html">Anders Sandberg: "Re: future pets"</a>
<li> <b>In reply to:</b> <a href="0762.html">Henri Kluytmans: "Re: Ethics of being a Creator"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Henri Kluytmans &lt;hkl@stack.nl&gt; writes:<br>
<p>
<i>&gt; &gt;First, some of the entities might suffer a gradual decline (like </i><br>
<i>&gt; &gt;humans with Alzheimer's disease), would it be nice to save backup </i><br>
<i>&gt; &gt;copies of their final, degraded state or should one save copies </i><br>
<i>&gt; &gt;from an earlier state? If so, why just one, why not several? </i><br>
<i>&gt; </i><br>
<i>&gt; When the creator has enough power to simulate a universe, he most </i><br>
<i>&gt; also have enough power to merge the backup copies of earlier states </i><br>
<i>&gt; with the later backups of the degraded states, and to construct </i><br>
<i>&gt; from those multiple states a "healthy" state, containing all memories </i><br>
<i>&gt; of the being. Of course one has to define what a "healthy" state is </i><br>
<i>&gt; first.</i><br>
<p>
Huh? This doesn't follow. I can set up a big game of life simulation<br>
(or something similar, perhaps Tierra) right now on this computer, but<br>
even if I discovered intelligent entities inside it after a while I<br>
wouldn't know what to do with them. Sure, that huge stretch of digital<br>
code is an intelligent entity, but I have no idea of how it works, how<br>
to merge its memories (which might be impossible even in principle for<br>
an Alzheimer-like case, where the memory substrate would be<br>
non-isomorphic over time) or what it would consider "healthy" ("Poor<br>
humans, guts riddled with bacteria. I'll resurrect them completely<br>
without any bacteria, then they will be healthy and happy!").<br>
<p>
Even if I happen to be a posthuman jupiter brain, I will still be<br>
limited in my knowledge about the behavior of complex systems such as<br>
simulated universes.<br>
<p>
<i>&gt; If sentient beings will develop (that are also able to selfreproduce) </i><br>
<i>&gt; it is very likely they will transform their environment considerably. </i><br>
<i>&gt; This process is exponential. (Look at how the human beings have </i><br>
<i>&gt; already transformed our environment (Earth). And how we could likely </i><br>
<i>&gt; transform are whole galaxy within a million years.)</i><br>
<i>&gt; This reasoning presumes that reproducing sentient beings will at one </i><br>
<i>&gt; time develop a technological civilization. I think however that this </i><br>
<i>&gt; is quite likely. </i><br>
<p>
Only in worlds like ours where there exists objects that can be used<br>
as tools. I would expect that an intelligent game of life pattern<br>
might live in a world where this is almost impossible - there is no<br>
way of moving anything without changing it. And in Tierra, the tools<br>
might be code integral to the entities or symbiotic creatures instead.<br>
<p>
<i>&gt; At a point in time the technological civilization will have transformed </i><br>
<i>&gt; its environment a considerable amount, it should then be quite feasible </i><br>
<i>&gt; to detect this pattern in the simulation. Now, the creator only has to </i><br>
<i>&gt; play back the simulation and extract al the sentient beings of the </i><br>
<i>&gt; earlier generations of this civilization. </i><br>
<p>
I think you are on the right track, looking for changes creating<br>
highly ordered states that spread quickly is a good way. But it is not<br>
trivial, since the order may be well hidden.<br>
<p>
<i>&gt; &gt;Third, the afterlife is rather underdetermined: how to</i><br>
<i>&gt; &gt;keep the entities from pain *there*?</i><br>
<i>&gt; </i><br>
<i>&gt; When the entities are restored in a separate simulation there </i><br>
<i>&gt; is now no reason anymore not to communicate with them. So the </i><br>
<i>&gt; creator could ask the sentient beings themselves if they are happy </i><br>
<i>&gt; or not. Or even give them the capabilities to alter themselves </i><br>
<i>&gt; in to a state they individually prefer.</i><br>
<p>
This assumes we can easily communicate with them. But how do we start<br>
exchanging communication with entities with a fundamentally extremely<br>
different world (like Tierrans, with a world consisting of computing<br>
nodes and nonlocal memory indexed by templates and energy appearing if<br>
you do certain things but not other things)? I think it can be done,<br>
but it would be extremely hard to do, and the process might be rather<br>
painful for the entities again (imagine being resurrected all alone in<br>
a weird caricature of the real world where *something* tries to<br>
communicate with you - and if you die, you are immediately<br>
resurrected).<br>
<p>
<p>
<p>
<pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
asa@nada.kth.se                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0765.html">den Otter: "Re: future pets"</a>
<li> <b>Previous message:</b> <a href="0763.html">Anders Sandberg: "Re: future pets"</a>
<li> <b>In reply to:</b> <a href="0762.html">Henri Kluytmans: "Re: Ethics of being a Creator"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
