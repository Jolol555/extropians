<!-- received="Fri Apr 24 09:35:10 1998 MDT" -->
<!-- sent="24 Apr 1998 17:35:02 +0200" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: future pets" -->
<!-- id="3.0.1.32.19980424170818.00687278@popserver.stack.nl" -->
<!-- inreplyto="Fri, 24 Apr 1998 07:55:50 -0600" -->
<title>extropians: Re: future pets</title>
<h1>Re: future pets</h1>
Anders Sandberg (<i>asa@nada.kth.se</i>)<br>
<i>24 Apr 1998 17:35:02 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#763">[ date ]</a><a href="index.html#763">[ thread ]</a><a href="subject.html#763">[ subject ]</a><a href="author.html#763">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0764.html">Anders Sandberg: "Re: Ethics of being a Creator"</a>
<li> <b>Previous message:</b> <a href="0762.html">Henri Kluytmans: "Re: Ethics of being a Creator"</a>
<li> <b>In reply to:</b> <a href="0756.html">Tony Belding: "RE: future pets"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
tlbelding@htcomp.net (Tony Belding) writes:<br>
<p>
<i>&gt; I had an idea at one time, based on what I said before about an evolutionary</i><br>
<i>&gt; legacy.  If a creature has evolved to have instincts and emotions for its own</i><br>
<i>&gt; survival and reproduction, and if it becomes sufficiently sophisticated to</i><br>
<i>&gt; demand rights, then it should have rights.</i><br>
<p>
Sounds like a good model to me, although one likely has to interpret<br>
"evolve" in a fairly wide sense.<br>
<p>
<i>&gt; Also, if you created an artificial intelligence based on a neural net,</i><br>
<i>&gt; which is "programmed" by a regimen of stimulus and feedback instead of</i><br>
<i>&gt; cut-and-dried rules, then it *might* be sentient.  This is one reason</i><br>
<i>&gt; why I find neural-net research vaguely distasteful. </i><br>
<p>
Interesting. I must admit I have also thought about this. I daily<br>
create, train, use and erase neural networks in my research. Do they<br>
have some kind of experience? Do they have a right to existence? As<br>
for the nets I'm currently using (small fully connected<br>
autoassociative networks with an incremental Bayesian learning rule),<br>
I realize that their complexity is less than the chemical networks of<br>
many bacteria, so I don't feel too bad about them. But this may become<br>
a real problem in the future - the other graduate students here (me<br>
included) are quite interested in creating a sentinent system if<br>
possible, and once we start to get close to that, then we are going to<br>
need to think much more about ethics.<br>
<p>
<i>&gt; It could lead to</i><br>
<i>&gt; some form of sentience, and I feel this is not a high-priority goal.</i><br>
<i>&gt; We already know how to create sentient beings, we've got almost six</i><br>
<i>&gt; billion of them.  Do we really need more? </i><br>
<p>
No, but we might need *different* kinds of entities. I think it would<br>
be healthy if we humans weren't the only kind of intelligent entity in<br>
society, the existence of other kinds of thinking and experiencing<br>
might have profound and healthy effects on ourselves.<br>
<p>
<i>&gt; Far more useful to create</i><br>
<i>&gt; intelligent but non-sentient servants, IMHO.</i><br>
<p>
Sure, for practical work. But sentinent beings are ends in themselves<br>
in some sense.<br>
<p>
<i>&gt; That was my idea.  However, I've continued thinking about it, and I see that</i><br>
<i>&gt; there could still be some problems.  Depending on how radical things get, we</i><br>
<i>&gt; could end up with entities splitting and recombining right and left in various</i><br>
<i>&gt; ways.  Under such circumstances, it could be hard to keep track of such an</i><br>
<i>&gt; arbitrary definition of sentience, and the system could be subject to abuse. </i><br>
<p>
This might not be a very strong objection, the basic idea might be<br>
sound even if it is hard to implement in a society we could<br>
imagine. It might be a good starting point for further examination.<br>
<p>
<i>&gt; There's also the political obstacle -- I'm talking about a *legal* definition</i><br>
<i>&gt; of sentience.  A legal definition isn't useful unless you can explain it to</i><br>
<i>&gt; enough people and convince them to accept it.  I'm afraid my ideas don't</i><br>
<i>&gt; exactly have mass appeal in their current form.  :-)</i><br>
<p>
Just you wait until the first AI is interviewed on CNN and starts to<br>
quote Martin Luther King... :-)<br>
<p>
<pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
asa@nada.kth.se                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="0764.html">Anders Sandberg: "Re: Ethics of being a Creator"</a>
<li> <b>Previous message:</b> <a href="0762.html">Henri Kluytmans: "Re: Ethics of being a Creator"</a>
<li> <b>In reply to:</b> <a href="0756.html">Tony Belding: "RE: future pets"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
