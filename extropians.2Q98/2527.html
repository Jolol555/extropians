<!-- received="Mon Jun 22 19:45:00 1998 MDT" -->
<!-- sent="Mon, 22 Jun 1998 21:44:53 -0400 (EDT)" -->
<!-- name="Daniel Fabulich" -->
<!-- email="daniel.fabulich@yale.edu" -->
<!-- subject="Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)" -->
<!-- id="Pine.SUN.3.91.980622181111.4913C-100000@tangelo.phys.unm.edu" -->
<!-- inreplyto="358EE746.BB0@geocities.com" -->
<title>extropians: Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)</title>
<h1>Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)</h1>
Daniel Fabulich (<i>daniel.fabulich@yale.edu</i>)<br>
<i>Mon, 22 Jun 1998 21:44:53 -0400 (EDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2527">[ date ]</a><a href="index.html#2527">[ thread ]</a><a href="subject.html#2527">[ subject ]</a><a href="author.html#2527">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2528.html">Kathryn Aegis: "Re:  AI ethics (was The Truman Show)"</a>
<li> <b>Previous message:</b> <a href="2526.html">Michelle Jones: "Re: extropian memes"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Tue, 23 Jun 1998, den Otter wrote:<br>
<p>
<i>&gt; Emotions are important for us because they help us to survive. AIs</i><br>
<i>&gt; don't need to fend for themselves in a difficult environment; they</i><br>
<i>&gt; get all the energy, protection &amp; imput they need from humans and</i><br>
<i>&gt; other machines. All they have to do is solve puzzles (of biology,</i><br>
<i>&gt; programming etc). If you program it with an "urge" to solve puzzles</i><br>
<i>&gt; (just like your PC has an "urge" to execute your typed orders), it</i><br>
<i>&gt; will work just fine. No (other) "emotions" are needed, imo. It's</i><br>
<i>&gt; like with the birds and planes: both can fly, only their methods</i><br>
<i>&gt; are different, and both have their specialties.</i><br>
<p>
I think you're approaching this question from a very different direction<br>
from many of those who subscribe to this list.  Many of us suspect that if<br>
and when we create an AI, it will not be any easier to "program" than<br>
people are; that on some level the architecture behind AI will<br>
sufficiently similiar to that within our own brains that we will not be<br>
able to selectively remove anything fundamental from this mix, including<br>
emotions, survival instincts, irrationalities and quirks.<br>
<p>
Unlike flight, intelligence may well require something as complicated as a<br>
bird or a person in order to work.  If so, there's no reason to assume<br>
that we could strip emotions out of intelligence like we could take the<br>
feathers out of flight.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2528.html">Kathryn Aegis: "Re:  AI ethics (was The Truman Show)"</a>
<li> <b>Previous message:</b> <a href="2526.html">Michelle Jones: "Re: extropian memes"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
