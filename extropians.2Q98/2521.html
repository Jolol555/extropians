<!-- received="Mon Jun 22 17:26:07 1998 MDT" -->
<!-- sent="Tue, 23 Jun 1998 01:22:46 +0200" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="The AI revolution (Was: Re: &gt;H ART: The Truman Show)" -->
<!-- id="Pine.SUN.3.91.980622152943.2711C-100000@tangelo.phys.unm.edu" -->
<!-- inreplyto="Re: &gt;H ART: The Truman Show)" -->
<title>extropians: The AI revolution (Was: Re: &gt;H ART: The Truman Show)</title>
<h1>The AI revolution (Was: Re: &gt;H ART: The Truman Show)</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Tue, 23 Jun 1998 01:22:46 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2521">[ date ]</a><a href="index.html#2521">[ thread ]</a><a href="subject.html#2521">[ subject ]</a><a href="author.html#2521">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2522.html">Michael Nielsen: "Re: AI ethics [was: The Truman Show]"</a>
<li> <b>Previous message:</b> <a href="2520.html">Michael Nielsen: "Re: &gt;H ART: The Truman Show"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2542.html">den Otter: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Maybe reply:</b> <a href="2542.html">den Otter: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Reply:</b> <a href="2544.html">Anders Sandberg: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Maybe reply:</b> <a href="2609.html">Max M: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Maybe reply:</b> <a href="2611.html">den Otter: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Maybe reply:</b> <a href="2685.html">Max M: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Anders Sandberg wrote:<br>
<i>&gt; </i><br>
<i>&gt; den Otter &lt;neosapient@geocities.com&gt; writes:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; Since AIs will presumably be made without emotions, or at least with</i><br>
<i>&gt; &gt; a much more limited number of emotions than humans, you don't have</i><br>
<i>&gt; &gt; to worry about their "feelings".</i><br>
<i>&gt; </i><br>
<i>&gt; I think this is a fallacy. </i><br>
<p>
I beg to differ.<br>
<p>
<i>&gt;&gt;First, why would AI have no emotions or a</i><br>
limited repertoar? Given current research into cognitive neuroscience<br>
it seems that emotions are instead enormously important for rational<br>
thinking since they provide the valuations and heuristics necessary<br>
for making decisions well.&lt;&lt; <br>
<p>
Emotions are important for us because they help us to survive. AIs<br>
don't need to fend for themselves in a difficult environment; they<br>
get all the energy, protection &amp; imput they need from humans and<br>
other machines. All they have to do is solve puzzles (of biology,<br>
programming etc). If you program it with an "urge" to solve puzzles<br>
(just like your PC has an "urge" to execute your typed orders), it<br>
will work just fine. No (other) "emotions" are needed, imo. It's<br>
like with the birds and planes: both can fly, only their methods<br>
are different, and both have their specialties.<br>
<p>
<i>&gt;&gt;Secondly, even if they have less feelings</i><br>
than humans, why does that mean we can treat them as we like?&lt;&lt; <br>
<p>
If it doesn't care about abuse, if it feels no pain or emotional<br>
trauma, then there is no reason to worry about it's well-being<br>
(for it's own sake). AIs can almost certainly be made this way, <br>
with no more emotions than a PC. <br>
<p>
<i>&gt; &gt; Also, one of the first things you</i><br>
<i>&gt; &gt; would ask an AI is to develop uploading &amp; computer-neuron interfaces,</i><br>
<i>&gt; &gt; so that you can make the AI's intelligence part of your own.</i><br>
<i>&gt; </i><br>
<i>&gt; This is the old "Superintelligence will solve every problem"</i><br>
<i>&gt; fallacy. </i><br>
<p>
It just has to solve a couple of problems. After that, we'll have all<br>
the time in the world to work on the rest (as uploaded, supersmart/fast<br>
immortals). The increased speed alone will undoubtedly result in some<br>
impressive breaktroughs.<br>
<p>
<i>&gt; If I manage to create a human-or-above-level AI living inside</i><br>
<i>&gt; a virtual world of candy, it will not necessarily be able to solve</i><br>
<i>&gt; real world problems (it only knows about candy engineering), and given</i><br>
<i>&gt; access to the physical world and a good education its basic cognitive</i><br>
<i>&gt; structure (which was good for a candy world) might still make it very</i><br>
<i>&gt; bad at developing uploadning.</i><br>
<p>
If the AI has no emotions (no survival drive), there is no reason to <br>
shield it from the world. <br>
 <br>
<i>&gt; &gt; This would</i><br>
<i>&gt; &gt; pretty much solve the whole "rights problem" (which is largely</i><br>
<i>&gt; &gt; artificial anyway), since you don't grant rights to specific parts</i><br>
<i>&gt; &gt; of your brain.</i><br>
<i>&gt; </i><br>
<i>&gt; Let me see. Overheard on the SubSpace network:</i><br>
<i>&gt; </i><br>
<i>&gt; Borg Hive 19117632: "What about the ethics of creating those</i><br>
<i>&gt; 'individuals' you created on Earth a few megayears ago?"</i><br>
<i>&gt; </i><br>
<i>&gt; Borg Hive 54874378: "No problem. I will assimilate them all in a</i><br>
<i>&gt; moment. Then there will be no ethical problem since they will be part</i><br>
<i>&gt; of me."</i><br>
<i>&gt; </i><br>
<i>&gt; I think you are again getting into the 'might is right' position you</i><br>
<i>&gt; had on the posthuman ethics thread on the transhumanist list. Am I</i><br>
<i>&gt; completely wrong?</i><br>
<p>
Eternal truth: unless 19117632 or some other being can and wants to<br>
stop 54874378, it will do what it bloody well pleases. Might is<br>
the *source* of right. Right = privilege, and always exists within<br>
a context of power. A SI may have many "autonomous" AIs in it's<br>
"head". For example, it could decide to simulate life on earth,<br>
with "real" players to make it more interesting. Are these simulated<br>
beings, mere thoughts to the SI, entitled to rights and protection?<br>
If so, who or what could force the SI to grant it's thoughts rights<br>
(a rather rude invasion of privacy). How do you enforce such rules?<br>
Clearly a difficult matter, but it always comes down to "firepower"<br>
in the end (can you blackmail the other into doing something he<br>
doesn't like?-- that's the question).<br>
 <br>
<i>&gt; &gt; A failure to integrate with the AIs asap would</i><br>
<i>&gt; &gt; undoubtedly result in AI domination, and human extinction.</i><br>
<i>&gt; </i><br>
<i>&gt; Again a highly doubtful assertion. As I argued in my essay about</i><br>
<i>&gt; posthuman ethics, even without integration (which I really think is a</i><br>
<i>&gt; great idea, it is just that I want to integrate with AI developed</i><br>
<i>&gt; specifically for that purpose and not just to get rid of unnecessary</i><br>
<i>&gt; ethical subjects) human extinction is not a rational consequence of</i><br>
<i>&gt; superintelligent AI under a very general set of assumptions.</i><br>
<p>
It is rational because in the development of AI, just like in any<br>
economic or arms race, people will sacrifice safety to get that<br>
extra edge. If it turns out that a highly "emotionally" unstable<br>
AI with delusions of grandeur is more productive, then there<br>
will always be some folks that will make it. Terrorists or<br>
dictators could make "evil" AIs on purpose, there might be<br>
a freak error that sends one or more AIs out of control, or<br>
they might simply decide that humans are just dangerous pests,<br>
and kill them. There is *plenty* of stuff that could go wrong,<br>
and in a time of nano-built basement nukes and the like,<br>
small mistakes can have big consequences. Intelligence is<br>
a powerful weapon indeed.<br>
<p>
<i>&gt; Somehow I think it is our mammalian territoriality and xenophobia</i><br>
<i>&gt; speaking rather than a careful analysis of consequences that makes</i><br>
<i>&gt; people so fond of setting up AI as invincible conquerors.</i><br>
<p>
That territoriality and xenophobia helped to keep us alive, and<br>
still do. The world is a dangerous place, and it won't become<br>
all of sudden peachy &amp; rosy when AI arrives. Remember: this isn't<br>
comparable to the invention of gun powder, the industrial revolution,<br>
nukes or the internet. For the first time in history, there will<br>
be beings that are (a lot) smarter and faster than us. If this<br>
goes unchecked, mankind will be completely at the mercy of <br>
machines with unknown (unkowable?) motives. Gods really.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2522.html">Michael Nielsen: "Re: AI ethics [was: The Truman Show]"</a>
<li> <b>Previous message:</b> <a href="2520.html">Michael Nielsen: "Re: &gt;H ART: The Truman Show"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2542.html">den Otter: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Maybe reply:</b> <a href="2542.html">den Otter: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Reply:</b> <a href="2544.html">Anders Sandberg: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Maybe reply:</b> <a href="2609.html">Max M: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Maybe reply:</b> <a href="2611.html">den Otter: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Maybe reply:</b> <a href="2685.html">Max M: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<!-- reply="end" -->
</ul>
