<!-- received="Mon Jun 22 11:45:13 1998 MDT" -->
<!-- sent="Mon, 22 Jun 1998 13:43:57 -0400" -->
<!-- name="Alexander 'Sasha' Chislenko" -->
<!-- email="sasha1@netcom.com" -->
<!-- subject="AI ethics [was:  The Truman Show]" -->
<!-- id="199806221744.KAA29606@mail.netcom.com" -->
<!-- inreplyto="Pine.SUN.3.91.980616171206.19600A-100000@tangelo.phys.unm." -->
<title>extropians: AI ethics [was:  The Truman Show]</title>
<h1>AI ethics [was:  The Truman Show]</h1>
Alexander 'Sasha' Chislenko (<i>sasha1@netcom.com</i>)<br>
<i>Mon, 22 Jun 1998 13:43:57 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2503">[ date ]</a><a href="index.html#2503">[ thread ]</a><a href="subject.html#2503">[ subject ]</a><a href="author.html#2503">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2504.html">Ian Goddard: "Re: All-Zero-Sum Counter... Not!"</a>
<li> <b>Previous message:</b> <a href="2502.html">Verdop: "Re: Creationists"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2522.html">Michael Nielsen: "Re: AI ethics [was: The Truman Show]"</a>
<li> <b>Reply:</b> <a href="2522.html">Michael Nielsen: "Re: AI ethics [was: The Truman Show]"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
At 10:31 06/21/98 , Michael Nielsen wrote:<br>
<i>&gt;</i><br>
<i>&gt;Is it ethical to contain an AI in a limited world? This is an especially </i><br>
<i>&gt;interesting question if one takes the point of view that the most likely </i><br>
<i>&gt;path to Artificial Intelligence is an approach based on evolutionary </i><br>
<i>&gt;programming.</i><br>
<i>&gt;</i><br>
<i>&gt;Is it ethical to broadcast details of an AI's "life" to other </i><br>
<i>&gt;researchers or interested parties?</i><br>
<i>&gt;</i><br>
<i>&gt;Is it ethical to profit from the actions of an AI?</i><br>
<p>
I would suggest two interpretations to the above questions.<br>
<p>
Human ethics evolved to regulate people's relations with other persons<br>
and their possessions in the physical world.  Some of the ethical<br>
principles must be generic and good for regulation relations between<br>
different entities (the degree of identity granulation in an AI world will<br>
be much different from human, so I don't really expect "personalities"<br>
there).  Other principles may be just carried to the new world, because<br>
we "feel like it".  I think this is a wrong approach.   Should we cover<br>
software agents with a decency crypto-veil in the moments of replication,<br>
or tax successful computer programs to provide a decent level of<br>
operation to their underprivileged inefficient brethren?  This can get<br>
quite ridiculous.<br>
We should better think of how to efficiently set up new systems, and<br>
what kinds of protocols may provide us - and AIs - with greatest mutual<br>
benefits.<br>
<p>
If there are any ethical controversies with AIs, they will probably happen<br>
only in the short period of time (10 to 20 years?) when AIs have near-human<br>
intelligence.<br>
After that, AIs will take care of their benefit, and of ours as well.  They would<br>
not be concerned about our watching them either, as we are not concerned<br>
about our pets watching us.<br>
<p>
<p>
-------------------------------------------------------------------<br>
Alexander Chislenko  &lt;<a href="http://www.lucifer.com/~sasha/home.html">http://www.lucifer.com/~sasha/home.html</a>&gt;<br>
Extropy Online           &lt;<a href="http://www.extropy.org/eo/index.htm">http://www.extropy.org/eo/index.htm</a>&gt;<br>
-------------------------------------------------------------------<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2504.html">Ian Goddard: "Re: All-Zero-Sum Counter... Not!"</a>
<li> <b>Previous message:</b> <a href="2502.html">Verdop: "Re: Creationists"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2522.html">Michael Nielsen: "Re: AI ethics [was: The Truman Show]"</a>
<li> <b>Reply:</b> <a href="2522.html">Michael Nielsen: "Re: AI ethics [was: The Truman Show]"</a>
<!-- reply="end" -->
</ul>
