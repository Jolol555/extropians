<!-- received="Mon Jun 22 02:46:56 1998 MDT" -->
<!-- sent="22 Jun 1998 10:46:46 +0200" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: &gt;H ART: The Truman Show" -->
<!-- id="199806220354.UAA12487@well.com" -->
<!-- inreplyto="Sun, 21 Jun 1998 08:31:47 -0600 (MDT)" -->
<title>extropians: Re: &gt;H ART: The Truman Show</title>
<h1>Re: &gt;H ART: The Truman Show</h1>
Anders Sandberg (<i>asa@nada.kth.se</i>)<br>
<i>22 Jun 1998 10:46:46 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2495">[ date ]</a><a href="index.html#2495">[ thread ]</a><a href="subject.html#2495">[ subject ]</a><a href="author.html#2495">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2496.html">Michael Lorrey: "Re: The Best Possible World"</a>
<li> <b>Previous message:</b> <a href="2494.html">CALYK@aol.com: "Re: The Best Possible World"</a>
<li> <b>In reply to:</b> <a href="2480.html">Michael Nielsen: "Re: &gt;H ART: The Truman Show"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Michael Nielsen &lt;mnielsen@tangelo.phys.unm.edu&gt; writes:<br>
<p>
<i>&gt; Is it ethical to contain an AI in a limited world? This is an especially </i><br>
<i>&gt; interesting question if one takes the point of view that the most likely </i><br>
<i>&gt; path to Artificial Intelligence is an approach based on evolutionary </i><br>
<i>&gt; programming.</i><br>
<i>&gt; </i><br>
<i>&gt; Is it ethical to broadcast details of an AI's "life" to other </i><br>
<i>&gt; researchers or interested parties?</i><br>
<p>
These are interesting questions. Overall the field of creator-created<br>
ethics is little explored so far (for obvious reasons). <br>
<p>
I must admit I have no good consistent idea about how to answer these<br>
questions. We might need to create AIs in limited worlds, and they<br>
might be extremely useful in there. "Opening the box" and allowing<br>
them out might be troubling for them, but at the same time that would<br>
put them on an equal footing (at least equal ontological footing) with<br>
us, suggesting that at least then they should definitely get<br>
rights. The problem here seems to be that although the AIs are<br>
rational subjects the rules for rights and ethics we have developed<br>
doesn't seem to work well across ontological levels.<br>
<p>
<i>&gt; Is it ethical to profit from the actions of an AI?</i><br>
<p>
Is it ethical to profit from the actions of a human? I would say so,<br>
if the human gets part of the earnings (ideally making a contract with<br>
me). Things get tricky when the AI/human doesn't know I profit, but<br>
I'm sure the legal system already has some rules saying it is not<br>
proper behavior. Laws about parent/child interaction might also be<br>
applicable here.<br>
<p>
<pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
asa@nada.kth.se                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2496.html">Michael Lorrey: "Re: The Best Possible World"</a>
<li> <b>Previous message:</b> <a href="2494.html">CALYK@aol.com: "Re: The Best Possible World"</a>
<li> <b>In reply to:</b> <a href="2480.html">Michael Nielsen: "Re: &gt;H ART: The Truman Show"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
