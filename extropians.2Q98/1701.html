<!-- received="Sun May 17 10:14:00 1998 MDT" -->
<!-- sent="Sun, 17 May 1998 12:10:26 -0400" -->
<!-- name="Ian Goddard" -->
<!-- email="igoddard@netkonnect.net" -->
<!-- subject="Re: ETHICS: Utility First--&gt; Fraud" -->
<!-- id="3.0.3.32.19980517121026.03213580@208.198.32.3" -->
<!-- inreplyto="Pine.GSO.3.94.980516161346.18018C-100000@minerva.cis.yale." -->
<title>extropians: Re: ETHICS: Utility First--&gt; Fraud</title>
<h1>Re: ETHICS: Utility First--&gt; Fraud</h1>
Ian Goddard (<i>igoddard@netkonnect.net</i>)<br>
<i>Sun, 17 May 1998 12:10:26 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1701">[ date ]</a><a href="index.html#1701">[ thread ]</a><a href="subject.html#1701">[ subject ]</a><a href="author.html#1701">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1702.html">Ian Goddard: "Re: ExI = Truth First ?"</a>
<li> <b>Previous message:</b> <a href="1700.html">Dan Clemmensen: "Re: Near-Term Scenarios"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
At 04:37 PM 5/16/98 -0400, Daniel Fabulich wrote:<br>
<p>
<i>&gt;On Sat, 16 May 1998, Ian Goddard wrote:</i><br>
<i>&gt;</i><br>
<i>&gt;&gt; </i><br>
<i>&gt;&gt; If my principle is utility first and I have</i><br>
<i>&gt;&gt; a hypothesis about what nature is, then I do</i><br>
<i>&gt;&gt; research, gathering data about nature, add </i><br>
<i>&gt;&gt; it all up, and lo and behold the numbers </i><br>
<i>&gt;&gt; suggest my hypothesis is not true, I </i><br>
<i>&gt;&gt; have one of two options:</i><br>
<i>&gt;&gt; </i><br>
<i>&gt;&gt; 1) conclude that my hypothesis is not true.</i><br>
<i>&gt;&gt; </i><br>
<i>&gt;&gt; 2) alter the data to make my theory appear true.</i><br>
<i>&gt;&gt; </i><br>
<i>&gt;&gt; Option 1 adheres to a "truth first me second" </i><br>
<i>&gt;&gt; principle, option two adheres to a "me first</i><br>
<i>&gt;&gt; truth maybe later," principle. A "me first" </i><br>
<i>&gt;&gt; principle is obviously useful to me, so a </i><br>
<i>&gt;&gt; "utility first, truth maybe" standard</i><br>
<i>&gt;&gt; will automatically promote fraud</i><br>
<i>&gt;&gt; and abolish a code of ethical</i><br>
<i>&gt;&gt; conduct that is the only</i><br>
<i>&gt;&gt; hope for science &amp; humanity...</i><br>
<i>&gt;&gt; </i><br>
<i>&gt;</i><br>
<i>&gt;First, if choosing 2 would hurt humanity, then utilitarianism demands that</i><br>
<i>&gt;we choose 1, not 2.  You're arguing against EGOISM, the philosophy that</i><br>
<i>&gt;each agent should make choices in such a way as to maximize his-her OWN</i><br>
<i>&gt;utility, as opposed to utilitarianism which argues for greatest utility</i><br>
<i>&gt;for greatest number.  </i><br>
<p>
<p>
   IAN: The issue I've raised does not exclude <br>
   any type of utility, such as egoic. The issue <br>
   raised addresses the idea that "useless truths"<br>
   (where "useless" is defined as not causing mon-<br>
   itary profits) should be rejected. The truth <br>
   that the planet Pluto exists may not be causing<br>
   monetary profits, does that mean all data about<br>
   Pluto should be erased from science books? I <br>
   say no. I say that truth is implicitly valu-<br>
   able to inquiry into the nature of things.<br>
<p>
<p>
<i>&gt;Goddard, utilitarianism is a consequentialist philosophy, which means that</i><br>
<i>&gt;it evaluates choices and actions based on their consequences, just as you</i><br>
<i>&gt;do and have.  It states that an action is right if and only if it results</i><br>
<i>&gt;in the best possible consequences.  It evaluates consequences based on the</i><br>
<i>&gt;utility of all people, where utility is defind as their happiness,</i><br>
<i>&gt;satisfaction, etc.  One consequence is better than another if it results</i><br>
<i>&gt;in greater happiness for a greater number.  Thus, the right action results</i><br>
<i>&gt;in the greatest utility for the greatest number.</i><br>
<i>&gt;</i><br>
<i>&gt;THIS is the philosophy you have so maligned, and so completely</i><br>
<i>&gt;misunderstood.  </i><br>
<p>
<p>
   IAN: No. Your defining "utility" "the greater good,"<br>
   I have not misunderstood that the greater good was<br>
   anything but the greater good. I've simply addressed<br>
   the issue in which "utility" could be defined by some<br>
   other standard, such as egoic. In fact, there can be<br>
   no other standard than egoic, because even if you do <br>
   things for others, you do them because you want to;<br>
   your measure of the "greater good" is YOUR measure,<br>
   for some the "greater good" is mass sterilization.<br>
<p>
   I really don't care to get into a discussion of uti-<br>
   litarianism, I just think that truths should not be <br>
   eliminated because they may not cause profits now.<br>
<p>
<p>
<p>
********************************************************<br>
Visit Ian W Goddard ---&gt;  <a href="http://www.erols.com/igoddard">http://www.erols.com/igoddard</a><br>
________________________________________________________<br>
Statements      T r u t h           A defines -A<br>
                a                   -A defines A<br>
 A: x is A      b   A -A<br>
                l   T  F          A set is defined<br>
-A: x is -A     e   F  T        by its members, thus<br>
                    ?  ?     A &amp; -A contain each other.<br>
--------------------------------------------------------<br>
H O L I S M ---&gt;  <a href="http://www.erols.com/igoddard/meta.htm">http://www.erols.com/igoddard/meta.htm</a><br>
________________________________________________________<br>
<p>
<p>
 <br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1702.html">Ian Goddard: "Re: ExI = Truth First ?"</a>
<li> <b>Previous message:</b> <a href="1700.html">Dan Clemmensen: "Re: Near-Term Scenarios"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
