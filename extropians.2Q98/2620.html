<!-- received="Wed Jun 24 19:01:21 1998 MDT" -->
<!-- sent="Wed, 24 Jun 1998 21:01:13 -0400 (EDT)" -->
<!-- name="Daniel Fabulich" -->
<!-- email="daniel.fabulich@yale.edu" -->
<!-- subject="Re: Ethics" -->
<!-- id="s5923757.005@MAIL.TAIT.CO.NZ" -->
<!-- inreplyto="007c01bd9fad$2563d4a0$446545c2@bungle" -->
<title>extropians: Re: Ethics</title>
<h1>Re: Ethics</h1>
Daniel Fabulich (<i>daniel.fabulich@yale.edu</i>)<br>
<i>Wed, 24 Jun 1998 21:01:13 -0400 (EDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2620">[ date ]</a><a href="index.html#2620">[ thread ]</a><a href="subject.html#2620">[ subject ]</a><a href="author.html#2620">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2621.html">Technotranscendence: "FWD: The Happy Robot"</a>
<li> <b>Previous message:</b> <a href="2619.html">Michael Lorrey: "Re: Ethics"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2624.html">Ian Goddard: "Net Difference = 0 ?"</a>
<li> <b>Reply:</b> <a href="2624.html">Ian Goddard: "Net Difference = 0 ?"</a>
<li> <b>Reply:</b> <a href="2630.html">Daniel Fabulich: "Re: Special Relativity"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Wed, 24 Jun 1998, Bryan Moss wrote:<br>
<p>
<i>&gt; I know how much you all hate discussing the</i><br>
<i>&gt; basics, but what exactly makes something unethical</i><br>
<i>&gt; and why should I care?</i><br>
<p>
I'll answer the second question first because it's much easier.  The<br>
"right" action, in the ethical sense of the term, is defined as the thing<br>
which you should do.  Therefore, it isn't really sensible to ask "why<br>
should I care what I should do?"  If you've already agreed that a<br>
particular action is the one which you should do, then there's no further<br>
question to be asked; you should do what you should do.<br>
<p>
The question of ethics is properly stated: "What *should* I (we?) do?"  If<br>
we answer that question, if we can find out what you should do, then we<br>
can answer your first question by saying that an action is unethical if<br>
you shouldn't do it; ie it is any action which is not what you should do.<br>
Of course, we can *only* say this meaningfully if we have solved the<br>
problem of ethics;  otherwise, you could just as easily ask "What<br>
shouldn't I do?" and get no more useful a response.<br>
<p>
Within this context, it is usually posited by a variety of thinkers that<br>
the right answer to the question is necessarily the rational answer to the<br>
question.  This is somewhat contested; the answer may well be unavailable<br>
to reason, instead available only to instinct, or derived from an<br>
"objective list" which is not itself derived from axioms.  There are also<br>
some who claim that we cannot answer the question at all; that ultimately<br>
no actions are better than others.  I happen to disagree with these<br>
thinkers, but one should keep the non-rationalists in mind, since there<br>
are so many of them out there. <br>
<p>
Those aside, assuming that rationality is correct usually leads thinkers<br>
to define what rationality is, show how the rules of rationality dictate<br>
certain actions, declare them to be right, and claim to have solved the<br>
problem of ethics.  Kant is probably the most famous and well-respected of<br>
such philosophers, though he has competition; this strategy goes all the<br>
way back to Aristotle, who basically defined logic in the first place.<br>
<p>
That being said, there is no specific extropian answer to the problem of<br>
ethics, though we could easily suggest that one extreme answer might be:<br>
"The right action is that which maxmizes extropy over all other possible<br>
actions."  However, I think many extropians would not agree with this<br>
answer to the question; while extropy is good, it may not be the only<br>
important thing in the universe.  Following this rule would demand that we<br>
kill, steal, torture, and even commit suicide if doing so would even<br>
marginally increase universal extropy compared to that which would be lost<br>
by doing so. <br>
<p>
The answers I find the most compelling are probably these:  "The right<br>
action is that which is most likely to maximize the total well-being and<br>
total number of all people."  "The right action is that which would<br>
promote our own well-being if everybody was doing the right thing."  "The<br>
right action is that which is most likely to maximize the agent's<br>
well-being (without preventing others from doing the right thing)."  The<br>
first answer is basically utilitarianism, the second answer is a gross<br>
simplification of Kant's categorical imperitive, and the third answer is<br>
egoism (the modification in parentheses is close to Rand's objectivism<br>
or some forms of libertarianism).<br>
<p>
Speaking personally, I believe that the first answer is correct, though<br>
the modified third answer is running a close second.  I tend to reject the<br>
second answer, on the grounds that in many cases doing the "right" action<br>
as dictated by the second answer makes the agent and others worse off. <br>
Without carefully defining practical rationality, it seems to me that<br>
doing an action which makes oneself and all those around one worse off is<br>
not very rational at all.<br>
<p>
I tend to reject the third answer because simple egoism tends to promote<br>
actions which seriously hurt others, if the agent can pull them off<br>
without getting more punishment than pleasure from doing the action.  This<br>
is not a logical inconsistency, except to the extent that it would<br>
probably be worse for us if we were all pure egoists; while I do not<br>
agree with the categorical imperitive as written, I do think that any<br>
rational answer to the question ought to be generalizible in a way that<br>
simple egoism isn't.<br>
<p>
Meanwhile, objectivism seems to rely on a kind of irrationality: while it<br>
is objectively true that "preventing others from acting morally" is wrong<br>
according to utilitarianism and the categorical imperitive, it does not<br>
follow from simple egoism; indeed, egoism seems to demand that we prevent<br>
others from acting morally if we would benefit from doing so.  That being<br>
said, however, I find that utilitarianism and objectivism/libertarianism<br>
usually agree on what the right action is.  Both are generalizible and<br>
both seem to obey some definition of practical rationality. <br>
<p>
Ultimately I believe utilitarianism is correct because even the modified<br>
version of egoism may prevent us from doing something obviously rational<br>
under a variety of situations.  A common example is one in which you might<br>
steal from someone in order to save lives, possibly your own.  An<br>
objectivist would quickly point out that stealing is wrong, and therefore<br>
one should never do so.  However, under situations of sufficient urgency,<br>
no other choice may be available (besides death), and within that context<br>
we again seem to be forced into the position of accepting bad consequences<br>
when we do the right thing; a situation which I believe to be paradoxical.<br>
Utilitarianism, on the other hand, allows us to get the consequences we<br>
strive for without sacrificing practical rationality.<br>
<p>
Utilitarianism is not without its problems, however, especially when<br>
maximizing total happiness would fail to maximize total numbers, or vice<br>
versa.  It also seems to have problems of justice; for example, if somehow<br>
by torturing one person you could make a hundred others positively elated,<br>
utilitarianism may require that you do this.  I doubt that situations<br>
quite like this actually exist, though I'm sure some smarty here will try<br>
and step forward and prove me wrong. <br>
<p>
Anyway, the point is that this question is important because it determines<br>
what you should do.  As for the correct answer, if we posit that the<br>
correct answer can be found rationally, then the problem of ethics may one<br>
day not be so problematic after all.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2621.html">Technotranscendence: "FWD: The Happy Robot"</a>
<li> <b>Previous message:</b> <a href="2619.html">Michael Lorrey: "Re: Ethics"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2624.html">Ian Goddard: "Net Difference = 0 ?"</a>
<li> <b>Reply:</b> <a href="2624.html">Ian Goddard: "Net Difference = 0 ?"</a>
<li> <b>Reply:</b> <a href="2630.html">Daniel Fabulich: "Re: Special Relativity"</a>
<!-- reply="end" -->
</ul>
