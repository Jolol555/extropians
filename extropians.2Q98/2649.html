<!-- received="Thu Jun 25 12:21:59 1998 MDT" -->
<!-- sent="Thu, 25 Jun 1998 14:21:39 -0400 (EDT)" -->
<!-- name="Daniel Fabulich" -->
<!-- email="daniel.fabulich@yale.edu" -->
<!-- subject="Re: Ethics" -->
<!-- id="199806251727.AA236175629@raptor.fc.hp.com" -->
<!-- inreplyto="001401bda05c$0e454ce0$755a95c1@bungle" -->
<title>extropians: Re: Ethics</title>
<h1>Re: Ethics</h1>
Daniel Fabulich (<i>daniel.fabulich@yale.edu</i>)<br>
<i>Thu, 25 Jun 1998 14:21:39 -0400 (EDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2649">[ date ]</a><a href="index.html#2649">[ thread ]</a><a href="subject.html#2649">[ subject ]</a><a href="author.html#2649">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2650.html">Doug Bailey: "MEDIA: Forbes Magazine discusses AI and the Singularity (well, kinda)"</a>
<li> <b>Previous message:</b> <a href="2648.html">Jim Barnebee: "IMO corrections about western religons"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2656.html">Anders Sandberg: "Re: Ethics"</a>
<li> <b>Reply:</b> <a href="2656.html">Anders Sandberg: "Re: Ethics"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Thu, 25 Jun 1998, Bryan Moss wrote:<br>
<p>
<i>&gt; In the above paragraph you are judging one answer,</i><br>
<i>&gt; "the right action is that which maximises extropy</i><br>
<i>&gt; over all other possible actions" by another,</i><br>
<i>&gt; Utilitarianism. Clearly this is not logical.</i><br>
<p>
I should explain:  this answer is not wrong simply because it disagrees<br>
with utilitarianism.  If that were the case, I would have no defense:<br>
utilitarianism could be declared wrong by the same argument.  (Though, if<br>
utilitarianism is right, as I think it may be, then this argument would be<br>
valid.)<br>
<p>
However, I assert that most extropians would not agree with the<br>
extreme ethical rule I described in my last post because it would require<br>
actions which most extropians *already* consider wrong.  Since we know<br>
that most extropians do not believe killing, stealing and suicide to be a<br>
good idea, we may use this to show that most extropians do not support a<br>
strong consequentialist version of extropianism.  I did not attempt to<br>
show that they were right or wrong here; I was simply observing that they<br>
do not support that particular moral rule.<br>
<p>
<i>&gt; But you're judging Egoism by Utilitarianism, which</i><br>
<i>&gt; does not seem to be a rational way of finding the</i><br>
<i>&gt; correct answer. You cannot say one answer is wrong</i><br>
<i>&gt; because it is not equal to another answer when you</i><br>
<i>&gt; do not know the other answer to be correct.</i><br>
<p>
You left off the second half of that paragraph, which was my *actual*<br>
response to egoism.  I believe I said something like this: the fact that<br>
egoists will hurt others to benefit themselves is not logically<br>
inconsistent, but it is not consistently generalizible: if everyone were<br>
egoistic, we would actually be worse off.  On some level, in order to be<br>
egoistic, we must reject egoism.  THIS is my real critique, which was more<br>
or less the underpinning of Kant's argument: the correct answer to the<br>
problem of ethics ought to work just as well (if not better!) when<br>
everyone is acting ethically as when very few people are.<br>
<p>
<i>&gt; I'm not sure this is obviously rational, I fail to</i><br>
<i>&gt; see why saving lives is a rational act.</i><br>
<p>
Saving lives is not, in itself, rational; unless you get great pleasure<br>
from doing so.  Saving your OWN life, however, is.  This is fine for the<br>
simple egoist; the simple egoist would steal to save his own life without<br>
a second thought.  The objectivist/libertarian, however, will do anything<br>
to save her own life *except* steal, which leaves her in a bad place under<br>
situations of sufficient urgency.<br>
<p>
I argue that objectivism is wrong because it requires one to perform<br>
irrational actions in order to do the right thing, which is contrary to<br>
our earlier presumption that the right action is the rational one.  With<br>
this alone, we could not reject simple egoism; if we add generalization to<br>
this rule, however, egoism fails the test, leaving us, IMO, with<br>
utilitarianism. <br>
<p>
<i>&gt; But doesn't Utilitarianism promote self-sacrifice?</i><br>
<i>&gt; If your death saves lives (or just makes others</i><br>
<i>&gt; happy), then your death is a good thing.</i><br>
<p>
Possibly.  However, I believe that people greatly exaggerate the number of<br>
situations in which one might reasonably give one's life to save others;<br>
there is usually a better option which doesn't involve suicide.  At the<br>
same time, I will not say that there are never situations under which it<br>
would be good (at least according to utilitarianism) to give one's life to<br>
save others.  The only justificaiton I can give for this is, again, the<br>
generalization principle: if no one would ever give any of their happiness<br>
or risk their lives to save others, we would actually be more likely to<br>
die/suffer unhappiness as a result; we must reject this policy in order to<br>
pursue it.<br>
<p>
Meanwhile, both of the other two answers require us to give our lives<br>
under sufficently extreme circumstances.  Even simple egoism may require<br>
us to commit suicide if we will experience more pain than pleasure in the<br>
remainder of our lives (though this is arguable).  Utilitarianism provides<br>
an answer which, IMO, agrees with both the generalization princple as well<br>
as the conditions of rationality; that it requires suicide under extreme<br>
conditions is not a sufficient counter: suicide may indeed BE rational<br>
under very extreme circumstances.<br>
<p>
You may choose to reject the generalization principle; if so, simple<br>
egoism seems the most rational to me.  However, if we DO accept the<br>
generalization principle, I think that we must accept utilitarianism.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2650.html">Doug Bailey: "MEDIA: Forbes Magazine discusses AI and the Singularity (well, kinda)"</a>
<li> <b>Previous message:</b> <a href="2648.html">Jim Barnebee: "IMO corrections about western religons"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2656.html">Anders Sandberg: "Re: Ethics"</a>
<li> <b>Reply:</b> <a href="2656.html">Anders Sandberg: "Re: Ethics"</a>
<!-- reply="end" -->
</ul>
