<!-- received="Tue Jun 23 22:53:27 1998 MDT" -->
<!-- sent="Tue, 23 Jun 1998 21:53:19 -0700 (PDT)" -->
<!-- name="Damien R. Sullivan" -->
<!-- email="phoenix@ugcs.caltech.edu" -->
<!-- subject="Emotions and ethics" -->
<!-- id="199806240453.VAA06082@sloth.ugcs.caltech.edu" -->
<!-- inreplyto="" -->
<title>extropians: Emotions and ethics</title>
<h1>Emotions and ethics</h1>
Damien R. Sullivan (<i>phoenix@ugcs.caltech.edu</i>)<br>
<i>Tue, 23 Jun 1998 21:53:19 -0700 (PDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2579">[ date ]</a><a href="index.html#2579">[ thread ]</a><a href="subject.html#2579">[ subject ]</a><a href="author.html#2579">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2580.html">Ian Goddard: "Special Relativity"</a>
<li> <b>Previous message:</b> <a href="2578.html">Ian Goddard: "Re: Also Re: Special Relativity"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; To put it bluntly: if God decides that rape and destruction of art is good,</i><br>
<i>&gt; will it really be ethically good?</i><br>
<p>
What would it mean for it to be ethically bad, under these circumstances?<br>
<p>
Is a negative feedback automatically the equivalent of pain or hunger?  Does a<br>
robot turtle feel pain when its batteries are low?  Or could a complex AI<br>
think "oops, these are unproductive thoughts, I'll be rebooted, and I don't<br>
want that" in a quasi-Zen fashion, without 'real' subjective pain?<br>
<p>
Can we answer these questions without neurological insight into subjective<br>
feeling?  I doubt it.<br>
<p>
At any rate, an AI could have emotions, but ones more appropriate for its<br>
environment.  Curiosity and boredom and happiness when it fulfills the orders<br>
of its human, instead of dignity or desires for freedom or fear of snakes.<br>
<p>
S. M. Stirling's Draka timeline provides an interesting question.  The Draka<br>
are rational quasi-Nazis, or modern Spartans, descended from Southern<br>
Loyalists resettled in South Africa after the American Revolution.  Nasty<br>
folk.  Somewhat power-seductive, though.  At any rate, eventually they go<br>
posthuman, and Earth is occupied by two new species.  The serfs are Homo<br>
servus, similar to H. sapiens, but more docile, sensitive to Draka pheromones,<br>
worshipful of the Draka, and probably smarter and healthier.  The Draka<br>
replace themselves with Homo drakensis, with more genetic distance than chimps<br>
or maybe gorillas; generally superhuman. <br>
<p>
The question: regardless of Draka atrocities, what do you do with the existing<br>
society, if you could force a peace with the drakensis?  It seems unethical to<br>
a lot of people, but would interfering with it really be ethical?  The servus<br>
don't want freedom; they're not made for it.  Like Niven's Moties, but<br>
artificial, not natural.<br>
<p>
Hmm.  Are artificial biological castes worse than natural ones?<br>
<p>
The ethical sense being violated here would seem to be absolute, or<br>
transcendental, not empirical or evolutionary.  Evolutionary ethics seem to be<br>
misapplies in these cases, and empirical ones have no clear objection.<br>
<p>
-xx- Damien R. Sullivan X-) <br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2580.html">Ian Goddard: "Special Relativity"</a>
<li> <b>Previous message:</b> <a href="2578.html">Ian Goddard: "Re: Also Re: Special Relativity"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
