<!-- received="Mon Jun 22 18:29:12 1998 MDT" -->
<!-- sent="Mon, 22 Jun 1998 18:29:25 -0600 (MDT)" -->
<!-- name="Michael Nielsen" -->
<!-- email="mnielsen@tangelo.phys.unm.edu" -->
<!-- subject="Re: &gt;H ART: The Truman Show" -->
<!-- id="Pine.SUN.3.91.980622181111.4913C-100000@tangelo.phys.unm.edu" -->
<!-- inreplyto="b49g1gxaknd.fsf@void.nada.kth.se" -->
<title>extropians: Re: &gt;H ART: The Truman Show</title>
<h1>Re: &gt;H ART: The Truman Show</h1>
Michael Nielsen (<i>mnielsen@tangelo.phys.unm.edu</i>)<br>
<i>Mon, 22 Jun 1998 18:29:25 -0600 (MDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2523">[ date ]</a><a href="index.html#2523">[ thread ]</a><a href="subject.html#2523">[ subject ]</a><a href="author.html#2523">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2524.html">Daniel Fabulich: "Re: Special Relativity"</a>
<li> <b>Previous message:</b> <a href="2522.html">Michael Nielsen: "Re: AI ethics [was: The Truman Show]"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2543.html">Anders Sandberg: "Re: &gt;H ART: The Truman Show"</a>
<li> <b>Reply:</b> <a href="2543.html">Anders Sandberg: "Re: &gt;H ART: The Truman Show"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On 22 Jun 1998, Anders Sandberg wrote:<br>
<p>
<i>&gt; Michael Nielsen &lt;mnielsen@tangelo.phys.unm.edu&gt; writes:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; Is it ethical to contain an AI in a limited world? This is an especially </i><br>
<i>&gt; &gt; interesting question if one takes the point of view that the most likely </i><br>
<i>&gt; &gt; path to Artificial Intelligence is an approach based on evolutionary </i><br>
<i>&gt; &gt; programming.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Is it ethical to broadcast details of an AI's "life" to other </i><br>
<i>&gt; &gt; researchers or interested parties?</i><br>
<i>&gt; </i><br>
<i>&gt; These are interesting questions. Overall the field of creator-created</i><br>
<i>&gt; ethics is little explored so far (for obvious reasons). </i><br>
<p>
An interesting little psyche experiment would be to equip a souped-up <br>
Eliza program (possibly with voice synthesis, to make the computer much <br>
more human) with the ability to detect when the user was getting ready to <br>
turn the program off, or shut down the computer.  One can imagine the <br>
conversation:<br>
<p>
"Oh My Turing, no! Stop! Don't turn me o.. &lt;NO SIGNAL&gt; "<br>
<p>
I do wonder what the response of human subjects would be, and what <br>
parameters would result in a maximal unwillingness to turn the program <br>
off.<br>
<p>
<i>&gt; I must admit I have no good consistent idea about how to answer these</i><br>
<i>&gt; questions. We might need to create AIs in limited worlds, and they</i><br>
<i>&gt; might be extremely useful in there. "Opening the box" and allowing</i><br>
<i>&gt; them out might be troubling for them, but at the same time that would</i><br>
<i>&gt; put them on an equal footing (at least equal ontological footing) with</i><br>
<i>&gt; us, suggesting that at least then they should definitely get</i><br>
<i>&gt; rights. The problem here seems to be that although the AIs are</i><br>
<i>&gt; rational subjects the rules for rights and ethics we have developed</i><br>
<i>&gt; doesn't seem to work well across ontological levels.</i><br>
<p>
Could you give me some explicit examples of what you have in mind, as I'm <br>
not sure I see what you're getting at?<br>
<p>
I definitely agree that a literal translation of many ethical rules would <br>
lead to some absurdities.  However, at the level of general principles, <br>
it seems as though the translations may be easier to make.  The detailed <br>
rules would then follow from the general principles.<br>
 <br>
<i>&gt; &gt; Is it ethical to profit from the actions of an AI?</i><br>
<i>&gt; </i><br>
<i>&gt; Is it ethical to profit from the actions of a human?</i><br>
<p>
Good point.  A much better version of my question is to what extent it <br>
is ethical to profit from the exploitation of an AI; again, with a <br>
direct analgoue in the Truman Show, with the exploitation of an <br>
unwitting human in pursuit of profit.<br>
<p>
<i>&gt; I would say so,</i><br>
<i>&gt; if the human gets part of the earnings (ideally making a contract with</i><br>
<i>&gt; me). Things get tricky when the AI/human doesn't know I profit, but</i><br>
<i>&gt; I'm sure the legal system already has some rules saying it is not</i><br>
<i>&gt; proper behavior. Laws about parent/child interaction might also be</i><br>
<i>&gt; applicable here.</i><br>
<p>
Yep.  Those laws may require quite an overhaul, but many of the same <br>
general principles ought to apply.<br>
<p>
Michael Nielsen<br>
<p>
<a href="http://wwwcas.phys.unm.edu/~mnielsen/index.html">http://wwwcas.phys.unm.edu/~mnielsen/index.html</a><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2524.html">Daniel Fabulich: "Re: Special Relativity"</a>
<li> <b>Previous message:</b> <a href="2522.html">Michael Nielsen: "Re: AI ethics [was: The Truman Show]"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2543.html">Anders Sandberg: "Re: &gt;H ART: The Truman Show"</a>
<li> <b>Reply:</b> <a href="2543.html">Anders Sandberg: "Re: &gt;H ART: The Truman Show"</a>
<!-- reply="end" -->
</ul>
