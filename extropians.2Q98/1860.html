<!-- received="Sat May 23 08:55:55 1998 MDT" -->
<!-- sent="Sat, 23 May 1998 10:56:05 -0400" -->
<!-- name="Dan Clemmensen" -->
<!-- email="Dan@Clemmensen.ShireNet.com" -->
<!-- subject="Re: Near-Term Scenarios -- Nanotech" -->
<!-- id="199805231432.KAA23103@mars.superlink.net" -->
<!-- inreplyto="Near-Term Scenarios -- Nanotech" -->
<title>extropians: Re: Near-Term Scenarios -- Nanotech</title>
<h1>Re: Near-Term Scenarios -- Nanotech</h1>
Dan Clemmensen (<i>Dan@Clemmensen.ShireNet.com</i>)<br>
<i>Sat, 23 May 1998 10:56:05 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1860">[ date ]</a><a href="index.html#1860">[ thread ]</a><a href="subject.html#1860">[ subject ]</a><a href="author.html#1860">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1861.html">Ian Goddard: "Utilitarian Contradiction ?"</a>
<li> <b>Previous message:</b> <a href="1859.html">Technotranscendence: "Re: Near-Term Scenarios -- Space"</a>
<li> <b>Maybe in reply to:</b> <a href="1703.html">GBurch1: "Near-Term Scenarios -- Nanotech"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
GBurch1 wrote:<br>
<i>&gt; </i><br>
<i>&gt; A critique of this idea occurs to me after only part of one cup of coffee this</i><br>
<i>&gt; morning.  I can imagine that employment of genetic algorithms alone might</i><br>
<i>&gt; entail an accretion of inefficiency through multiple generations of</i><br>
<i>&gt; algorithmic evolution that eventually either offsets gains in hardware speed</i><br>
<i>&gt; or even effectively nullifies it.</i><br>
<p>
Is I understand it, one of the hard parts of GA is a rigorous specification of<br>
the goal toward which you are optimizing. As it happens, speed and space efficiency<br>
are just about the easiest goals to specify, and the early GA success stories<br>
frequently mention speed and space optimization.<br>
<i>&gt; </i><br>
<i>&gt; More probable seems a subtler loss of the ability to control the specific</i><br>
<i>&gt; direction algorithmic evolution takes as it progresses in complexity.  Perhaps</i><br>
<i>&gt; there is a dynamic tension between control and evolution that could offset</i><br>
<i>&gt; much of the speed-derived gains, such that the necessity to intervene in the</i><br>
<i>&gt; process to check-and-direct with human (even augmented human) intervention in</i><br>
<i>&gt; the algorithmic evolutionary process imposes a relatively low upper limit on</i><br>
<i>&gt; at least the first stages of "SI fetal development".</i><br>
<i>&gt; </i><br>
I agree that goal-setting is the big problem, but I'm more optimistic about<br>
increasing the goal-setting efficiency. In my favorite model, the SI is a<br>
collaboration, and the human is doing the goal-setting. The effectiveness of<br>
this process is dramatically and continuously enhanced by improvements in<br>
the data presentation and visualization algorithms that let the human<br>
understand the problem to be solved.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1861.html">Ian Goddard: "Utilitarian Contradiction ?"</a>
<li> <b>Previous message:</b> <a href="1859.html">Technotranscendence: "Re: Near-Term Scenarios -- Space"</a>
<li> <b>Maybe in reply to:</b> <a href="1703.html">GBurch1: "Near-Term Scenarios -- Nanotech"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
