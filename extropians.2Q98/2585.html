<!-- received="Wed Jun 24 06:06:43 1998 MDT" -->
<!-- sent="24 Jun 1998 14:06:36 +0200" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: AI's and Emotions" -->
<!-- id="s59138d5.031@MAIL.TAIT.CO.NZ" -->
<!-- inreplyto="Tue, 23 Jun 1998 18:46:47 -0500" -->
<title>extropians: Re: AI's and Emotions</title>
<h1>Re: AI's and Emotions</h1>
Anders Sandberg (<i>asa@nada.kth.se</i>)<br>
<i>24 Jun 1998 14:06:36 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2585">[ date ]</a><a href="index.html#2585">[ thread ]</a><a href="subject.html#2585">[ subject ]</a><a href="author.html#2585">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2586.html">Anders Sandberg: "Re: Information -Reply"</a>
<li> <b>Previous message:</b> <a href="2584.html">Anders Sandberg: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>In reply to:</b> <a href="2565.html">Scott Badger: "Re: AI's and Emotions"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
"Scott Badger" &lt;wbadger@psyberlink.net&gt; writes:<br>
<p>
<i>&gt; Just a couple points that came to mind on this topic.  I think perhaps that</i><br>
<i>&gt; the term *emotion* is being used a bit loosely here. </i><br>
<p>
Yes, we better be more careful here.<br>
<p>
<i>&gt; It's true that people</i><br>
<i>&gt; commonly use phrases like *I feel motivated* or *I feel like watching TV*.</i><br>
<i>&gt; If a thought just seems to pop into my mind (i.e. I want ice cream), I will</i><br>
<i>&gt; probably say *I feel like some ice cream* but it's really a cognition, not</i><br>
<i>&gt; an emotion.  The basic emotions are typically thought to be anger,</i><br>
<i>&gt; exhiliration, sadness, or fear.  But even these basic emotional states have</i><br>
<i>&gt; questionable objective validity.</i><br>
<p>
Or rather, we certainly do experience them, but it is not certain they<br>
are are basic. I'm fairly certain for example that what we commonly<br>
call pleasure is composed of several subsystems (such as the "liking"<br>
and "wanting" dopaminergic systems). <br>
<p>
<i>&gt; The most widely supported theory of emotion suggests the following (1) we</i><br>
<i>&gt; see the bear; (2) we cognitively interpret  the experience as threatening;</i><br>
<i>&gt; (3) we become physically aroused; (4) we cognitively assign a label to the</i><br>
<i>&gt; aroused state based on the context, our personal history, whether anyone</i><br>
<i>&gt; else is watching, etc.; and (4) the label we choose (i.e. the meaning we</i><br>
<i>&gt; assign to the arousal) subsequently dictates our behavior (i.e. we run, cry,</i><br>
<i>&gt; wrestle, etc.).  The objective quality of these aroused states does not vary</i><br>
<i>&gt; across the different emotions, though the subjective interpretation of them</i><br>
<i>&gt; does.  The chemistry is the same.  We construct the rest.  The level of</i><br>
<i>&gt; arousal we experience springs from the value and relevance we assign to the</i><br>
<i>&gt; experience.</i><br>
<p>
Actually, there are some chemical differences in which neuromodulator<br>
systems get activated - noradrenaline (arousal, aversive experience),<br>
serotonin (safety?), acetylcholine (attention, arousal), dopamine<br>
(motivation) and so on, but they are surprisingly small. Measured with<br>
a polygraph, anger and happiness are indistinguishable.<br>
<p>
<i>&gt; I think Anders made a reference to DeMato's (sp?) book, Descartes Error, and</i><br>
<i>&gt; his  assertion that emotions are necessary for good decision making.  That</i><br>
<i>&gt; may be true but what aspect of emotions?</i><br>
<p>
Damasio. <br>
<p>
I went for lunch with my research group, and we discussed this<br>
problem. We ended up with the following conclusions:<br>
<p>
In the brain the dopaminergic reward systems seem to be involved in<br>
reinforcement learning. They seem to turn our sensory input and<br>
deductions into a scalar reinforcement estimate, telling us how good<br>
or bad we did, and directly influencing plasticity in the basal<br>
ganglia to change our behavior to become better. <br>
<p>
Since intelligent entities need to learn, and learn in an unsupervised<br>
setting where the results might not even be present now but appear in<br>
the future, it is hard to avoid some kind of reinforcement learning<br>
with an internal reinforcement estimator that can do credit<br>
assignment. This means the entities need to be able to evaluate<br>
outcomes as good or bad, and to make estimates of how good the results<br>
of different actions will be. I would say this correlates quite well<br>
with what we would call pleasure or pain.<br>
<p>
Damasio makes a good case that we use these valences to do 'alpha-beta<br>
pruning' of our decision trees: ignore possible actions whose outcomes<br>
are likely to be bad, concentrate on those that will lead to<br>
good. Without this we would get trapped doing needless thinking about<br>
bad ideas, beside the risk of choosing a bad action because we cannot<br>
evaluate it as bad.<br>
<p>
On the other hand, our emotions change the mode of our thinking. For<br>
example, when aroused we tend to use behaviors that are strongly<br>
learned instead of exploring new possibilities (this is true even for<br>
high levels of happiness, apparently), when feeling content we are<br>
less likely to act in ways that will involve risk, and so on. These<br>
modes (moods?) doesn't appear necessary for rational thinking, so an<br>
AI might do without them. The effect would likely be rather like a<br>
caricature of Spock - constantly calm and efficient. But it is likely<br>
that having cognitive modes is a good thing in many circumstances; in<br>
a crisis situation it might be very useful not to be too calm, when<br>
encountering something unexpected it might be useful to go into a<br>
rapid analysis mode and so on.<br>
<p>
So our consensus became that AI will most likely have at least<br>
reinforcement emotions, and for efficiency reasons likely moods. On<br>
the other hand, we have not yet agreed on what motivations need to be<br>
included in an useful AI. A likely guess would be novelty detection<br>
and approach, otherwise it would learn very badly and likely get stuck<br>
in a rut. <br>
<p>
<i>&gt; I don't see why AI's couldn't use the assignment of value and the</i><br>
<i>&gt; determination of  relevance to make good decisions without the need for</i><br>
<i>&gt; aroused states.</i><br>
<p>
Note that we do not need arousal for decision making, it just<br>
influences it. But assignment of value, I would say that is the basis<br>
of emotion.<br>
<p>
<i>&gt; I certainly don't want to lose my capacity for positive exhiliration (i.e. I</i><br>
<i>&gt; love laughing).  I would hope that part of the transhuman condition would</i><br>
<i>&gt; involve greater control over, rather than the elimination of emotional</i><br>
<i>&gt; states.</i><br>
<p>
Exactly! We humans already have an impressive control over our<br>
emotions (most of them are actually caused at least in adults by<br>
internal cognitive processes) and hardware connections between the<br>
limbic system and the frontal cortex that are denser than among most<br>
mammals. That is something we can develop further.<br>
<p>
<p>
<pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
asa@nada.kth.se                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2586.html">Anders Sandberg: "Re: Information -Reply"</a>
<li> <b>Previous message:</b> <a href="2584.html">Anders Sandberg: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>In reply to:</b> <a href="2565.html">Scott Badger: "Re: AI's and Emotions"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
