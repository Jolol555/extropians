<!-- received="Sat May 16 14:08:01 1998 MDT" -->
<!-- sent="Sat, 16 May 1998 16:07:55 -0400 (EDT)" -->
<!-- name="Daniel Fabulich" -->
<!-- email="daniel.fabulich@yale.edu" -->
<!-- subject="Re: ExI = Truth First ?" -->
<!-- id="3.0.3.32.19980516154514.0321d128@pop.erols.com" -->
<!-- inreplyto="3.0.3.32.19980516073551.0320cb68@208.198.32.3" -->
<title>extropians: Re: ExI = Truth First ?</title>
<h1>Re: ExI = Truth First ?</h1>
Daniel Fabulich (<i>daniel.fabulich@yale.edu</i>)<br>
<i>Sat, 16 May 1998 16:07:55 -0400 (EDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1681">[ date ]</a><a href="index.html#1681">[ thread ]</a><a href="subject.html#1681">[ subject ]</a><a href="author.html#1681">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1682.html">Ian Goddard: "Truth Here, Utility There"</a>
<li> <b>Previous message:</b> <a href="1680.html">Ian Goddard: "ETHICS: Utility First--&gt; Fraud"</a>
<li> <b>In reply to:</b> <a href="1661.html">Ian Goddard: "ExI = Truth First ?"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1683.html">Daniel Fabulich: "Re: ETHICS: Utility First--&gt; Fraud"</a>
<li> <b>Reply:</b> <a href="1683.html">Daniel Fabulich: "Re: ETHICS: Utility First--&gt; Fraud"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Sat, 16 May 1998, Ian Goddard wrote:<br>
<p>
<i>&gt;    IAN: Your point is that function = utility.</i><br>
<i>&gt;    But we could have two functions, (1) a truth</i><br>
<i>&gt;    fuction that maps physical reality, and (2) a</i><br>
<i>&gt;    Big Lie function that masks physical reality.</i><br>
<i>&gt;    Both are useful to some, but only one is true.</i><br>
<i>&gt;    If "utility first" is my directive, I can choose</i><br>
<i>&gt;    either; if "truth first" is my principle I can</i><br>
<i>&gt;    choose only one. This limits my options, but </i><br>
<i>&gt;    as a rule it maximizes social outcomes, which </i><br>
<i>&gt;    is utilitarian, but utility comes second.</i><br>
<p>
I think you misunderstood me.  My point was simply that your reasons for<br>
accepting a "truth-first" principle were utilitarian in origin: that<br>
people would be better off if we all accepted "truth-first."  Well, if we<br>
WOULD be better off thanks to "truth-first," then utilitarianism demands<br>
that we act according to "truth-first;" thus we would accept it BECAUSE of<br>
utilitarisnism, not in spite of it.<br>
<p>
<i>&gt;    What is more, since there is only one reality,</i><br>
<i>&gt;    yet myriad claims about it, the Big Lie function</i><br>
<i>&gt;    will be useful to most people and thus the demand</i><br>
<i>&gt;    for the utility of BLF will be greater than for </i><br>
<i>&gt;    TF, so if our directive is  "whatever works to</i><br>
<i>&gt;    promote your idea," Big Lies will win the day.</i><br>
<p>
But, as I think we're both agreed, Big Lies are BAD for us in the long<br>
run.  Therefore, according to utilitarianism, we should avoid Big Lies.<br>
<p>
<i>&gt; </i><br>
<i>&gt;    When I apply a "truth first" principle based </i><br>
<i>&gt;    up a scientific defintion of truth, I submit </i><br>
<i>&gt;    to a "higher authority" that will judge my </i><br>
<i>&gt;    ideas (about any topic) accordingly. If I</i><br>
<i>&gt;    apply "utility first" then it's whatever</i><br>
<i>&gt;    I can get away with to promote my ideas.</i><br>
<i>&gt; </i><br>
<p>
How about the benevolence principle, an invention (I think) of JS Mill?<br>
"Will this action which I am choosing create the most happiness for the<br>
largest number of people out of all other actions I might choose?"<br>
<p>
<i>&gt; </i><br>
<i>&gt; &gt;&lt;thought experiment&gt; Suppose you are studying an important effect in</i><br>
<i>&gt; &gt;quantum mechanics, but one which can be put off until later without</i><br>
<i>&gt; &gt;significant losses in utility.  Then, you look out the window and you</i><br>
<i>&gt; &gt;see: A Drowning Child (tm) [the philosopher's favorite play toy!]. </i><br>
<i>&gt; &gt;You could save that child now and study quantum effects later, </i><br>
<i>&gt; &gt;OR you could just ignore the child and continue the pursuit of the</i><br>
<i>&gt; &gt;one-to-one truth function.  Despite the fact that utilitarianism demands</i><br>
<i>&gt; &gt;that you save that child, truth-first demands that truth comes first, and</i><br>
<i>&gt; &gt;utility second.  You ignore the child, and finish observing your quantum</i><br>
<i>&gt; &gt;effect before even considering saving him/her. &lt;/thought experiment&gt;</i><br>
<i>&gt; </i><br>
<i>&gt; </i><br>
<i>&gt;    IAN: An interesting dilemma, but I think it </i><br>
<i>&gt;    may be a false dilemma. Maybe I cannot swim,</i><br>
<i>&gt;    and I have no rope, or just don't care, so</i><br>
<i>&gt;    saving the drowning child is not useful to </i><br>
<i>&gt;    me, afterall, that kid was a real nuisance.</i><br>
<p>
Uhm...  SINCE this is a thought experiment, I can always tweak it.  I<br>
should have mentioned that you are ABLE to save the child, if you decided<br>
to, at a trivial loss of utility to yourself.  (This was actually part of<br>
the reason why I mentioned the "Drowning Child (tm):" ehtical philosophers<br>
REGULARLY use this as a thought experiment: you could either save the<br>
child or do something else.  Which do you do?)<br>
<p>
<i>&gt; </i><br>
<i>&gt;    So it doesn't follow that utility dictates </i><br>
<i>&gt;    that I stop my work and save the child.</i><br>
<p>
It does if you CAN save the child, and if saving the child would result in<br>
minimal utility loss for you. <br>
<p>
<i>&gt;    But if I have an axiom, "All human life is </i><br>
<i>&gt;    sacred," and I see a life in eminent peril,</i><br>
<i>&gt;    I say "I believe that axiom is true, thus </i><br>
<i>&gt;    I must stop my work and save that child." </i><br>
<i>&gt;    It seems to me that the act of stopping my</i><br>
<i>&gt;    work to save another must rest on a truth.</i><br>
<i>&gt; </i><br>
<p>
What you're talking about is extensional equivalence, which is what<br>
happens when two separate ethical theories require the same actions. <br>
Interesting questions arise when two ethical theories are COMPETELY<br>
extensionally equivalent, but use different premises and arguments to<br>
reach their conclusions.  However, as I think thought experiment can<br>
show, truth and utility are NOT completely extensionally equivalent, and<br>
when they disagree, utility ought to win out.<br>
<p>
<i>&gt;    IAN: It is TRUE that Germany would be better off </i><br>
<i>&gt;    if they did not start WWII and commit mass murder.</i><br>
<i>&gt;    It was useful for the Nazis to mask that truth, </i><br>
<i>&gt;    for their goals were not making Germany better</i><br>
<i>&gt;    but enemy extermination and global conquest.</i><br>
<p>
OK, let's stretch the imagination for a moment and pretend that the reason<br>
Naziism happened was because, and only because, they thought it would make<br>
the people better off.  Otherwise, you're not criticizing utilitarianism,<br>
but something more akin to nationalism, which we both agree is a silly<br>
moral premise. <br>
<p>
If Germany WAS acting solely to make the German people better off, then we<br>
can see clearly that they made a mistake in that regard: Germany was made<br>
worse off by WWII, not better.  As a definition of utility, if something<br>
makes you worse off, then it is not compatible with your utility.<br>
Therefore, I make the trivial conclusion that mass murder was NOT useful<br>
to Germany, because it caused WWII, which went quite badly for Germany.<br>
<p>
Utilitarianism does NOT demand, nor could it in any way be construed to<br>
support, Naziism.  (It may have been used at the time as rhetoric, but we<br>
can now see that people who argued that Naziism would make the Germans<br>
better off were wrong, and that utilitarianism was NOT on their side.)<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1682.html">Ian Goddard: "Truth Here, Utility There"</a>
<li> <b>Previous message:</b> <a href="1680.html">Ian Goddard: "ETHICS: Utility First--&gt; Fraud"</a>
<li> <b>In reply to:</b> <a href="1661.html">Ian Goddard: "ExI = Truth First ?"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1683.html">Daniel Fabulich: "Re: ETHICS: Utility First--&gt; Fraud"</a>
<li> <b>Reply:</b> <a href="1683.html">Daniel Fabulich: "Re: ETHICS: Utility First--&gt; Fraud"</a>
<!-- reply="end" -->
</ul>
