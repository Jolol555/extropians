<!-- received="Mon Jun 22 15:20:20 1998 MDT" -->
<!-- sent="22 Jun 1998 23:20:12 +0200" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: &gt;H ART: The Truman Show" -->
<!-- id="s58f768f.054@MAIL.TAIT.CO.NZ" -->
<!-- inreplyto="Mon, 22 Jun 1998 21:59:15 +0200" -->
<title>extropians: Re: &gt;H ART: The Truman Show</title>
<h1>Re: &gt;H ART: The Truman Show</h1>
Anders Sandberg (<i>asa@nada.kth.se</i>)<br>
<i>22 Jun 1998 23:20:12 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2513">[ date ]</a><a href="index.html#2513">[ thread ]</a><a href="subject.html#2513">[ subject ]</a><a href="author.html#2513">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2514.html">Randall R Randall: "Re: &gt;H ART: The Truman Show"</a>
<li> <b>Previous message:</b> <a href="2512.html">Gerhard Kessell-Haak: "Turtles, turtles, turtles... (was Re: Creationists)"</a>
<li> <b>In reply to:</b> <a href="2509.html">den Otter: "Re: &gt;H ART: The Truman Show"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
den Otter &lt;neosapient@geocities.com&gt; writes:<br>
<p>
<i>&gt; Since AIs will presumably be made without emotions, or at least with</i><br>
<i>&gt; a much more limited number of emotions than humans, you don't have</i><br>
<i>&gt; to worry about their "feelings".</i><br>
<p>
I think this is a fallacy. First, why would AI have no emotions or a<br>
limited repertoar? Given current research into cognitive neuroscience<br>
it seems that emotions are instead enormously important for rational<br>
thinking since they provide the valuations and heuristics necessary<br>
for making decisions well. Secondly, even if they have less feelings<br>
than humans, why does that mean we can treat them as we like? If it<br>
turns out that I have less emotions than most people, does that mean I<br>
also have less rights? Obviously this kind of reasoning doesn't work,<br>
beside the obvious impossibilities of finding out how AI or other<br>
humans experience their existence and putative emotions. We need to<br>
base our ethics on something more observable and stable than that.<br>
<p>
<i>&gt; Also, one of the first things you</i><br>
<i>&gt; would ask an AI is to develop uploading &amp; computer-neuron interfaces,</i><br>
<i>&gt; so that you can make the AI's intelligence part of your own.</i><br>
<p>
This is the old "Superintelligence will solve every problem"<br>
fallacy. If I manage to create a human-or-above-level AI living inside<br>
a virtual world of candy, it will not necessarily be able to solve<br>
real world problems (it only knows about candy engineering), and given<br>
access to the physical world and a good education its basic cognitive<br>
structure (which was good for a candy world) might still make it very<br>
bad at developing uploadning.<br>
<p>
<i>&gt; This would</i><br>
<i>&gt; pretty much solve the whole "rights problem" (which is largely</i><br>
<i>&gt; artificial anyway), since you don't grant rights to specific parts</i><br>
<i>&gt; of your brain.</i><br>
<p>
Let me see. Overheard on the SubSpace network: <br>
<p>
Borg Hive 19117632: "What about the ethics of creating those<br>
'individuals' you created on Earth a few megayears ago?"<br>
<p>
Borg Hive 54874378: "No problem. I will assimilate them all in a<br>
moment. Then there will be no ethical problem since they will be part<br>
of me."<br>
<p>
I think you are again getting into the 'might is right' position you<br>
had on the posthuman ethics thread on the transhumanist list. Am I<br>
completely wrong?<br>
<p>
<i>&gt; A failure to integrate with the AIs asap would </i><br>
<i>&gt; undoubtedly result in AI domination, and human extinction.</i><br>
<p>
Again a highly doubtful assertion. As I argued in my essay about<br>
posthuman ethics, even without integration (which I really think is a<br>
great idea, it is just that I want to integrate with AI developed<br>
specifically for that purpose and not just to get rid of unnecessary<br>
ethical subjects) human extinction is not a rational consequence of<br>
superintelligent AI under a very general set of assumptions. <br>
<p>
Somehow I think it is our mammalian territoriality and xenophobia<br>
speaking rather than a careful analysis of consequences that makes<br>
people so fond of setting up AI as invincible conquerors.<br>
<p>
<pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
asa@nada.kth.se                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2514.html">Randall R Randall: "Re: &gt;H ART: The Truman Show"</a>
<li> <b>Previous message:</b> <a href="2512.html">Gerhard Kessell-Haak: "Turtles, turtles, turtles... (was Re: Creationists)"</a>
<li> <b>In reply to:</b> <a href="2509.html">den Otter: "Re: &gt;H ART: The Truman Show"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
