<!-- received="Tue Jun 23 06:18:20 1998 MDT" -->
<!-- sent="23 Jun 1998 14:18:15 +0200" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)" -->
<!-- id="3.0.3.32.19980623041154.0309e594@208.198.32.3" -->
<!-- inreplyto="Tue, 23 Jun 1998 01:22:46 +0200" -->
<title>extropians: Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)</title>
<h1>Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)</h1>
Anders Sandberg (<i>asa@nada.kth.se</i>)<br>
<i>23 Jun 1998 14:18:15 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2544">[ date ]</a><a href="index.html#2544">[ thread ]</a><a href="subject.html#2544">[ subject ]</a><a href="author.html#2544">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2545.html">Berrie Staring: "TransVision '98  Pics &amp; stuf....."</a>
<li> <b>Previous message:</b> <a href="2543.html">Anders Sandberg: "Re: &gt;H ART: The Truman Show"</a>
<li> <b>In reply to:</b> <a href="2521.html">den Otter: "The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2567.html">Daniel Fabulich: "Re: Special Relativity"</a>
<li> <b>Reply:</b> <a href="2567.html">Daniel Fabulich: "Re: Special Relativity"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
den Otter &lt;neosapient@geocities.com&gt; writes:<br>
<p>
<i>&gt; Emotions are important for us because they help us to survive. AIs</i><br>
<i>&gt; don't need to fend for themselves in a difficult environment; they</i><br>
<i>&gt; get all the energy, protection &amp; imput they need from humans and</i><br>
<i>&gt; other machines. All they have to do is solve puzzles (of biology,</i><br>
<i>&gt; programming etc).</i><br>
<p>
Isn't solving puzzles a difficult environment? The space of scientific<br>
research is a very weird place, with quite complex hinders and even<br>
some dangers (if the AI gets stuck in an infinite loop it will be<br>
reset, if it doesn't produce anything considered useful by the humans<br>
it will be discontinued, research money might run out etc).<br>
<p>
<p>
<i>&gt; If you program it with an "urge" to solve puzzles</i><br>
<i>&gt; (just like your PC has an "urge" to execute your typed orders), it</i><br>
<i>&gt; will work just fine. No (other) "emotions" are needed, imo.</i><br>
<p>
[Slight spoiler] I'm reminded of the AI in Greg Bear's _Moving Mars_<br>
who becomes fascinated by a space of alternate truths while doing a<br>
kind of hyperspace jump. The effects are nearly deadly to the<br>
explorers and the AI, as the laws of physics in the ship begins to<br>
shift, but they manage to return to normal space and survive. The<br>
person linked to the AI assures the others that it won't happen again<br>
since the AI now has realized that if it explores that kind of stuff<br>
it will cease to exist, and that would be the end of its ability to<br>
explore. Note that the AI did not have self-preservation in the first<br>
place, and this was nearly deadly. Second, it had some kind of<br>
motivation to solve more problems and learn more, but this can be<br>
generalized into new emotions. If it can deduce self preservation from<br>
this, why not other emotions we have not desired, like territoriality<br>
(if you remove resources from it it will not be able to solve problems<br>
well, so it should prevent this).<br>
<p>
<i>&gt; &gt;&gt;Secondly, even if they have less feelings</i><br>
<i>&gt; than humans, why does that mean we can treat them as we like?&lt;&lt; </i><br>
<i>&gt; </i><br>
<i>&gt; If it doesn't care about abuse, if it feels no pain or emotional</i><br>
<i>&gt; trauma, then there is no reason to worry about it's well-being</i><br>
<i>&gt; (for it's own sake). AIs can almost certainly be made this way, </i><br>
<i>&gt; with no more emotions than a PC. </i><br>
<p>
So then it would be no problem with using humans without any sense of<br>
pain or emotion for medical experiments? Why does emotions imply<br>
ethical rights?<br>
<p>
<i>&gt; &gt; &gt; This would</i><br>
<i>&gt; &gt; &gt; pretty much solve the whole "rights problem" (which is largely</i><br>
<i>&gt; &gt; &gt; artificial anyway), since you don't grant rights to specific parts</i><br>
<i>&gt; &gt; &gt; of your brain.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Let me see. Overheard on the SubSpace network:</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Borg Hive 19117632: "What about the ethics of creating those</i><br>
<i>&gt; &gt; 'individuals' you created on Earth a few megayears ago?"</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Borg Hive 54874378: "No problem. I will assimilate them all in a</i><br>
<i>&gt; &gt; moment. Then there will be no ethical problem since they will be part</i><br>
<i>&gt; &gt; of me."</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; I think you are again getting into the 'might is right' position you</i><br>
<i>&gt; &gt; had on the posthuman ethics thread on the transhumanist list. Am I</i><br>
<i>&gt; &gt; completely wrong?</i><br>
<i>&gt; </i><br>
<i>&gt; Eternal truth: unless 19117632 or some other being can and wants to</i><br>
<i>&gt; stop 54874378, it will do what it bloody well pleases. Might is</i><br>
<i>&gt; the *source* of right. Right = privilege, and always exists within</i><br>
<i>&gt; a context of power. A SI may have many "autonomous" AIs in it's</i><br>
<i>&gt; "head". For example, it could decide to simulate life on earth,</i><br>
<i>&gt; with "real" players to make it more interesting. Are these simulated</i><br>
<i>&gt; beings, mere thoughts to the SI, entitled to rights and protection?</i><br>
<i>&gt; If so, who or what could force the SI to grant it's thoughts rights</i><br>
<i>&gt; (a rather rude invasion of privacy). How do you enforce such rules?</i><br>
<i>&gt; Clearly a difficult matter, but it always comes down to "firepower"</i><br>
<i>&gt; in the end (can you blackmail the other into doing something he</i><br>
<i>&gt; doesn't like?-- that's the question).</i><br>
<p>
<p>
<i>&gt;  </i><br>
<i>&gt; &gt; &gt; A failure to integrate with the AIs asap would</i><br>
<i>&gt; &gt; &gt; undoubtedly result in AI domination, and human extinction.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Again a highly doubtful assertion. As I argued in my essay about</i><br>
<i>&gt; &gt; posthuman ethics, even without integration (which I really think is a</i><br>
<i>&gt; &gt; great idea, it is just that I want to integrate with AI developed</i><br>
<i>&gt; &gt; specifically for that purpose and not just to get rid of unnecessary</i><br>
<i>&gt; &gt; ethical subjects) human extinction is not a rational consequence of</i><br>
<i>&gt; &gt; superintelligent AI under a very general set of assumptions.</i><br>
<i>&gt; </i><br>
<i>&gt; It is rational because in the development of AI, just like in any</i><br>
<i>&gt; economic or arms race, people will sacrifice safety to get that</i><br>
<i>&gt; extra edge.</i><br>
<p>
Do they? Note that even taking risks is subject to rational<br>
analysis. Some risks are acceptable, others aren't, and you can<br>
estimate this before taking them. Taking arbitrarily large risks<br>
doesn't work in the long run since they outweigh the benefits, and we<br>
get a clustering around the rational level of risks by the<br>
survivors/people with experience.<br>
<p>
<i>&gt; If it turns out that a highly "emotionally" unstable</i><br>
<i>&gt; AI with delusions of grandeur is more productive, then there</i><br>
<i>&gt; will always be some folks that will make it. Terrorists or</i><br>
<i>&gt; dictators could make "evil" AIs on purpose, there might be</i><br>
<i>&gt; a freak error that sends one or more AIs out of control, or</i><br>
<i>&gt; they might simply decide that humans are just dangerous pests,</i><br>
<i>&gt; and kill them. There is *plenty* of stuff that could go wrong,</i><br>
<i>&gt; and in a time of nano-built basement nukes and the like,</i><br>
<i>&gt; small mistakes can have big consequences. Intelligence is</i><br>
<i>&gt; a powerful weapon indeed.</i><br>
<p>
Exactly. But how large is the ratio between irrational people (the<br>
dictators, terrorists or extreme risk takers) and rational people? It<br>
is not that large really, so as long as there exists<br>
counter-technologies or the possibility or retaliation the irrational<br>
people have a distinct disadvantage. Things change if you introduce<br>
technologies that cannot be defended well against or cannot be traced;<br>
now the situation is unstable. However, one cannot just say that AI<br>
will by necessity lead to this situation, one has to analyze it more<br>
carefully. (In addition, irrational people seem to be much worse at<br>
creating new technology than rational people)<br>
<p>
For example, in a world with a lot of human-level AIs, would a<br>
above-human-level AI with nasty goals have a decisive advantage? Many<br>
seem to assume this, but I have not seen any believable analysis that<br>
really show it (other than the fallacy that greater intelligence can<br>
always outsmart lower intelligence; if that is so, how come that many<br>
brilliant generals have still been defeated by less brilliant<br>
opponents?). In fact, it might turn out that being a lone<br>
superintelligence is a disadvantage when faced with a multitude of<br>
opponents, it would be better to be a larger but less smart group.<br>
<p>
Unfortunately this kind of theoretical discussion is plagued by<br>
thought experiments that gladly ignore the constraints of psychology,<br>
economics, game theory and technology. <br>
<p>
<i>&gt; &gt; Somehow I think it is our mammalian territoriality and xenophobia</i><br>
<i>&gt; &gt; speaking rather than a careful analysis of consequences that makes</i><br>
<i>&gt; &gt; people so fond of setting up AI as invincible conquerors.</i><br>
<i>&gt; </i><br>
<i>&gt; That territoriality and xenophobia helped to keep us alive, and</i><br>
<i>&gt; still do. The world is a dangerous place, and it won't become</i><br>
<i>&gt; all of sudden peachy &amp; rosy when AI arrives. Remember: this isn't</i><br>
<i>&gt; comparable to the invention of gun powder, the industrial revolution,</i><br>
<i>&gt; nukes or the internet. For the first time in history, there will</i><br>
<i>&gt; be beings that are (a lot) smarter and faster than us. If this</i><br>
<i>&gt; goes unchecked, mankind will be completely at the mercy of </i><br>
<i>&gt; machines with unknown (unkowable?) motives. Gods really.</i><br>
<p>
But they won't come into existence or develop in a vacuum. Most<br>
discussions of this kind on this list tend to implicitely assume that<br>
first there is nothing, and then there are Gods, with so little time<br>
in between that it can safely be ignored. I am highly sceptical of<br>
this, and feel justified in this by my knowledge of both the history<br>
of technological development, the economics of technology, the<br>
complexity of scientific research and the trickiness of improving very<br>
complex systems. AI will not develop overnight, it will emerge under a<br>
period of time which will most likely be decades rather than days (as<br>
some singularitians like to think); a very fast development, but not<br>
something instantaneous. During this time the AI and humans will<br>
interact and both adapt to each other in various ways. I would be very<br>
surprised if we ended up with just humans and super-AIs. A more likely<br>
result would be a broad spectrum of entities of different levels, at<br>
least able to communicate with those just above or below them.<br>
<p>
<pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
asa@nada.kth.se                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2545.html">Berrie Staring: "TransVision '98  Pics &amp; stuf....."</a>
<li> <b>Previous message:</b> <a href="2543.html">Anders Sandberg: "Re: &gt;H ART: The Truman Show"</a>
<li> <b>In reply to:</b> <a href="2521.html">den Otter: "The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2567.html">Daniel Fabulich: "Re: Special Relativity"</a>
<li> <b>Reply:</b> <a href="2567.html">Daniel Fabulich: "Re: Special Relativity"</a>
<!-- reply="end" -->
</ul>
