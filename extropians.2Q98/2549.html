<!-- received="Tue Jun 23 11:41:17 1998 MDT" -->
<!-- sent="23 Jun 1998 19:41:08 +0200" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: &gt;H ART: The Truman Show" -->
<!-- id="3.0.2.32.19980623095559.0093f100@shell9.ba.best.com" -->
<!-- inreplyto="Tue, 23 Jun 1998 12:14:12 +0200" -->
<title>extropians: Re: &gt;H ART: The Truman Show</title>
<h1>Re: &gt;H ART: The Truman Show</h1>
Anders Sandberg (<i>asa@nada.kth.se</i>)<br>
<i>23 Jun 1998 19:41:08 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2549">[ date ]</a><a href="index.html#2549">[ thread ]</a><a href="subject.html#2549">[ subject ]</a><a href="author.html#2549">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2550.html">Ralph Lewis: "Re: Creationists"</a>
<li> <b>Previous message:</b> <a href="2548.html">James Rogers: "Re: Creationists"</a>
<li> <b>In reply to:</b> <a href="2540.html">den Otter: "Re: &gt;H ART: The Truman Show"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
den Otter &lt;neosapient@geocities.com&gt; writes:<br>
<p>
<i>&gt; The fact that our brains happen to work with certain emotions doesn't</i><br>
<i>&gt; automatically mean that this is the *only* way to achieve intelligence.</i><br>
<p>
I agree with this. The big question is what invariants are needed to<br>
achieve intelligence, and if emotions are part of them, specific just<br>
to intelligences in a certain kind of enviornment/evolutionary past or<br>
just one possibility among many.<br>
<p>
<i>&gt; Well, I *don't want* AIs to resemble humans with their complex emotion-</i><br>
<i>&gt; based value systems. We need obedient servants, not competition. So if </i><br>
<i>&gt; it turns out that you *can* have intelligence without a "will", then </i><br>
<i>&gt; that should be used to make useful "genie-AIs". If this isn't possible,</i><br>
<i>&gt; it might be better to make no AIs at all.</i><br>
<p>
But even the humility of humble servants is an emotion. I understand<br>
what you mean, and I agree that "genie-AIs" would be very useful<br>
(humans could in fact provide them with the emotional-motivational<br>
framework they might need). I'm not that convinced that enotional AI<br>
on the other hand is bad; I do not think it is an unmanageable<br>
problem.<br>
<p>
<i>&gt; &gt; &gt; Also, one of the first things you</i><br>
<i>&gt; &gt; &gt; would ask an AI is to develop uploading &amp; computer-neuron interfaces,</i><br>
<i>&gt; &gt; &gt; so that you can make the AI's intelligence part of your own. This would</i><br>
<i>&gt; &gt; &gt; pretty much solve the whole "rights problem" (which is largely</i><br>
<i>&gt; &gt; &gt; artificial anyway),</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; What do you mean, the rights problem is "artificial"?</i><br>
<i>&gt; </i><br>
<i>&gt; It means it's not an "absolute" problem like escaping the earth's</i><br>
<i>&gt; gravity, or breathing under water etc. It is only a problem</i><br>
<i>&gt; because certain people *make* it a problem, just like there</i><br>
<i>&gt; was no real "drug problem" before the war on drugs started.</i><br>
<p>
Aha, so we transhumanists are creating the "death problem" by not just<br>
accepting that people die young? That people die, destroy their lives<br>
with drugs or demand rights are real objective facts. Since we (or at<br>
least the people involved) do not like this, we try to do something<br>
about it. This means we consider the current situation as a problem<br>
and try to find a solution (which might be more or less good, of<br>
course). All problems are to some extent artificial, or more properly<br>
subjective to intelligent entities. <br>
<p>
<i>&gt; Unless the AIs start demanding rights themselves,</i><br>
<i>&gt; there is no reason to grant them any. If we're smart, we'll</i><br>
<i>&gt; make the AIs so that they'll never feel the need to do this.</i><br>
<p>
Which in itself is an interesting ethical problem. Suppose a certain<br>
form of education created citizens that did not feel a need for<br>
rights, would it be ethical for a dictator to use it?<br>
<p>
<i>&gt; &gt; &gt; since you don't grant rights to specific parts</i><br>
<i>&gt; &gt; &gt; of your brain. A failure to integrate with the AIs asap would</i><br>
<i>&gt; &gt; &gt; undoubtedly result in AI domination, and human extinction.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; This seems to be an unjustified assumption.  All other forms of life in the</i><br>
<i>&gt; &gt; world haven't died off with the coming of human beings.  </i><br>
<i>&gt; </i><br>
<i>&gt; No, but many *have* died. Besides, although clearly inferior, humans</i><br>
<i>&gt; would</i><br>
<i>&gt; probably still be a (potential) threat to SI (after all, they have</i><br>
<i>&gt; created it, or at least it's predecessors). Just like early man</i><br>
<i>&gt; hunted many dumber, yet clearly dangerous predators to extinction,</i><br>
<i>&gt; the SIs might decide to do the same. Just to be on the safe side.</i><br>
<i>&gt; After all, they won't dependent on the humans in any way.</i><br>
<p>
Actually, as I outlined in my essay they might very well be dependent<br>
or linked to humans for simple economical reasons. The law of<br>
comparative advantage clearly suggests that it would be more<br>
beneficial for the SI to trade with the humans than not; both sides<br>
can profit by specializing and depending on the other. <br>
<p>
If humans are a potential threat, then the SI would also be less<br>
motivated to wipe them out since the danger would be much larger. If<br>
second-strike capability exists, then the SI would be hurt even if it<br>
succeeds in wiping out the humans. And in order for the attack to be<br>
rational, the benefit (safety) must outweigh the negative utility<br>
(resources spent on attack, risk for losses or death, no more trading<br>
opportunities). Simple game theory.<br>
<p>
<pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
asa@nada.kth.se                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2550.html">Ralph Lewis: "Re: Creationists"</a>
<li> <b>Previous message:</b> <a href="2548.html">James Rogers: "Re: Creationists"</a>
<li> <b>In reply to:</b> <a href="2540.html">den Otter: "Re: &gt;H ART: The Truman Show"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
