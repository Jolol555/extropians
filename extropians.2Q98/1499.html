<!-- received="Thu May  7 19:53:50 1998 MDT" -->
<!-- sent="Wed, 06 May 1998 20:11:21 -0400" -->
<!-- name="Dan Clemmensen" -->
<!-- email="Dan@Clemmensen.ShireNet.com" -->
<!-- subject="Re: Hyper-AI's vs Transhumans" -->
<!-- id="199805080002.RAA28814@igce.igc.org" -->
<!-- inreplyto="Hyper-AI's vs Transhumans" -->
<title>extropians: Re: Hyper-AI's vs Transhumans</title>
<h1>Re: Hyper-AI's vs Transhumans</h1>
Dan Clemmensen (<i>Dan@Clemmensen.ShireNet.com</i>)<br>
<i>Wed, 06 May 1998 20:11:21 -0400</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1499">[ date ]</a><a href="index.html#1499">[ thread ]</a><a href="subject.html#1499">[ subject ]</a><a href="author.html#1499">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1500.html">Borys Wrobel: "Re: Hyper-AI's vs Transhumans"</a>
<li> <b>Previous message:</b> <a href="1498.html">Dan Clemmensen: "Re: Utopia"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1516.html">Anders Sandberg: "Re: Hyper-AI's vs Transhumans"</a>
<li> <b>Reply:</b> <a href="1516.html">Anders Sandberg: "Re: Hyper-AI's vs Transhumans"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Paul Hughes wrote:<br>
<i>&gt; </i><br>
<i>&gt; Dan Clemmensen wrote:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; IMO an SI will very likely start as an augmented human,</i><br>
<i>&gt; &gt; a collaboration between one human and a computer. This SI can</i><br>
<i>&gt; &gt; rapidly augment the computer part of itself, so there is no</i><br>
<i>&gt; &gt; reason to assume that a second SI will come into existence prior</i><br>
<i>&gt; &gt; to the first SI taking over the world, or that a de novo AI will</i><br>
<i>&gt; &gt; ever come into existence. The SI may choose to decouple from its</i><br>
<i>&gt; &gt; human component, but such a decision, and indeed any decision made</i><br>
<i>&gt; &gt; by the SI, is beyond analysis by humans.</i><br>
<i>&gt; &gt; Please see:   <a href="http://www.shirenet.com/~dgc/singularity/singularity.htm">http://www.shirenet.com/~dgc/singularity/singularity.htm</a></i><br>
<i>&gt; </i><br>
<i>&gt; I've read over your essay a couple of times now. Compelling.</i><br>
<i>&gt; </i><br>
<i>&gt; Are you suggesting that the singularity will essentially consist of only</i><br>
<i>&gt; one intelligence (i.e. one entity, one ego) with, if we are lucky, an</i><br>
<i>&gt; arbitrary number of uploaded humans as cells in its brain? A borganism?</i><br>
<i>&gt; </i><br>
This is my opinion. However, I realize that it is only one of a wide range<br>
of possibilities. There are a whole bunch of interesting things for use to<br>
consider, and in retrospect I feel that dispite my own admonishments to the<br>
contrary I have focused in on a particular point within the space of possible<br>
singularity initiators: I've picked the "single human" spot on the<br>
"initiator size" axis, and the "instantaneous" point on the "rate" axis.<br>
I've also picked the "single entity" on the "post-human population size"<br>
axis. All of these choices are in my little paper, but none of them are<br>
sufficiently explicit and I spend little time on explaining the alternatives.<br>
<p>
I rejected the "zero humans" point on the initiator axis because think AI<br>
(i.e., no human component of the SI) is harder to achieve than a collaboration.<br>
I rejected the larger numbers of humans because I guess I thought a multi-<br>
human collaborative component was harder than a single-human component, but<br>
I'd now guess that this is a weaker argument. Now, I would still assign<br>
a near-zero probablity to the pure AI and a large probability to an initiator<br>
size of one human, but I think a small group (say a workgroup at a lab) is<br>
possible, and a larger colloboration {say an internet mailing list :-) }<br>
is also possible.<br>
<p>
I picked the "instant" point on the rate axis for the reasons stated in the<br>
paper. Of course, I started with Vernor Vinge's "singularity" as part<br>
of the title of my paper, so I guess I'm biased. This choice is easy to<br>
attack on all kinds of perfectly reasonable grounds, mostly because the<br>
term "singluarity" isn't the correct mathematical/physical analogy. It's<br>
better to use a phase change as the analogy: The liquid-to-solid transition<br>
of a supercooled liguid when a seed crystal is added. The analogy is, I think,<br>
clear: the raw available computing power on the internet represents the<br>
supercooled liguid and some as-yet-undeveloped software represents the<br>
crystal.<br>
<p>
It might be nice if some of the participants from the prior round of the<br>
"&gt;Web" discussions would add any new thought they may have had on this matter.<br>
<p>
As to uploads: the SI may or may not choose to permit or compel uploads, and<br>
may or may not choose to permit the continued existence of humans in any form.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1500.html">Borys Wrobel: "Re: Hyper-AI's vs Transhumans"</a>
<li> <b>Previous message:</b> <a href="1498.html">Dan Clemmensen: "Re: Utopia"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1516.html">Anders Sandberg: "Re: Hyper-AI's vs Transhumans"</a>
<li> <b>Reply:</b> <a href="1516.html">Anders Sandberg: "Re: Hyper-AI's vs Transhumans"</a>
<!-- reply="end" -->
</ul>
