<!-- received="Mon Jun 22 17:27:42 1998 MDT" -->
<!-- sent="Mon, 22 Jun 1998 17:27:54 -0600 (MDT)" -->
<!-- name="Michael Nielsen" -->
<!-- email="mnielsen@tangelo.phys.unm.edu" -->
<!-- subject="Re: &gt;H ART: The Truman Show" -->
<!-- id="Pine.SUN.3.91.980622152943.2711C-100000@tangelo.phys.unm.edu" -->
<!-- inreplyto="358EB793.39ED@geocities.com" -->
<title>extropians: Re: &gt;H ART: The Truman Show</title>
<h1>Re: &gt;H ART: The Truman Show</h1>
Michael Nielsen (<i>mnielsen@tangelo.phys.unm.edu</i>)<br>
<i>Mon, 22 Jun 1998 17:27:54 -0600 (MDT)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2520">[ date ]</a><a href="index.html#2520">[ thread ]</a><a href="subject.html#2520">[ subject ]</a><a href="author.html#2520">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2521.html">den Otter: "The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Previous message:</b> <a href="2519.html">Dan Clemmensen: "Re: Turtles, turtles, turtles... (was Re: Creationists)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Mon, 22 Jun 1998, den Otter wrote:<br>
<p>
<i>&gt; Michael Nielsen wrote:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; Is it ethical to contain an AI in a limited world? This is an especially</i><br>
<i>&gt; &gt; interesting question if one takes the point of view that the most likely</i><br>
<i>&gt; &gt; path to Artificial Intelligence is an approach based on evolutionary</i><br>
<i>&gt; &gt; programming.</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Is it ethical to broadcast details of an AI's "life" to other</i><br>
<i>&gt; &gt; researchers or interested parties?</i><br>
<i>&gt; &gt; </i><br>
<i>&gt; &gt; Is it ethical to profit from the actions of an AI?</i><br>
<i>&gt; </i><br>
<i>&gt; Since AIs will presumably be made without emotions, or at least with</i><br>
<i>&gt; a much more limited number of emotions than humans, you don't have</i><br>
<i>&gt; to worry about their "feelings".</i><br>
<p>
For the record, I may as well note that I think this is a highly <br>
questionable assumption.  On what do you base it?<br>
<p>
One thing I don't doubt is that AIs will exhibit occasionally strange <br>
behaviour.  Even "rational" behaviour (whatever that means) is <br>
surprisingly subjective, depending as it does upon what information is <br>
avaliable. Small variations in the available information can have a <br>
large impact on behaviour, even if that behaviour is governed by a small <br>
number of rigidly adhered-to rules.<br>
<p>
In turn, the available information varies quite a bit from intelligence <br>
to intelligence, as does the available resources which can be devoted to <br>
analysis.<br>
<p>
One final questgion, before moving on to your next comment: Upon what do <br>
we base our values, if not some form of emotional / irrational <br>
attachment? It is certainly advantageous to have a reasonably strongly <br>
held value system; apathy and inaction is the alternative.   Emotions <br>
seem to be a key factor in maintaining such value systems.<br>
<p>
<i>&gt; Also, one of the first things you</i><br>
<i>&gt; would ask an AI is to develop uploading &amp; computer-neuron interfaces,</i><br>
<i>&gt; so that you can make the AI's intelligence part of your own. This would</i><br>
<i>&gt; pretty much solve the whole "rights problem" (which is largely</i><br>
<i>&gt; artificial anyway),</i><br>
<p>
What do you mean, the rights problem is "artificial"? <br>
<p>
<i>&gt; since you don't grant rights to specific parts</i><br>
<i>&gt; of your brain. A failure to integrate with the AIs asap would </i><br>
<i>&gt; undoubtedly result in AI domination, and human extinction.</i><br>
<p>
This seems to be an unjustified assumption.  All other forms of life in the <br>
world haven't died off with the coming of human beings.  Some of our near <br>
relatives amongst the primates are still doing okay.<br>
<p>
<i>&gt; P.s: I'm almost certain that our ethics will become obsolete with</i><br>
<i>&gt; the rise of SI, they are simply too much shaped by our specific</i><br>
<i>&gt; evolution etc.</i><br>
<p>
This may be a tip as to why an SI may share our ethics: their <br>
evolutionary path includes us.  It depends upon how fast their own <br>
evolution continues.<br>
<p>
Michael Nielsen<br>
<p>
<a href="http://wwwcas.phys.unm.edu/~mnielsen/index.html">http://wwwcas.phys.unm.edu/~mnielsen/index.html</a><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2521.html">den Otter: "The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Previous message:</b> <a href="2519.html">Dan Clemmensen: "Re: Turtles, turtles, turtles... (was Re: Creationists)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
