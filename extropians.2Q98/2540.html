<!-- received="Tue Jun 23 04:17:13 1998 MDT" -->
<!-- sent="Tue, 23 Jun 1998 12:14:12 +0200" -->
<!-- name="den Otter" -->
<!-- email="neosapient@geocities.com" -->
<!-- subject="Re: &gt;H ART: The Truman Show" -->
<!-- id="3.0.3.32.19980623041154.0309e594@208.198.32.3" -->
<!-- inreplyto="&gt;H ART: The Truman Show" -->
<title>extropians: Re: &gt;H ART: The Truman Show</title>
<h1>Re: &gt;H ART: The Truman Show</h1>
den Otter (<i>neosapient@geocities.com</i>)<br>
<i>Tue, 23 Jun 1998 12:14:12 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2540">[ date ]</a><a href="index.html#2540">[ thread ]</a><a href="subject.html#2540">[ subject ]</a><a href="author.html#2540">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2541.html">Anders Sandberg: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Previous message:</b> <a href="2539.html">Ian Goddard: "Special Relativity"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2549.html">Anders Sandberg: "Re: &gt;H ART: The Truman Show"</a>
<li> <b>Reply:</b> <a href="2549.html">Anders Sandberg: "Re: &gt;H ART: The Truman Show"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Michael Nielsen wrote:<br>
<i>&gt; </i><br>
<i>&gt; On Mon, 22 Jun 1998, den Otter wrote:</i><br>
<p>
<i>&gt; &gt; Since AIs will presumably be made without emotions, or at least with</i><br>
<i>&gt; &gt; a much more limited number of emotions than humans, you don't have</i><br>
<i>&gt; &gt; to worry about their "feelings".</i><br>
<i>&gt; </i><br>
<i>&gt; For the record, I may as well note that I think this is a highly</i><br>
<i>&gt; questionable assumption.  On what do you base it?</i><br>
<p>
The fact that our brains happen to work with certain emotions doesn't<br>
automatically mean that this is the *only* way to achieve intelligence.<br>
Evolution is blind, doesn't seek perfection like we do. Humans have<br>
surpassed nature in many other areas, so why not with AI? <br>
 <br>
<i>&gt; One final questgion, before moving on to your next comment: Upon what do</i><br>
<i>&gt; we base our values, if not some form of emotional / irrational</i><br>
<i>&gt; attachment? It is certainly advantageous to have a reasonably strongly</i><br>
<i>&gt; held value system; apathy and inaction is the alternative.   Emotions</i><br>
<i>&gt; seem to be a key factor in maintaining such value systems.</i><br>
<p>
Well, I *don't want* AIs to resemble humans with their complex emotion-<br>
based value systems. We need obedient servants, not competition. So if <br>
it turns out that you *can* have intelligence without a "will", then <br>
that should be used to make useful "genie-AIs". If this isn't possible,<br>
it might be better to make no AIs at all.<br>
 <br>
<i>&gt; &gt; Also, one of the first things you</i><br>
<i>&gt; &gt; would ask an AI is to develop uploading &amp; computer-neuron interfaces,</i><br>
<i>&gt; &gt; so that you can make the AI's intelligence part of your own. This would</i><br>
<i>&gt; &gt; pretty much solve the whole "rights problem" (which is largely</i><br>
<i>&gt; &gt; artificial anyway),</i><br>
<i>&gt; </i><br>
<i>&gt; What do you mean, the rights problem is "artificial"?</i><br>
<p>
It means it's not an "absolute" problem like escaping the earth's<br>
gravity, or breathing under water etc. It is only a problem<br>
because certain people *make* it a problem, just like there<br>
was no real "drug problem" before the war on drugs started.<br>
Calling something a problem is more or less a self-fulfilling<br>
prophecy. Unless the AIs start demanding rights themselves,<br>
there is no reason to grant them any. If we're smart, we'll<br>
make the AIs so that they'll never feel the need to do this.<br>
<p>
<i>&gt; &gt; since you don't grant rights to specific parts</i><br>
<i>&gt; &gt; of your brain. A failure to integrate with the AIs asap would</i><br>
<i>&gt; &gt; undoubtedly result in AI domination, and human extinction.</i><br>
<i>&gt; </i><br>
<i>&gt; This seems to be an unjustified assumption.  All other forms of life in the</i><br>
<i>&gt; world haven't died off with the coming of human beings.  </i><br>
<p>
No, but many *have* died. Besides, although clearly inferior, humans<br>
would<br>
probably still be a (potential) threat to SI (after all, they have<br>
created it, or at least it's predecessors). Just like early man<br>
hunted many dumber, yet clearly dangerous predators to extinction,<br>
the SIs might decide to do the same. Just to be on the safe side.<br>
After all, they won't dependent on the humans in any way.<br>
<p>
<i>&gt;&gt;Some of our near relatives amongst the primates are still doing okay.&lt;&lt;</i><br>
<p>
Yes, after being nearly hunted to extinction they now get the privilege<br>
to sit in cages for our amusement, or (if they're really lucky) they<br>
may hang around in their original habitat, with only the occasional<br>
poacher or camera team coming by. What a glorious existence!<br>
 <br>
<i>&gt; &gt; P.s: I'm almost certain that our ethics will become obsolete with</i><br>
<i>&gt; &gt; the rise of SI, they are simply too much shaped by our specific</i><br>
<i>&gt; &gt; evolution etc.</i><br>
<i>&gt; </i><br>
<i>&gt; This may be a tip as to why an SI may share our ethics: their</i><br>
<i>&gt; evolutionary path includes us.  It depends upon how fast their own</i><br>
<i>&gt; evolution continues.</i><br>
<p>
An electronic SI (or even AI) will be able to outperform, at least in<br>
matters of speed, raw power &amp; capacity any (biological) human by many<br>
orders of magnitude. Therefore, it's better to make sure that the<br>
first SIs will be *us*. <br>
<p>
As for our ethics: most of them are shaped by our weaknesses and<br>
resulting interdependence. A SI could very well be the first<br>
(intelligent) life form that is totally self-sufficient, and <br>
thus independent. This definitely changes the rules of the game.<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2541.html">Anders Sandberg: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Previous message:</b> <a href="2539.html">Ian Goddard: "Special Relativity"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2549.html">Anders Sandberg: "Re: &gt;H ART: The Truman Show"</a>
<li> <b>Reply:</b> <a href="2549.html">Anders Sandberg: "Re: &gt;H ART: The Truman Show"</a>
<!-- reply="end" -->
</ul>
