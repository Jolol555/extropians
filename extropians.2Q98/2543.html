<!-- received="Tue Jun 23 05:43:25 1998 MDT" -->
<!-- sent="23 Jun 1998 13:43:17 +0200" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: &gt;H ART: The Truman Show" -->
<!-- id="3.0.3.32.19980623041154.0309e594@208.198.32.3" -->
<!-- inreplyto="Mon, 22 Jun 1998 18:29:25 -0600 (MDT)" -->
<title>extropians: Re: &gt;H ART: The Truman Show</title>
<h1>Re: &gt;H ART: The Truman Show</h1>
Anders Sandberg (<i>asa@nada.kth.se</i>)<br>
<i>23 Jun 1998 13:43:17 +0200</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2543">[ date ]</a><a href="index.html#2543">[ thread ]</a><a href="subject.html#2543">[ subject ]</a><a href="author.html#2543">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2544.html">Anders Sandberg: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Previous message:</b> <a href="2542.html">den Otter: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>In reply to:</b> <a href="2523.html">Michael Nielsen: "Re: &gt;H ART: The Truman Show"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Michael Nielsen &lt;mnielsen@tangelo.phys.unm.edu&gt; writes:<br>
<p>
<i>&gt; An interesting little psyche experiment would be to equip a souped-up </i><br>
<i>&gt; Eliza program (possibly with voice synthesis, to make the computer much </i><br>
<i>&gt; more human) with the ability to detect when the user was getting ready to </i><br>
<i>&gt; turn the program off, or shut down the computer.  One can imagine the </i><br>
<i>&gt; conversation:</i><br>
<i>&gt; </i><br>
<i>&gt; "Oh My Turing, no! Stop! Don't turn me o.. &lt;NO SIGNAL&gt; "</i><br>
<i>&gt; </i><br>
<i>&gt; I do wonder what the response of human subjects would be, and what </i><br>
<i>&gt; parameters would result in a maximal unwillingness to turn the program </i><br>
<i>&gt; off.</i><br>
<p>
I think there was a short story in the anthology _The Mind's I_<br>
(Dennet and Hofstadter eds.) where somebody claimed computers were<br>
mindless and just machines, but felt compassion and pangs of<br>
conscience when given a mallet to smash a small robot. It could be<br>
that just a few simple behaviors (like trying escape damage, looking<br>
vulnerable etc) are enough to make humans regard the system as worthy<br>
compassion.<br>
<p>
<i>&gt; &gt; I must admit I have no good consistent idea about how to answer these</i><br>
<i>&gt; &gt; questions. We might need to create AIs in limited worlds, and they</i><br>
<i>&gt; &gt; might be extremely useful in there. "Opening the box" and allowing</i><br>
<i>&gt; &gt; them out might be troubling for them, but at the same time that would</i><br>
<i>&gt; &gt; put them on an equal footing (at least equal ontological footing) with</i><br>
<i>&gt; &gt; us, suggesting that at least then they should definitely get</i><br>
<i>&gt; &gt; rights. The problem here seems to be that although the AIs are</i><br>
<i>&gt; &gt; rational subjects the rules for rights and ethics we have developed</i><br>
<i>&gt; &gt; doesn't seem to work well across ontological levels.</i><br>
<i>&gt; </i><br>
<i>&gt; Could you give me some explicit examples of what you have in mind, as I'm </i><br>
<i>&gt; not sure I see what you're getting at?</i><br>
<p>
Are authors responsible for what happens to their fictive characters?<br>
They certainly decide what will happen, but nobody has so far claimed<br>
it is unethical to give a character a tragic ending (just imagine<br>
being protagonist in an Iain Banks novel!). But what if the characters<br>
are low-level AI programs with motivation systems running in a virtual<br>
world generating story? What if they are of comparable complexity to<br>
humans? David Brin has written an excellent short story about this<br>
problem, "Stones of Significance" (I'm not sure it has been published<br>
yet).<br>
<p>
A classic example would otherwise be the relationship between a god<br>
and humans. Is God pantheon ethically responsible for the state of the<br>
world? What rights do humans have versus God, and vice versa? Or does<br>
God by definition stand above any ethics, instead being the<br>
fundamental ethical bedrock we should start from, as many<br>
theologicians claim? To put it bluntly: if God decides that rape and<br>
destruction of art is good, will it really be ethically good?<br>
<p>
<i>&gt; I definitely agree that a literal translation of many ethical rules would </i><br>
<i>&gt; lead to some absurdities.  However, at the level of general principles, </i><br>
<i>&gt; it seems as though the translations may be easier to make.  The detailed </i><br>
<i>&gt; rules would then follow from the general principles.</i><br>
<p>
Quite likely. We need a set of principles that can deal with different<br>
entities of very different capacity and motivation, which exist on<br>
different ontological levels and might be created by each<br>
other. Tricky, but might be doable.<br>
<p>
<pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
asa@nada.kth.se                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2544.html">Anders Sandberg: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>Previous message:</b> <a href="2542.html">den Otter: "Re: The AI revolution (Was: Re: &gt;H ART: The Truman Show)"</a>
<li> <b>In reply to:</b> <a href="2523.html">Michael Nielsen: "Re: &gt;H ART: The Truman Show"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
