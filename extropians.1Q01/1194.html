<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Paradox--was Re: Active shields, was Re: Critic</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Paradox--was Re: Active shields, was Re: Criticism depth, was    Re:     Homework, Nuke,  etc..">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Paradox--was Re: Active shields, was Re: Criticism depth, was    Re:     Homework, Nuke,  etc..</h1>
<!-- received="Sun Jan 14 00:36:01 2001" -->
<!-- isoreceived="20010114073601" -->
<!-- sent="Sun, 14 Jan 2001 02:36:03 -0500" -->
<!-- isosent="20010114073603" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Paradox--was Re: Active shields, was Re: Criticism depth, was    Re:     Homework, Nuke,  etc.." -->
<!-- id="3A6156E3.200E174F@pobox.com" -->
<!-- inreplyto="20010114051737.3006.qmail@web4403.mail.yahoo.com" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Paradox--was%20Re:%20Active%20shields,%20was%20Re:%20Criticism%20depth,%20was%20%20%20%20Re:%20%20%20%20%20Homework,%20Nuke,%20%20etc..&In-Reply-To=&lt;3A6156E3.200E174F@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 14 2001 - 00:36:03 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1195.html">Emlyn: "Re: &quot;God Does Not Play Dice...&quot;   AE"</a>
<li><strong>Previous message:</strong> <a href="1193.html">John  M Grigg: "GM food:  the Outer Limits episode tonight revolves around it"</a>
<li><strong>In reply to:</strong> <a href="1185.html">John Marlow: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was   Re:     Homework, Nuke,  etc.."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1198.html">John Marlow: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was    Re:     Homework, Nuke,  etc.."</a>
<li><strong>Maybe reply:</strong> <a href="1198.html">John Marlow: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was    Re:     Homework, Nuke,  etc.."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1194">[ date ]</a>
<a href="index.html#1194">[ thread ]</a>
<a href="subject.html#1194">[ subject ]</a>
<a href="author.html#1194">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
John Marlow wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; **One: Yeah--the thing behaves like a human because
</em><br>
<em>&gt; it's programmed by humans and likely monkeyed around
</em><br>
<em>&gt; with by military types, whose thinking I don't care
</em><br>
<em>&gt; for.
</em><br>
<p>I hope not.  I hope that we can finish before the military types start
<br>
monkeying.
<br>
<p><em>&gt; **Two: However the thing starts off, it glitches/goes
</em><br>
<em>&gt; chaotic/turns psychotic and can't be shut off.
</em><br>
<p>Yes, that is the risk.  It is an *unavoidable* risk - at least,
<br>
unavoidable except by exterminating all sentient life in the solar
<br>
system.  It was implicit in the dawn of intelligence - the moment when
<br>
intelligence begins to enhance itself.
<br>
<p><em>&gt; **Three: The thing behaves NOTHING like a human--is,
</em><br>
<em>&gt; in fact, completely alien. The basis of its decisions
</em><br>
<em>&gt; will therefore be unknowable.
</em><br>
<p>Nothing is completely knowable.  The basis of a Friendly AI's decisions is
<br>
certainly not completely unknowable.  Beating the maximum human kindliness
<br>
is probably more than good enough.
<br>
<p><em>&gt; It might push us to the
</em><br>
<em>&gt; limits of the universe and help us create an
</em><br>
<em>&gt; everlasting utopia--or exterminate us all tomorrow for
</em><br>
<em>&gt; a reason we couldn't even guess BECAUSE it's nothing
</em><br>
<em>&gt; like a human.
</em><br>
<p>Yes, that is the Great Coinflip.  Again, my point is that this is an
<br>
ineradicable coinflip unless you really think that humanity can spend the
<br>
next billion years at exactly this intelligence level.  If not, sooner or
<br>
later we need to confront transhumanity.  Every extra year is another year
<br>
we have the opportunity to exterminate ourselves, so sooner is better. 
<br>
Can't make it to utopia if the goo eats through an artery.
<br>
<p><em>&gt; It is therefore a constant threat whose
</em><br>
<em>&gt; level at any given instant is not unly unknown but
</em><br>
<em>&gt; unknowable. And, being alien--it may well view us the
</em><br>
<em>&gt; same way.
</em><br>
<p>Why are you attributing anthropomorphic xenophobia to an alien you just
<br>
got through describing as unknowable?
<br>
<p><em>&gt; **Never said it would. The more power it/they have,
</em><br>
<em>&gt; the greater the danger, was my point.
</em><br>
<p>Anything above and beyond the simple fact of superintelligence is
<br>
irrelevant overkill.
<br>
<p><em>&gt; &gt; As it happens, AI is much less risky than
</em><br>
<em>&gt; &gt; nanotechnology.  I think I know
</em><br>
<em>&gt; &gt; how to build a Friendly AI.  I don't see *any*
</em><br>
<em>&gt; &gt; development scenario for
</em><br>
<em>&gt; &gt; nanotechnology that doesn't end in a short war.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; **Yah--a VERY short war...
</em><br>
<p>Yes, that was the implication.
<br>
<p><em>&gt; &gt; Offensive technology has
</em><br>
<em>&gt; &gt; overpowered defensive technology ever since the
</em><br>
<em>&gt; &gt; invention of the ICBM, and
</em><br>
<em>&gt; &gt; nanotechnology (1) makes the imbalance even worse
</em><br>
<em>&gt; &gt; and (2) removes all the
</em><br>
<em>&gt; &gt; stabilizing factors that have prevented nuclear war
</em><br>
<em>&gt; &gt; so far.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; **Bingo.
</em><br>
<p>Okay, you get this, you get the massive cloud of doom, you even get the
<br>
part where transhumanity is our chance out of it, so why are you still
<br>
focusing on the *totally ineradicable* risk of unfriendly transhumanity?
<br>
<p><em>&gt;   If nanotech,
</em><br>
<em>&gt; &gt; we're screwed.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; **If nano goes amok or is in the wrong hands, you
</em><br>
<em>&gt; mean, I think. Of course, any hands may be wrong
</em><br>
<em>&gt; hands.
</em><br>
<p>Let's see, what is the probability that nano winds up in at least one pair
<br>
of wrong hands... this is Earth, right?  We're screwed.
<br>
<p><em>&gt;   The probability of humanity's
</em><br>
<em>&gt; &gt; survival is the probability
</em><br>
<em>&gt; &gt; that AI comes first times the probability that
</em><br>
<em>&gt; &gt; Friendly AI is possible
</em><br>
<em>&gt; &gt; times the probability that Friendliness is
</em><br>
<em>&gt; &gt; successfully implemented by the
</em><br>
<em>&gt; &gt; first group to create AI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; **What, in your estimation, is the probability that
</em><br>
<em>&gt; military interests will seize the operation/the AI at
</em><br>
<em>&gt; the critical time? That they have/will infiltrate
</em><br>
<em>&gt; AI-creation efforts for this and monitoring purposes?
</em><br>
<em>&gt; Remember--they're thinking like hunter-gatherers,
</em><br>
<em>&gt; too...
</em><br>
<p>This requires a military mind sufficiently intelligent to get why AI is
<br>
important and sufficiently stupid to not get why Friendliness is
<br>
important.  Besides, what would you have me do about it?
<br>
<p><em>&gt;   If Friendly AI is
</em><br>
<em>&gt; &gt; impossible, then humanity is
</em><br>
<em>&gt; &gt; screwed, period.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; **Best info sources on this issue and on your take of
</em><br>
<em>&gt; this issue? On AI/SI?
</em><br>
<p>I thought you'd never ask.  I'd recommend:
<br>
<p><a href="http://singinst.org/intro.html">http://singinst.org/intro.html</a>
<br>
<a href="http://sysopmind.com/sing/PtS/navigation/deadlines.html">http://sysopmind.com/sing/PtS/navigation/deadlines.html</a>
<br>
<a href="http://singinst.org/CaTAI.html">http://singinst.org/CaTAI.html</a>
<br>
<p>You can find some non-Yudkowskian material at:
<br>
<p><a href="http://dmoz.org/Society/Philosophy/Current_Movements/Transhumanism/">http://dmoz.org/Society/Philosophy/Current_Movements/Transhumanism/</a>
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1195.html">Emlyn: "Re: &quot;God Does Not Play Dice...&quot;   AE"</a>
<li><strong>Previous message:</strong> <a href="1193.html">John  M Grigg: "GM food:  the Outer Limits episode tonight revolves around it"</a>
<li><strong>In reply to:</strong> <a href="1185.html">John Marlow: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was   Re:     Homework, Nuke,  etc.."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1198.html">John Marlow: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was    Re:     Homework, Nuke,  etc.."</a>
<li><strong>Maybe reply:</strong> <a href="1198.html">John Marlow: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was    Re:     Homework, Nuke,  etc.."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1194">[ date ]</a>
<a href="index.html#1194">[ thread ]</a>
<a href="subject.html#1194">[ subject ]</a>
<a href="author.html#1194">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:56:19 MDT</em>
</em>
</small>
</body>
</html>
