<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=Windows-1252">
<title>extropians: Re: Kurzweil's new Singularity/AI page</title>
<meta name="Author" content="J. R. Molloy (jr@shasta.com)">
<meta name="Subject" content="Re: Kurzweil's new Singularity/AI page">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Kurzweil's new Singularity/AI page</h1>
<!-- received="Sun Feb 25 12:07:54 2001" -->
<!-- isoreceived="20010225190754" -->
<!-- sent="Sat, 24 Feb 2001 15:57:39 -0800" -->
<!-- isosent="20010224235739" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: Kurzweil's new Singularity/AI page" -->
<!-- id="006901c09ebd$95dd74e0$c45c2a42@jrmolloy" -->
<!-- charset="Windows-1252" -->
<!-- inreplyto="200102242320.PAA17715@finney.org" -->
<strong>From:</strong> J. R. Molloy (<a href="mailto:jr@shasta.com?Subject=Re:%20Kurzweil's%20new%20Singularity/AI%20page&In-Reply-To=&lt;006901c09ebd$95dd74e0$c45c2a42@jrmolloy&gt;"><em>jr@shasta.com</em></a>)<br>
<strong>Date:</strong> Sat Feb 24 2001 - 16:57:39 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4700.html">Max More: "The non-existence of posthumans [was: Re: Heston Speech]"</a>
<li><strong>Previous message:</strong> <a href="4698.html">Emlyn: "Jonathan Lebed article on NYT"</a>
<li><strong>In reply to:</strong> <a href="4656.html">hal@finney.org: "Kurzweil's new Singularity/AI page"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4699">[ date ]</a>
<a href="index.html#4699">[ thread ]</a>
<a href="subject.html#4699">[ subject ]</a>
<a href="author.html#4699">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
<em>&gt; I have to admit he hooked me with this.  Not to give anything away, I
</em><br>
must
<br>
<em>&gt; reveal that I don't have an extra $40 trillion in my pockets right now.
</em><br>
<em>&gt; But it's a pretty good essay anyway.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Hal
</em><br>
<em>&gt;
</em><br>
<p>You never know when these things might disappear from the Net, so here's
<br>
the whole thing:
<br>
<p><p><a href="http://www.kurzweilai.net/meme1/frame.html?main=/articles/art0012.html">http://www.kurzweilai.net/meme1/frame.html?main=/articles/art0012.html</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The Singularity is Near
<br>
A Book Précis by   Raymond Kurzweil
<br>
<p>You will get $40 trillion just by reading this précis and understanding
<br>
what it says. For complete details, see below. (It's true that authors
<br>
will do just about anything to keep your attention, but I'm serious about
<br>
this statement. Until I return to a further explanation, however, do read
<br>
the first sentence of this paragraph carefully.)
<br>
<p>Now back to the future: it's widely misunderstood. Our forebears expected
<br>
the future to be pretty much like their present, which had been pretty
<br>
much like their past. Although exponential trends did exist a thousand
<br>
years ago, they were at that very early stage where an exponential trend
<br>
is so flat that it looks like no trend at all. So their lack of
<br>
expectations was largely fulfilled. Today, in accordance with the common
<br>
wisdom, everyone expects continuous technological progress and the social
<br>
repercussions that follow. But the future will be far more surprising than
<br>
most observers realize: few have truly internalized the implications of
<br>
the fact that the rate of change itself is accelerating.
<br>
<p>The Intuitive Linear View versus the Historical Exponential View
<br>
Most long range forecasts of technical feasibility in future time periods
<br>
dramatically underestimate the power of future technology because they are
<br>
based on what I call the &quot;intuitive linear&quot; view of technological progress
<br>
rather than the &quot;historical exponential view.&quot; To express this another
<br>
way, it is not the case that we will experience a hundred years of
<br>
progress in the twenty-first century; rather we will witness on the order
<br>
of twenty thousand years of progress (at today's rate of progress, that
<br>
is).
<br>
<p>This disparity in outlook comes up frequently in a variety of contexts,
<br>
for example, the discussion of the ethical issues that Bill Joy raised in
<br>
his controversial WIRED cover story, Why The Future Doesn't Need Us. Bill
<br>
and I have been frequently paired in a variety of venues as pessimist and
<br>
optimist respectively. Although I'm expected to criticize Bill's position,
<br>
and indeed I do take issue with his prescription of relinquishment, I
<br>
nonetheless usually end up defending Joy on the key issue of feasibility.
<br>
Recently a Noble Prize winning panelist dismissed Bill's concerns,
<br>
exclaiming that, &quot;we're not going to see self-replicating nanoengineered
<br>
entities for a hundred years.&quot; I pointed out that 100 years was indeed a
<br>
reasonable estimate of the amount of technical progress required to
<br>
achieve this particular milestone at today's rate of progress. But because
<br>
we're doubling the rate of progress every decade, we'll see a century of
<br>
progress--at today's rate--in only 25 calendar years.
<br>
<p>When people think of a future period, they intuitively assume that the
<br>
current rate of progress will continue for future periods. However,
<br>
careful consideration of the pace of technology shows that the rate of
<br>
progress is not constant, but it is human nature to adapt to the changing
<br>
pace, so the intuitive view is that the pace will continue at the current
<br>
rate. Even for those of us who have been around long enough to experience
<br>
how the pace increases over time, our unexamined intuition nonetheless
<br>
provides the impression that progress changes at the rate that we have
<br>
experienced recently. From the mathematician's perspective, a primary
<br>
reason for this is that an exponential curve approximates a straight line
<br>
when viewed for a brief duration. So even though the rate of progress in
<br>
the very recent past (e.g., this past year) is far greater than it was ten
<br>
years ago (let alone a hundred or a thousand years ago), our memories are
<br>
nonetheless dominated by our very recent experience. It is typical,
<br>
therefore, that even sophisticated commentators, when considering the
<br>
future, extrapolate the current pace of change over the next 10 years or
<br>
100 years to determine their expectations. This is why I call this way of
<br>
looking at the future the &quot;intuitive linear&quot; view.
<br>
<p>But a serious assessment of the history of technology shows that
<br>
technological change is exponential. In exponential growth, we find that a
<br>
key measurement such as computational power is multiplied by a constant
<br>
factor for each unit of time (e.g., doubling every year) rather than just
<br>
being added to incrementally. Exponential growth is a feature of any
<br>
evolutionary process, of which technology is a primary example. One can
<br>
examine the data
<br>
in different ways, on different time scales, and for a wide variety of
<br>
technologies ranging from electronic to biological, and the acceleration
<br>
of progress and growth applies. Indeed, we find not just simple
<br>
exponential growth, but &quot;double&quot; exponential growth, meaning that the rate
<br>
of exponential growth is itself growing exponentially. These observations
<br>
do not rely merely on an assumption of the continuation of Moore's law
<br>
(i.e., the exponential shrinking of transistor sizes on an integrated
<br>
circuit), but is based on a rich model of diverse technological processes.
<br>
What it clearly shows is that technology, particularly the pace of
<br>
technological change, advances (at least) exponentially, not linearly, and
<br>
has been doing so since the advent of technology, indeed since the advent
<br>
of evolution on Earth.
<br>
<p>I emphasize this point because it is the most important failure that
<br>
would-be prognosticators make in considering future trends. Most
<br>
technology forecasts ignore altogether this &quot;historical exponential view&quot;
<br>
of technological progress. That is why people tend to overestimate what
<br>
can be achieved in the short term (because we tend to leave out necessary
<br>
details), but underestimate what can be achieved in the long term (because
<br>
the exponential growth is ignored).
<br>
<p>The Law of Accelerating Returns
<br>
We can organize these observations into what I call the law of
<br>
accelerating returns as follows:
<br>
Evolution applies positive feedback in that the more capable methods
<br>
resulting from one stage of evolutionary progress are used to create the
<br>
next stage. As a result, the
<br>
rate of progress of an evolutionary process increases exponentially over
<br>
time. Over time, the &quot;order&quot; of the information embedded in the
<br>
evolutionary process (i.e., the measure of how well the information fits a
<br>
purpose, which in evolution is survival) increases.
<br>
A correlate of the above observation is that the &quot;returns&quot; of an
<br>
evolutionary process (e.g., the speed, cost-effectiveness, or overall
<br>
&quot;power&quot; of a process) increase exponentially over time.
<br>
In another positive feedback loop, as a particular evolutionary process
<br>
(e.g., computation) becomes more effective (e.g., cost effective), greater
<br>
resources are deployed toward the further progress of that process. This
<br>
results in a second level of exponential growth (i.e., the rate of
<br>
exponential growth itself grows exponentially).
<br>
Biological evolution is one such evolutionary process.
<br>
Technological evolution is another such evolutionary process. Indeed, the
<br>
emergence of the first technology creating species resulted in the new
<br>
evolutionary process of technology. Therefore, technological evolution is
<br>
an outgrowth of--and a continuation of--biological evolution.
<br>
A specific paradigm (a method or approach to solving a problem, e.g.,
<br>
shrinking transistors on an integrated circuit as an approach to making
<br>
more powerful computers) provides exponential growth until the method
<br>
exhausts its potential. When this happens, a paradigm shift (i.e., a
<br>
fundamental change in the approach) occurs, which enables exponential
<br>
growth to continue.
<br>
If we apply these principles at the highest level of evolution on Earth,
<br>
the first step, the creation of cells, introduced the paradigm of biology.
<br>
The subsequent emergence of DNA provided a digital method to record the
<br>
results of evolutionary experiments. Then, the evolution of a species who
<br>
combined rational thought with an opposable appendage (i.e., the thumb)
<br>
caused a fundamental paradigm shift from biology to technology. The
<br>
upcoming primary paradigm shift will be from biological thinking to a
<br>
hybrid combining biological and nonbiological thinking. This hybrid will
<br>
include &quot;biologically inspired&quot; processes resulting from the reverse
<br>
engineering of biological brains.
<br>
<p>If we examine the timing of these steps, we see that the process has
<br>
continuously accelerated. The evolution of life forms required billions of
<br>
years for the first steps (e.g., primitive cells); later on progress
<br>
accelerated. During the Cambrian explosion, major paradigm shifts took
<br>
only tens of millions of years. Later on, Humanoids developed over a
<br>
period of millions of years, and Homo sapiens over a period of only
<br>
hundreds of thousands of years.
<br>
<p>With the advent of a technology-creating species, the exponential pace
<br>
became too fast for evolution through DNA-guided protein synthesis and
<br>
moved on to human-created technology. Technology goes beyond mere tool
<br>
making; it is a process of creating ever more powerful technology using
<br>
the tools from the previous round of innovation. In this way, human
<br>
technology is distinguished from the tool making of other species. There
<br>
is a record of each stage of technology, and each new stage of technology
<br>
builds on the order of the previous stage.
<br>
<p>The first technological steps-sharp edges, fire, the wheel--took tens of
<br>
thousands of years. For people living in this era, there was little
<br>
noticeable technological change in even a thousand years. By 1000 A.D.,
<br>
progress was much faster and a paradigm shift required only a century or
<br>
two. In the nineteenth century, we saw more technological change than in
<br>
the nine centuries preceding it. Then in the first twenty years of the
<br>
twentieth century, we saw more advancement than in all of the nineteenth
<br>
century. Now, paradigm shifts occur in only a few years time. The World
<br>
Wide Web did not exist in anything like its present form just a few years
<br>
ago; it didn't exist at all a decade ago.
<br>
<p><p>The paradigm shift rate (i.e., the overall rate of technical progress) is
<br>
currently doubling (approximately) every decade; that is, paradigm shift
<br>
times are halving every decade (and the rate of acceleration is itself
<br>
growing exponentially). So, the technological progress in the twenty-first
<br>
century will be equivalent to what would require (in the linear view) on
<br>
the order of 200 centuries. In contrast, the twentieth century saw only
<br>
about 25 years of progress (again at today's rate of progress) since we
<br>
have been speeding up to current rates. So the twenty-first century will
<br>
see almost a thousand times greater technological change than its
<br>
predecessor.
<br>
<p><p>The Singularity Is Near
<br>
To appreciate the nature and significance of the coming &quot;singularity,&quot; it
<br>
is important to ponder the nature of exponential growth. Toward this end,
<br>
I am fond of telling the tale of the inventor of chess and his patron, the
<br>
emperor of China. In response to the emperor's offer of a reward for his
<br>
new beloved game, the inventor asked for a single grain of rice on the
<br>
first square, two on the second square, four on the third, and so on. The
<br>
Emperor quickly granted this seemingly benign and humble request. One
<br>
version of the story has the emperor going bankrupt as the 63 doublings
<br>
ultimately totaled 18 million trillion grains of rice. At ten grains of
<br>
rice per square inch, this requires rice fields covering twice the surface
<br>
area of the Earth, oceans included. Another version of the story has the
<br>
inventor losing his head.
<br>
<p>It should be pointed out that as the emperor and the inventor went through
<br>
the first half of the chess board, things were fairly uneventful. The
<br>
inventor was given spoonfuls of rice, then bowls of rice, then barrels. By
<br>
the end of the first half of the chess board, the inventor had accumulated
<br>
one large field's worth (4 billion grains), and the emperor did start to
<br>
take notice. It was as they progressed through the second half of the
<br>
chessboard that the situation quickly deteriorated. Incidentally, with
<br>
regard to the doublings of computation, that's about where we stand
<br>
now--there have been slightly more than 32 doublings of performance since
<br>
the first programmable computers were invented during World War II.
<br>
<p>This is the nature of exponential growth. Although technology grows in the
<br>
exponential domain, we humans live in a linear world. So technological
<br>
trends are not noticed as small levels of technological power are doubled.
<br>
Then seemingly out of nowhere, a technology explodes into view. For
<br>
example, when the Internet went from 20,000 to 80,000 nodes over a two
<br>
year period during the 1980s, this progress remained hidden from the
<br>
general public. A decade later, when it went from 20 million to 80 million
<br>
nodes in the same amount of time, the impact was rather conspicuous.
<br>
<p>As exponential growth continues to accelerate into the first half of the
<br>
twenty-first century, it will appear to explode into infinity, at least
<br>
from the limited and linear perspective of contemporary humans. The
<br>
progress will ultimately become so fast that it will rupture our ability
<br>
to follow it. It will literally get out of our control. The illusion that
<br>
we have our hand &quot;on the plug,&quot; will be dispelled.
<br>
<p>Can the pace of technological progress continue to speed up indefinitely?
<br>
Is there not a point where humans are unable to think fast enough to keep
<br>
up with it? With regard to unenhanced humans, clearly so. But what would a
<br>
thousand scientists, each a thousand times more intelligent than human
<br>
scientists today, and each operating a thousand times faster than
<br>
contemporary humans (because the information processing in their primarily
<br>
nonbiological brains is faster) accomplish? One year would be like a
<br>
millennium. What would they come up with?
<br>
<p>Well, for one thing, they would come up with technology to become even
<br>
more intelligent (because their intelligence is no longer of fixed
<br>
capacity). They would change their own thought processes to think even
<br>
faster. When the scientists evolve to be a million times more intelligent
<br>
and operate a million times faster, then an hour would result in a century
<br>
of progress (in today's terms).
<br>
<p>This, then, is the Singularity. The Singularity is technological change so
<br>
rapid and so profound that it represents a rupture in the fabric of human
<br>
history. Some would say that we cannot comprehend the Singularity, at
<br>
least with our current level of understanding, and that it is impossible,
<br>
therefore, to look past its &quot;event horizon&quot; and make sense of what lies
<br>
beyond.
<br>
<p>My view is that despite our profound limitations of thought, constrained
<br>
as we are today to a mere hundred trillion interneuronal connections in
<br>
our biological brains, we nonetheless have sufficient powers of
<br>
abstraction to make meaningful statements about the nature of life after
<br>
the Singularity. Most importantly, it is my view that the intelligence
<br>
that will emerge will continue to represent the human civilization, which
<br>
is already a human-machine civilization. This will be the next step in
<br>
evolution, the next high level paradigm shift.
<br>
<p>To put the concept of Singularity into perspective, let's explore the
<br>
history of the word itself. Singularity is a familiar word meaning a
<br>
unique event with profound implications. In mathematics, the term implies
<br>
infinity, the explosion of value that occurs when dividing a constant by a
<br>
number that gets closer and closer to zero. In physics, similarly, a
<br>
singularity denotes an event or location of infinite power. At the center
<br>
of a black hole, matter is so dense that its gravity is infinite. As
<br>
nearby matter and energy are drawn into the black hole, an event horizon
<br>
separates the region from the rest of the Universe. It constitutes a
<br>
rupture in the fabric of space and time. The Universe itself is said to
<br>
have begun with just such a Singularity.
<br>
<p>In the 1950s, John Von Neumann was quoted as saying that &quot;the ever
<br>
accelerating progress of technology...gives the appearance of approaching
<br>
some essential singularity in the history of the race beyond which human
<br>
affairs, as we know them, could not continue.&quot; In the 1960s, I. J. Good
<br>
wrote of an &quot;intelligence explosion,&quot; resulting from intelligent machines
<br>
designing their next generation without human intervention. In 1986,
<br>
Vernor Vinge, a mathematician and computer scientist at San Diego State
<br>
University, wrote about a rapidly approaching technological &quot;singularity&quot;
<br>
in his science fiction novel, Marooned in Realtime. Then in 1993, Vinge
<br>
presented a paper to a NASA-organized symposium which described the
<br>
Singularity as an impending event resulting primarily from the advent of
<br>
&quot;entities with greater than human intelligence,&quot; which Vinge saw as the
<br>
harbinger of a run-away phenomenon.
<br>
<p><em>&gt;From my perspective, the Singularity has many faces. It represents the
</em><br>
nearly vertical phase of exponential growth where the rate of growth is so
<br>
extreme that technology appears to be growing at infinite speed. Of
<br>
course, from a mathematical perspective, there is no discontinuity, no
<br>
rupture, and the growth rates remain finite, albeit extraordinarily large.
<br>
But from our currently limited perspective, this imminent event appears to
<br>
be an acute and abrupt break in the continuity of progress. However, I
<br>
emphasize the word &quot;currently,&quot; because one of the salient implications of
<br>
the Singularity will be a change in the nature of our ability to
<br>
understand. In other words, we will become vastly smarter as we merge with
<br>
our technology.
<br>
<p>When I wrote my first book, The Age of Intelligent Machines, in the 1980s,
<br>
I ended the book with the specter of the emergence of machine intelligence
<br>
greater than human intelligence, but found it difficult to look beyond
<br>
this event horizon. Now having thought about its implications for the past
<br>
20 years, I feel that we are indeed capable of understanding the many
<br>
facets of this threshold, one that will transform all spheres of human
<br>
life.
<br>
<p>Consider a few examples of the implications. The bulk of our experiences
<br>
will shift from real reality to virtual reality. Most of the intelligence
<br>
of our civilization will ultimately be nonbiological, which by the end of
<br>
this century will be trillions of trillions of times more powerful than
<br>
human intelligence. However, to address often expressed concerns, this
<br>
does not imply the end of biological intelligence, even if thrown from its
<br>
perch of evolutionary superiority. Moreover, it is important to note that
<br>
the nonbiological forms will be derivative of biological design. In other
<br>
words, our civilization will remain human, indeed in many ways more
<br>
exemplary of what we regard as human than it is today, although our
<br>
understanding of the term will move beyond its strictly biological
<br>
origins.
<br>
<p>Many observers have nonetheless expressed alarm at the emergence of forms
<br>
of nonbiological intelligence superior to human intelligence. The
<br>
potential to augment our own intelligence through intimate connection with
<br>
other thinking mediums does not necessarily alleviate the concern, as some
<br>
people have expressed the wish to remain &quot;unenhanced&quot; while at the same
<br>
time keeping their place at the top of the intellectual food chain. My
<br>
view is that the likely outcome is that on the one hand, from the
<br>
perspective of biological humanity, these superhuman intelligences will
<br>
appear to be their transcendent servants, satisfying their needs and
<br>
desires. On the other hand, fulfilling the wishes of a revered biological
<br>
legacy will occupy only a trivial portion of the intellectual power that
<br>
the Singularity will bring.
<br>
<p>Needless to say, the Singularity will transform all aspects of our lives,
<br>
social, sexual, and economic, which I explore herewith.
<br>
<p>Wherefrom Moore's Law
<br>
Before considering further the implications of the Singularity, let's
<br>
examine the wide range of technologies that are subject to the law of
<br>
accelerating returns. The exponential trend that has gained the greatest
<br>
public recognition has become known as &quot;Moore's Law.&quot; Gordon Moore, one of
<br>
the inventors of integrated circuits, and then Chairman of Intel, noted in
<br>
the mid 1970s that we could squeeze twice as many transistors on an
<br>
integrated circuit every 24 months. Given that the electrons have less
<br>
distance to travel, the circuits also run twice as fast, providing an
<br>
overall quadrupling of computational power.
<br>
<p>After sixty years of devoted service, Moore's Law will die a dignified
<br>
death no later than the year 2019. By that time, transistor features will
<br>
be just a few atoms in width, and the strategy of ever finer
<br>
photolithography will have run its course. So, will that be the end of the
<br>
exponential growth of computing?
<br>
<p>Don't bet on it.
<br>
<p>If we plot the speed (in instructions per second) per $1000 (in constant
<br>
dollars) of 49 famous calculators and computers spanning the entire
<br>
twentieth century, we note some interesting observations.
<br>
<p>Moore's Law Was Not the First, but the Fifth Paradigm To Provide
<br>
Exponential Growth of Computing
<br>
Each time one paradigm runs out of steam, another picks up the pace
<br>
<p>It is important to note that Moore's Law of Integrated Circuits was not
<br>
the first, but the fifth paradigm to provide accelerating
<br>
price-performance. Computing devices have been consistently multiplying in
<br>
power (per unit of time) from the mechanical calculating devices used in
<br>
the 1890 U.S. Census, to Turing's relay-based &quot;Robinson&quot; machine that
<br>
cracked the Nazi enigma code, to the CBS vacuum tube computer that
<br>
predicted the election of Eisenhower, to the transistor-based machines
<br>
used in the first space launches, to the integrated-circuit-based personal
<br>
computer which I used to dictate (and automatically transcribe) this book
<br>
précis.
<br>
<p>But I noticed something else surprising. When I plotted the 49 machines on
<br>
an exponential graph (where a straight line means exponential growth), I
<br>
didn't get a straight line. What I got was another exponential curve. In
<br>
other words, there's exponential growth in the rate of exponential growth.
<br>
Computer speed (per unit cost) doubled every three years between 1910 and
<br>
1950, doubled every two years between 1950 and 1966, and is now doubling
<br>
every year.
<br>
<p>But where does Moore's Law come from? What is behind this remarkably
<br>
predictable phenomenon? I have seen relatively little written about the
<br>
ultimate source of this trend. Is it just &quot;a set of industry expectations
<br>
and goals,&quot; as Randy Isaac, head of basic science at IBM contends? Or is
<br>
there something more profound going on?
<br>
<p>In my view, it is one manifestation (among many) of the exponential growth
<br>
of the evolutionary process that is technology. The exponential growth of
<br>
computing is a marvelous quantitative example of the exponentially growing
<br>
returns from an evolutionary process. We can also express the exponential
<br>
growth of computing in terms of an accelerating pace: it took ninety years
<br>
to achieve the first MIPS (million instructions per second) per thousand
<br>
dollars, now we add one MIPS per thousand dollars every day.
<br>
<p>Moore's Law narrowly refers to the number of transistors on an integrated
<br>
circuit of fixed size, and sometimes has been expressed even more narrowly
<br>
in terms of transistor feature size. But rather than feature size (which
<br>
is only one contributing factor), or even number of transistors, I think
<br>
the most appropriate measure to track is computational speed per unit
<br>
cost. This takes into account many levels of &quot;cleverness&quot; (i.e.,
<br>
innovation, which is to say, technological evolution). In addition to all
<br>
of the innovation in integrated circuits, there are multiple layers of
<br>
innovation in computer design, e.g., pipelining, parallel processing,
<br>
instruction look-ahead, instruction and memory caching, and many others.
<br>
<p><em>&gt;From the above chart, we see that the exponential growth of computing
</em><br>
didn't start with integrated circuits (around 1958), or even transistors
<br>
(around 1947), but goes back to the electromechanical calculators used in
<br>
the 1890 and 1900 U.S. Census. This chart spans at least five distinct
<br>
paradigms of computing, of which Moore's Law pertains to only the latest
<br>
one.
<br>
<p>It's obvious what the sixth paradigm will be after Moore's Law runs out of
<br>
steam during the second decade of this century. Chips today are flat
<br>
(although it does require up to 20 layers of material to produce one layer
<br>
of circuitry). Our brain, in contrast, is organized in three dimensions.
<br>
We live in a three dimensional world, why not use the third dimension? The
<br>
human brain actually uses a very inefficient electrochemical digital
<br>
controlled analog computational process. The bulk of the calculations are
<br>
done in the interneuronal connections at a speed of only about 200
<br>
calculations per second (in each connection), which is about ten million
<br>
times slower than contemporary electronic circuits. But the brain gains
<br>
its prodigious powers from its extremely parallel organization in three
<br>
dimensions. There are many technologies in the wings that build circuitry
<br>
in three dimensions. Nanotubes, for example, which are already working in
<br>
laboratories, build circuits from pentagonal arrays of carbon atoms. One
<br>
cubic inch of nanotube circuitry would be a million times more powerful
<br>
than the human brain. There are more than enough new computing
<br>
technologies now being researched, including three-dimensional silicon
<br>
chips, optical computing, crystalline computing, DNA computing, and
<br>
quantum computing, to keep the law of accelerating returns as applied to
<br>
computation going for a long time.
<br>
<p>Thus the (double) exponential growth of computing is broader than Moore's
<br>
Law, which refers to only one of its paradigms. And this accelerating
<br>
growth of computing is, in turn, part of the yet broader phenomenon of the
<br>
accelerating pace of any evolutionary process. Observers are quick to
<br>
criticize extrapolations of an exponential trend on the basis that the
<br>
trend is bound to run out of &quot;resources.&quot; The classical example is when a
<br>
species happens upon a new habitat (e.g., rabbits in Australia), the
<br>
species' numbers will grow exponentially for a time, but then hit a limit
<br>
when resources such as food and space run out.
<br>
<p>But the resources underlying the exponential growth of an evolutionary
<br>
process are relatively unbounded:
<br>
<p>(i) The (ever growing) order of the evolutionary process itself. Each
<br>
stage of evolution provides more powerful tools for the next. In
<br>
biological evolution, the advent of DNA allowed more powerful and faster
<br>
evolutionary &quot;experiments.&quot; Later, setting the &quot;designs&quot; of animal body
<br>
plans during the Cambrian explosion allowed rapid evolutionary development
<br>
of other body organs such as the brain. Or to take a more recent example,
<br>
the advent of computer assisted design tools allows rapid development of
<br>
the next generation of computers.
<br>
(ii) The &quot;chaos&quot; of the environment in which the evolutionary process
<br>
takes place and which provides the options for further diversity. In
<br>
biological evolution, diversity enters the process in the form of
<br>
mutations and ever changing environmental conditions. In technological
<br>
evolution, human ingenuity combined with ever changing market conditions
<br>
keep the process of innovation going.
<br>
The maximum potential of matter and energy to contain intelligent
<br>
processes is a valid issue. But according to my models, we won't approach
<br>
those limits during this century (but this will become an issue within a
<br>
couple of centuries).
<br>
<p>We also need to distinguish between the &quot;S&quot; curve (an &quot;S&quot; stretched to the
<br>
right, comprising very slow, virtually unnoticeable growth--followed by
<br>
very rapid growth--followed by a flattening out as the process approaches
<br>
an asymptote) that is characteristic of any specific technological
<br>
paradigm and the continuing exponential growth that is characteristic of
<br>
the ongoing evolutionary process of technology. Specific paradigms, such
<br>
as Moore's Law, do ultimately reach levels at which exponential growth is
<br>
no longer feasible. Thus Moore's Law is an S curve. But the growth of
<br>
computation is an ongoing exponential (at least until we &quot;saturate&quot; the
<br>
Universe with the intelligence of our human-machine civilization, but that
<br>
will not be a limit in this coming century). In accordance with the law of
<br>
accelerating returns, paradigm shift, also called innovation, turns the S
<br>
curve of any specific paradigm into a continuing exponential. A new
<br>
paradigm (e.g., three-dimensional circuits) takes over when the old
<br>
paradigm approaches its natural limit. This has already happened at least
<br>
four times in the history of computation. This difference also
<br>
distinguishes the tool making of non-human species, in which the mastery
<br>
of a tool-making (or using) skill by each animal is characterized by an
<br>
abruptly ending S shaped learning curve, versus human-created technology,
<br>
which has followed an exponential pattern of growth and acceleration since
<br>
its inception.
<br>
<p>DNA Sequencing, Memory, Communications, the Internet, and Miniaturization
<br>
This &quot;law of accelerating returns&quot; applies to all of technology, indeed to
<br>
any true evolutionary process, and can be measured with remarkable
<br>
precision in information based technologies. There are a great many
<br>
examples of the exponential growth implied by the law of accelerating
<br>
returns in technologies as varied as DNA sequencing, communication speeds,
<br>
electronics of all kinds, and even in the rapidly shrinking size of
<br>
technology. The Singularity results not from the exponential explosion of
<br>
computation alone, but rather from the interplay and myriad synergies that
<br>
will result from manifold intertwined technological revolutions. Also,
<br>
keep in mind that every point on the exponential growth curves underlying
<br>
these panoply of technologies (see the graphs below) represents an intense
<br>
human drama of innovation and competition. It is remarkable therefore that
<br>
these chaotic processes result in such smooth and predictable exponential
<br>
trends.
<br>
<p>For example, when the human genome scan started fourteen years ago,
<br>
critics pointed out that given the speed with which the genome could then
<br>
be scanned, it would take thousands of years to finish the project. Yet
<br>
the fifteen year project was nonetheless completed slightly ahead of
<br>
schedule.
<br>
<p>Of course, we expect to see exponential growth in electronic memories such
<br>
as RAM.
<br>
<p>Notice How Exponential Growth Continued through Paradigm Shifts from
<br>
Vacuum Tubes to Discrete Transistors to Integrated Circuits
<br>
However, growth in magnetic memory is not primarily a matter of Moore's
<br>
law, but includes advances in mechanical and electromagnetic systems.
<br>
<p>Exponential growth in communications technology has been even more
<br>
explosive than in computation and is no less significant in its
<br>
implications. Again, this progression involves far more than just
<br>
shrinking transistors on an integrated circuit, but includes accelerating
<br>
advances in fiber optics, optical switching, electromagnetic technologies,
<br>
and others.
<br>
<p>Notice how the explosion of the Internet appears to be a surprise from the
<br>
Linear Chart, but was perfectly predictable from the Exponential Chart
<br>
<p>Ultimately we will get away from the tangle of wires in our cities and in
<br>
our lives through wireless communication, the power of which is doubling
<br>
every 10 to 11 months.
<br>
<p>Another technology that will have profound implications for the
<br>
twenty-first century is the pervasive trend toward making things smaller,
<br>
i.e., miniaturization. The salient implementation sizes of a broad range
<br>
of technologies, both electronic and mechanical, are shrinking, also at a
<br>
double exponential rate. At present, we are shrinking technology by a
<br>
factor of approximately 5.6 per linear dimension per decade.
<br>
<p><p>The Exponential Growth of Computation Revisited
<br>
If we view the exponential growth of computation in its proper perspective
<br>
as one example of the pervasiveness of the exponential growth of
<br>
information based technology, that is, as one example of many of the law
<br>
of accelerating returns, then we can confidently predict its continuation.
<br>
<p>In the accompanying sidebar, I include a simplified mathematical model of
<br>
the law of accelerating returns as it pertains to the (double) exponential
<br>
growth of computing. The formulas below result in the above graph of the
<br>
continued growth of computation. This graph matches the available data for
<br>
the twentieth century through all five paradigms and provides projections
<br>
for the twenty-first century. Note how the Growth Rate is growing slowly,
<br>
but nonetheless exponentially.
<br>
<p>The Law of Accelerating Returns Applied to the Growth of Computation
<br>
The following provides a brief overview of the law of accelerating returns
<br>
as it applies to the double exponential growth of computation. This model
<br>
considers the impact of the growing power of the technology to foster its
<br>
own next generation. For example, with more powerful computers and related
<br>
technology, we have the tools and the knowledge to design yet more
<br>
powerful computers, and to do so more quickly.
<br>
<p>Note that the data for the year 2000 and beyond assume neural net
<br>
connection calculations as it is expected that this type of calculation
<br>
will ultimately dominate, particularly in emulating human brain functions.
<br>
This type of calculation is less expensive than conventional (e.g.,
<br>
Pentium III / IV) calculations by a factor of at least 100 (particularly
<br>
if implemented using digital controlled analog electronics, which would
<br>
correspond well to the brain's digital controlled analog electrochemical
<br>
processes). A factor of 100 translates into approximately 6 years (today)
<br>
and less than 6 years later in the twenty-first century.
<br>
<p>My estimate of brain capacity is 100 billion neurons times an average
<br>
1,000 connections per neuron (with the calculations taking place primarily
<br>
in the connections) times 200 calculations per second. Although these
<br>
estimates are conservatively high, one can find higher and lower
<br>
estimates. However, even much higher (or lower) estimates by orders of
<br>
magnitude only shift the prediction by a relatively small number of years.
<br>
<p>Some prominent dates from this analysis include the following:
<br>
<p>We achieve one Human Brain capability (2 * 10^16 cps) for $1,000 around
<br>
the year 2023.
<br>
We achieve one Human Brain capability (2 * 10^16 cps) for one cent around
<br>
the year 2037.
<br>
We achieve one Human Race capability (2 * 10^26 cps) for $1,000 around the
<br>
year 2049.
<br>
We achieve one Human Race capability (2 * 10^26 cps) for one cent around
<br>
the year 2059.
<br>
The Model considers the following variables:
<br>
<p>V: Velocity (i.e., power) of computing (measured in CPS/unit cost)
<br>
W: World Knowledge as it pertains to designing and building computational
<br>
devices
<br>
t: Time
<br>
The assumptions of the model are:
<br>
<p>(1) V = C1 * W
<br>
In other words, computer power is a linear function of the knowledge of
<br>
how to build computers. This is actually a conservative assumption. In
<br>
general, innovations improve V (computer power) by a multiple, not in an
<br>
additive way. Independent innovations multiply each other's effect. For
<br>
example, a circuit advance such as CMOS, a more efficient IC wiring
<br>
methodology, and a processor innovation such as pipelining all increase V
<br>
by independent multiples.
<br>
<p>(2) W = C2 * Integral (0 to t) V
<br>
In other words, W (knowledge) is cumulative, and the instantaneous
<br>
increment to knowledge is proportional to V.
<br>
<p>This gives us:
<br>
<p>W = C1 * C2 * Integral (0 to t) W
<br>
W = C1 * C2 * C3 ^ (C4 * t)
<br>
V = C1 ^ 2 * C2 * C3 ^ (C4 * t)
<br>
(Note on notation: a^b means a raised to the b power.)
<br>
Simplifying the constants, we get:
<br>
<p>V = Ca * Cb ^ (Cc * t)
<br>
So this is a formula for &quot;accelerating&quot; (i.e., exponentially growing)
<br>
returns, a &quot;regular Moore's Law.&quot;
<br>
<p>As I mentioned above, the data shows exponential growth in the rate of
<br>
exponential growth. (We doubled computer power every three years early in
<br>
the twentieth century, every two years in the middle of the century, and
<br>
close to every one year during the 1990s.)
<br>
<p>Let's factor in another exponential phenomenon, which is the growing
<br>
resources for computation. Not only is each (constant cost) device getting
<br>
more powerful as a function of W, but the resources deployed for
<br>
computation are also growing exponentially.
<br>
<p>We now have:
<br>
<p>N: Expenditures for computation
<br>
V = C1 * W (as before)
<br>
N = C4 ^ (C5 * t) (Expenditure for computation is growing at its own
<br>
exponential rate)
<br>
W = C2 * Integral(0 to t) (N * V)
<br>
As before, world knowledge is accumulating, and the instantaneous
<br>
increment is proportional to the amount of computation, which equals the
<br>
resources deployed for computation (N) * the power of each (constant cost)
<br>
device.
<br>
<p>This gives us:
<br>
<p>W = C1 * C2 * Integral(0 to t) (C4 ^ (C5 * t) * W)
<br>
W = C1 * C2 * (C3 ^ (C6 * t)) ^ (C7 * t)
<br>
V = C1 ^ 2 * C2 * (C3 ^ (C6 * t)) ^ (C7 * t)
<br>
Simplifying the constants, we get:
<br>
<p>V = Ca * (Cb ^ (Cc * t)) ^ (Cd * t)
<br>
This is a double exponential--an exponential curve in which the rate of
<br>
exponential growth is growing at a different exponential rate.
<br>
<p>Now let's consider real-world data. Considering the data for actual
<br>
calculating devices and computers during the twentieth century:
<br>
<p>CPS/$1K: Calculations Per Second for $1,000
<br>
Twentieth century computing data matches:
<br>
<p>CPS/$1K = 10^(6.00*((20.40/6.00)^((A13-1900)/100))-11.00)
<br>
We can determine the growth rate over a period of time:
<br>
<p>Growth Rate =10^((LOG(CPS/$1K for Current Year) - LOG(CPS/$1K for Previous
<br>
Year))/(Current Year - Previous Year))
<br>
Human Brain = 100 Billion (10^11) neurons * 1000 (10^3) Connections/Neuron
<br>
* 200 (2 * 10^2) Calculations Per Second Per Connection = 2 * 10^16
<br>
Calculations Per Second
<br>
Human Race = 10 Billion (10^10) Human Brains = 2 * 10^26 Calculations Per
<br>
Second
<br>
These formulas produce the graph above.
<br>
<p>Already, IBM's &quot;Blue Gene&quot; supercomputer, now being built and scheduled to
<br>
be completed by 2005, is projected to provide 1 million billion
<br>
calculations per second (i.e., one billion megaflops). This is already one
<br>
twentieth of the capacity of the human brain, which I estimate at a
<br>
conservatively high 20 million billion calculations per second (100
<br>
billion neurons times 1,000 connections per neuron times 200 calculations
<br>
per second per connection). In line with my earlier predictions,
<br>
supercomputers will achieve one human brain capacity by 2010, and personal
<br>
computers will do so by around 2020. By 2030, it will take a village of
<br>
human brains (around a thousand) to match $1000 of computing. By 2050,
<br>
$1000 of computing will equal the processing power of all human brains on
<br>
Earth. Of course, this only includes those brains still using carbon-based
<br>
neurons. While human neurons are wondrous creations in a way, we wouldn't
<br>
(and don't) design computing circuits the same way. Our electronic
<br>
circuits are already more than ten million times faster than a neuron's
<br>
electrochemical processes. Most of the complexity of a human neuron is
<br>
devoted to maintaining its life support functions, not its information
<br>
processing capabilities. Ultimately, we will need to port our mental
<br>
processes to a more suitable computational substrate. Then our minds won't
<br>
have to stay so small, being constrained as they are today to a mere
<br>
hundred trillion neural connections each operating at a ponderous 200
<br>
digitally controlled analog calculations per second.
<br>
<p>The Software of Intelligence
<br>
So far, I've been talking about the hardware of computing. The software is
<br>
even more salient. One of the principal assumptions underlying the
<br>
expectation of the Singularity is the ability of nonbiological mediums to
<br>
emulate the richness, subtlety, and depth of human thinking. Achieving the
<br>
computational capacity of the human brain, or even villages and nations of
<br>
human brains will not automatically produce human levels of capability. By
<br>
human levels I include all the diverse and subtle ways in which humans are
<br>
intelligent, including musical and artistic aptitude, creativity,
<br>
physically moving through the world, and understanding and responding
<br>
appropriately to emotion. The requisite hardware capacity is a necessary
<br>
but not sufficient condition. The organization and content of these
<br>
resources--the software of intelligence--is also critical.
<br>
<p>Before addressing this issue, it is important to note that once a computer
<br>
achieves a human level of intelligence, it will necessarily soar past it.
<br>
A key advantage of nonbiological intelligence is that machines can easily
<br>
share their knowledge. If I learn French, or read War and Peace, I can't
<br>
readily download that learning to you. You have to acquire that
<br>
scholarship the same painstaking way that I did. My knowledge, embedded in
<br>
a vast pattern of neurotransmitter concentrations and interneuronal
<br>
connections, cannot be quickly accessed or transmitted. But we won't leave
<br>
out quick downloading ports in our nonbiological equivalents of human
<br>
neuron clusters. When one computer learns a skill or gains an insight, it
<br>
can immediately share that wisdom with billions of other machines.
<br>
<p>As a contemporary example, we spent years teaching one research computer
<br>
how to recognize continuous human speech. We exposed it to thousands of
<br>
hours of recorded speech, corrected its errors, and patiently improved its
<br>
performance. Finally, it became quite adept at recognizing speech (I
<br>
dictated most of my recent book to it). Now if you want your own personal
<br>
computer to recognize speech, it doesn't have to go through the same
<br>
process; you can just download the fully trained patterns in seconds.
<br>
Ultimately, billions of nonbiological entities can be the master of all
<br>
human and machine acquired knowledge.
<br>
<p>In addition, computers are potentially millions of times faster than human
<br>
neural circuits. A computer can also remember billions or even trillions
<br>
of facts perfectly, while we are hard pressed to remember a handful of
<br>
phone numbers. The combination of human level intelligence in a machine
<br>
with a computer's inherent superiority in the speed, accuracy, and sharing
<br>
ability of its memory will be formidable.
<br>
<p>There are a number of compelling scenarios to achieve higher levels of
<br>
intelligence in our computers, and ultimately human levels and beyond. We
<br>
will be able to evolve and train a system combining massively parallel
<br>
neural nets with other paradigms to understand language and model
<br>
knowledge, including the ability to read and model the knowledge contained
<br>
in written documents. Unlike many contemporary &quot;neural net&quot; machines,
<br>
which use mathematically simplified models of human neurons, some
<br>
contemporary neural nets are already using highly detailed models of human
<br>
neurons, including detailed nonlinear analog activation functions and
<br>
other relevant details. Although the ability of today's computers to
<br>
extract and learn knowledge from natural language documents is limited,
<br>
their capabilities in this domain are improving rapidly. Computers will be
<br>
able to read on their own, understanding and modeling what they have read,
<br>
by the second decade of the twenty-first century. We can then have our
<br>
computers read all of the world's literature--books, magazines, scientific
<br>
journals, and other available material. Ultimately, the machines will
<br>
gather knowledge on their own by venturing out on the web, or even into
<br>
the physical world, drawing from the full spectrum of media and
<br>
information services, and sharing knowledge with each other (which
<br>
machines can do far more easily than their human creators).
<br>
<p>Reverse Engineering the Human Brain
<br>
The most compelling scenario for mastering the software of intelligence is
<br>
to tap into the blueprint of the best example we can get our hands on of
<br>
an intelligent process. There is no reason why we cannot reverse engineer
<br>
the human brain, and essentially copy its design. Although it took its
<br>
original designer several billion years to develop, it's readily available
<br>
to us, and not (yet) copyrighted. Although there's a skull around the
<br>
brain, it is not hidden from our view.
<br>
<p>The most immediately accessible way to accomplish this is through
<br>
destructive scanning: we take a frozen brain, preferably one frozen just
<br>
slightly before rather than slightly after it was going to die anyway, and
<br>
examine one brain layer--one very thin slice--at a time. We can readily
<br>
see every neuron and every connection and every neurotransmitter
<br>
concentration represented in each synapse-thin layer.
<br>
<p>Human brain scanning has already started. A condemned killer allowed his
<br>
brain and body to be scanned and you can access all 10 billion bytes of
<br>
him on the Internet
<br>
<a href="http://www.nlm.nih.gov/research/visible/visible_human.html">http://www.nlm.nih.gov/research/visible/visible_human.html</a>.
<br>
<p>He has a 25 billion byte female companion on the site as well in case he
<br>
gets lonely. This scan is not high enough in resolution for our purposes,
<br>
but then, we probably don't want to base our templates of machine
<br>
intelligence on the brain of a convicted killer, anyway.
<br>
<p>But scanning a frozen brain is feasible today, albeit not yet at a
<br>
sufficient speed or bandwidth, but again, the law of accelerating returns
<br>
will provide the requisite speed of scanning, just as it did for the human
<br>
genome scan. Carnegie Mellon University's Andreas Nowatzyk plans to scan
<br>
the nervous system of the brain and body of a mouse with a resolution of
<br>
less than 200 nanometers, which is getting very close to the resolution
<br>
needed for reverse engineering.
<br>
<p>We also have noninvasive scanning techniques today, including
<br>
high-resolution magnetic resonance imaging (MRI) scans, optical imaging,
<br>
near-infrared scanning, and other technologies which are capable in
<br>
certain instances of resolving individual somas, or neuron cell bodies.
<br>
Brain scanning technologies are also increasing their resolution with each
<br>
new generation, just what we would expect from the law of accelerating
<br>
returns. Future generations will enable us to resolve the connections
<br>
between neurons and to peer inside the synapses and record the
<br>
neurotransmitter concentrations.
<br>
<p>We can peer inside someone's brain today with noninvasive scanners, which
<br>
are increasing their resolution with each new generation of this
<br>
technology. There are a number of technical challenges in accomplishing
<br>
this, including achieving suitable resolution, bandwidth, lack of
<br>
vibration, and safety. For a variety of reasons it is easier to scan the
<br>
brain of someone recently deceased than of someone still living. It is
<br>
easier to get someone deceased to sit still, for one thing. But
<br>
noninvasively scanning a living brain will ultimately become feasible as
<br>
MRI, optical, and other scanning technologies continue to improve in
<br>
resolution and speed.
<br>
<p>Scanning from Inside
<br>
Although noninvasive means of scanning the brain from outside the skull
<br>
are rapidly improving, the most practical approach to capturing every
<br>
salient neural detail will be to scan it from inside. By 2030, &quot;nanobot&quot;
<br>
(i.e., nano robot) technology will be viable, and brain scanning will be a
<br>
prominent application. Nanobots are robots that are the size of human
<br>
blood cells, or even smaller. Billions of them could travel through every
<br>
brain capillary and scan every relevant feature from up close. Using high
<br>
speed wireless communication, the nanobots would communicate with each
<br>
other, and with other computers that are compiling the brain scan data
<br>
base (in other words, the nanobots will all be on a wireless local area
<br>
network).
<br>
<p>This scenario involves only capabilities that we can touch and feel today.
<br>
We already have technology capable of producing very high resolution
<br>
scans, provided that the scanner is physically proximate to the neural
<br>
features. The basic computational and communication methods are also
<br>
essentially feasible today. The primary features that are not yet
<br>
practical are nanobot size and cost. As I discussed above, we can project
<br>
the exponentially declining cost of computation, and the rapidly declining
<br>
size of both electronic and mechanical technologies. We can conservatively
<br>
expect, therefore, the requisite nanobot technology by around 2030.
<br>
Because of its ability to place each scanner in very close physical
<br>
proximity to every neural feature, nanobot-based scanning will be more
<br>
practical than scanning the brain from outside.
<br>
<p>How to Use Your Brain Scan
<br>
How will we apply the thousands of trillions of bytes of information
<br>
derived from each brain scan? One approach is to use the results to design
<br>
more intelligent parallel algorithms for our machines, particularly those
<br>
based on one of the neural net paradigms. With this approach, we don't
<br>
have to copy every single connection. There is a great deal of repetition
<br>
and redundancy within any particular brain region. Although the
<br>
information contained in a human brain would require thousands of
<br>
trillions of bytes of information (on the order of 100 billion neurons
<br>
times an average of 1,000 connections per neuron, each with multiple
<br>
neurotransmitter concentrations and connection data), the design of the
<br>
brain is characterized by a human genome of only about a billion bytes.
<br>
<p>Furthermore, most of the genome is redundant, so the initial design of the
<br>
brain is characterized by approximately one hundred million bytes, about
<br>
the size of Microsoft Word. Of course, the complexity of our brains
<br>
greatly increases as we interact with the world (by a factor of more than
<br>
ten million). Because of the highly repetitive patterns found in each
<br>
specific brain region, it is not necessary to capture each detail in order
<br>
to reverse engineer the significant digital-analog algorithms. With this
<br>
information, we can design simulated nets that operate similarly. There
<br>
are already multiple efforts under way to scan the human brain and apply
<br>
the insights derived to the design of intelligent machines.
<br>
<p>The pace of brain reverse engineering is only slightly behind the
<br>
availability of the brain scanning and neuron structure information. A
<br>
contemporary example is a comprehensive model of a significant portion of
<br>
the human auditory processing system that Lloyd Watts (www.lloydwatts.com)
<br>
has developed from both neurobiology studies of specific neuron types and
<br>
brain interneuronal connection information. Watts' model includes five
<br>
parallel paths and includes the actual intermediate representations of
<br>
auditory information at each stage of neural processing. Watts has
<br>
implemented his model as real-time software which can locate and identify
<br>
sounds with many of the same properties as human hearing. Although a work
<br>
in progress, the model illustrates the feasibility of converting
<br>
neurobiological models and brain connection data into working simulations.
<br>
Also, as Hans Moravec and others have speculated, these efficient
<br>
simulations require about 1,000 times less computation than the
<br>
theoretical potential of the biological neurons being simulated.
<br>
<p>Reverse Engineering the Human Brain: Five Parallel Auditory Pathways
<br>
<p>Cochlea: Sense organ of hearing. 30,000 fibers converts motion of the
<br>
stapes into spectro-temporal representation of sound.
<br>
<p>MC: Multipolar Cells. Measure spectral energy.
<br>
<p>GBC: Globular Bushy Cells. Relays spikes from the auditory nerve to the
<br>
Lateral Superior.
<br>
<p>Olivary Complex (includes LSO and MSO). Encoding of timing and amplitude
<br>
of signals for binaural comparison of level.
<br>
<p>SBC: Spherical Bushy Cells. Provide temporal sharpening of time of
<br>
arrival, as a pre-processor for interaural time difference calculation.
<br>
<p>OC: Octopus Cells. Detection of transients.
<br>
<p>DCN: Dorsal Cochlear Nucleus. Detection of spectral edges and calibrating
<br>
for noise levels.
<br>
<p>VNTB: Ventral Nucleus of the Trapezoid Body. Feedback signals to modulate
<br>
outer hair cell function in the cochlea.
<br>
<p>VNLL, PON: Ventral Nucleus of the Lateral Lemniscus, Peri-Olivary Nuclei.
<br>
Processing transients from the Octopus Cells.
<br>
<p>MSO: Medial Superior Olive. Computing inter-aural time difference
<br>
(difference in time of arrival between the two ears, used to tell where a
<br>
sound is coming from).
<br>
<p>LSO: Lateral Superior Olive. Also involved in computing inter-aural level
<br>
difference.
<br>
<p>ICC: Central Nucleus of the Inferior Colliculus. The site of major
<br>
integration of multiple representations of sound.
<br>
<p>ICx: Exterior Nucleus of the Inferior Colliculus. Further refinement of
<br>
sound localization.
<br>
<p>SC: Superior Colliculus. Location of auditory/visual merging.
<br>
<p>MGB: Medial Geniculate Body. The auditory portion of the thalamus.
<br>
<p>LS: Limbic System. Comprising many structures associated with emotion,
<br>
memory, territory, etc.
<br>
<p>AC: Auditory Cortex.
<br>
<p>The brain is not one huge &quot;tabula rasa&quot; (i.e., undifferentiated blank
<br>
slate), but rather an intricate and intertwined collection of hundreds of
<br>
specialized regions. The process of &quot;peeling the onion&quot; to understand
<br>
these interleaved regions is well underway. As the requisite neuron models
<br>
and brain interconnection data becomes available, detailed and
<br>
implementable models such as the auditory example above will be developed
<br>
for all brain regions.
<br>
<p>After the algorithms of a region are understood, they can be refined and
<br>
extended before being implemented in synthetic neural equivalents. For one
<br>
thing, they can be run on a computational substrate that is already more
<br>
than ten million times faster than neural circuitry. And we can also throw
<br>
in the methods for building intelligent machines that we already
<br>
understand.
<br>
<p>Downloading the Human Brain
<br>
A more controversial application than this
<br>
scanning-the-brain-to-understand-it scenario is
<br>
scanning-the-brain-to-download-it. Here we scan someone's brain to map the
<br>
locations, interconnections, and contents of all the somas, axons,
<br>
dendrites, presynaptic vesicles, neurotransmitter concentrations, and
<br>
other neural components and levels. Its entire organization can then be
<br>
re-created on a neural computer of sufficient capacity, including the
<br>
contents of its memory.
<br>
<p>To do this, we need to understand local brain processes, although not
<br>
necessarily all of the higher level processes. Scanning a brain with
<br>
sufficient detail to download it may sound daunting, but so did the human
<br>
genome scan. All of the basic technologies exist today, just not with the
<br>
requisite speed, cost, and size, but these are the attributes that are
<br>
improving at a double exponential pace.
<br>
<p>The computationally pertinent aspects of individual neurons are
<br>
complicated, but definitely not beyond our ability to accurately model.
<br>
For example, Ted Berger and his colleagues at Hedco Neurosciences have
<br>
built integrated circuits that precisely match the digital and analog
<br>
information processing characteristics of neurons, including clusters with
<br>
hundreds of neurons. Carver Mead and his colleagues at CalTech have built
<br>
a variety of integrated circuits that emulate the digital-analog
<br>
characteristics of mammalian neural circuits.
<br>
<p>A recent experiment at San Diego's Institute for Nonlinear Science
<br>
demonstrates the potential for electronic neurons to precisely emulate
<br>
biological ones. Neurons (biological or otherwise) are a prime example of
<br>
what is often called &quot;chaotic computing.&quot; Each neuron acts in an
<br>
essentially unpredictable fashion. When an entire network of neurons
<br>
receives input (from the outside world or from other networks of neurons),
<br>
the signaling amongst them appears at first to be frenzied and random.
<br>
Over time, typically a fraction of a second or so, the chaotic interplay
<br>
of the neurons dies down, and a stable pattern emerges. This pattern
<br>
represents the &quot;decision&quot; of the neural network. If the neural network is
<br>
performing a pattern recognition task (which, incidentally, comprises the
<br>
bulk of the activity in the human brain), then the emergent pattern
<br>
represents the appropriate recognition.
<br>
<p>So the question addressed by the San Diego researchers was whether
<br>
electronic neurons could engage in this chaotic dance alongside biological
<br>
ones. They hooked up their artificial neurons with those from spiney
<br>
lobsters in a single network, and their hybrid biological-nonbiological
<br>
network performed in the same way (i.e., chaotic interplay followed by a
<br>
stable emergent pattern) and with the same type of results as an all
<br>
biological net of neurons. Essentially, the biological neurons accepted
<br>
their electronic peers. It indicates that their mathematical model of
<br>
these neurons was reasonably accurate.
<br>
<p>There are many projects around the world which are creating nonbiological
<br>
devices to recreate in great detail the functionality of human neuron
<br>
clusters. The accuracy and scale of these neuron-cluster replications are
<br>
rapidly increasing. We started with functionally equivalent recreations of
<br>
single neurons, then clusters of tens, then hundreds, and now thousands.
<br>
Scaling up technical processes at an exponential pace is what technology
<br>
is good at.
<br>
<p>As the computational power to emulate the human brain becomes
<br>
available--we're not there yet, but we will be there within a couple of
<br>
decades--projects already under way to scan the human brain will be
<br>
accelerated, with a view both to understand the human brain in general, as
<br>
well as providing a detailed description of the contents and design of
<br>
specific brains. By the third decade of the twenty-first century, we will
<br>
be in a position to create highly detailed and complete maps of all
<br>
relevant features of all neurons, neural connections and synapses in the
<br>
human brain, all of the neural details that play a role in the behavior
<br>
and functionality of the brain, and to recreate these designs in suitably
<br>
advanced neural computers.
<br>
<p>Is the Human Brain Different from a Computer?
<br>
Is the human brain different from a computer?
<br>
<p>The answer depends on what we mean by the word &quot;computer.&quot; Certainly the
<br>
brain uses very different methods from conventional contemporary
<br>
computers. Most computers today are all digital and perform one (or
<br>
perhaps a few) computations at a time at extremely high speed. In
<br>
contrast, the human brain combines digital and analog methods with most
<br>
computations performed in the analog domain. The brain is massively
<br>
parallel, performing on the order of a hundred trillion computations at
<br>
the same time, but at extremely slow speeds.
<br>
<p>With regard to digital versus analog computing, we know that digital
<br>
computing can be functionally equivalent to analog computing (although the
<br>
reverse is not true), so we can perform all of the capabilities of a
<br>
hybrid digital--analog network with an all digital computer. On the other
<br>
hand, there is an engineering advantage to analog circuits in that analog
<br>
computing is potentially thousands of times more efficient. An analog
<br>
computation can be performed by a few transistors, or, in the case of
<br>
mammalian neurons, specific electrochemical processes. A digital
<br>
computation, in contrast, requires thousands or tens of thousands of
<br>
transistors. So there is a significant engineering advantage to emulating
<br>
the brain's analog methods.
<br>
<p>The massive parallelism of the human brain is the key to its pattern
<br>
recognition abilities, which reflects the strength of human thinking. As I
<br>
discussed above, mammalian neurons engage in a chaotic dance, and if the
<br>
neural network has learned its lessons well, then a stable pattern will
<br>
emerge reflecting the network's decision. There is no reason why our
<br>
nonbiological functionally equivalent recreations of biological neural
<br>
networks cannot be built using these same principles, and indeed there are
<br>
dozens of projects around the world that have succeeded in doing this. My
<br>
own technical field is pattern recognition, and the projects that I have
<br>
been involved in for over thirty years use this form of chaotic computing.
<br>
Particularly successful examples are Carver Mead's neural chips, which are
<br>
highly parallel, use digital controlled analog computing, and are intended
<br>
as functionally similar recreations of biological networks.
<br>
<p>Objective and Subjective
<br>
The Singularity envisions the emergence of human-like intelligent entities
<br>
of astonishing diversity and scope. Although these entities will be
<br>
capable of passing the &quot;Turing test&quot; (i.e., able to fool humans that they
<br>
are human), the question arises as to whether these &quot;people&quot; are
<br>
conscious, or just appear that way. To gain some insight as to why this is
<br>
an extremely subtle question (albeit an ultimately important one) it is
<br>
useful to consider some of the paradoxes that emerge from the concept of
<br>
downloading specific human brains.
<br>
<p>Although I anticipate that the most common application of the knowledge
<br>
gained from reverse engineering the human brain will be creating more
<br>
intelligent machines that are not necessarily modeled on specific
<br>
biological human individuals, the scenario of scanning and reinstantiating
<br>
all of the neural details of a specific person raises the most immediate
<br>
questions of identity. Let's consider the question of what we will find
<br>
when we do this.
<br>
<p>We have to consider this question on both the objective and subjective
<br>
levels. &quot;Objective&quot; means everyone except me, so let's start with that.
<br>
Objectively, when we scan someone's brain and reinstantiate their personal
<br>
mind file into a suitable computing medium, the newly emergent &quot;person&quot;
<br>
will appear to other observers to have very much the same personality,
<br>
history, and memory as the person originally scanned. That is, once the
<br>
technology has been refined and perfected. Like any new technology, it
<br>
won't be perfect at first. But ultimately, the scans and recreations will
<br>
be very accurate and realistic.
<br>
<p>Interacting with the newly instantiated person will feel like interacting
<br>
with the original person. The new person will claim to be that same old
<br>
person and will have a memory of having been that person. The new person
<br>
will have all of the patterns of knowledge, skill, and personality of the
<br>
original. We are already creating functionally equivalent recreations of
<br>
neurons and neuron clusters with sufficient accuracy that biological
<br>
neurons accept their nonbiological equivalents and work with them as if
<br>
they were biological. There are no natural limits that prevent us from
<br>
doing the same with the hundred billion neuron cluster of clusters we call
<br>
the human brain.
<br>
<p>Subjectively, the issue is more subtle and profound, but first we need to
<br>
reflect on one additional objective issue: our physical self.
<br>
<p>The Importance of Having a Body
<br>
Consider how many of our thoughts and thinking are directed toward our
<br>
body and its survival, security, nutrition, and image, not to mention
<br>
affection, sexuality, and reproduction. Many, if not most, of the goals we
<br>
attempt to advance using our brains have to do with our bodies: protecting
<br>
them, providing them with fuel, making them attractive, making them feel
<br>
good, providing for their myriad needs and desires. Some philosophers
<br>
maintain that achieving human level intelligence is impossible without a
<br>
body. If we're going to port a human's mind to a new computational medium,
<br>
we'd better provide a body. A disembodied mind will quickly get depressed.
<br>
<p>There are a variety of bodies that we will provide for our machines, and
<br>
that they will provide for themselves: bodies built through nanotechnology
<br>
(i.e., building highly complex physical systems atom by atom), virtual
<br>
bodies (that exist only in virtual reality), bodies comprised of swarms of
<br>
nanobots, and other technologies.
<br>
<p>A common scenario will be to enhance a person's biological brain with
<br>
intimate connection to nonbiological intelligence. In this case, the body
<br>
remains the good old human body that we're familiar with, although this
<br>
too will become greatly enhanced through biotechnology (gene enhancement
<br>
and replacement) and, later on, through nanotechnology. A detailed
<br>
examination of twenty-first century bodies is beyond the scope of this
<br>
précis, but recreating and enhancing our bodies will be (and has been) an
<br>
easier task than recreating our minds.
<br>
<p>So Just Who Are These People?
<br>
To return to the issue of subjectivity, consider: is the reinstantiated
<br>
mind the same consciousness as the person we just scanned? Are these
<br>
&quot;people&quot; conscious at all? Is this a mind or just a brain?
<br>
<p>Consciousness in our twenty-first century machines will be a critically
<br>
important issue. But it is not easily resolved, or even readily
<br>
understood. People tend to have strong views on the subject, and often
<br>
just can't understand how anyone else could possibly see the issue from a
<br>
different perspective. Marvin Minsky observed that &quot;there's something
<br>
queer about describing consciousness. Whatever people mean to say, they
<br>
just can't seem to make it clear.&quot;
<br>
<p>We don't worry, at least not yet, about causing pain and suffering to our
<br>
computer programs. But at what point do we consider an entity, a process,
<br>
to be conscious, to feel pain and discomfort, to have its own
<br>
intentionality, its own free will? How do we determine if an entity is
<br>
conscious; if it has subjective experience? How do we distinguish a
<br>
process that is conscious from one that just acts as if it is conscious?
<br>
<p>We can't simply ask it. If it says &quot;Hey I'm conscious,&quot; does that settle
<br>
the issue? No, we have computer games today that effectively do that, and
<br>
they're not terribly convincing.
<br>
<p>How about if the entity is very convincing and compelling when it says
<br>
&quot;I'm lonely, please keep me company.&quot; Does that settle the issue?
<br>
<p>If we look inside its circuits, and see essentially the identical kinds of
<br>
feedback loops and other mechanisms in its brain that we see in a human
<br>
brain (albeit implemented using nonbiological equivalents), does that
<br>
settle the issue?
<br>
<p>And just who are these people in the machine, anyway? The answer will
<br>
depend on who you ask. If you ask the people in the machine, they will
<br>
strenuously claim to be the original persons. For example, if we
<br>
scan--let's say myself--and record the exact state, level, and position of
<br>
every neurotransmitter, synapse, neural connection, and every other
<br>
relevant detail, and then reinstantiate this massive data base of
<br>
information (which I estimate at thousands of trillions of bytes) into a
<br>
neural computer of sufficient capacity, the person who then emerges in the
<br>
machine will think that &quot;he&quot; is (and had been) me, or at least he will act
<br>
that way. He will say &quot;I grew up in Queens, New York, went to college at
<br>
MIT, stayed in the Boston area, started and sold a few artificial
<br>
intelligence companies, walked into a scanner there, and woke up in the
<br>
machine here. Hey, this technology really works.&quot;
<br>
<p>But wait.
<br>
<p>Is this really me? For one thing, old biological Ray (that's me) still
<br>
exists. I'll still be here in my carbon-cell-based brain. Alas, I will
<br>
have to sit back and watch the new Ray succeed in endeavors that I could
<br>
only dream of.
<br>
<p>A Thought Experiment
<br>
Let's consider the issue of just who I am, and who the new Ray is a little
<br>
more carefully. First of all, am I the stuff in my brain and body?
<br>
<p>Consider that the particles making up my body and brain are constantly
<br>
changing. We are not at all permanent collections of particles. The cells
<br>
in our bodies turn over at different rates, but the particles (e.g., atoms
<br>
and molecules) that comprise our cells are exchanged at a very rapid rate.
<br>
I am just not the same collection of particles that I was even a month
<br>
ago. It is the patterns of matter and energy that are semipermanent (that
<br>
is, changing only gradually), but our actual material content is changing
<br>
constantly, and very quickly. We are rather like the patterns that water
<br>
makes in a stream. The rushing water around a formation of rocks makes a
<br>
particular, unique pattern. This pattern may remain relatively unchanged
<br>
for hours, even years. Of course, the actual material constituting the
<br>
pattern--the water--is replaced in milliseconds. The same is true for Ray
<br>
Kurzweil. Like the water in a stream, my particles are constantly
<br>
changing, but the pattern that people recognize as Ray has a reasonable
<br>
level of continuity. This argues that we should not associate our
<br>
fundamental identity with a specific set of particles, but rather the
<br>
pattern of matter and energy that we represent. Many contemporary
<br>
philosophers seem partial to this &quot;identify from pattern&quot; argument.
<br>
<p>But (again) wait.
<br>
<p>If you were to scan my brain and reinstantiate new Ray while I was
<br>
sleeping, I would not necessarily even know about it (with the nanobots,
<br>
this will be a feasible scenario). If you then come to me, and say, &quot;good
<br>
news, Ray, we've successfully reinstantiated your mind file, so we won't
<br>
be needing your old brain anymore,&quot; I may suddenly realize the flaw in
<br>
this &quot;identity from pattern&quot; argument. I may wish new Ray well, and
<br>
realize that he shares my &quot;pattern,&quot; but I would nonetheless conclude that
<br>
he's not me, because I'm still here. How could he be me? After all, I
<br>
would not necessarily know that he even existed.
<br>
<p>Let's consider another perplexing scenario. Suppose I replace a small
<br>
number of biological neurons with functionally equivalent nonbiological
<br>
ones (they may provide certain benefits such as greater reliability and
<br>
longevity, but that's not relevant to this thought experiment). After I
<br>
have this procedure performed, am I still the same person? My friends
<br>
certainly think so. I still have the same self-deprecating humor, the same
<br>
silly grin--yes, I'm still the same guy.
<br>
<p>It should be clear where I'm going with this. Bit by bit, region by
<br>
region, I ultimately replace my entire brain with essentially identical
<br>
(perhaps improved) nonbiological equivalents (preserving all of the
<br>
neurotransmitter concentrations and other details that represent my
<br>
learning, skills, and memories). At each point, I feel the procedures were
<br>
successful. At each point, I feel that I am the same guy. After each
<br>
procedure, I claim to be the same guy. My friends concur. There is no old
<br>
Ray and new Ray, just one Ray, one that never appears to fundamentally
<br>
change.
<br>
<p>But consider this. This gradual replacement of my brain with a
<br>
nonbiological equivalent is essentially identical to the following
<br>
sequence:
<br>
<p>(i) scan Ray and reinstantiate Ray's mind file into new (nonbiological)
<br>
Ray, and, then
<br>
(ii) terminate old Ray. But we concluded above that in such a scenario new
<br>
Ray is not the same as old Ray. And if old Ray is terminated, well then
<br>
that's the end of Ray. So the gradual replacement scenario essentially
<br>
ends with the same result: New Ray has been created, and old Ray has been
<br>
destroyed, even if we never saw him missing. So what appears to be the
<br>
continuing existence of just one Ray is really the creation of new Ray and
<br>
the termination of old Ray.
<br>
On yet another hand (we're running out of philosophical hands here), the
<br>
gradual replacement scenario is not altogether different from what happens
<br>
normally to our biological selves, in that our particles are always
<br>
rapidly being replaced. So am I constantly being replaced with someone
<br>
else who just happens to be very similar to my old self?
<br>
<p>I am trying to illustrate why consciousness is not an easy issue. If we
<br>
talk about consciousness as just a certain type of intelligent skill: the
<br>
ability to reflect on one's own self and situation, for example, then the
<br>
issue is not difficult at all because any skill or capability or form of
<br>
intelligence that one cares to define will be replicated in nonbiological
<br>
entities (i.e., machines) within a few decades. With this type of
<br>
objective view of consciousness, the conundrums do go away. But a fully
<br>
objective view does not penetrate to the core of the issue, because the
<br>
essence of consciousness is subjective experience, not objective
<br>
correlates of that experience.
<br>
<p>Will these future machines be capable of having spiritual experiences?
<br>
<p>They certainly will claim to. They will claim to be people, and to have
<br>
the full range of emotional and spiritual experiences that people claim to
<br>
have. And these will not be idle claims; they will evidence the sort of
<br>
rich, complex, and subtle behavior one associates with these feelings. How
<br>
do the claims and behaviors--compelling as they will be--relate to the
<br>
subjective experience of these reinstantiated people? We keep coming back
<br>
to the very real but ultimately unmeasurable issue of consciousness.
<br>
<p>People often talk about consciousness as if it were a clear property of an
<br>
entity that can readily be identified, detected, and gauged. If there is
<br>
one crucial insight that we can make regarding why the issue of
<br>
consciousness is so contentious, it is the following:
<br>
<p>There exists no objective test that can conclusively determine its
<br>
presence.
<br>
<p>Science is about objective measurement and logical implications therefrom,
<br>
but the very nature of objectivity is that you cannot measure subjective
<br>
experience-you can only measure correlates of it, such as behavior (and by
<br>
behavior, I include the actions of components of an entity, such as
<br>
neurons). This limitation has to do with the very nature of the concepts
<br>
&quot;objective&quot; and &quot;subjective.&quot; Fundamentally, we cannot penetrate the
<br>
subjective experience of another entity with direct objective measurement.
<br>
We can certainly make arguments about it: i.e., &quot;look inside the brain of
<br>
this nonhuman entity, see how its methods are just like a human brain.&quot;
<br>
Or, &quot;see how its behavior is just like human behavior.&quot; But in the end,
<br>
these remain just arguments. No matter how convincing the behavior of a
<br>
reinstantiated person, some observers will refuse to accept the
<br>
consciousness of an entity unless it squirts neurotransmitters, or is
<br>
based on DNA-guided protein synthesis, or has some other specific
<br>
biologically human attribute.
<br>
<p>We assume that other humans are conscious, but that is still an
<br>
assumption, and there is no consensus amongst humans about the
<br>
consciousness of nonhuman entities, such as higher non-human animals. The
<br>
issue will be even more contentious with regard to future nonbiological
<br>
entities with human-like behavior and intelligence.
<br>
<p>So how will we resolve the claimed consciousness of nonbiological
<br>
intelligence (claimed, that is, by the machines)? From a practical
<br>
perspective, we'll accept their claims. Keep in mind that nonbiological
<br>
entities in the twenty-first century will be extremely intelligent, so
<br>
they'll be able to convince us that they are conscious. They'll have all
<br>
the delicate and emotional cues that convince us today that humans are
<br>
conscious. They will be able to make us laugh and cry. And they'll get mad
<br>
if we don't accept their claims. But fundamentally this is a political
<br>
prediction, not a philosophical argument.
<br>
<p>On Tubules and Quantum Computing
<br>
Over the past several years, Roger Penrose, a noted physicist and
<br>
philosopher, has suggested that fine structures in the neurons called
<br>
tubules perform an exotic form of computation called &quot;quantum computing.&quot;
<br>
Quantum computing is computing using what are called &quot;qu bits&quot; which take
<br>
on all possible combinations of solutions simultaneously. It can be
<br>
considered to be an extreme form of parallel processing (because every
<br>
combination of values of the qu bits are tested simultaneously). Penrose
<br>
suggests that the tubules and their quantum computing capabilities
<br>
complicate the concept of recreating neurons and reinstantiating mind
<br>
files.
<br>
<p>However, there is little to suggest that the tubules contribute to the
<br>
thinking process. Even generous models of human knowledge and capability
<br>
are more than accounted for by current estimates of brain size, based on
<br>
contemporary models of neuron functioning that do not include tubules. In
<br>
fact, even with these tubule-less models, it appears that the brain is
<br>
conservatively designed with many more connections (by several orders of
<br>
magnitude) than it needs for its capabilities and capacity. Recent
<br>
experiments (e.g., the San Diego Institute for Nonlinear Science
<br>
experiments) showing that hybrid biological-nonbiological networks perform
<br>
similarly to all biological networks, while not definitive, are strongly
<br>
suggestive that our tubule-less models of neuron functioning are adequate.
<br>
Lloyd Watts' software simulation of his intricate model of human auditory
<br>
processing uses orders of magnitude less computation than the networks of
<br>
neurons he is simulating, and there is no suggestion that quantum
<br>
computing is needed.
<br>
<p>However, even if the tubules are important, it doesn't change the
<br>
projections I have discussed above to any significant degree. According to
<br>
my model of computational growth, if the tubules multiplied neuron
<br>
complexity by a factor of a thousand (and keep in mind that our current
<br>
tubule-less neuron models are already complex, including on the order of a
<br>
thousand connections per neuron, multiple nonlinearities and other
<br>
details), this would delay our reaching brain capacity by only about 9
<br>
years. If we're off by a factor of a million, that's still only a delay of
<br>
17 years. A factor of a billion is around 24 years (keep in mind
<br>
computation is growing by a double exponential).
<br>
<p>With regard to quantum computing, once again there is nothing to suggest
<br>
that the brain does quantum computing. Just because quantum technology may
<br>
be feasible does not suggest that the brain is capable of it. After all,
<br>
we don't have lasers or even radios in our brains. Although some
<br>
scientists have claimed to detect quantum wave collapse in the brain, no
<br>
one has suggested human capabilities that actually require a capacity for
<br>
quantum computing.
<br>
<p>However, even if the brain does do quantum computing, this does not
<br>
significantly change the outlook for human-level computing (and beyond)
<br>
nor does it suggest that brain downloading is infeasible. First of all, if
<br>
the brain does do quantum computing this would only verify that quantum
<br>
computing is feasible. There would be nothing in such a finding to suggest
<br>
that quantum computing is restricted to biological mechanisms. Biological
<br>
quantum computing mechanisms, if they exist, could be replicated. Indeed,
<br>
recent experiments with small scale quantum computers appear to be
<br>
successful. Even the conventional transistor relies on the quantum effect
<br>
of electron tunneling.
<br>
<p>Penrose suggests that it is impossible to perfectly replicate a set of
<br>
quantum states, so therefore, perfect downloading is impossible. Well, how
<br>
perfect does a download have to be? I am at this moment in a very
<br>
different quantum state (and different in non-quantum ways as well) than I
<br>
was a minute ago (certainly in a very different state than I was before I
<br>
wrote this paragraph). If we develop downloading technology to the point
<br>
where the &quot;copies&quot; are as close to the original as the original person
<br>
changes anyway in the course of one minute, that would be good enough for
<br>
any conceivable purpose, yet does not require copying quantum states. As
<br>
the technology improves, the accuracy of the copy could become as close as
<br>
the original changes within ever briefer periods of time (e.g., one
<br>
second, one millisecond, one microsecond).
<br>
<p>When it was pointed out to Penrose that neurons (and even neural
<br>
connections) were too big for quantum computing, he came up with the
<br>
tubule theory as a possible mechanism for neural quantum computing. So the
<br>
concern with quantum computing and tubules have been introduced together.
<br>
If one is searching for barriers to replicating brain function, it is an
<br>
ingenious theory, but it fails to introduce any genuine barriers. There is
<br>
no evidence for it, and even if true, it only delays matters by a decade
<br>
or two. There is no reason to believe that biological mechanisms
<br>
(including quantum computing) are inherently impossible to replicate using
<br>
nonbiological materials and mechanisms. Dozens of contemporary experiments
<br>
are successfully performing just such replications.
<br>
<p>The Noninvasive Surgery-Free Reversible Programmable Distributed Brain
<br>
Implant, Full-Immersion Shared Virtual Reality Environments, Experience
<br>
Beamers, and Brain Expansion
<br>
How will we apply technology that is more intelligent than its creators?
<br>
One might be tempted to respond &quot;Carefully!&quot; But let's take a look at some
<br>
examples.
<br>
<p>Consider several examples of the nanobot technology, which, based on
<br>
miniaturization and cost reduction trends, will be feasible within 30
<br>
years. In addition to scanning your brain, the nanobots will also be able
<br>
to expand our experiences and our capabilities.
<br>
<p>Nanobot technology will provide fully immersive, totally convincing
<br>
virtual reality in the following way. The nanobots take up positions in
<br>
close physical proximity to every interneuronal connection coming from all
<br>
of our senses (e.g., eyes, ears, skin). We already have the technology for
<br>
electronic devices to communicate with neurons in both directions that
<br>
requires no direct physical contact with the neurons. For example,
<br>
scientists at the Max Planck Institute have developed &quot;neuron transistors&quot;
<br>
that can detect the firing of a nearby neuron, or alternatively, can cause
<br>
a nearby neuron to fire, or suppress it from firing. This amounts to
<br>
two-way communication between neurons and the electronic-based neuron
<br>
transistors. The Institute scientists demonstrated their invention by
<br>
controlling the movement of a living leech from their computer. Again, the
<br>
primary aspect of nanobot-based virtual reality that is not yet feasible
<br>
is size and cost.
<br>
<p>When we want to experience real reality, the nanobots just stay in
<br>
position (in the capillaries) and do nothing. If we want to enter virtual
<br>
reality, they suppress all of the inputs coming from the real senses, and
<br>
replace them with the signals that would be appropriate for the virtual
<br>
environment. You (i.e., your brain) could decide to cause your muscles and
<br>
limbs to move as you normally would, but the nanobots again intercept
<br>
these interneuronal signals, suppress your real limbs from moving, and
<br>
instead cause your virtual limbs to move and provide the appropriate
<br>
movement and reorientation in the virtual environment.
<br>
<p>The web will provide a panoply of virtual environments to explore. Some
<br>
will be recreations of real places, others will be fanciful environments
<br>
that have no &quot;real&quot; counterpart. Some indeed would be impossible in the
<br>
physical world (perhaps, because they violate the laws of physics). We
<br>
will be able to &quot;go&quot; to these virtual environments by ourselves, or we
<br>
will meet other people there, both real people and simulated people. Of
<br>
course, ultimately there won't be a clear distinction between the two.
<br>
<p>By 2030, going to a web site will mean entering a full immersion virtual
<br>
reality environment. In addition to encompassing all of the senses, these
<br>
shared environments can include emotional overlays as the nanobots will be
<br>
capable of triggering the neurological correlates of emotions, sexual
<br>
pleasure, and other derivatives of our sensory experience and mental
<br>
reactions.
<br>
<p>In the same way that people today beam their lives from web cams in their
<br>
bedrooms, &quot;experience beamers&quot; circa 2030 will beam their entire flow of
<br>
sensory experiences, and if so desired, their emotions and other secondary
<br>
reactions. We'll be able to plug in (by going to the appropriate web site)
<br>
and experience other people's lives as in the plot concept of 'Being John
<br>
Malkovich.' Particularly interesting experiences can be archived and
<br>
relived at any time.
<br>
<p>We won't need to wait until 2030 to experience shared virtual reality
<br>
environments, at least for the visual and auditory senses. Full immersion
<br>
visual-auditory environments will be available by the end of this decade
<br>
with images written directly onto our retinas by our eyeglasses and
<br>
contact lenses. All of the electronics for the computation, image
<br>
reconstruction, and very high bandwidth wireless connection to the
<br>
Internet will be embedded in our glasses and woven into our clothing, so
<br>
computers as distinct objects will disappear.
<br>
<p>In my view, the most significant implication of the Singularity will be
<br>
the merger of biological and nonbiological intelligence. First, it is
<br>
important to point out that well before the end of the twenty-first
<br>
century, thinking on nonbiological substrates will dominate. Biological
<br>
thinking is stuck at 1026 calculations per second (for all biological
<br>
human brains), and that figure will not appreciably change, even with
<br>
bioengineering changes to our genome. Nonbiological intelligence, on the
<br>
other hand, is growing at a double exponential rate and will vastly exceed
<br>
biological intelligence well before the middle of this century. However,
<br>
in my view, this nonbiological intelligence should still be considered
<br>
human as it is fully derivative of the human-machine civilization. The
<br>
merger of these two worlds of intelligence is not merely a merger of
<br>
biological and nonbiological thinking mediums, but more importantly one of
<br>
method and organization of thinking.
<br>
<p>One of the key ways in which the two worlds can interact will be through
<br>
the nanobots. Nanobot technology will be able to expand our minds in
<br>
virtually any imaginable way. Our brains today are relatively fixed in
<br>
design. Although we do add patterns of interneuronal connections and
<br>
neurotransmitter concentrations as a normal part of the learning process,
<br>
the current overall capacity of the human brain is highly constrained,
<br>
restricted to a mere hundred trillion connections. Brain implants based on
<br>
massively distributed intelligent nanobots will ultimately expand our
<br>
memories a trillion fold, and otherwise vastly improve all of our sensory,
<br>
pattern recognition, and cognitive abilities. Since the nanobots are
<br>
communicating with each other over a wireless local area network, they can
<br>
create any set of new neural connections, can break existing connections
<br>
(by suppressing neural firing), can create new hybrid
<br>
biological-nonbiological networks, as well as add vast new nonbiological
<br>
networks.
<br>
<p>Using nanobots as brain extenders is a significant improvement over the
<br>
idea of surgically installed neural implants, which are beginning to be
<br>
used today (e.g., ventral posterior nucleus, subthalmic nucleus, and
<br>
ventral lateral thalamus neural implants to counteract Parkinson's Disease
<br>
and tremors from other neurological disorders, cochlear implants, and
<br>
others.) Nanobots will be introduced without surgery, essentially just by
<br>
injecting or even swallowing them. They can all be directed to leave, so
<br>
the process is easily reversible. They are programmable, in that they can
<br>
provide virtual reality one minute, and a variety of brain extensions the
<br>
next. They can change their configuration, and clearly can alter their
<br>
software. Perhaps most importantly, they are massively distributed and
<br>
therefore can take up billions or trillions of positions throughout the
<br>
brain, whereas a surgically introduced neural implant can only be placed
<br>
in one or at most a few locations.
<br>
<p>The Double Exponential Growth of the Economy During the 1990s Was Not a
<br>
Bubble
<br>
Yet another manifestation of the law of accelerating returns as it rushes
<br>
toward the Singularity can be found in the world of economics, a world
<br>
vital to both the genesis of the law of accelerating returns, and to its
<br>
implications. It is the economic imperative of a competitive marketplace
<br>
that is driving technology forward and fueling the law of accelerating
<br>
returns. In turn, the law of accelerating returns, particularly as it
<br>
approaches the Singularity, is transforming economic relationships.
<br>
<p>Virtually all of the economic models taught in economics classes, used by
<br>
the Federal Reserve Board to set monetary policy, by Government agencies
<br>
to set economic policy, and by economic forecasters of all kinds are
<br>
fundamentally flawed because they are based on the intuitive linear view
<br>
of history rather than the historically based exponential view. The reason
<br>
that these linear models appear to work for a while is for the same reason
<br>
that most people adopt the intuitive linear view in the first place:
<br>
exponential trends appear to be linear when viewed (and experienced) for a
<br>
brief period of time, particularly in the early stages of an exponential
<br>
trend when not much is happening. But once the &quot;knee of the curve&quot; is
<br>
achieved and the exponential growth explodes, the linear models break
<br>
down. The exponential trends underlying productivity growth are just
<br>
beginning this explosive phase.
<br>
<p>The economy (viewed either in total or per capita) has been growing
<br>
exponentially throughout this century:
<br>
<p>There is also a second level of exponential growth, but up until recently
<br>
the second exponent has been in the early phase so that the growth in the
<br>
growth rate has not been noticed. However, this has changed in this past
<br>
decade, during which the rate of growth has been noticeably exponential.
<br>
<p>Productivity (economic output per worker) has also been growing
<br>
exponentially. Even these statistics are greatly understated because they
<br>
do not fully reflect significant improvements in the quality and features
<br>
of products and services. It is not the case that &quot;a car is a car;&quot; there
<br>
have been significant improvements in safety, reliability, and features.
<br>
There are a myriad of such examples. Pharmaceutical drugs are increasingly
<br>
effective. Groceries ordered in five minutes on the web and delivered to
<br>
your door are worth more than groceries on a supermarket shelf that you
<br>
have to fetch yourself. Clothes custom manufactured for your unique body
<br>
scan are worth more than clothes you happen to find left on a store rack.
<br>
These sorts of improvements are true for most product categories, and none
<br>
of them are reflected in the productivity statistics.
<br>
<p>The statistical methods underlying the productivity measurements tend to
<br>
factor out gains by essentially concluding that we still only get one
<br>
dollar of products and services for a dollar despite the fact that we get
<br>
much more for a dollar (e.g., compare a $1,000 computer today to one ten
<br>
years ago). University of Chicago Professor Pete Klenow and University of
<br>
Rochester Professor Mark Bils estimate that the value of existing goods
<br>
has been increasing at 1.5% per year for the past 20 years because of
<br>
qualitative improvements. This still does not account for the introduction
<br>
of entirely new products and product categories. The Bureau of Labor
<br>
Statistics, which is responsible for the inflation statistics, uses a
<br>
model that incorporates an estimate of quality growth at only 0.5% per
<br>
year, reflecting a systematic underestimate of quality improvement and a
<br>
resulting overestimate of inflation by at least 1 percent per year.
<br>
<p>Despite these weaknesses in the productivity statistical methods, the
<br>
gains in productivity are now reaching the steep part of the exponential
<br>
curve. Labor productivity grew at 1.6% per year until 1994, then rose at
<br>
2.4% per year, and is now growing even more rapidly. In the quarter ending
<br>
July 30, 2000, labor productivity grew at 5.3%. Manufacturing productivity
<br>
grew at 4.4% annually from 1995 to 1999, durables manufacturing at 6.5%
<br>
per year.
<br>
<p>The 1990s have seen the most powerful deflationary forces in history. This
<br>
is why we are not seeing inflation. Yes, it's true that low unemployment,
<br>
high asset values, economic growth, and other such factors are
<br>
inflationary, but these factors are offset by the double exponential
<br>
trends in the price-performance of all information based technologies:
<br>
computation, memory, communications, biotechnology, miniaturization, and
<br>
even the overall rate of technical progress. These technologies deeply
<br>
affect all industries.
<br>
<p>We are also undergoing massive disintermediation in the channels of
<br>
distribution through the web and other new communication technologies, as
<br>
well as escalating efficiencies in operations and administration.
<br>
<p>All of the technology trend charts in this précis e represent massive
<br>
deflation. There are many examples of the impact of these escalating
<br>
efficiencies. BP Amoco's cost for finding oil is now less than $1 per
<br>
barrel, down from nearly $10 in 1991. Processing an internet transaction
<br>
costs a bank one penny, compared to over $1 using a teller ten years ago.
<br>
A Roland Berger / Deutsche Bank study estimates a cost savings of $1200
<br>
per North American car over the next five years. A more optimistic Morgan
<br>
Stanley study estimates that Internet-based procurement will save Ford,
<br>
GM, and DaimlerChrysler about $2700 per vehicle. Software prices are
<br>
deflating even more quickly than computer hardware.
<br>
<p>Software Price-Performance Has Also Improved at an Exponential Rate
<br>
(Example: Automatic Speech Recognition Software
<br>
&nbsp;1985 1995 2000
<br>
Price  $5,000 $500 $50
<br>
Vocabulary Size (# words)  1,000 10,000 100,000
<br>
Continuous Speech?  No No Yes
<br>
User Training Required (Minutes)  180 60 5
<br>
Accuracy  Poor Fair Good
<br>
<p>Current economic policy is based on outdated models which include energy
<br>
prices, commodity prices, and capital investment in plant and equipment as
<br>
key driving factors, but do not adequately model bandwidth, MIPs,
<br>
megabytes, intellectual property, knowledge, and other increasingly vital
<br>
(and increasingly increasing) constituents that are driving the economy.
<br>
<p>The economy &quot;wants&quot; to grow more than the 3.5% per year, which constitutes
<br>
the current &quot;speed limit&quot; that the Federal Reserve bank and other policy
<br>
makers have established as &quot;safe,&quot; meaning noninflationary. But in keeping
<br>
with the law of accelerating returns, the economy is capable of &quot;safely&quot;
<br>
establishing this level of growth in less than a year, implying a growth
<br>
rate in an entire year of greater than 3.5%. Recently, the growth rate has
<br>
exceeded 5%.
<br>
<p>None of this means that cycles of recession will disappear immediately.
<br>
The economy still has some of the underlying dynamics that historically
<br>
have caused cycles of recession, specifically excessive commitments such
<br>
as capital intensive projects and the overstocking of inventories.
<br>
However, the rapid dissemination of information, sophisticated forms of
<br>
online procurement, and increasingly transparent markets in all industries
<br>
have diminished the impact of this cycle. So &quot;recessions&quot; are likely to be
<br>
shallow and short lived. The underlying long-term growth rate will
<br>
continue at a double exponential rate.
<br>
<p>The overall growth of the economy reflects completely new forms and layers
<br>
of wealth and value that did not previously exist, or least that did not p
<br>
reviously constitute a significant portion of the economy (but do now):
<br>
intellectual property, communication portals, web sites, bandwidth,
<br>
software, data bases, and many other new technology based categories.
<br>
<p>There is no need for high interest rates to counter an inflation that
<br>
doesn't exist. The inflationary pressures which exist are counterbalanced
<br>
by all of the deflationary forces I've mentioned. The current high
<br>
interest rates fostered by the Federal Reserve Bank are destructive, are
<br>
causing trillions of dollars of lost wealth, are regressive, hurt business
<br>
and the middle class, and are completely unnecessary.
<br>
<p>The Fed's monetary policy is only influential because people believe it to
<br>
be. It has little real power. The economy today is largely backed by
<br>
private capital in the form of a growing variety of equity instruments.
<br>
The portion of available liquidity in the economy that the Fed actually
<br>
controls is relatively insignificant. The reserves that banks and
<br>
financial institutions maintain with the Federal Reserve System are less
<br>
than $50 billion, which is only 0.6% of the GDP, and 0.25% of the
<br>
liquidity available in stocks.
<br>
<p>Restricting the growth rate of the economy to an arbitrary limit makes as
<br>
much sense as restricting the rate at which a company can grow its
<br>
revenues--or its market cap. Speculative fever will certainly occur and
<br>
there will necessarily continue to be high profile failures and market
<br>
corrections. However the ability of technology companies to rapidly create
<br>
new--real--wealth is just one of the factors that will continue to fuel
<br>
ongoing double exponential growth in the economy. These policies have led
<br>
to an &quot;Alice in Wonderland&quot; situation in which the market goes up on bad
<br>
economic news (because it means that more unnecessary punishment will be
<br>
avoided) and goes down on good economic news.
<br>
<p>Speaking of market speculative fever and market corrections, the stock
<br>
market values for so-called &quot;B to B&quot; (Business to Business) and &quot;B to C&quot;
<br>
(Business to Consumer) web portals and enabling technologies is likely to
<br>
come back strongly as it becomes clear that economic transactions are
<br>
indeed escalating toward e-commerce, and that the (surviving) contenders
<br>
are capable of demonstrating profitable business models.
<br>
<p>The intuitive linear assumption underlying economic thinking reaches its
<br>
most ludicrous conclusions in the political debate surrounding the
<br>
long-term future of the social security system. The economic models used
<br>
for the social security projections are entirely linear, i.e., they
<br>
reflect fixed economic growth. This might be viewed as conservative
<br>
planning if we were talking about projections of only a few years, but
<br>
they become utterly unrealistic for the three to four decades being
<br>
discussed. These projections actually assume a fixed rate of growth of
<br>
3.5% per year for the next fifty years! There are incredibly naïve
<br>
assumptions that bear on both sides of the argument. On the one hand,
<br>
there will be radical extensions to human longevity, while on the other
<br>
hand, we will benefit from far greater economic expansion. These factors
<br>
do not rule each other out, however, as the positive factors are stronger,
<br>
and will ultimately dominate. Moreover, we are certain to rethink social
<br>
security when we have centenarians who look and act like 30 year-olds (but
<br>
who will think much faster than 30 year-olds circa the year 2000).
<br>
<p>Another implication of the law of accelerating returns is exponential
<br>
growth in education and learning. Over the past 120 years, we have
<br>
increased our investment in K-12 education (per student and in constant
<br>
dollars) by a factor of ten. We have a one hundred fold increase in the
<br>
number of college students. Automation started by amplifying the power of
<br>
our muscles, and in recent times has been amplifying the power of our
<br>
minds. Thus, for the past two centuries, automation has been eliminating
<br>
jobs at the bottom of the skill ladder while creating new (and better
<br>
paying) jobs at the top of the skill ladder. So the ladder has been moving
<br>
up, and thus we have been exponentially increasing investments in
<br>
education at all levels.
<br>
<p>Oh, and about that &quot;offer&quot; at the beginning of this précis, consider that
<br>
present stock values are based on future expectations. Given that the
<br>
(literally) short sighted linear intuitive view represents the ubiquitous
<br>
outlook, the common wisdom in economic expectations are dramatically
<br>
understated. Although stock prices reflect the consensus of a buyer-seller
<br>
market, it nonetheless reflects the underlying linear assumption regarding
<br>
future economic growth. But the law of accelerating returns clearly
<br>
implies that the growth rate will continue to grow exponentially because
<br>
the rate of progress will continue to accelerate. Although (weakening)
<br>
recessionary cycles will continue to cause immediate growth rates to
<br>
fluctuate, the underlying rate of growth will continue to double
<br>
approximately every decade.
<br>
<p>But wait a second, you said that I would get $40 trillion if I read and
<br>
understood this précis .
<br>
<p>That's right. According to my models, if we replace the linear outlook
<br>
with the more appropriate exponential outlook, current stock prices should
<br>
triple. Since there's about $20 trillion in the equity markets, that's $40
<br>
trillion in additional wealth.
<br>
<p>But you said I would get that money.
<br>
<p>No, I said &quot;you&quot; would get the money, and that's why I suggested reading
<br>
the sentence carefully. The English word &quot;you&quot; can be singular or plural.
<br>
I meant it in the sense of &quot;all of you.&quot;
<br>
<p>I see, all of us as in the whole world. But not everyone will read this
<br>
précis .
<br>
<p>Well, but everyone could. So if all of you read this précis and understand
<br>
it, then economic expectations would be based on the historical
<br>
exponential model, and thus stock values would increase.
<br>
<p>You mean if everyone understands it, and agrees with it.
<br>
<p>Okay, I suppose I was assuming that.
<br>
<p>Is that what you expect to happen.
<br>
<p>Well, actually, no. Putting on my futurist hat again, my prediction is
<br>
that indeed these views will prevail, but only over time, as more and more
<br>
evidence of the exponential nature of technology and its impact on the
<br>
economy becomes apparent. This will happen gradually over the next several
<br>
years, which will represent a strong continuing updraft for the market.
<br>
<p>A Clear and Future Danger
<br>
Technology has always been a double edged sword, bringing us longer and
<br>
healthier life spans, freedom from physical and mental drudgery, and many
<br>
new creative possibilities on the one hand, while introducing new and
<br>
salient dangers on the other. We still live today with sufficient nuclear
<br>
weapons (not all of which appear to be well accounted for) to end all
<br>
mammalian life on the planet. Bioengineering is in the early stages of
<br>
enormous strides in reversing disease and aging processes. However, the
<br>
means and knowledge will soon exist in a routine college bioengineering
<br>
lab (and already exists in more sophisticated labs) to create unfriendly
<br>
pathogens more dangerous than nuclear weapons. As technology accelerates
<br>
toward the Singularity, we will see the same intertwined potentials: a
<br>
feast of creativity resulting from human intelligence expanded a
<br>
trillion-fold combined with many grave new dangers.
<br>
<p>Consider unrestrained nanobot replication. Nanobot technology requires
<br>
billions or trillions of such intelligent devices to be useful. The most
<br>
cost effective way to scale up to such levels is through self-replication,
<br>
essentially the same approach used in the biological world. And in the
<br>
same way that biological self-replication gone awry (i.e., cancer) results
<br>
in biological destruction, a defect in the mechanism curtailing nanobot
<br>
self-replication would endanger all physical entities, biological or
<br>
otherwise.
<br>
<p>Other primary concerns include &quot;who is controlling the nanobots?&quot; and &quot;who
<br>
are the nanobots talking to?&quot; Organizations (e.g., governments, extremist
<br>
groups) or just a clever individual could put trillions of undetectable
<br>
nanobots in the water or food supply of an individual or of an entire
<br>
population. These &quot;spy&quot; nanobots could then monitor, influence, and even
<br>
control our thoughts and actions. In addition to introducing physical spy
<br>
nanobots, existing nanobots could be influenced through software viruses
<br>
and other software &quot;hacking&quot; techniques. When there is software running in
<br>
our brains, issues of privacy and security will take on a new urgency.
<br>
<p>My own expectation is that the creative and constructive applications of
<br>
this technology will dominate, as I believe they do today. But there will
<br>
be a valuable (and increasingly vocal) role for a concerned and
<br>
constructive Luddite movement (i.e., anti-technologists inspired by early
<br>
nineteenth century weavers who destroyed labor-saving machinery in
<br>
protest).
<br>
<p>If we imagine describing the dangers that exist today to people who lived
<br>
a couple of hundred years ago, they would think it mad to take such risks.
<br>
On the other hand, how many people in the year 2000 would really want to
<br>
go back to the short, brutish, disease-filled, poverty-stricken,
<br>
disaster-prone lives that 99 percent of the human race struggled through a
<br>
couple of centuries ago? We may romanticize the past, but up until fairly
<br>
recently, most of humanity lived extremely fragile lives where one all too
<br>
common misfortune could spell disaster. Substantial portions of our
<br>
species still live in this precarious way, which is at least one reason to
<br>
continue technological progress and the economic enhancement that
<br>
accompanies it.
<br>
<p>People often go through three stages in examining the impact of future
<br>
technology: awe and wonderment at its potential to overcome age old
<br>
problems, then a sense of dread at a new set of grave dangers that
<br>
accompany these new technologies, followed, finally and hopefully, by the
<br>
realization that the only viable and responsible path is to set a careful
<br>
course that can realize the promise while managing the peril.
<br>
<p>In his cover story for WIRED Why The Future Doesn't Need Us, Bill Joy
<br>
eloquently described the plagues of centuries' past, and how new
<br>
self-replicating technologies, such as mutant bioengineered pathogens, and
<br>
&quot;nanobots&quot; run amok, may bring back long forgotten pestilence. Indeed
<br>
these are real dangers. It is also the case, which Joy acknowledges, that
<br>
it has been technological advances, such as antibiotics and improved
<br>
sanitation, which has freed us from the prevalence of such plagues.
<br>
Suffering in the world continues and demands our steadfast attention.
<br>
Should we tell the millions of people afflicted with cancer and other
<br>
devastating conditions that we are canceling the development of all
<br>
bioengineered treatments because there is a risk that these same
<br>
technologies may someday be used for malevolent purposes? Having asked the
<br>
rhetorical question, I realize that there is a movement to do exactly
<br>
that, but I think most people would agree that such broad based
<br>
relinquishment is not the answer.
<br>
<p>The continued opportunity to alleviate human distress is one important
<br>
motivation for continuing technological advancement. Also compelling are
<br>
the already apparent economic gains I discussed above which will continue
<br>
to hasten in the decades ahead. The continued acceleration of many
<br>
intertwined technologies are roads paved with gold (I use the plural here
<br>
because technology is clearly not a single path). In a competitive
<br>
environment, it is an economic imperative to go down these roads.
<br>
Relinquishing technological advancement would be economic suicide for
<br>
individuals, companies, and nations.
<br>
<p>Which brings us to the issue of relinquishment, which is Bill Joy's most
<br>
controversial recommendation and personal commitment. I do feel that
<br>
relinquishment at the right level is part of a responsible and
<br>
constructive response to these genuine perils. The issue, however, is
<br>
exactly this: at what level are we to relinquish technology?
<br>
<p>Ted Kaczynski would have us renounce all of it. This, in my view, is
<br>
neither desirable nor feasible, and the futility of such a position is
<br>
only underscored by the senselessness of Kaczynski's deplorable tactics.
<br>
<p>Another level would be to forego certain fields; nanotechnology, for
<br>
example, that might be regarded as too dangerous. But such sweeping
<br>
strokes of relinquishment are equally untenable. Nanotechnology is simply
<br>
the inevitable end result of the persistent trend toward miniaturization
<br>
which pervades all of technology. It is far from a single centralized
<br>
effort, but is being pursued by a myriad of projects with many diverse
<br>
goals.
<br>
<p>One observer wrote:
<br>
<p>&quot;A further reason why industrial society cannot be reformed. . . is that
<br>
modern technology is a unified system in which all parts are dependent on
<br>
one another. You can't get rid of the &quot;bad&quot; parts of technology and retain
<br>
only the &quot;good&quot; parts. Take modern medicine, for example. Progress in
<br>
medical science depends on progress in chemistry, physics, biology,
<br>
computer science and other fields. Advanced medical treatments require
<br>
expensive, high-tech equipment that can be made available only by a
<br>
technologically progressive, economically rich society. Clearly you can't
<br>
have much progress in medicine without the whole technological system and
<br>
everything that goes with it.&quot;
<br>
The observer I am quoting is, again, Ted Kaczynski. Although one might
<br>
properly resist Kaczynski as an authority, I believe he is correct on the
<br>
deeply entangled nature of the benefits and risks. However, Kaczynski and
<br>
I clearly part company on our overall assessment on the relative balance
<br>
between the two. Bill Joy and I have dialogued on this issue both publicly
<br>
and privately, and we both believe that technology will and should
<br>
progress, and that we need to be actively concerned with the dark side. If
<br>
Bill and I disagree, it's on the granularity of relinquishment that is
<br>
both feasible and desirable.
<br>
<p>Abandonment of broad areas of technology will only push them underground
<br>
where development would continue unimpeded by ethics and regulation. In
<br>
such a situation, it would be the less stable, less responsible
<br>
practitioners (e.g., the terrorists) who would have all the expertise.
<br>
<p>I do think that relinquishment at the right level needs to be part of our
<br>
ethical response to the dangers of twenty first century technologies. One
<br>
constructive example of this is the proposed ethical guideline by the
<br>
Foresight Institute, founded by nanotechnology pioneer Eric Drexler, that
<br>
nanotechnologists agree to relinquish the development of physical entities
<br>
that can self-replicate in a natural environment. Another is a ban on
<br>
self-replicating physical entities that contain their own codes for
<br>
self-replication. In what nanotechnologist Ralph Merkle calls the
<br>
&quot;Broadcast Architecture,&quot; such entities would have to obtain such codes
<br>
from a centralized secure server, which would guard against undesirable
<br>
replication. The Broadcast Architecture is impossible in the biological
<br>
world, which represents at least one way in which nanotechnology can be
<br>
made safer than biotechnology. In other ways, nanotech is potentially more
<br>
dangerous because nanobots can be physically stronger than protein-based
<br>
entities and more intelligent. It will eventually be possible to combine
<br>
the two by having nanotechnology provide the codes within biological
<br>
entities (replacing DNA), in which case biological entities can use the
<br>
much safer Broadcast Architecture.
<br>
<p>Our ethics as responsible technologists should include such &quot;fine grained&quot;
<br>
relinquishment, among other professional ethical guidelines. Other
<br>
protections will need to include oversight by regulatory bodies, the
<br>
development of technology-specific &quot;immune&quot; responses, as well as computer
<br>
assisted surveillance by law enforcement organizations. Many people are
<br>
not aware that our intelligence agencies already use advanced technologies
<br>
such as automated word spotting to monitor a substantial flow of telephone
<br>
conversations. As we go forward, balancing our cherished rights of privacy
<br>
with our need to be protected from the malicious use of powerful twenty
<br>
first century technologies will be one of many profound challenges. This
<br>
is one reason that such issues as an encryption &quot;trap door&quot; (in which law
<br>
enforcement authorities would have access to otherwise secure information)
<br>
and the FBI &quot;Carnivore&quot; email-snooping system have been so contentious.
<br>
<p>As a test case, we can take a small measure of comfort from how we have
<br>
dealt with one recent technological challenge. There exists today a new
<br>
form of fully nonbiological self replicating entity that didn't exist just
<br>
a few decades ago: the computer virus. When this form of destructive
<br>
intruder first appeared, strong concerns were voiced that as they became
<br>
more sophisticated, software pathogens had the potential to destroy the
<br>
computer network medium they live in. Yet the &quot;immune system&quot; that has
<br>
evolved in response to this challenge has been largely effective. Although
<br>
destructive self-replicating software entities do cause damage from time
<br>
to time, the injury is but a small fraction of the benefit we receive from
<br>
the computers and communication links that harbor them. No one would
<br>
suggest we do away with computers, local area networks, and the Internet
<br>
because of software viruses.
<br>
<p>One might counter that computer viruses do not have the lethal potential
<br>
of biological viruses or of destructive nanotechnology. Although true,
<br>
this strengthens my observation. The fact that computer viruses are not
<br>
usually deadly to humans only means that more people are willing to create
<br>
and release them. It also means that our response to the danger is that
<br>
much less intense. Conversely, when it comes to self replicating entities
<br>
that are potentially lethal on a large scale, our response on all levels
<br>
will be vastly more serious.
<br>
<p>Technology will remain a double edged sword, and the story of the Twenty
<br>
First century has not yet been written. It represents vast power to be
<br>
used for all humankind's purposes. We have no choice but to work hard to
<br>
apply these quickening technologies to advance our human values, despite
<br>
what often appears to be a lack of consensus on what those values should
<br>
be.
<br>
<p>Living Forever
<br>
Once brain porting technology has been refined and fully developed, will
<br>
this enable us to live forever? The answer depends on what we mean by
<br>
living and dying. Consider what we do today with our personal computer
<br>
files. When we change from one personal computer to a less obsolete model,
<br>
we don't throw all our files away; rather we copy them over to the new
<br>
hardware. Although our software files do not necessary continue their
<br>
existence forever, the longevity of our personal computer software is
<br>
completely separate and disconnected from the hardware that it runs on.
<br>
When it comes to our personal mind file, however, when our human hardware
<br>
crashes, the software of our lives dies with it. However, this will not
<br>
continue to be the case when we have the means to store and restore the
<br>
thousands of trillions of bytes of information represented in the pattern
<br>
that we call our brains.
<br>
<p>The longevity of one's mind file will not be dependent, therefore, on the
<br>
continued viability of any particular hardware medium. Ultimately
<br>
software-based humans, albeit vastly extended beyond the severe
<br>
limitations of humans as we know them today, will live out on the web,
<br>
projecting bodies whenever they need or want them, including virtual
<br>
bodies in diverse realms of virtual reality, holographically projected
<br>
bodies, physical bodies comprised of nanobot swarms, and other forms of
<br>
nanotechnology.
<br>
<p>A software-based human will be free, therefore, from the constraints of
<br>
any particular thinking medium. Today, we are each confined to a mere
<br>
hundred trillion connections, but humans at the end of the twenty-first
<br>
century can grow their thinking and thoughts without limit. We may regard
<br>
this as a form of immortality, although it is worth pointing out that data
<br>
and information do not necessarily last forever. Although not dependent on
<br>
the viability of the hardware it runs on, the longevity of information
<br>
depends on its relevance, utility, and accessibility. If you've ever tried
<br>
to retrieve information from an obsolete form of data storage in an old
<br>
obscure format (e.g., a reel of magnetic tape from a 1970 minicomputer),
<br>
you will understand the challenges in keeping software viable. However, if
<br>
we are diligent in maintaining our mind file, keeping current backups, and
<br>
porting to current formats and mediums, then a form of immortality can be
<br>
attained, at least for software-based humans. Our mind file--our
<br>
personality, skills, memories--all of that is lost today when our
<br>
biological hardware crashes. When we can access, store, and restore that
<br>
information, then its longevity will no longer be tied to our hardware
<br>
permanence.
<br>
<p>Is this form of immortality the same concept as a physical human, as we
<br>
know them today, living forever? In one sense it is, because as I pointed
<br>
out earlier, our contemporary selves are not a constant collection of
<br>
matter either. Only our pattern of matter and energy persists, and even
<br>
that gradually changes. Similarly, it will be the pattern of a software
<br>
human that persists and develops and changes gradually.
<br>
<p>But is that person based on my mind file, who migrates across many
<br>
computational substrates, and who outlives any particular thinking medium,
<br>
really me? We come back to the same questions of consciousness and
<br>
identity, issues that have been debated since the Platonic dialogues. As
<br>
we go through the twenty-first century, these will not remain polite
<br>
philosophical debates, but will be confronted as vital, practical,
<br>
political, and legal issues.
<br>
<p>A related question is &quot;is death desirable?&quot; A great deal of our effort
<br>
goes into avoiding it. We make extraordinary efforts to delay it, and
<br>
indeed often consider its intrusion a tragic event. Yet we might find it
<br>
hard to live without it. We consider death as giving meaning to our lives.
<br>
It gives importance and value to time. Time could become meaningless if
<br>
there were too much of it.
<br>
<p>The Next Step in Evolution and the Purpose of Life
<br>
But I regard the freeing of the human mind from its severe physical
<br>
limitations of scope and duration as the necessary next step in evolution.
<br>
Evolution, in my view, represents the purpose of life. That is, the
<br>
purpose of life--and of our lives--is to evolve. The Singularity then is
<br>
not a grave danger to be avoided. In my view, this next paradigm shift
<br>
represents the goal of our civilization.
<br>
<p>What does it mean to evolve? Evolution moves toward greater complexity,
<br>
greater elegance, greater knowledge, greater intelligence, greater beauty,
<br>
greater creativity, and more of other abstract and subtle attributes such
<br>
as love. And God has been called all these things, only without any
<br>
limitation: infinite knowledge, infinite intelligence, infinite beauty,
<br>
infinite creativity, infinite love, and so on. Of course, even the
<br>
accelerating growth of evolution never achieves an infinite level, but as
<br>
it explodes exponentially, it certainly moves rapidly in that direction.
<br>
So evolution moves inexorably toward our conception of God, albeit never
<br>
quite reaching this ideal. Thus the freeing of our thinking from the
<br>
severe limitations of its biological form may be regarded as an essential
<br>
spiritual quest.
<br>
<p>In making this statement, it is important to emphasize that terms like
<br>
evolution, destiny, and spiritual quest are observations about the end
<br>
result, not the basis for these predictions. I am not saying that
<br>
technology will evolve to human levels and beyond simply because it is our
<br>
destiny and because of the satisfaction of a spiritual quest. Rather my
<br>
projections result from a methodology based on the dynamics underlying the
<br>
(double) exponential growth of technological processes. The primary force
<br>
driving technology is economic imperative. We are moving toward machines
<br>
with human level intelligence (and beyond) as the result of millions of
<br>
small advances, each with their own particular economic justification.
<br>
<p>To use an example from my own experience at one of my companies (Kurzweil
<br>
Applied Intelligence), whenever we came up with a slightly more
<br>
intelligent version of speech recognition, the new version invariably had
<br>
greater value than the earlier generation and, as a result, sales
<br>
increased. It is interesting to note that in the example of speech
<br>
recognition software, the three primary surviving competitors stayed very
<br>
close to each other in the intelligence of their software. A few other
<br>
companies that failed to do so (e.g., Speech Systems) went out of
<br>
business. At any point in time, we would be able to sell the version prior
<br>
to the latest version for perhaps a quarter of the price of the current
<br>
version. As for versions of our technology that were two generations old,
<br>
we couldn't even give those away. This phenomenon is not only true for
<br>
pattern recognition and other &quot;AI&quot; software, but applies to all products,
<br>
from bread makers to cars. And if the product itself doesn't exhibit some
<br>
level of intelligence, then intelligence in the manufacturing and
<br>
marketing methods have a major effect on the success and profitability of
<br>
an enterprise.
<br>
<p>There is a vital economic imperative to create more intelligent
<br>
technology. Intelligent machines have enormous value. That is why they are
<br>
being built. There are tens of thousands of projects that are advancing
<br>
intelligent machines in diverse incremental ways. The support for &quot;high
<br>
tech&quot; in the business community (mostly software) has grown enormously.
<br>
When I started my optical character recognition (OCR) and speech synthesis
<br>
company (Kurzweil Computer Products, Inc.) in 1974, there were only a
<br>
half-dozen high technology IPO's that year. The number of such deals has
<br>
increased one hundred fold and the number of dollars invested has
<br>
increased by more than one thousand fold in the past 25 years. In the four
<br>
years between 1995 and 1999 alone, high tech venture capital deals
<br>
increased from just over $1 billion to approximately $15 billion.
<br>
<p>We will continue to build more powerful computational mechanisms because
<br>
it creates enormous value. We will reverse-engineer the human brain not
<br>
simply because it is our destiny, but because there is valuable
<br>
information to be found there that will provide insights in building more
<br>
intelligent (and more valuable) machines. We would have to repeal
<br>
capitalism and every visage of economic competition to stop this
<br>
progression.
<br>
<p>By the second half of this next century, there will be no clear
<br>
distinction between human and machine intelligence. On the one hand, we
<br>
will have biological brains vastly expanded through distributed
<br>
nanobot-based implants. On the other hand, we will have fully
<br>
nonbiological brains that are copies of human brains, albeit also vastly
<br>
extended. And we will have a myriad of other varieties of intimate
<br>
connection between human thinking and the technology it has fostered.
<br>
<p>Ultimately, nonbiological intelligence will dominate because it is growing
<br>
at a double exponential rate, whereas for all practical purposes
<br>
biological intelligence is at a standstill. Human thinking is stuck at
<br>
1026 calculations per second (for all biological humans), and that figure
<br>
will never appreciably change (except for a small increase resulting from
<br>
genetic engineering). Nonbiological thinking is still millions of times
<br>
less today, but the cross over will occur before 2030. By the end of the
<br>
twenty-first century, nonbiological thinking will be trillions of
<br>
trillions of times more powerful than that of its biological progenitors,
<br>
although still of human origin. It will continue to be the human-machine
<br>
civilization taking the next step in evolution.
<br>
<p>Most forecasts of the future seem to ignore the revolutionary impact of
<br>
the Singularity in our human destiny: the inevitable emergence of
<br>
computers that match and ultimately vastly exceed the capabilities of the
<br>
human brain, a development that will be no less important than the
<br>
evolution of human intelligence itself some thousands of centuries ago.
<br>
And the primary reason for this failure is that they are based on the
<br>
intuitive but short sighted linear view of history.
<br>
<p>Before the next century is over, the Earth's technology-creating species
<br>
will merge with its computational technology. There will not be a clear
<br>
distinction between human and machine. After all, what is the difference
<br>
between a human brain enhanced a trillion fold by nanobot-based implants,
<br>
and a computer whose design is based on high resolution scans of the human
<br>
brain, and then extended a trillion-fold?
<br>
<p>Why SETI Will Fail (and why we are alone in the Universe)
<br>
The law of accelerating returns implies that by 2099, the intelligence
<br>
that will have emerged from human-machine civilization will be trillions
<br>
of trillions of times more powerful than it is today, dominated of course
<br>
by its nonbiological form.
<br>
<p>So what does this have to do with SETI (the Search for Extra Terrestrial
<br>
Intelligence)? The naïve view, going back to pre-Copernican days, was that
<br>
the Earth was at the center of the Universe, and human intelligence its
<br>
greatest gift (next to God). The more informed recent view is that even if
<br>
the likelihood of a star having a planet with a technology creating
<br>
species is very low (e.g., one in a million), there are so many stars
<br>
(i.e., billions of trillions of them), that there are bound to be many
<br>
with advanced technology.
<br>
<p>This is the view behind SETI, was my view until recently, and is the
<br>
common informed view today. Although SETI has not yet looked everywhere,
<br>
it has already covered a substantial portion of the Universe.
<br>
<p>In the above diagram (courtesy of Scientific American), we can see that
<br>
SETI has already thoroughly searched all star systems within 107
<br>
light-years from Earth for alien civilizations capable (and willing) to
<br>
transmit at a power of at least 1025 watts, a so-called Type II
<br>
civilization (and all star systems within 106 light-years for transmission
<br>
of at least 1018 watts, and so on). No sign of intelligence has been found
<br>
as of yet.
<br>
<p>In a recent email to my research assistant, Dr. Seth Shostak of the SETI
<br>
Institute points out that a new comprehensive targeted search, called
<br>
Project Phoenix, which has up to 100 times the sensitivity and covers a
<br>
greater range of the radio dial as compared to previous searches, has only
<br>
been applied thus far to 500 star systems, which is, of course only a
<br>
minute fraction of the half trillion star systems in just our own galaxy.
<br>
<p>However, according to my model, once a civilization achieves our own level
<br>
(&quot;Earth-level&quot;) of radio transmission, it takes no more than one century,
<br>
two at the most, to achieve what SETI calls a Type II civilization. If the
<br>
assumption that there are at least millions of radio capable civilizations
<br>
out there, and that these civilizations are spread out over millions
<br>
(indeed billions) of years of development, then surely there ought to be
<br>
millions that have achieved Type II status.
<br>
<p>Incidentally, this is not an argument against the SETI project, which in
<br>
my view should have the highest possible priority because the negative
<br>
finding is no less significant than a positive result.
<br>
<p>It is odd that we find the cosmos so silent. Where is everybody? There
<br>
should be millions of civilizations vastly more advanced than our own, so
<br>
we should be noticing their broadcasts. A sufficiently advanced
<br>
civilization would not be likely to restrict its broadcasts to subtle
<br>
signals on obscure frequencies. Why are they so silent, and so shy?
<br>
<p>As I have studied the implications of the law of accelerating returns, I
<br>
have come to a different view.
<br>
<p>Because exponential growth is so explosive, it is the case that once a
<br>
species develops computing technology, it is only a matter of a couple of
<br>
centuries before the nonbiological form of their intelligence explodes. It
<br>
permeates virtually all matter in their vicinity, and then inevitably
<br>
expands outward close to the maximum speed that information can travel.
<br>
Once the nonbiological intelligence emerging from that species' technology
<br>
has saturated its vicinity (and the nature of this saturation is another
<br>
complex issue, which I won't deal with in this précis), it has no other
<br>
way to continue to evolve but to expand outwardly. The expansion does not
<br>
start out at the maximum speed, but quickly achieves a speed within a
<br>
vanishingly small delta from the maximum speed.
<br>
<p>What is the maximum speed? We currently understand this to be the speed of
<br>
light, but there are already tantalizing hints that this may not be an
<br>
absolute limit. There were recent experiments that measured the flight
<br>
time of photons at nearly twice the speed of light, a result of quantum
<br>
uncertainty on their position. However, this result is actually not useful
<br>
for this analysis, because it does not actually allow information to be
<br>
communicated at faster than the speed of light, and we are fundamentally
<br>
interested in communication speed.
<br>
<p>Quantum disentanglement has been measured at many times the speed of
<br>
light, but this is only communicating randomness (profound quantum
<br>
randomness) at speeds far greater than the speed of light; again, this is
<br>
not communication of information (but is of great interest for restoring
<br>
encryption, after quantum computing destroys it). There is the potential
<br>
for worm holes (or folds of the Universe in dimensions beyond the three
<br>
visible ones), but this is not really traveling at faster than the speed
<br>
of light, it just means that the topology of the Universe is not the
<br>
simple three dimensional space that naïve physics implies. But we already
<br>
knew that. However, if worm holes or folds in the Universe are ubiquitous,
<br>
then perhaps these short cuts would allow us to get everywhere quickly.
<br>
Would anyone be shocked if some subtle ways of getting around this speed
<br>
limit were discovered? And no matter how subtle, sufficiently subtle
<br>
technology will find ways to apply it. The point is that if there are ways
<br>
around this limit (or any other currently understood limit), then the
<br>
extraordinary levels of intelligence that our human-machine civilization
<br>
will achieve will find those ways and exploit them.
<br>
<p>So for now, we can say that ultra high levels of intelligence will expand
<br>
outward at the speed of light, but recognize that this may not be the
<br>
actual limit of the speed of expansion, or even if the limit is the speed
<br>
of light that this limit may not restrict reaching other locations
<br>
quickly.
<br>
<p>Consider that the time spans for biological evolution are measured in
<br>
millions and billions of years, so if there are other civilizations out
<br>
there, they would be spread out by huge spans of time. If there are a lot
<br>
of them, as contemporary thinking implies, then it would be very unlikely
<br>
that at least some of them would not be ahead of us. That at least is the
<br>
SETI assumption. And if they are ahead of us, they likely would be ahead
<br>
of us by huge spans of time. The likelihood that any civilization that is
<br>
ahead of us is ahead of us by only a few decades is extremely small.
<br>
<p>If the SETI assumption that there are many (e.g., millions) of
<br>
technological (at least radio capable) civilizations is correct, then at
<br>
least some of them (i.e., millions of them) would be way ahead of us. But
<br>
it takes only a few centuries at most from the advent of computation for
<br>
that civilization to expand outward at at least light speed. Given this,
<br>
how can it be that we have not noticed them?
<br>
<p>The conclusion I reach is that it is likely that there are no such other
<br>
civilizations. In other words, we are in the lead. That's right, our
<br>
humble civilization with its Dodge pick up trucks, fried chicken fast
<br>
food, and ethnic cleansings (and computation!) is in the lead.
<br>
<p>Now how can that be? Isn't this extremely unlikely given the billions of
<br>
trillions of likely planets? Indeed it is very unlikely. But equally
<br>
unlikely is the existence of our Universe with a set of laws of physics so
<br>
exquisitely precisely what is needed for the evolution of life to be
<br>
possible. But by the Anthropic principle, if the Universe didn't allow the
<br>
evolution of life we wouldn't be here to notice it. Yet here we are. So by
<br>
the same Anthropic principle, we're here in the lead in the Universe.
<br>
Again, if we weren't here, we would not be noticing it.
<br>
<p>Let's consider some arguments against this perspective.
<br>
<p>Perhaps there are extremely advanced technological civilizations out
<br>
there, but we are outside their light sphere of intelligence. That is,
<br>
they haven't gotten here yet. Okay, in this case, SETI will still fail
<br>
because we won't be able to see (or hear) them, at least not before we
<br>
reach Singularity.
<br>
<p>Perhaps they are amongst us, but have decided to remain invisible to us.
<br>
Incidentally, I have always considered the science fiction notion of large
<br>
space ships with large squishy creatures similar to us to be very
<br>
unlikely. Any civilization sophisticated enough to make the trip here
<br>
would have long since passed the point of merging with their technology
<br>
and would not need to send such physically bulky organisms and equipment.
<br>
Such a civilization would not have any unmet material needs that require
<br>
it to steal physical resources from us. They would be here for observation
<br>
only, to gather knowledge, which is the only resource of value to such a
<br>
civilization. The intelligence and equipment needed for such observation
<br>
would be extremely small. In this case, SETI will still fail because if
<br>
this civilization decided that it did not want us to notice it, then it
<br>
would succeed in that desire. Keep in mind that they would be vastly more
<br>
intelligent than we are today. Perhaps they will reveal themselves to us
<br>
when we achieve the next level of our evolution, specifically merging our
<br>
biological brains with our technology, which is to say, after the
<br>
Singularity. Moreover, given that the SETI assumption implies that there
<br>
are millions of such highly developed civilizations, it seems odd that all
<br>
of them have made the same decision to stay out of our way.
<br>
<p>Why Intelligence is More Powerful than Physics
<br>
As intelligence saturates the matter and energy available to it, it turns
<br>
dumb matter into smart matter. Although smart matter still nominally
<br>
follows the laws of physics, it is so exquisitely intelligent that it can
<br>
harness the most subtle aspects of the laws to manipulate matter and
<br>
energy to its will. So it would at least appear that intelligence is more
<br>
powerful than physics.
<br>
<p>Perhaps what I should say is that intelligence is more powerful than
<br>
cosmology. That is, once matter evolves into smart matter (matter fully
<br>
saturated with intelligence), it can manipulate matter and energy to do
<br>
whatever it wants. This perspective has not been considered in discussions
<br>
of future cosmology. It is assumed that intelligence is irrelevant to
<br>
events and processes on a cosmological scale. Stars are born and die;
<br>
galaxies go through their cycles of creation and destruction. The Universe
<br>
itself was born in a big bang and will end with a crunch or a whimper,
<br>
we're not yet sure which. But intelligence has little to do with it.
<br>
Intelligence is just a bit of froth, an ebullition of little creatures
<br>
darting in and out of inexorable universal forces. The mindless mechanism
<br>
of the Universe is winding up or down to a distant future, and there's
<br>
nothing intelligence can do about it.
<br>
<p>That's the common wisdom, but I don't agree with it. Intelligence will be
<br>
more powerful than these impersonal forces. Once a planet yields a
<br>
technology creating species and that species creates computation (as has
<br>
happened here on Earth), it is only a matter of a few centuries before its
<br>
intelligence saturates the matter and energy in its vicinity, and it
<br>
begins to expand outward at the speed of light or greater. It will then
<br>
overcome gravity (through exquisite and vast technology) and other
<br>
cosmological forces (or, to be fully accurate, will maneuver and control
<br>
these forces) and create the Universe it wants. This is the goal of the
<br>
Singularity.
<br>
<p>What kind of Universe will that be? Well, just wait and see.
<br>
<p>Plan to Stick Around
<br>
Most of you (again I'm using the plural form of the word) are likely to be
<br>
around to see the Singularity. The expanding human life span is another
<br>
one of those exponential trends. In the eighteenth century, we added a few
<br>
days every year to human longevity; during the nineteenth century we added
<br>
a couple of weeks each year; and now we're adding almost a half a year
<br>
every year. With the revolutions in genomics, proteomics, rational drug
<br>
design, therapeutic cloning of our own organs and tissues, and related
<br>
developments in bio-information sciences, we will be adding more than a
<br>
year every year within ten years. So take care of yourself the old
<br>
fashioned way for just a little while longer, and you may actually get to
<br>
experience the next fundamental paradigm shift in our destiny.
<br>
<p>Copyright (C) Raymond Kurzweil 2001
<br>
--------------------------------------------------
<br>
ô¿ô
<br>
<p>Stay hungry,
<br>
<p>--J. R.
<br>
<p>Useless hypotheses: consciousness, phlogiston, philosophy, vitalism, mind,
<br>
free will
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4700.html">Max More: "The non-existence of posthumans [was: Re: Heston Speech]"</a>
<li><strong>Previous message:</strong> <a href="4698.html">Emlyn: "Jonathan Lebed article on NYT"</a>
<li><strong>In reply to:</strong> <a href="4656.html">hal@finney.org: "Kurzweil's new Singularity/AI page"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4699">[ date ]</a>
<a href="index.html#4699">[ thread ]</a>
<a href="subject.html#4699">[ subject ]</a>
<a href="author.html#4699">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:56:48 MDT</em>
</em>
</small>
</body>
</html>
