<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Paradox--was Re: Active shields, was Re: Critic</title>
<meta name="Author" content="John Marlow (johnmarrek@yahoo.com)">
<meta name="Subject" content="Re: Paradox--was Re: Active shields, was Re: Criticism depth, was   Re:     Homework, Nuke,  etc..">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Paradox--was Re: Active shields, was Re: Criticism depth, was   Re:     Homework, Nuke,  etc..</h1>
<!-- received="Sat Jan 13 22:17:39 2001" -->
<!-- isoreceived="20010114051739" -->
<!-- sent="Sat, 13 Jan 2001 21:17:37 -0800 (PST)" -->
<!-- isosent="20010114051737" -->
<!-- name="John Marlow" -->
<!-- email="johnmarrek@yahoo.com" -->
<!-- subject="Re: Paradox--was Re: Active shields, was Re: Criticism depth, was   Re:     Homework, Nuke,  etc.." -->
<!-- id="20010114051737.3006.qmail@web4403.mail.yahoo.com" -->
<!-- inreplyto="Paradox--was Re: Active shields, was Re: Criticism depth, was   Re:     Homework, Nuke,  etc.." -->
<strong>From:</strong> John Marlow (<a href="mailto:johnmarrek@yahoo.com?Subject=Re:%20Paradox--was%20Re:%20Active%20shields,%20was%20Re:%20Criticism%20depth,%20was%20%20%20Re:%20%20%20%20%20Homework,%20Nuke,%20%20etc..&In-Reply-To=&lt;20010114051737.3006.qmail@web4403.mail.yahoo.com&gt;"><em>johnmarrek@yahoo.com</em></a>)<br>
<strong>Date:</strong> Sat Jan 13 2001 - 22:17:37 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1186.html">Technotranscendence: "Re: That idiot Darwin"</a>
<li><strong>Previous message:</strong> <a href="1184.html">John Marlow: "Re: &quot;God Does Not Play Dice...&quot;   AE"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1194.html">Eliezer S. Yudkowsky: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was    Re:     Homework, Nuke,  etc.."</a>
<li><strong>Reply:</strong> <a href="1194.html">Eliezer S. Yudkowsky: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was    Re:     Homework, Nuke,  etc.."</a>
<li><strong>Reply:</strong> <a href="1220.html">xgl: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was  Re:     Homework, Nuke,  etc.."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1185">[ date ]</a>
<a href="index.html#1185">[ thread ]</a>
<a href="subject.html#1185">[ subject ]</a>
<a href="author.html#1185">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
**What can I say? But for the goal, you seem eminently
<br>
reasonable, as always.
<br>
<p>--- &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Paradox--was%20Re:%20Active%20shields,%20was%20Re:%20Criticism%20depth,%20was%20%20%20Re:%20%20%20%20%20Homework,%20Nuke,%20%20etc..&In-Reply-To=&lt;20010114051737.3006.qmail@web4403.mail.yahoo.com&gt;">sentience@pobox.com</a>&gt;
<br>
wrote:
<br>
<em>&gt; John Marlow wrote:
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; &gt; &gt; In which case, there may not be a damned
</em><br>
<em>&gt; &gt; &gt; &gt; thing we can do about it.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Yep.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; **GAAAK! And you proceed! Or are you simply that
</em><br>
<em>&gt; &gt; convinced of our own incompetence? (But then, this
</em><br>
<em>&gt; &gt; would actually argue AGAINST proceeding, if you
</em><br>
<em>&gt; think
</em><br>
<em>&gt; &gt; about it...)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Why, oh why is it that the machinophobes of this
</em><br>
<em>&gt; world assume that
</em><br>
<em>&gt; researchers never think about the implications of
</em><br>
<em>&gt; their research?
</em><br>
<p>**Me no machinophobe; see nanotech advocacy.
<br>
<p>&nbsp;&nbsp;When I
<br>
<em>&gt; was a kid, I thought I'd grow up to be a physicist
</em><br>
<em>&gt; like my father... maybe
</em><br>
<em>&gt; work on nanotechnology...  I am here, today, in this
</em><br>
<em>&gt; profession, working
</em><br>
<em>&gt; on AI, because I believe that this is not just the
</em><br>
<em>&gt; best, but the *only*
</em><br>
<em>&gt; path to survival for humanity.
</em><br>
<p>**Why?
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; &gt; **No--I am, for example, strongly in favor of
</em><br>
<em>&gt; &gt; developing nanotechnology, and that is by far the
</em><br>
<em>&gt; &gt; biggest risk I can see.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; **Building an AI brighter than we are might run a
</em><br>
<em>&gt; &gt; close second. That I'm not for.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; **The first offers nearly limitless advantages for
</em><br>
<em>&gt; the
</em><br>
<em>&gt; &gt; risk; the second offers only certain disaster.
</em><br>
<em>&gt; It's a
</em><br>
<em>&gt; &gt; risk-benefit analysis.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; On this planet *right now* there exists enough
</em><br>
<em>&gt; networked computing power
</em><br>
<em>&gt; to create an AI.
</em><br>
<p>**Now, forgive me if I've missed something here
<br>
(please point it out)--but isn't AI a matter of
<br>
quality rather than quantity?
<br>
<p>&nbsp;&nbsp;Stopping progress wouldn't be
<br>
<em>&gt; enough.  You'd have to
</em><br>
<em>&gt; move the entire world backwards, and keep it there,
</em><br>
<em>&gt; not just for ten
</em><br>
<em>&gt; years, or for a century, but forever, or else some
</em><br>
<em>&gt; other generation just
</em><br>
<em>&gt; winds up facing the same problem.  It doesn't
</em><br>
<em>&gt; matter, as you describe the
</em><br>
<em>&gt; future, whether AI presents the greater or the
</em><br>
<em>&gt; lesser risk; what matters
</em><br>
<em>&gt; is that a success in Friendly AI makes navigating
</em><br>
<em>&gt; nanotech easier,
</em><br>
<p>**Whoa. Lemme get this straight--you want to create
<br>
something brighter than us, and then put IT in charge
<br>
of developing nanotech? (Handing it, to be &quot;paranoid,&quot;
<br>
the best and most rapid means with which to get rid of
<br>
us, should it choose to do so.)
<br>
<p><p><p>&nbsp;while
<br>
<em>&gt; success in nanotechnology doesn't help AI.  In fact,
</em><br>
<em>&gt; nanotechnology
</em><br>
<em>&gt; provides for computers with thousands or millions of
</em><br>
<em>&gt; times the power of a
</em><br>
<em>&gt; single human brain, at which point it takes no
</em><br>
<em>&gt; subtlety, no knowledge, no
</em><br>
<em>&gt; wisdom to create AI; anyone with a nanocomputer can
</em><br>
<em>&gt; just brute-force it.
</em><br>
<p>**See above--quality v quantity (brute-force).
<br>
<p><em>&gt; 
</em><br>
<em>&gt; It's very easy for me to understand why you're
</em><br>
<em>&gt; concerned.  You've seen a
</em><br>
<em>&gt; bunch of bad-guy machines on TV, and maybe a few
</em><br>
<em>&gt; good-guy machines, and
</em><br>
<em>&gt; every last one of them behaved just like a human.
</em><br>
<p>**Oh don't blame it all on Jim Cameron. On the
<br>
contrary, I have three concerns:
<br>
<p>**One: Yeah--the thing behaves like a human because
<br>
it's programmed by humans and likely monkeyed around
<br>
with by military types, whose thinking I don't care
<br>
for.
<br>
<p>**Two: However the thing starts off, it glitches/goes
<br>
chaotic/turns psychotic and can't be shut off.
<br>
<p>**Three: The thing behaves NOTHING like a human--is,
<br>
in fact, completely alien. The basis of its decisions
<br>
will therefore be unknowable. It might push us to the
<br>
limits of the universe and help us create an
<br>
everlasting utopia--or exterminate us all tomorrow for
<br>
a reason we couldn't even guess BECAUSE it's nothing
<br>
like a human. It is therefore a constant threat whose
<br>
level at any given instant is not unly unknown but
<br>
unknowable. And, being alien--it may well view us the
<br>
same way.
<br>
<p><p><em>&gt; Your reluctance to
</em><br>
<em>&gt; entrust a single AI with power is a case in point. 
</em><br>
<em>&gt; How much power an AI
</em><br>
<em>&gt; has, and whether it's a single AI or a group, would
</em><br>
<em>&gt; not have the slightest
</em><br>
<em>&gt; impact on the AI's trustworthiness.
</em><br>
<p>**Never said it would. The more power it/they have,
<br>
the greater the danger, was my point.
<br>
<p>&nbsp;&nbsp;That only
<br>
<em>&gt; happens with humans who've
</em><br>
<em>&gt; spent the last three million years evolving to win
</em><br>
<em>&gt; power struggles in
</em><br>
<em>&gt; hunter-gatherer tribes.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; As it happens, AI is much less risky than
</em><br>
<em>&gt; nanotechnology.  I think I know
</em><br>
<em>&gt; how to build a Friendly AI.  I don't see *any*
</em><br>
<em>&gt; development scenario for
</em><br>
<em>&gt; nanotechnology that doesn't end in a short war. 
</em><br>
<p>**Yah--a VERY short war...
<br>
<p><em>&gt; Offensive technology has
</em><br>
<em>&gt; overpowered defensive technology ever since the
</em><br>
<em>&gt; invention of the ICBM, and
</em><br>
<em>&gt; nanotechnology (1) makes the imbalance even worse
</em><br>
<em>&gt; and (2) removes all the
</em><br>
<em>&gt; stabilizing factors that have prevented nuclear war
</em><br>
<em>&gt; so far.
</em><br>
<p>**Bingo.
<br>
<p>&nbsp;&nbsp;If nanotech,
<br>
<em>&gt; we're screwed.
</em><br>
<p>**If nano goes amok or is in the wrong hands, you
<br>
mean, I think. Of course, any hands may be wrong
<br>
hands.
<br>
<p>&nbsp;&nbsp;The probability of humanity's
<br>
<em>&gt; survival is the probability
</em><br>
<em>&gt; that AI comes first times the probability that
</em><br>
<em>&gt; Friendly AI is possible
</em><br>
<em>&gt; times the probability that Friendliness is
</em><br>
<em>&gt; successfully implemented by the
</em><br>
<em>&gt; first group to create AI.
</em><br>
<p>**What, in your estimation, is the probability that
<br>
military interests will seize the operation/the AI at
<br>
the critical time? That they have/will infiltrate
<br>
AI-creation efforts for this and monitoring purposes?
<br>
Remember--they're thinking like hunter-gatherers,
<br>
too...
<br>
<p>&nbsp;&nbsp;If Friendly AI is
<br>
<em>&gt; impossible, then humanity is
</em><br>
<em>&gt; screwed, period.
</em><br>
<p>**Best info sources on this issue and on your take of
<br>
this issue? On AI/SI?
<br>
<p>john marlow
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; --              --              --              --  
</em><br>
<em>&gt;            -- 
</em><br>
<em>&gt; Eliezer S. Yudkowsky                         
</em><br>
<em>&gt; <a href="http://singinst.org/">http://singinst.org/</a> 
</em><br>
<em>&gt; Research Fellow, Singularity Institute for
</em><br>
<em>&gt; Artificial Intelligence
</em><br>
<p><p>__________________________________________________
<br>
Do You Yahoo!?
<br>
Get email at your own domain with Yahoo! Mail. 
<br>
<a href="http://personal.mail.yahoo.com/">http://personal.mail.yahoo.com/</a>
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1186.html">Technotranscendence: "Re: That idiot Darwin"</a>
<li><strong>Previous message:</strong> <a href="1184.html">John Marlow: "Re: &quot;God Does Not Play Dice...&quot;   AE"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1194.html">Eliezer S. Yudkowsky: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was    Re:     Homework, Nuke,  etc.."</a>
<li><strong>Reply:</strong> <a href="1194.html">Eliezer S. Yudkowsky: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was    Re:     Homework, Nuke,  etc.."</a>
<li><strong>Reply:</strong> <a href="1220.html">xgl: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was  Re:     Homework, Nuke,  etc.."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1185">[ date ]</a>
<a href="index.html#1185">[ thread ]</a>
<a href="subject.html#1185">[ subject ]</a>
<a href="author.html#1185">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:56:19 MDT</em>
</em>
</small>
</body>
</html>
