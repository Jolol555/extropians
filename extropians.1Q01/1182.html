<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Paradox--was Re: Active shields, was Re: Critic</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Paradox--was Re: Active shields, was Re: Criticism depth, was   Re:     Homework, Nuke,  etc..">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Paradox--was Re: Active shields, was Re: Criticism depth, was   Re:     Homework, Nuke,  etc..</h1>
<!-- received="Sat Jan 13 21:35:58 2001" -->
<!-- isoreceived="20010114043558" -->
<!-- sent="Sat, 13 Jan 2001 23:35:59 -0500" -->
<!-- isosent="20010114043559" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Paradox--was Re: Active shields, was Re: Criticism depth, was   Re:     Homework, Nuke,  etc.." -->
<!-- id="3A612CAF.C7C5B43A@pobox.com" -->
<!-- inreplyto="20010114025314.7429.qmail@web4402.mail.yahoo.com" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Paradox--was%20Re:%20Active%20shields,%20was%20Re:%20Criticism%20depth,%20was%20%20%20Re:%20%20%20%20%20Homework,%20Nuke,%20%20etc..&In-Reply-To=&lt;3A612CAF.C7C5B43A@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jan 13 2001 - 21:35:59 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1183.html">Damien Broderick: "Stewart Brand's The Clock of the Long Now"</a>
<li><strong>Previous message:</strong> <a href="1181.html">GBurch1@aol.com: "Re: That idiot Darwin"</a>
<li><strong>In reply to:</strong> <a href="1168.html">John Marlow: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was  Re:     Homework, Nuke,  etc.."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1185.html">John Marlow: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was   Re:     Homework, Nuke,  etc.."</a>
<li><strong>Maybe reply:</strong> <a href="1185.html">John Marlow: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was   Re:     Homework, Nuke,  etc.."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1182">[ date ]</a>
<a href="index.html#1182">[ thread ]</a>
<a href="subject.html#1182">[ subject ]</a>
<a href="author.html#1182">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
John Marlow wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt; &gt; In which case, there may not be a damned
</em><br>
<em>&gt; &gt; &gt; thing we can do about it.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Yep.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; **GAAAK! And you proceed! Or are you simply that
</em><br>
<em>&gt; convinced of our own incompetence? (But then, this
</em><br>
<em>&gt; would actually argue AGAINST proceeding, if you think
</em><br>
<em>&gt; about it...)
</em><br>
<p>Why, oh why is it that the machinophobes of this world assume that
<br>
researchers never think about the implications of their research?  When I
<br>
was a kid, I thought I'd grow up to be a physicist like my father... maybe
<br>
work on nanotechnology...  I am here, today, in this profession, working
<br>
on AI, because I believe that this is not just the best, but the *only*
<br>
path to survival for humanity.
<br>
<p><em>&gt; **No--I am, for example, strongly in favor of
</em><br>
<em>&gt; developing nanotechnology, and that is by far the
</em><br>
<em>&gt; biggest risk I can see.
</em><br>
<em>&gt;
</em><br>
<em>&gt; **Building an AI brighter than we are might run a
</em><br>
<em>&gt; close second. That I'm not for.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; **The first offers nearly limitless advantages for the
</em><br>
<em>&gt; risk; the second offers only certain disaster. It's a
</em><br>
<em>&gt; risk-benefit analysis.
</em><br>
<p>On this planet *right now* there exists enough networked computing power
<br>
to create an AI.  Stopping progress wouldn't be enough.  You'd have to
<br>
move the entire world backwards, and keep it there, not just for ten
<br>
years, or for a century, but forever, or else some other generation just
<br>
winds up facing the same problem.  It doesn't matter, as you describe the
<br>
future, whether AI presents the greater or the lesser risk; what matters
<br>
is that a success in Friendly AI makes navigating nanotech easier, while
<br>
success in nanotechnology doesn't help AI.  In fact, nanotechnology
<br>
provides for computers with thousands or millions of times the power of a
<br>
single human brain, at which point it takes no subtlety, no knowledge, no
<br>
wisdom to create AI; anyone with a nanocomputer can just brute-force it.
<br>
<p>It's very easy for me to understand why you're concerned.  You've seen a
<br>
bunch of bad-guy machines on TV, and maybe a few good-guy machines, and
<br>
every last one of them behaved just like a human.  Your reluctance to
<br>
entrust a single AI with power is a case in point.  How much power an AI
<br>
has, and whether it's a single AI or a group, would not have the slightest
<br>
impact on the AI's trustworthiness.  That only happens with humans who've
<br>
spent the last three million years evolving to win power struggles in
<br>
hunter-gatherer tribes.
<br>
<p>As it happens, AI is much less risky than nanotechnology.  I think I know
<br>
how to build a Friendly AI.  I don't see *any* development scenario for
<br>
nanotechnology that doesn't end in a short war.  Offensive technology has
<br>
overpowered defensive technology ever since the invention of the ICBM, and
<br>
nanotechnology (1) makes the imbalance even worse and (2) removes all the
<br>
stabilizing factors that have prevented nuclear war so far.  If nanotech,
<br>
we're screwed.  The probability of humanity's survival is the probability
<br>
that AI comes first times the probability that Friendly AI is possible
<br>
times the probability that Friendliness is successfully implemented by the
<br>
first group to create AI.  If Friendly AI is impossible, then humanity is
<br>
screwed, period.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1183.html">Damien Broderick: "Stewart Brand's The Clock of the Long Now"</a>
<li><strong>Previous message:</strong> <a href="1181.html">GBurch1@aol.com: "Re: That idiot Darwin"</a>
<li><strong>In reply to:</strong> <a href="1168.html">John Marlow: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was  Re:     Homework, Nuke,  etc.."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1185.html">John Marlow: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was   Re:     Homework, Nuke,  etc.."</a>
<li><strong>Maybe reply:</strong> <a href="1185.html">John Marlow: "Re: Paradox--was Re: Active shields, was Re: Criticism depth, was   Re:     Homework, Nuke,  etc.."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1182">[ date ]</a>
<a href="index.html#1182">[ thread ]</a>
<a href="subject.html#1182">[ subject ]</a>
<a href="author.html#1182">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:56:19 MDT</em>
</em>
</small>
</body>
</html>
