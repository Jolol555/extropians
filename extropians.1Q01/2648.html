<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: Singularity optimization [Was: Colossus and the</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Singularity optimization [Was: Colossus and the Singularity]">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: Singularity optimization [Was: Colossus and the Singularity]</h1>
<!-- received="Sat Jan 27 17:20:08 2001" -->
<!-- isoreceived="20010128002008" -->
<!-- sent="Sat, 27 Jan 2001 19:19:22 -0500" -->
<!-- isosent="20010128001922" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Singularity optimization [Was: Colossus and the Singularity]" -->
<!-- id="3A73658A.B4420E@pobox.com" -->
<!-- inreplyto="b49elxowool.fsf@sans04.nada.kth.se" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Singularity%20optimization%20[Was:%20Colossus%20and%20the%20Singularity]&In-Reply-To=&lt;3A73658A.B4420E@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jan 27 2001 - 17:19:22 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2649.html">Anders Sandberg: "Re: Things extropians agree on"</a>
<li><strong>Previous message:</strong> <a href="2647.html">slash@extropy.org: "24 Hours of Extropy Institute Portal Headlines For Extropy Mailing List"</a>
<li><strong>In reply to:</strong> <a href="2645.html">Anders Sandberg: "Re: Singularity optimization [Was: Colossus and the Singularity]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2654.html">Damien Broderick: "Re: Singularity optimization [Was: Colossus and the Singularity]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2648">[ date ]</a>
<a href="index.html#2648">[ thread ]</a>
<a href="subject.html#2648">[ subject ]</a>
<a href="author.html#2648">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Anders Sandberg wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Singularity%20optimization%20[Was:%20Colossus%20and%20the%20Singularity]&In-Reply-To=&lt;3A73658A.B4420E@pobox.com&gt;">sentience@pobox.com</a>&gt; writes:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; In the &quot;general case&quot;, the landscape is not subject to optimization.
</em><br>
<em>&gt; &gt; Intelligence is an evolutionary advantage because it enables the organism
</em><br>
<em>&gt; &gt; to model, predict, and manipulate regularities in reality.  In a
</em><br>
<em>&gt; &gt; maximum-entropy Universe, which is what you're talking about, intelligence
</em><br>
<em>&gt; &gt; is impossible and so is evolution.  The fitness landscape you're talking
</em><br>
<em>&gt; &gt; about, and that the paper you cited is talking about, bears no useful
</em><br>
<em>&gt; &gt; resemblance to our own low-entropy reality.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; True. My point is that if you want to build something that functions
</em><br>
<em>&gt; in the real low-entropy world, then you have a good chance. But if it
</em><br>
<em>&gt; is only going on inside the high-entropy world of algorithms then you
</em><br>
<em>&gt; will likely not get any good results. This is why I consider
</em><br>
<em>&gt; &quot;transcendence in a box&quot; scenarios so misleading. Having stuff
</em><br>
<em>&gt; transcend in the real world is another matter - but here we also get
</em><br>
<em>&gt; more slow interactions as a limiting factor.
</em><br>
<p>Okay, I don't understand this at all.  I don't understand why you think
<br>
that there's higher entropy inside the box than outside the box.  The box
<br>
is a part of our Universe, isn't it?  And one that's built by highly
<br>
unentropic programmers and sealed away from thermodynamics by a layer of
<br>
abstraction.
<br>
<p><em>&gt; Hmm, my description may not have been clear enough then. What I was
</em><br>
<em>&gt; looking at was a sequence where program P_n searches for a replacement
</em><br>
<em>&gt; program P_{n+1}.
</em><br>
<p>Yep, and it's possible to say all kinds of reasonable things about P_x
<br>
searching for P_y that suddenly become absurd if you imagine a specific
<br>
uploaded human pouring on the neurons or a seed AI transferring itself
<br>
into a rod logic.  Does it even matter where the curve tops out, or
<br>
whether it tops out at all, when there are all these enormous improvements
<br>
dangling *just* out of reach?  The improvements we *already know* how to
<br>
make are more than enough to qualify for a Singularity.
<br>
<p><em>&gt; &gt; Finally, if the improvement curve is so horribly logarithmic, then why
</em><br>
<em>&gt; &gt; didn't the vast majority of BLIND evolution on this planet take place in
</em><br>
<em>&gt; &gt; the first million years?  If increasing complexity or increasing
</em><br>
<em>&gt; &gt; improvement renders further improvements more difficult to find, then why
</em><br>
<em>&gt; &gt; doesn't BLIND evolution show a logarithmic curve?  These mathematical
</em><br>
<em>&gt; &gt; theories bear no resemblance to *any* observable reality.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You see it very much in alife simulations. This is why so many people
</em><br>
<em>&gt; try to find ways of promoting continual evolution in them; the holy
</em><br>
<em>&gt; grail would be to get some kind of cambrian explosion of
</em><br>
<em>&gt; complexity.
</em><br>
<p>Yes, and you see it in Eurisko as well.  Where you don't see it is
<br>
real-life evolution, the accumulation of knowledge as a function of
<br>
existing knowledge, human intelligence as a function of time, the progress
<br>
of technology (*not* some specific bright idea, but the succession of
<br>
bright ideas over time), and all the other places where sufficient seed
<br>
complexity exists for open-ended improvement.
<br>
<p><em>&gt; The question is how you measure evolutionary improvement. In alife you
</em><br>
<em>&gt; can just measure fitness. In real life the best thing is to look at
</em><br>
<em>&gt; the rate of extinction, which could be seen as a measure of the
</em><br>
<em>&gt; average fitness of entire species. In
</em><br>
<em>&gt; <a href="http://xxx.lanl.gov/abs/adap-org/9811003">http://xxx.lanl.gov/abs/adap-org/9811003</a> it is mentioned that we see a
</em><br>
<em>&gt; general decrease in extinction rate in the phanerozoic; it seems to be
</em><br>
<em>&gt; a 1/t decline according to them.
</em><br>
<p>I looked over this (cool) paper, but it seems a bit suspect when
<br>
considered as a measure of evolutionary improvement rates, given that I've
<br>
yet to hear any argument for functional complexity accumulating at
<br>
inverse-t (*across* successions of punctuated equilibria, not within a
<br>
single equilibrium).  It sure doesn't show up in any graphs of
<br>
progress-with-time that I'm familiar with; those graphs usually resemble
<br>
the more familiar picture where half the total progress happened merely
<br>
within the last century or the last million years or whatever.
<br>
<p>I'm sorry, but this still looks to me like the &quot;Each incremental
<br>
improvement in human intelligence required a doubling of brain size&quot;
<br>
argument.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2649.html">Anders Sandberg: "Re: Things extropians agree on"</a>
<li><strong>Previous message:</strong> <a href="2647.html">slash@extropy.org: "24 Hours of Extropy Institute Portal Headlines For Extropy Mailing List"</a>
<li><strong>In reply to:</strong> <a href="2645.html">Anders Sandberg: "Re: Singularity optimization [Was: Colossus and the Singularity]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2654.html">Damien Broderick: "Re: Singularity optimization [Was: Colossus and the Singularity]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2648">[ date ]</a>
<a href="index.html#2648">[ thread ]</a>
<a href="subject.html#2648">[ subject ]</a>
<a href="author.html#2648">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:56:25 MDT</em>
</em>
</small>
</body>
</html>
