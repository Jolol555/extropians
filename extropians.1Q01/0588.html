<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: AI: The generic conversation (was: Will we be viped</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="AI: The generic conversation (was: Will we be viped out by SI?)">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>AI: The generic conversation (was: Will we be viped out by SI?)</h1>
<!-- received="Sun Jan  7 21:10:13 2001" -->
<!-- isoreceived="20010108041013" -->
<!-- sent="Sun, 07 Jan 2001 23:10:13 -0500" -->
<!-- isosent="20010108041013" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="AI: The generic conversation (was: Will we be viped out by SI?)" -->
<!-- id="3A593DA5.A0E68B7D@pobox.com" -->
<!-- inreplyto="147f01c078d1$edeffd40$39bc473f@jrmolloy" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20AI:%20The%20generic%20conversation%20(was:%20Will%20we%20be%20viped%20out%20by%20SI?)&In-Reply-To=&lt;3A593DA5.A0E68B7D@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 07 2001 - 21:10:13 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0589.html">Adrian Tymes: "Re: riddles"</a>
<li><strong>Previous message:</strong> <a href="0587.html">Spike Jones: "Re: Fish in Space"</a>
<li><strong>In reply to:</strong> <a href="0539.html">J. R. Molloy: "Re: Will we be viped out by SI? WAS: RE: CULTURE: Chicago Tribune ..."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0567.html">Ziana Astralos: "Re: Will we be viped out by SI? WAS: RE: CULTURE: Chicago Tribune ..."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#588">[ date ]</a>
<a href="index.html#588">[ thread ]</a>
<a href="subject.html#588">[ subject ]</a>
<a href="author.html#588">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
And he sat there, screaming, paralyzed by pain, as every cell of his body
<br>
was racked with agonizing, massive waves of deja vu.
<br>
<p>==
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The Generic Conversation
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(as adapted from _Friendly AI_, a work in progress)
<br>
<p><p>Somebody:  &quot;But what happens if the AI decides to do [something only a
<br>
human would want]?&quot;
<br>
<p>Eliezer:  &quot;Ve won't want to do [whatever] because the instinct for doing
<br>
[whatever] is a complex functional adaptation, and complex functional
<br>
adaptations don't just materialize in source code.  I mean, it's
<br>
understandable that humans want to do [whatever] because of [insert
<br>
selection pressure], but you can't reason from that to AIs.&quot;
<br>
<p>Somebody:  &quot;But everyone needs to do [whatever] because [insert personal
<br>
philosophy], so the AI will decide to do it as well.&quot;
<br>
<p>Eliezer:  &quot;Yes, doing [whatever] is sometimes useful.  But even if the AI
<br>
decides to do [whatever] because it serves [insert Friendliness supergoal]
<br>
under [insert contrived scenario], that's not the same as having an
<br>
independent desire to do [whatever].&quot;
<br>
<p>Somebody:  &quot;Yes, that's what I've been saying:  The AI will see that
<br>
[whatever] is useful and decide to start doing it.  So now we need to
<br>
worry about [some scenario in which doing &lt;whatever&gt; is catastrophically
<br>
unFriendly].&quot;
<br>
<p>Eliezer:  &quot;But the AI won't have an independent desire to do [whatever]. 
<br>
The AI will only do [whatever] when it serves the supergoals.  A Friendly
<br>
AI would never do [whatever] if it stomps on the Friendliness supergoals.&quot;
<br>
<p>Somebody:  &quot;I don't understand.  You've admitted that [whatever] is
<br>
useful.  Obviously, the AI will alter itself so it does [whatever]
<br>
instinctively.&quot;
<br>
<p>Eliezer:  &quot;The AI doesn't need to give verself an instinct in order to do
<br>
[whatever]; if doing [whatever] really is useful, then the AI can see that
<br>
and do [whatever] as a consequence of pre-existing supergoals, and only
<br>
when [whatever] serves those supergoals.&quot;
<br>
<p>Somebody:  &quot;But an instinct is more efficient, so the AI will alter itself
<br>
to do [whatever] automatically.&quot;
<br>
<p>Eliezer:  &quot;Only for humans.  For an AI, [insert complex explanation of the
<br>
cognitive differences between having 32 2-gigahertz processors and 100
<br>
trillion 200-hertz synapses], so making [whatever] an independent
<br>
supergoal would only be infinitesimally more efficient.&quot;
<br>
<p>Somebody:  &quot;Yes, but it is more efficient!  So the AI will do it.&quot;
<br>
<p>Eliezer:  &quot;It's not more efficient from the perspective of a Friendly AI
<br>
if it results in [something catastrophically unFriendly].  To the exact
<br>
extent that an instinct is context-insensitive, which is what you're
<br>
worried about, a Friendly AI won't think that making [whatever]
<br>
context-insensitive, with all the [insert horrifying consequences], is
<br>
worth the infinitesimal improvement in speed.&quot;
<br>
<p>Somebody:  &quot;Oh.  Well, maybe -&quot;
<br>
<p>Eugene Leitl (interrupts):  &quot;But you can only build AIs using evolution. 
<br>
So the AI will wind up with [exactly the same instinct that humans have].&quot;
<br>
<p>Eliezer:  &quot;One, I don't plan on using evolution to build seed AIs.  Two,
<br>
even if I did use controlled evolution, winding up with [whatever] would
<br>
require exactly duplicating [some exotic selection pressure].  Three, even
<br>
if I duplicated [some exotic selection pressure], my breeding population
<br>
would start out with a set of Friendly supergoals, with the understanding
<br>
that competing in controlled evolution serves the Friendliness supergoals
<br>
by developing better AI, and with the knowledge that [whatever] helps them
<br>
compete, so the breeding population could do [whatever] as a consequence
<br>
of Friendliness supergoals, and a mutation for doing [whatever]
<br>
instinctively wouldn't add any additional adaptive behaviors.  There'd be
<br>
no selection pressure.&quot;
<br>
<p>Eugene Leitl:  &quot;Yeah?  Well, I do plan to use evolution.  And I plan to
<br>
run the AIs using scenarios with [some exotic selection pressure].  And I
<br>
don't plan on starting out with a breeding population of Friendly AIs. 
<br>
And even if I did, I wouldn't try to create any selection pressures that
<br>
would ensure that [whatever] was executed in a context-sensitive and
<br>
Friendliness-aware way, so even the infinitesimal performance improvement
<br>
that comes from total context-insensitivity would be an evolutionary
<br>
advantage.&quot;
<br>
<p>Eliezer:  &quot;Okay, so your AIs will destroy the world.  Mine won't.&quot;
<br>
<p>Eugene Leitl:  &quot;But evolution is the only way to develop AIs.  And the
<br>
starting population of AIs will have [some kind of totally opaque code],
<br>
so you won't be able to start out with a Friendly breeding population, or
<br>
have any control over their evolution at all.  You'll be forced to do it
<br>
my way and then you'll destroy the world.&quot;
<br>
<p>Eliezer:  &quot;One, your way is so incredibly dangerous that I just wouldn't
<br>
do it, no matter how useful I thought it might be.  Two, according to my
<br>
best current theoretical understanding of intelligence, you are so totally
<br>
off-base that 'flat wrong' doesn't even begin to describe it.&quot;
<br>
<p>Eugene Leitl:  &quot;Yeah?  Well, *I* think *you're* off-base, and *I* went to
<br>
high school.&quot;
<br>
<p>Eliezer:  &quot;Are we done?  We're done.&quot;
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0589.html">Adrian Tymes: "Re: riddles"</a>
<li><strong>Previous message:</strong> <a href="0587.html">Spike Jones: "Re: Fish in Space"</a>
<li><strong>In reply to:</strong> <a href="0539.html">J. R. Molloy: "Re: Will we be viped out by SI? WAS: RE: CULTURE: Chicago Tribune ..."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0567.html">Ziana Astralos: "Re: Will we be viped out by SI? WAS: RE: CULTURE: Chicago Tribune ..."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#588">[ date ]</a>
<a href="index.html#588">[ thread ]</a>
<a href="subject.html#588">[ subject ]</a>
<a href="author.html#588">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:56:17 MDT</em>
</em>
</small>
</body>
</html>
