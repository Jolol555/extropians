<!-- received="Mon Feb  3 14:55:46 1997 MDT" -->
<!-- sent="Mon, 3 Feb 1997 13:31:10 -0800 (PST)" -->
<!-- name="Robin Hanson" -->
<!-- email="hanson@hss.caltech.edu" -->
<!-- subject="Superintelligences' motivation" -->
<!-- id="199702032131.NAA12952@hss.caltech.edu" -->
<!-- inreplyto="" -->
<title>extropians: Superintelligences' motivation</title>
<h1>Superintelligences' motivation</h1>
Robin Hanson (<i>hanson@hss.caltech.edu</i>)<br>
<i>Mon, 3 Feb 1997 13:31:10 -0800 (PST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2177">[ date ]</a><a href="index.html#2177">[ thread ]</a><a href="subject.html#2177">[ subject ]</a><a href="author.html#2177">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2178.html">Robert Schrader: "economic catastrophe"</a>
<li> <b>Previous message:</b> <a href="2176.html">Eugene Leitl: "Re: Venus wonders what all the fuss is about"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1632.html">Michael Lorrey: "Re: Superintelligences' motivation"</a>
<li> <b>Maybe reply:</b> <a href="1632.html">Michael Lorrey: "Re: Superintelligences' motivation"</a>
<li> <b>Maybe reply:</b> <a href="2218.html">N.BOSTROM@lse.ac.uk: "Re: Superintelligences' motivation"</a>
<li> <b>Maybe reply:</b> <a href="2263.html">Mike Cowar.: "Re: Superintelligences' motivation"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
N.BOSTROM@lse.ac.uk writes:<br>
<i>&gt;... call such a system autopotent: it hascomplete power over and</i><br>
<i>&gt;knowledge of itself.  ...  Suppose we tried to operate such a system</i><br>
<i>&gt;on the pain/pleasure principle.  ... It would simply turn on the</i><br>
<i>&gt;pleasure directly. ...  We may thus begin to wonder whether an</i><br>
<i>&gt;autopotent system could be made to function at all; perhaps it would</i><br>
<i>&gt;be unstable? The solution seems to be to substitute an external</i><br>
<i>&gt;ultimate goal for the internal ultimate goal of pleasure.</i><br>
<p>
Another alternative is for the system to prefer stability in one of<br>
the areas under its control.  It wants to do what you say, and wants<br>
to continue to want this.  This idea is pursued by Minksy in his<br>
Soceity of Mind book.<br>
<p>
Robin D. Hanson  hanson@hss.caltech.edu  <a href="http://hss.caltech.edu/~hanson/">http://hss.caltech.edu/~hanson/</a><br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2178.html">Robert Schrader: "economic catastrophe"</a>
<li> <b>Previous message:</b> <a href="2176.html">Eugene Leitl: "Re: Venus wonders what all the fuss is about"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1632.html">Michael Lorrey: "Re: Superintelligences' motivation"</a>
<li> <b>Maybe reply:</b> <a href="1632.html">Michael Lorrey: "Re: Superintelligences' motivation"</a>
<li> <b>Maybe reply:</b> <a href="2218.html">N.BOSTROM@lse.ac.uk: "Re: Superintelligences' motivation"</a>
<li> <b>Maybe reply:</b> <a href="2263.html">Mike Cowar.: "Re: Superintelligences' motivation"</a>
<!-- reply="end" -->
</ul>
