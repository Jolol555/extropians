<!-- received="Fri Mar  7 10:59:02 1997 MDT" -->
<!-- sent="Fri, 7 Mar 1997 12:23:06 -0500" -->
<!-- name="Crosby_M" -->
<!-- email="CrosbyM@po1.cpi.bls.gov" -->
<!-- subject="RE: ART/NEURAL NETS Creativity" -->
<!-- id="c=US%a=_%p=BLS%l=BLS/PSB/00096F32@psbmailhub.psb.bls.gov" -->
<!-- inreplyto="" -->
<title>extropians: RE: ART/NEURAL NETS Creativity</title>
<h1>RE: ART/NEURAL NETS Creativity</h1>
Crosby_M (<i>CrosbyM@po1.cpi.bls.gov</i>)<br>
<i>Fri, 7 Mar 1997 12:23:06 -0500</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3278">[ date ]</a><a href="index.html#3278">[ thread ]</a><a href="subject.html#3278">[ subject ]</a><a href="author.html#3278">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3279.html">Max More: "Greetings (forward)"</a>
<li> <b>Previous message:</b> <a href="3277.html">Steve Edwards: "Clones a'coming"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Thu, 6 Mar 1997, I wrote:<br>
&lt;Human minds are not *merely* neural nets, even though neural nets may <br>
be an appropriate model for portions of the human mind.&gt;<br>
<p>
Robert Schrader then asked:<br>
&lt;Please elaborate.  What kind of part are these non-neural-net parts? <br>
And how would you model them?&gt;<br>
<p>
Sorry, I don't really know.  The point I was primarily trying to make <br>
was that something that appears to be an inessential artifact at one <br>
level can be appropriated as a driving force at another level.  When <br>
we do get a grip on the mechanisms involved in the *linking* of <br>
different hierarchical and relational levels in complex systems then I <br>
think we'll have a better model for such things as creativity than we <br>
have with uni-level 'hardware' models like Turing machines or basic <br>
neural nets.<br>
<p>
I was just reading a 9701 _Communications of the ACM_ article on <br>
"Artificial Intelligence and Virtual Organizations".  I like their <br>
concise definition of ontology: "Ontologies are specifications of <br>
discourse among multiple agents in the form of a shared vocabulary." <br>
 (This is sorta what Gregory Houston was recently talking about - on a <br>
semi-synchronous thread - as a precondition for close friendships, <br>
while Kathryn Aegis, on the other hand, was talking about friendships <br>
being based more on shared commitments than on common interests.)<br>
<p>
Anyway, this CACM article went on to describe some of the specialized <br>
agents that are needed in a 'virtual organization', e.g., domain <br>
experts, wrapper agents (to translate outputs from the domain experts <br>
to the shared vocabulary), facilitator or broker agents to provide a <br>
reliable communication layer between the specialized agents.<br>
<p>
The human brain also has similar specialized and generalized organs. <br>
So, I'm trying to find a way to say that /even if/ the underlying <br>
hardware or wetware is neural nets all the way up and down, at a <br>
certain level of analysis the functional elements and their roles in <br>
the overall ecology (e.g., the perceptual and intentional dynamics) <br>
are able to encode things that you won't see if you look only at the <br>
underlying mechanisms.<br>
<p>
Mark Crosby<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3279.html">Max More: "Greetings (forward)"</a>
<li> <b>Previous message:</b> <a href="3277.html">Steve Edwards: "Clones a'coming"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
