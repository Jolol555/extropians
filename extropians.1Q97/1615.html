<!-- received="Wed Jan 29 15:28:49 1997 MDT" -->
<!-- sent="Wed, 29 Jan 97 22:22:06 GMT" -->
<!-- name="N.BOSTROM@lse.ac.uk" -->
<!-- email="N.BOSTROM@lse.ac.uk" -->
<!-- subject="Superintelligences' motivation" -->
<!-- id="9700298546.AA854605634@ccgw9.lse.ac.uk" -->
<!-- inreplyto="" -->
<title>extropians: Superintelligences' motivation</title>
<h1>Superintelligences' motivation</h1>
<i>N.BOSTROM@lse.ac.uk</i><br>
<i>Wed, 29 Jan 97 22:22:06 GMT</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1615">[ date ]</a><a href="index.html#1615">[ thread ]</a><a href="subject.html#1615">[ subject ]</a><a href="author.html#1615">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1616.html">Richard Brodie: "NOTICE: Brodie, Drexler To Speak in LA in February"</a>
<li> <b>Previous message:</b> <a href="1614.html">Joy Williams: "Re: Venus Automorphs Into Great Mathematician, Loses Psyche"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
I repost my message from yesterday since it arrived in a very ugly format. I hope this will turn out better.<br>
 ----<br>
Consider a superintelligence that has full control over its internal <br>
machinery. This could be achieved by connecting it to a <br>
sophisticated robot arm with which it could rewire itself any way it <br>
wanted; or it could accomplished by some more direct means <br>
(rewriting its own program, thought control). Assume also that it has<br>
 complete self-knowledge - by which I do not mean that the system <br>
has completeness in the mathematical sense, but simply that it has <br>
a good general understanding of its own architecture (like a superb <br>
neuroscientist might have in the future when neuroscience has <br>
reached its full maturity). Let's call such a system autopotent: it hascomplete power over and knowledge of itself. We may note that <br>
it is not implausible to suppose that superintelligences will actually <br>
tend to be autopotent; they will easily obtain self-knowledge, and <br>
they might also obtain self-power (either because we allow them, or<br>
 through their own cunningness).<br>
<p>
        Suppose we tried to operate such a system on the <br>
pain/pleasure principle. We would give the autopotent system a <br>
goal (help us solve a difficult physics problem, for example) and it <br>
would try to achieve that goal because it would expect to be <br>
rewarded when it succeeded. But the superintelligence isn't stupid.<br>
 It would realise that if its ultimate goal was to experience the <br>
reward, there would be a much more efficient method to obtain it <br>
than trying to solve that physics problem. It would simply turn on the <br>
pleasure directly. It could even chose to rewire itself into exactly the <br>
same state as it would have been in after it had successfully solved <br>
the external task. And the pleasure could be made maximally <br>
intense and of indefinite duration. It follows that the system wouldn't <br>
care one bit about the physics problem, or any other problem for <br>
that matter: it would take the straight route to the maximally pleasant <br>
state.<br>
<p>
We may thus begin to wonder whether an autopotent system could <br>
be made to function at all; perhaps it would be unstable? The <br>
solution seems to be to substitute an external ultimate goal for the <br>
internal ultimate goal of pleasure. The pleasure/pain motivation <br>
principle couldn't work for an such a system: no stable autopotent <br>
agent could be an egoistic hedonist. But if the system's end goal <br>
were to solve that physical problem, then there is no reason why it <br>
should begin to manipulate itself into a state of feeling pleasure or <br>
even a state of (falsely) believing it had solved the problem. It would<br>
 know that none of this would achieve the goal, which is to solve <br>
the external problem; so it wouldn't do it.<br>
<p>
Thus we see that the pleasure/pain principle would not constitute a<br>
 workable modus operandi for an autopotent system. But such a <br>
system can be motivated, it seems, by a suitable basis of external <br>
values. The pleasure/pain principle could play a part of the <br>
motivation scheme, for example if the external value were to include<br>
 that it is bad to directly ply ones own motivation centre.<br>
<p>
One popular line of reasoning, which I find suspicious, is that <br>
superintelligences would be very intellectual/spiritual, in the sense <br>
that they would engage in all sorts of intellectual pursuits quite apart <br>
from any considerations of practical utility (such as personal safety, <br>
proliferation, influence, increase of computational resources etc.). It<br>
 is possible that superintelligences would do that if they were <br>
specifically constructed to cherish spiritual values, but otherwise <br>
there is not reason to suppose they would do something just for the<br>
 fun of it when they could have as much fun as they wanted simply <br>
by manipulating their pleasure centres. I mean, if you can associate <br>
pleasure with any activity whatsoever, why not associate it with an <br>
activity that also served a practical purpose? Now, there may be <br>
many subtle answers to that question; I just want to issue a general <br>
warning against uncritically assuming that laws about human <br>
psychology and motivation will automatically carry over to <br>
superintelligences.<br>
<p>
        One reason why the philosophy of motivation is important<br>
 is that the more knowledge and power we get, the more our <br>
desires will affect the external world. Thus, in order to predict what <br>
will happen in the external world, it will become more and more <br>
relevant to find out what our desires are --and how they are likely to<br>
 change as a consequence of our obtaining more knowledge and <br>
power. Of particular importance are those technologies that will <br>
allow us to modify our own desires (e.g. psychoactive drugs). Once <br>
such technologies become sufficiently powerful and well-known, <br>
they will in effect promote our second-order (or even higher-order!) <br>
desires into power. Our first-order desires will be determined by our <br>
second-order desires. This might drastically facilitate prediction of <br>
events in the external world. All we have to do is to find out what <br>
our higher-order desires are, for they will determine our lower order <br>
desires which in turn will determine an increasing number of <br>
features in the external world, as our technological might grows. <br>
Thus, in order to predict the long term development of the most <br>
interesting aspects of the world, the most relevant considerations <br>
will be (1) the fundamental physical constraints; and (2) the <br>
higher-order desires of the agents that have the most power at the <br>
time when technologies become available for choosing our <br>
first-order desires.<br>
<p>
<p>
Nicholas Bostrom     n.bostrom@lse.ac.uk<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1616.html">Richard Brodie: "NOTICE: Brodie, Drexler To Speak in LA in February"</a>
<li> <b>Previous message:</b> <a href="1614.html">Joy Williams: "Re: Venus Automorphs Into Great Mathematician, Loses Psyche"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
