<!-- received="Sat Feb  8 20:28:50 1997 MDT" -->
<!-- sent="Sat, 08 Feb 1997 20:28:49 -0600" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The Meaning of Life" -->
<!-- id="199702082309.AAA17268@inet.uni-c.dk" -->
<!-- inreplyto="The Meaning of Life" -->
<title>extropians: Re: The Meaning of Life</title>
<h1>Re: The Meaning of Life</h1>
Eliezer Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Sat, 08 Feb 1997 20:28:49 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2405">[ date ]</a><a href="index.html#2405">[ thread ]</a><a href="subject.html#2405">[ subject ]</a><a href="author.html#2405">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2406.html">E. Shaun Russell: "Re: Eurocracy (was Re: Immortality and Resources)"</a>
<li> <b>Previous message:</b> <a href="2404.html">Eliezer Yudkowsky: "The Religion of Arbitrary Value"</a>
<li> <b>Maybe in reply to:</b> <a href="2594.html">John K Clark: "The Meaning of Life"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2424.html">Anders Sandberg: "Re: Eurocracy (was Re: Immortality and Resources)"</a>
<li> <b>Reply:</b> <a href="2424.html">Anders Sandberg: "Re: Eurocracy (was Re: Immortality and Resources)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
[Eric Watt Forste:]<br>
<i>&gt; It's a little bit sad that you can't tell the difference between a human</i><br>
<i>&gt; being and a 386 programmed to spew out random sentences.</i><br>
<p>
Well?  What is it?  I'm waiting?  I realize that a human being is more<br>
complex than a 386, but what difference does it make?  A 386 can<br>
maximize about as well as a human, even if it has less resources to do<br>
it with.<br>
<p>
Again.  Why does something acting to maximize X make it valuable, and<br>
why is X valuable if a human acts to maximize it but not if a 386 acts<br>
to maximize it?<br>
<p>
I've stated that there's no ethical difference 'tween a human maximizing<br>
something and a thermostat following the same strategy.  Respond, remain<br>
silent, or claim I'm "obviously wrong", refuse to back it up, and change<br>
your name to Searle.<br>
<p>
<i>&gt; Assertions prove nothing, Eliezer. How would you like to go about</i><br>
<i>&gt; demonstrating to us the arguments behind your conviction that value</i><br>
<i>&gt; is independent of valuers? If this is your axiom, then you are</i><br>
<i>&gt; simply assuming what it is that you are setting out to prove: that</i><br>
<i>&gt; life has no value.</i><br>
<p>
I'm setting out to prove no such thing.  Nor does it follow from<br>
observer-independent value that life is valueless.  My claim is that<br>
observer-dependent value in the direct, pleasure-center sense requires a<br>
definition of "value" which is not intuitively compatible with what<br>
*anyone* means, including all participants in this dicussion, by asking<br>
whether life is worth living.  Thus making observer-dependent values no<br>
values at all.<br>
<p>
<i>&gt; Perhaps it's the extra fuss that makes the difference? A simple</i><br>
<i>&gt; computer can declare "I will resist if you turn me off." You turn</i><br>
<i>&gt; it off and nothing happens. I, on the other hand, can declare "I</i><br>
<i>&gt; will resist if you try to take away my raincoat." and you will find</i><br>
<i>&gt; it a considerably harder task to take away my raincoat than to turn</i><br>
<i>&gt; off the simple computer.</i><br>
<p>
Now you're citing something with a vague bit of justification:  You<br>
state that *meaningful* maximization requires *resistance* to<br>
de-maximizing forces.<br>
<p>
Someday, computer security systems may put up quite a fight if you try<br>
to turn them off over the 'Net.  This is not the place to retype GURPS<br>
Cyberpunk, but possible actions range from severing connections to<br>
repairing damaged programs to rewriting entrance architectures to impose<br>
new password-challenges.  I could write, right now, an application that<br>
resisted death in a dozen ways, from trying to copy itself elsewhere to<br>
disabling the commonly used off-switch.  If someone can't resist, do<br>
they lose their right to live?  This is the "fetuses/computers have no<br>
rights because they are dependent on us" absurdity all over again.  If I<br>
make you dependent on me, even for blood transfusions, do you lose your<br>
rights as a sentient being?<br>
<p>
There are dozens of far-from-sentient classical AIs that will attempt to<br>
achieve goals using combinations of actions.  From where I sit, I see a<br>
folder called "shrdlu" which contains a computer program that will<br>
attempt to stack up blocks and uses chained actions to that effect. <br>
Does this program have any ethical relevance?  Within its world, do<br>
stacked blocks have meaning?<br>
<p>
Don't give me that "sad that you can't tell the difference" copout. <br>
Tell me what the fundamental difference is, and why "maximizing X" is<br>
not sufficient to make X valuable, and how humans maximizing X includes<br>
the key ingredient.<br>
<p>
<i>&gt; Okay, so you're a nihilist who can't tell the difference between human</i><br>
<i>&gt; beings and themostats. This is hardly a new philosophy. People have been</i><br>
<i>&gt; messing around with nihilism extensively for the last hundred years, and</i><br>
<i>&gt; it goes back even farther than that. But nihilism is not really a</i><br>
<i>&gt; philosophy; nihilism is a disorder of the affects.</i><br>
<p>
Nope, I'm saying *your* philosophy leads to nihilism.  I'm saying your<br>
philosophy of value has not *told* me what the difference between humans<br>
and thermostats is.  If you're going to post condescending remarks, you<br>
should read enough of the thread to know who to condescend to!  Also,<br>
YOUR philosophy is a disorder of the affects TOO, so nyah nyah nyah nyah<br>
nyah!<br>
<p>
<i>&gt; There are many things in the world which cannot be maximized by</i><br>
<i>&gt; anything simpler than intelligence, consciousness, brains, thinking,</i><br>
<i>&gt; etc. There are many things in the world which cannot be maximized</i><br>
<i>&gt; by thermostats.</i><br>
<p>
Name as many as you want, because I can PROVE that a giant<br>
hashtable-thermostat can maximize anything a computational mind can. <br>
I.e. REALLY BIIG (but less than 3^^^3) lookup table, duplicates inputs<br>
and outputs, no mind, but works as well.  Again, what difference does it<br>
make what does the maximizing?<br>
<p>
<i>&gt; The Meaning of Life is staring you in the face. It's right in front</i><br>
<i>&gt; of your eyes. If you don't feel it right now, get outside. Find a</i><br>
<i>&gt; dandelion meadow or a beautiful man or woman or a symphony orchestra</i><br>
<i>&gt; or something.  Try reading some William Blake, who teaches us how</i><br>
<i>&gt; to see the world in a grain of sand.</i><br>
<p>
Why is tickling the pleasure centers significant?  You're simply ducking<br>
the question.  Evolution makes a lot of things *pleasurable*, this<br>
causes us to assign them value, but I want to know if things *really*<br>
have value that isn't just a trick of our genes!  If our genes made us<br>
think that sadism was inherently valuable, would it be?  You've named<br>
things that we think of as not merely pleasurable, but *holy* - but John<br>
K Clark would laugh at quite a few "holy" values.<br>
<p>
Now, it is possible that pleasure has inherent value.  David Pearce<br>
thinks so.  He could even be right; subjective conscious pleasure could<br>
be inherently valuable.  In which case the logical course would be<br>
wireheading on the greatest possible scale, using only the Power<br>
necessary to "turn on" the Universe and then devoting all those useless<br>
thinking circuits to pleasure-center emulation.  And yet, some deep part<br>
of us says:  "This isn't meaningful; this is a dead end."<br>
<p>
I've been at peace with the world around me, while listening to Bach, if<br>
at few other times.  It moves me.  I admit it.  But I still have the<br>
moral courage to defy my own sense of outrage and say, "So what?" <br>
Simply because I assign value to a thing does not necessarily make it<br>
valuable!  Why is assigning value to Bach better than assigning value to<br>
cats?  And if the latter is wrong, why not the first as well?<br>
<p>
Your intuitive definition of "value" is "All pleasurable activities that<br>
I have not categorized as 'dead ends'."  My response:  "So what? <br>
Justify!"<br>
<p>
<i>&gt; Eliezer, it's quite foolish to pretend that determinism means that our </i><br>
<i>&gt; behavior is determined by our genes. If you had been born with precisely</i><br>
<i>&gt; your present genes, and immediately put up for adoption and raised in </i><br>
<i>&gt; Korea, you would be quite a different person than you are now.</i><br>
<p>
Maybe... but unlike most people, I can't be sure which of my<br>
philosophies are genetic and which environmental.  As an Algernon, I<br>
might have wound up adopting close to exactly the same philosophical<br>
worldview if raised in Korea, although perhaps a bit later (no<br>
computers, no Internet).<br>
<p>
As I write this, I can think of two (possible) Algernons who seem almost<br>
eerily identical - not in knowledge, not in skills, probably not in life<br>
history... but in writing style and outlook on life, *indistinguishable*<br>
to me, and probably any normal human looking on.  Similarly, the only<br>
other known Countersphexist happened upon my Algernon site while looking<br>
up "Singularity" on Alta Vista.  And I'd bet money that a certain other<br>
happened on "Algernon's Law" while looking up "intelligence<br>
enhancement".<br>
<p>
So it could be true that I'd be a lot different if brought up in Korea. <br>
But the evidence available to me suggests that innate ability levels do<br>
an awful lot to determine what philosophy you choose, perhaps to the<br>
point of simple genetic determinism.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2406.html">E. Shaun Russell: "Re: Eurocracy (was Re: Immortality and Resources)"</a>
<li> <b>Previous message:</b> <a href="2404.html">Eliezer Yudkowsky: "The Religion of Arbitrary Value"</a>
<li> <b>Maybe in reply to:</b> <a href="2594.html">John K Clark: "The Meaning of Life"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2424.html">Anders Sandberg: "Re: Eurocracy (was Re: Immortality and Resources)"</a>
<li> <b>Reply:</b> <a href="2424.html">Anders Sandberg: "Re: Eurocracy (was Re: Immortality and Resources)"</a>
<!-- reply="end" -->
</ul>
