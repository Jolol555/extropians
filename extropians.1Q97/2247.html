<!-- received="Tue Feb  4 22:53:31 1997 MDT" -->
<!-- sent="Tue, 04 Feb 1997 20:34:33 -0600" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Ethical Groundings (was: Anders Sandberg's Value System)" -->
<!-- id="199702050213.SAA14540@web2.calweb.com" -->
<!-- inreplyto="" -->
<title>extropians: Ethical Groundings (was: Anders Sandberg's Value System)</title>
<h1>Ethical Groundings (was: Anders Sandberg's Value System)</h1>
Eliezer Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Tue, 04 Feb 1997 20:34:33 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2247">[ date ]</a><a href="index.html#2247">[ thread ]</a><a href="subject.html#2247">[ subject ]</a><a href="author.html#2247">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2248.html">J. de Lyser: "Re: An extropian ethic - how about Habitationalism ?(was"</a>
<li> <b>Previous message:</b> <a href="2246.html">Eliezer Yudkowsky: "Re: An extropian ethic (was Ecology)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2262.html">Eric Watt Forste: "Re: Ethical Groundings (was: Anders Sandberg's Value System)"</a>
<li> <b>Maybe reply:</b> <a href="2262.html">Eric Watt Forste: "Re: Ethical Groundings (was: Anders Sandberg's Value System)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Anders Sandburg views complex systems as a basic good.<br>
This reminds me deeply of one ethical system I tried on for size, long<br>
ago:  "The basic good is new ideas."<br>
<p>
In terms of how this concept actually originated, it's when I tried to<br>
figure out where mental energy came from.  I guessed "new ideas" on the<br>
basis that they would stir up new thought-currents and create mental<br>
energy.  In retrospect, I was completely wrong; new ideas may create<br>
mental energy but simply by originating new goals which are not worn by<br>
repeated viewed-as-futile efforts, or perhaps simply because gaining new<br>
knowledge is a basic goal and even an evolved warning signal for greater<br>
awareness.<br>
<p>
In any case, the more I thought about the "new ideas" goal, the more it<br>
seemed appropriate as an actual Meaning-Of-Life candidate - in<br>
retrospect, not an Interim, but an actual Meaning.  Consider that<br>
duplicated information is likely to be no more valuable than a single<br>
copy, and that whatever the Meaning of Life is, it is likely to viewable<br>
on some level as a complex system or informational object.  That is,<br>
you'd imagine the Meaning was *something*, and duplicated *somethings*<br>
would be no better than one *something*.<br>
<p>
Unlike Anders Sandburg, I view an entire ecology as being essentially<br>
valueless - because the complexity of an ecology is so much *less* than<br>
the complexity of a single human brain that I'd happily destroy a<br>
biosphere (one not supporting any sentient life, of course!) to save a<br>
human life.  Besides, ecologies aren't conscious, and whatever the<br>
Meaning of Life is, I strongly suspect that it requires consciousness<br>
(or some similar ontologically engineered substance) as substrate.  So<br>
therefore, increasing the number of new ideas held by sentients is the<br>
most likely course to maximizing the Meaning.  This was later replaced<br>
by the Singularity on the grounds that knowing what the Meaning was<br>
would be the way to maximize it, but anyway...<br>
<p>
Consider the consequences of this ethical system.  It urges<br>
individualism, critical thinking, the advancement of science and<br>
technology - and keeping up with the forefront, writing and reading<br>
science fiction, the exploration of new frontiers, and above all the<br>
invention of methods of intelligence enhancement.  Very Extropian,<br>
really - especially considering that this system was invented long<br>
before I'd heard of Extropy, although not before reading "Great Mambo<br>
Chicken" and being raised as a Libertarian.<br>
<p>
So that's my proposal for a definition of "extropy" as a substance:<br>
New ideas.<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2248.html">J. de Lyser: "Re: An extropian ethic - how about Habitationalism ?(was"</a>
<li> <b>Previous message:</b> <a href="2246.html">Eliezer Yudkowsky: "Re: An extropian ethic (was Ecology)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2262.html">Eric Watt Forste: "Re: Ethical Groundings (was: Anders Sandberg's Value System)"</a>
<li> <b>Maybe reply:</b> <a href="2262.html">Eric Watt Forste: "Re: Ethical Groundings (was: Anders Sandberg's Value System)"</a>
<!-- reply="end" -->
</ul>
