<!-- received="Sat Jan 25 01:23:25 1997 MDT" -->
<!-- sent="Fri, 24 Jan 1997 23:47:39 -0800" -->
<!-- name="Omega" -->
<!-- email="omega@pacific.net" -->
<!-- subject="Re: Trans-Human Intelligence (was: Gender issues)" -->
<!-- id="199701250641.XAA03374@primenet.com" -->
<!-- inreplyto="Trans-Human Intelligence (was: Gender issues)" -->
<title>extropians: Re: Trans-Human Intelligence (was: Gender issues)</title>
<h1>Re: Trans-Human Intelligence (was: Gender issues)</h1>
Omega (<i>omega@pacific.net</i>)<br>
<i>Fri, 24 Jan 1997 23:47:39 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1269">[ date ]</a><a href="index.html#1269">[ thread ]</a><a href="subject.html#1269">[ subject ]</a><a href="author.html#1269">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1270.html">Omega: "Re: "PC", the rest of the story (was Re: Gender issues (was HUMOR: A"</a>
<li> <b>Previous message:</b> <a href="1268.html">Dan Fabulich: "Re: Gender issues and throwing"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Eugene Leitl wrote:<br>
 <br>
<i>&gt; I know that. It's just that I don't believe that an exponential (or</i><br>
<i>&gt; hypergolic hyperbolic) curve describes future developments adequately.</i><br>
<i>&gt; (I wish it did, since liquid nitrogen is _very_ cold, and my belly</i><br>
<i>&gt; (emotion) does not buy everything my head (ratio) tells it about</i><br>
<i>&gt; identity conservation). The whisky is good, but the steak is substandard.</i><br>
<p>
Yes, the belly does not buy what the head tells it.  A very significant, <br>
and IMO greatly underrated consideration when humans are talking about<br>
either cyronics or anything even remotely related to trans-humanism.<br>
<p>
<i>&gt; &gt; him.  Decades I would agree with (if the number of them were low)</i><br>
<i>&gt; &gt; but, IMO, not centuries.</i><br>
<i>&gt; &gt;</i><br>
<i>&gt; &gt; There is also the possibility that technological development may</i><br>
<i>&gt; &gt; end up being hyper-exponential (with time) when "qualitative" and</i><br>
<i>&gt; &gt; "non-continuous" breakthroughs (e.g. as in that often mentioned</i><br>
<i>&gt; </i><br>
<i>&gt; Oh yes, Singularity is a nice concept. Saturation sounds much less</i><br>
<i>&gt; interesting. And developmental discontinuities (plateaus), caused</i><br>
<i>&gt; to JIT-unavailabilty of next-generation technologies sound outright</i><br>
<i>&gt; grim. So I'm a killjoy. Sue me. &lt;snip&gt;</i><br>
<i>&gt;</i><br>
<i>&gt; Yes, there's a lot of things happening. However, I think that</i><br>
<i>&gt; superintelligent machines are the key, and these will need truly</i><br>
<i>&gt; impressive hardware, which will need oodles of cash and manpower for R&amp;D,</i><br>
<i>&gt; which industry might be unwilling (local optimization), and the state</i><br>
<i>&gt; unable to spend and a couple decades to hatch (wet neurosci results,</i><br>
<i>&gt; molecular manufacturing, macroscopic von Neumann probes, etc.) which is</i><br>
<i>&gt; simply is not happening.</i><br>
<i>&gt; </i><br>
<i>&gt; Maybe we need better PR.</i><br>
<p>
I agree only to the extent that the things you mention reduce to human<br>
behavior, which is what I consider to be, by far, the biggest factor in<br>
this whole process, but not with what you characterize as the hardware<br>
requirements.<br>
 <br>
<i>&gt; &gt; Maybe it won't in fact be hyper-exponential, but I feel that your</i><br>
<i>&gt; &gt; conclusion still falls under what I would call the default way</i><br>
<i>&gt; &gt; of estimating the future that Russel describes as flawed.  What</i><br>
<i>&gt; &gt; do you think, do I have the beginnings of a compelling case?</i><br>
<i>&gt; </i><br>
<i>&gt; Eliezer makes a very good point, one which maestro Vinge described</i><br>
<i>&gt; in the blooming of the Blight at the beginning of AFUTD: each subsequent</i><br>
<i>&gt; second grew longer and longer. Arithmetically, the argument is</i><br>
<i>&gt; impeccable: a fast machine can be used to build a yet faster machine,</i><br>
<i>&gt; which in turn... However, remember the reasons why Lem's 'GODs' in</i><br>
<i>&gt; "Fiasco" were so tiny. Wormhole building is accessible (if at all) at</i><br>
<i>&gt; very high energies, which take vast structures, which take some time to</i><br>
<i>&gt; build. There might be even a natural barrier (killjoy, killjoy), the</i><br>
<i>&gt; grapes might be hanging a trifle too high for natural life (spake Michio</i><br>
<i>&gt; Kaku in "Hyperspace").</i><br>
<i>&gt; </i><br>
<i>&gt; So relativistic physics sets us a barrier. Building megascale physical</i><br>
<i>&gt; structures a yet another. Moreover, the future is in flux. We can't be</i><br>
<i>&gt; certain of anything. We all might be dead or thrown back noticeably by</i><br>
<i>&gt; 2030, because of a natural, or, engineered pandemia, a society breakdown,</i><br>
<i>&gt; a global war for resources, whatever (Boulding's War Trap, Population</i><br>
<i>&gt; Trap, Entropy Trap in the "Great Transition" (1964)). The only certain</i><br>
<i>&gt; thing about the future is that it is uncertain. Trivially? True.</i><br>
<p>
I agree that the future may well be uncertain (although the possibility<br>
exists that it may actually be fully deterministic in light of the new<br>
transactional interpretation of QM [a subject for the free-will thread])<br>
but I don't agree with your assessment of the relevant technology.<br>
<p>
Wormholes?  Relativistic physics?  Whoa, I know I mentioned the pos-<br>
sibility of hyper-exponential development, but my original context was<br>
sex changes within the context of nanotech designer life.  I agree there<br>
may be fundamental barriers, but I see these as being much further out<br>
than anything involved in simply getting to nanotech and designer life.<br>
I see no reason at all why early trans-human intelligences should press<br>
the limits of physics anymore than the human brain itself does.<br>
<p>
It's not at all clear to me that developing nanotech even requires <br>
trans-human intelligence, and even if it does, I'm not at all con-<br>
vinced that the difficulties of getting to trans-human intelligence<br>
are as you present them.  I mean if the development of trans-human <br>
intelligence really is going to take:<br>
<p>
<i>&gt; truly impressive hardware, which will need oodles of cash and manpower </i><br>
<i>&gt; for R&amp;D,</i><br>
<p>
Then maybe we should consider turning the job over from the computer <br>
science field to the gene splicers and the pharmaceutical industry; at<br>
least there, the starting platform is already one of "human intelligence".<br>
<p>
All in all, it seems like the greatest problems are not knowledge, but<br>
those amorphous things we call politics, wisdom, morality, motivation,<br>
and human behavior in general (not to mention trans-human behavior).<br>
<pre>
-- 
In the Ecstatic Service of Life -- Omega
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1270.html">Omega: "Re: "PC", the rest of the story (was Re: Gender issues (was HUMOR: A"</a>
<li> <b>Previous message:</b> <a href="1268.html">Dan Fabulich: "Re: Gender issues and throwing"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
