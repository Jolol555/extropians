<!-- received="Mon Feb  3 12:40:59 1997 MDT" -->
<!-- sent="Mon, 03 Feb 1997 11:05:46 -0800" -->
<!-- name="Eric Watt Forste" -->
<!-- email="arkuat@pobox.com" -->
<!-- subject="Re: Free will, still more" -->
<!-- id="199702031905.LAA07429@idiom.com" -->
<!-- inreplyto="Free will, still more" -->
<title>extropians: Re: Free will, still more</title>
<h1>Re: Free will, still more</h1>
Eric Watt Forste (<i>arkuat@pobox.com</i>)<br>
<i>Mon, 03 Feb 1997 11:05:46 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2167">[ date ]</a><a href="index.html#2167">[ thread ]</a><a href="subject.html#2167">[ subject ]</a><a href="author.html#2167">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2168.html">Eric Watt Forste: "Re: What is the definition of "definition"?"</a>
<li> <b>Previous message:</b> <a href="2166.html">J. de Lyser: "Re: ECON: What Jim Legg doesn't understand"</a>
<li> <b>Maybe in reply to:</b> <a href="2047.html">John K Clark: "Free will, still more"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2205.html">Eliezer Yudkowsky: "Re: Free will, still more"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Eliezer Yudkowsky writes:<br>
<i> &gt;	Our mind contains cognitive objects called 'symbols'.  Nobody knows</i><br>
<i> &gt;where or how they are stored, except that we think the hippocampus</i><br>
<i> &gt;creates them and the cerebellum retrieves them.</i><br>
<p>
I thought the cerebellum was involved in fine-scale motor coordination,<br>
not in conceptual recall. Are you sure you aren't meaning to say<br>
"cerebral cortex" instead of cerebellum? That's a guess on my<br>
part...  you've really lost me.<br>
<p>
<i> &gt;	The network does ground out, because semantic primitives other than</i><br>
<i> &gt;symbols exist.  As a general rule the non-symbolic primitives consist of</i><br>
<i> &gt;either some experiences, or a transformation performed on the current</i><br>
<i> &gt;working memory or visualization, often by attempting to alter the</i><br>
<i> &gt;visualization so that it is analogous to abstracted extracts from</i><br>
<i> &gt;previous experience.</i><br>
<p>
This is an interesting tack to take, but your assertion that<br>
"experiences" or "transformations performed on the current working<br>
memory or visualization" are nonsymbolic is unsupported.  The<br>
distinction you are trying to draw here seems spurious to me. I<br>
suspect that where you are using the Hofstadterian word "symbol"<br>
I prefer the word "concept", but it's not as if we have pinned<br>
these things down in the physical brain yet, so we can use what<br>
words we choose, I suppose.<br>
<p>
<i> &gt;	Classical AIs have no visualizational facilities and their symbols are</i><br>
<i> &gt;all defined in terms of other symbols, which is why classical AI is such</i><br>
<i> &gt;a horrible model of the mind.</i><br>
<p>
The formation of concepts from the interaction between nonlinguistic<br>
percepts and linguistic percepts is still quite poorly understood.<br>
Your criticism above fails to touch (for instance) Moravec's robots,<br>
since his machines do have visualizational facilities and their<br>
"symbols" (though actually they're probably much closer to naked<br>
percepts than to linguistically informed concepts) are constructed<br>
on the basis of data from these sensorics, and yet don't come<br>
anywhere near human minds yet. They are impressively moving down the<br>
sphexishness axis, though.<br>
<p>
I suppose I'm going to have to read Calvin's THE CEREBRAL CODE now, as<br>
it seems to be the latest hot book. (I wish Edelman were a better<br>
writer.) (And after rereading what I've written below, I'm thinking that<br>
I'm long overdue to get around to Bateson.)<br>
<p>
<i> &gt;Definition (n):  An explanation intended to convey all</i><br>
<i> &gt;information necessary for the formation of a symbol.</i><br>
<p>
I think the word "symbol" is a bit vague for this context, but I<br>
understand your wanting to follow Hofstadter's jargon. Perhaps a<br>
definition is an explanation intended to convey all *linguistic*<br>
perceptual information necessary for the initial differentiation<br>
of a concept from a preexisting (in the audience's mind) seed<br>
concept...  hence the traditional Aristotelian emphasis on genus<br>
and differentia in definitions. But definitions, being linguistic<br>
constructs, clearly cannot contain the nonlinguistic perceptual<br>
information necessary for the full and accurate differentiation of<br>
the new concept from the seed concept. Because the words used<br>
*within* the linguistic definition are presumably connected to<br>
nonlinguistic perceptual memories associated with their initial<br>
formation and development, a definition of a new word is an attempt<br>
to bootstrap nonlinguistic perceptual information (associated with<br>
the words used to construct the definition) into the newly<br>
differentiating concept.<br>
<p>
Clearly, the most effective definition is going be as dependent on<br>
the conceptual resources available for exploitation within the<br>
audience's mind as it is on the actual "content" of the concept<br>
within the speaker's mind being defined.<br>
<p>
Let me add my usual caveat that by "mind" I mean the information<br>
process or system instantiated by a human physical nervous system,<br>
and not anything particularly mystical.<br>
<p>
<pre>
--
Eric Watt Forste ++ arkuat@pobox.com ++ <a href="http://www.pobox.com/~arkuat/">http://www.pobox.com/~arkuat/</a> 
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2168.html">Eric Watt Forste: "Re: What is the definition of "definition"?"</a>
<li> <b>Previous message:</b> <a href="2166.html">J. de Lyser: "Re: ECON: What Jim Legg doesn't understand"</a>
<li> <b>Maybe in reply to:</b> <a href="2047.html">John K Clark: "Free will, still more"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2205.html">Eliezer Yudkowsky: "Re: Free will, still more"</a>
<!-- reply="end" -->
</ul>
