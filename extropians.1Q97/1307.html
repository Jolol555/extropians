<!-- received="Sat Jan 25 15:56:30 1997 MDT" -->
<!-- sent="Sat, 25 Jan 1997 14:51:12 -0800" -->
<!-- name="Omega" -->
<!-- email="omega@pacific.net" -->
<!-- subject="Re: Trans-Human Intelligence (was: Gender issues)" -->
<!-- id="v02140b07af1030c94b2f@[207.171.197.181]" -->
<!-- inreplyto="Trans-Human Intelligence (was: Gender issues)" -->
<title>extropians: Re: Trans-Human Intelligence (was: Gender issues)</title>
<h1>Re: Trans-Human Intelligence (was: Gender issues)</h1>
Omega (<i>omega@pacific.net</i>)<br>
<i>Sat, 25 Jan 1997 14:51:12 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1307">[ date ]</a><a href="index.html#1307">[ thread ]</a><a href="subject.html#1307">[ subject ]</a><a href="author.html#1307">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1308.html">Omega: "Re: How does this look?"</a>
<li> <b>Previous message:</b> <a href="1306.html">Michael Lorrey: "Re: Gender issues and throwing"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Eugene Leitl wrote:<br>
 <br>
<i>&gt; On Fri, 24 Jan 1997, Omega wrote:</i><br>
<i>&gt;</i><br>
<i>&gt; &gt; I agree only to the extent that the things you mention reduce to human</i><br>
<i>&gt; &gt; behavior, which is what I consider to be, by far, the biggest factor in</i><br>
<i>&gt; &gt; this whole process, but not with what you characterize as the hardware</i><br>
<i>&gt; &gt; requirements.</i><br>
<i>&gt; </i><br>
<i>&gt; Currently, only humans are doing research. Machines are but tools</i><br>
<i>&gt; (growing smarter, granted). Cancel the humans from the equation now, and</i><br>
<i>&gt; everything will come to a crashing halt. What we need is the</i><br>
<i>&gt; sustainability of R &amp; D progress (wonderful term, this) in the absence</i><br>
<i>&gt; of of humans. In a pinch, a pretty smart von Neumann ecosystem should</i><br>
<i>&gt; suffice, after we're gone.</i><br>
<p>
Agreed.  The day when humans are no longer necessary is the day we cross<br>
the event-horizon of Vinge's "singularity"; an event which will give the<br>
term "Brave New World" true meaning.  Till that day, the obvious require-<br>
ment for further advancement is work (albeit with preferably ever incre-<br>
asing applied intelligence and efficiency) combined with a deeper under-<br>
standing of human politics/behavior/motivation/emotion so that our <br>
actions don't collide with our underlying nature.<br>
<p>
<i>&gt; Hardware requirements for a human equivalence _are_ exorbitant. Do your</i><br>
<i>&gt; back of the envelope, semiconductors won't do. Molecular circuitry is</i><br>
<i>&gt; required, which is not easy to do at all.</i><br>
&lt;snip&gt;<br>
<i>&gt; While wormholes is lunatic fringe technology, our current circuitry is</i><br>
<i>&gt; already constrained by (relativistic) physics. Do your back of the envelope,</i><br>
<i>&gt; it's really revealing.</i><br>
<p>
I understand that electronic circuitry has these constraints, but I disagree<br>
as to the relevance of these constraints in our lives.  Human equivalent <br>
intelligence is readily available in humans themselves.  Babies may be <br>
expensive, but for what you get, the prices are not in any way exorbitant.<br>
I think we tend to forget that computers are the most extreme case of idiot<br>
savants imaginable, and that by focusing entirely on invented things, we <br>
forget that trans-human intelligence is at all times available to us through<br>
ourselves.<br>
<p>
At any moment in time, human intelligence can always be improved with:<br>
<p>
1. Computers and communications technology.<br>
2. Near future in vivo cybernetic supplementation.<br>
3. Pharmaceutical supplementation.<br>
4. Personal memetic reprogramming including:<br>
   a. Additional education.<br>
   b. Organizational improvement.<br>
   c. Any number of cognitive disciplines (how to think smarter).<br>
5. Improved pedagogical/educational practices within society.<br>
6. Near future application of non-nano in vivo biological modifaction.<br>
7. All the myriad things I can't think of at the moment.<br>
<p>
Yes, the brave new world of the created trans-human is a ways down the <br>
road, but if we focus on the created hardware and ignore our own role<br>
in the process, it's easy to get pessimistic.  But I am strongly inclined<br>
to believe the message your belly was telling you about how cold liquid<br>
nitrogen is, is very closely related to the pessimism we feel.  When we <br>
focus entirely on the created hardware, we are hiding ourselves from the<br>
fact that trans-humanism is going to be "in our faces" and very personal. <br>
Your next quote demonstrates this nicely:<br>
<p>
<i>&gt; I presume we are talking about uploads, which has excellent chances to be</i><br>
<i>&gt; the only life in existance. Having physical bodies appears to me as an</i><br>
<i>&gt; anachronism, a waste of resource. You are sure you are able to protect</i><br>
<i>&gt; your physical system if the ground you tread on will be disassembled to</i><br>
<i>&gt; become a part of the Dysonian orbiting computer cluster?</i><br>
 <br>
Really, how can we expect our bellies not to do flip flops when faced with<br>
this.  This is what turns off many people to extropian ideas, they're not<br>
just deathists as many would make them out to be, they're scared to death<br>
of both:<br>
<p>
a. This technology coming home to roost in their bodies.<br>
b. The human form becoming an evolutionary backwater.<br>
<p>
BTW: Having physical bodies may no be so much of an anachronism after all<br>
if the microtubules of our cells turn out to have signifificant computational<br>
power.  If this is the case not only may human intelligence just in the brain<br>
be several orders of magnitude higher than what orthodox AI theory believes,<br>
but so too would there be additional (albeit hidden) intelligence throughout<br>
one's entire body.<br>
<p>
I have no problem with your assessments of the relevant technology for<br>
created trans-human intelligences, but in the end, it's not how many <br>
tFlops we have in our computers, it's how many tFlops we have in bellies<br>
as we ponder this stuff.<br>
<p>
<i>&gt; &gt; All in all, it seems like the greatest problems are not knowledge, but</i><br>
<i>&gt; &gt; those amorphous things we call politics, wisdom, morality, motivation,</i><br>
<i>&gt; &gt; and human behavior in general (not to mention trans-human behavior).</i><br>
<i>&gt; </i><br>
<i>&gt; Exactly. Labs, lots of bright researchers, cash, time. A coordinated</i><br>
<i>&gt; world-wide programme to dwarf HUGO &amp; Co. Do we have the resources? Do we</i><br>
<i>&gt; have the motivation? Even more importantly, do we have the time? There</i><br>
<i>&gt; are also a number of different, less pleasant developmental</i><br>
<i>&gt; discontinuities predicted for 2030, or so. Considering, how monumentally</i><br>
<i>&gt; stupid or species behaves on the large scale, I do not think them that</i><br>
<i>&gt; unlikely. We only move our ass when we feel the heat. This time, we</i><br>
<i>&gt; shouldn't make a mistake. We won't get a second chance.</i><br>
<p>
Consider that if there really is a cellular level intelligence in the <br>
cytoskeletons of our cells, then not only are we possibly faced with a<br>
communication gap within ourselves beyond what we have yet imagined,<br>
but also the possibility exists that the monumental stupidity of our<br>
species that you talk about may well be addressable through new commun-<br>
ication modalities.<br>
<p>
I feel that your offhand comment the other day about what your belly is<br>
telling you, is really just the tip of the iceberg, and in my mind what<br>
I believe to be the most likely factor when considering if the Titanic<br>
we call human civilization goes down or not.<br>
<pre>
-- 
In the Ecstatic Service of Life -- Omega
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1308.html">Omega: "Re: How does this look?"</a>
<li> <b>Previous message:</b> <a href="1306.html">Michael Lorrey: "Re: Gender issues and throwing"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
