<!-- received="Mon Mar 17 14:52:20 1997 MDT" -->
<!-- sent="Mon, 17 Mar 1997 22:04:47 +0100 (MET)" -->
<!-- name="Eugene Leitl" -->
<!-- email="Eugene.Leitl@lrz.uni-muenchen.de" -->
<!-- subject="Re: NEURO: Advanced neurons?" -->
<!-- id="Pine.SOL.3.91.970317212320.21406B-100000@sun6" -->
<!-- inreplyto="Pine.SOL.3.91N2x.970317160235.5961D-100000@hemul.nada.kth.se" -->
<title>extropians: Re: NEURO: Advanced neurons?</title>
<h1>Re: NEURO: Advanced neurons?</h1>
Eugene Leitl (<i>Eugene.Leitl@lrz.uni-muenchen.de</i>)<br>
<i>Mon, 17 Mar 1997 22:04:47 +0100 (MET)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#3731">[ date ]</a><a href="index.html#3731">[ thread ]</a><a href="subject.html#3731">[ subject ]</a><a href="author.html#3731">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3732.html">Phil Goetz: "Re: History of Scientific Skepticism"</a>
<li> <b>Previous message:</b> <a href="3730.html">Guru George: "Re: Popper's 'Scientific' Irrationalism"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="3741.html">Anders Sandberg: "Re: NEURO: Advanced neurons?"</a>
<li> <b>Reply:</b> <a href="3741.html">Anders Sandberg: "Re: NEURO: Advanced neurons?"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Mon, 17 Mar 1997, Anders Sandberg wrote:<br>
<p>
<i>&gt; On Sun, 16 Mar 1997, The Low Golden Willow wrote:</i><br>
<i>&gt; </i><br>
<i>&gt; &gt; There's not just trickier to figure out; there's requiring more data for</i><br>
<i>&gt; &gt; storage.  John and Eugene have had their debates on how much data the</i><br>
<i>&gt; &gt; brain encodes; the article seems to be evidence for Eugene's position.</i><br>
<i>&gt; &gt; And it may argue against Drexler's extreme miniturization ideas; the</i><br>
<i>&gt; &gt; brain could be less shrinkable.</i><br>
<i>&gt; </i><br>
<i>&gt; I don't know how shrinkable the brain is, but being the brother/collaborator </i><br>
<i>&gt; of an Amiga demo programmer I know that a clever hack can achieve plenty </i><br>
<p>
I still own a fully operational Amiga (A2000, this mail is being written <br>
on it), and I have sure seen miraculous demos. However, we are talking not <br>
about a radical algorithm (as a raycasting renderer vs. a polygon one) <br>
written by a human hacker, we are talking about an long-term evolutionary <br>
optimized connectionist hardware, which seem to operate at the threshold <br>
of what is at all possible with biological structures (speed, accuracy, <br>
etc.). <br>
(Remember Moravec's double bogus argument upon collapsibility of the <br>
retina, and his grand sweeping extrapolation, judging from retina to the <br>
cortex, regarding merely the most trivial functions, assuming an identical <br>
architecture on no empiric grounds whatsoever).<br>
<p>
<i>&gt; of compression if you have the basic algorithm right. I'm fairly sure </i><br>
<i>&gt; that once we have suceeded at uploading, we will gradually find ways to </i><br>
<i>&gt; shrink the upload matrix.</i><br>
<p>
That I agree absolutely with -- we won't have to account for each ion <br>
channel/neurotransmitter vesicle/cytoskeleton. I think there might be a <br>
bandwidth of several (maspar/conectionist) hardwares in existance, <br>
capable of emulating biological neuronal tissue efficiently. <br>
 <br>
<i>&gt; Back to the subject: I think the truth is somewhere between Eugene</i><br>
<i>&gt; (neurons are tricky, simulate them all as carefully as possible) and John</i><br>
<p>
In the retrospect I realize that my estimations were somewhat obscurely <br>
stated. I did not mean we must emulate each single biologically realistic <br>
neuron for uploads. This is certainly an option, in fact, I think the only <br>
way to translate a certain neural circuitry into a more efficient, <br>
compact (e.g. a packet switched integer automaton network), is by means <br>
of GA-reverse-enginering of a given peculiar (sub)network. Which requires a <br>
transient existance of a detailed emulation, of course. A population of <br>
them, in fact.<br>
<p>
What I _wanted_ to convey, is tanstaafl; that there is no free lunch, that <br>
there is a minimal computational work to be done to simulate a given <br>
physical system realistically. The harder, the smarter (= more complex) <br>
the system is. And that minimal threshold may lie quite high for such <br>
complex objects as a mammal brain. Human equivalents the size of a sugar <br>
cube, running at speeds &gt;10^6 of realtime seem to reside firmly in the <br>
realm of science fiction, not even very good science fiction.<br>
<p>
<i>&gt; (neurons are important mainly in groups) - neurons indeed do a lot of</i><br>
<i>&gt; amazing stuff, but they also work together in populations. Evolution would</i><br>
<p>
I we consider the edelmanian brain, a population is an absolute <br>
prerequisite for thought. Darwin doesn't operate on individuals, but on <br>
populations.<br>
<p>
<i>&gt; favor beings whose minds had a fair bit of redundancy over beings where</i><br>
<i>&gt; every neuron matters (in situations where brain damage is likely during</i><br>
<p>
Molecular hardware will die continuously. You simply cannot avoid it: it <br>
starts having defects right from the start, and it goes on losing bits up <br>
to the end of its usability, when it has to be substitued for fresh <br>
circuitry. We simply can't ditch "redundancy".<br>
<p>
<i>&gt; the lifetime of the being). So Eugene is pointing out an upper bound to</i><br>
<i>&gt; upload matrix capacity, while John is speaking of the "compressed"</i><br>
<i>&gt; brain-algorithm (where we can chunk neurons). </i><br>
<p>
I think this level of compressability might be quite limited. An <br>
intrinsically digital system with error-correction redundancy might have <br>
some advantages in relation to attractor regeneration capability vs. a <br>
wet analog system, as is ours. But this is pure conjecture, we lack any <br>
data whatsoever.<br>
 <br>
<i>&gt; As a neural networks/neuroscience person I would guess that these new</i><br>
<i>&gt; properties would not increase the upper estimate with more than a</i><br>
<i>&gt; magnitude. If we assume 10^11 neurons with 10^4 synapses each, and 100</i><br>
<p>
That's very good news.<br>
<p>
<i>&gt; parameters in each synapse (I'll chunk the synapses and dendrites they sit</i><br>
<i>&gt; upon into one unit here), we get 10^17 state variables. Nonlinearity,</i><br>
<i>&gt; dendritic computation and similar stuff just makes calculating the next</i><br>
<i>&gt; state more heavy, and doesn't increase the storage needs much. Each</i><br>
<i>&gt; update, (say) every millisecond might depend on the other states in each</i><br>
<i>&gt; neuron + signals from connecting neurons, giving around 10^26 flops. </i><br>
<i>&gt; Diffuse modulation doesn't increase the estimate much. Of course, this is</i><br>
<i>&gt; a very rough estimate and should be taken with a large grain of salt. </i><br>
<p>
How large large? Please give the exact weight, and the error range ;)<br>
<p>
ciao,<br>
'gene<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="3732.html">Phil Goetz: "Re: History of Scientific Skepticism"</a>
<li> <b>Previous message:</b> <a href="3730.html">Guru George: "Re: Popper's 'Scientific' Irrationalism"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="3741.html">Anders Sandberg: "Re: NEURO: Advanced neurons?"</a>
<li> <b>Reply:</b> <a href="3741.html">Anders Sandberg: "Re: NEURO: Advanced neurons?"</a>
<!-- reply="end" -->
</ul>
