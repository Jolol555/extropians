<!-- received="Wed Jan 29 17:15:14 1997 MDT" -->
<!-- sent="Thu, 30 Jan 1997 00:59:20 +0100" -->
<!-- name="Max M" -->
<!-- email="maxmcorp@inet.uni-c.dk" -->
<!-- subject="Re: Humanrintelligences' motivation (Was: Superintelligences' motivation)" -->
<!-- id="199701292356.AAA20246@inet.uni-c.dk" -->
<!-- inreplyto="Humanrintelligences' motivation (Was: Superintelligences' motivation)" -->
<title>extropians: Re: Humanrintelligences' motivation (Was: Superintelligences' motivation)</title>
<h1>Re: Humanrintelligences' motivation (Was: Superintelligences' motivation)</h1>
Max M (<i>maxmcorp@inet.uni-c.dk</i>)<br>
<i>Thu, 30 Jan 1997 00:59:20 +0100</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1623">[ date ]</a><a href="index.html#1623">[ thread ]</a><a href="subject.html#1623">[ subject ]</a><a href="author.html#1623">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1624.html">The Low Willow: "Re: Venus Automorphs Into Great Mathematician, Loses Psyche"</a>
<li> <b>Previous message:</b> <a href="1622.html">Ray Peck: "Re: Evolution: Bipedalism and Baldness(was the aquatic ape)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1689.html">Anders Sandberg: "Re: Humanrintelligences' motivation (Was: Superintelligences' motivation)"</a>
<li> <b>Reply:</b> <a href="1689.html">Anders Sandberg: "Re: Humanrintelligences' motivation (Was: Superintelligences' motivation)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
----------<br>
<i>&gt; From: N.BOSTROM@lse.ac.uk</i><br>
<p>
<i>&gt; Thus, in order to predict the long term development of the most </i><br>
<i>&gt; interesting aspects of the world, the most relevant considerations </i><br>
<i>&gt; will be (1) the fundamental physical constraints; and (2) the </i><br>
<i>&gt; higher-order desires of the agents that have the most power at the </i><br>
<i>&gt; time when technologies become available for choosing our </i><br>
<i>&gt; first-order desires.</i><br>
<p>
Good posting, but theres another minor motivation issue, wich i actually<br>
think is the most dangerous thing about extropy, transhumanism, nanotech,<br>
replicators etc. And that is human goals and motivations.<br>
We don't need advanced AI or IA but just plain simple exponential growth to<br>
give extremists and minorites acces to weaons of massive destruction.<br>
Somehow we need to change the goals and motivations of these dangerous<br>
minorities. It's a problem that is often passed over lightly when "we" talk<br>
about our rosy future, the singularity etc. But it only takes one madman<br>
with the recipe for Gray Goo to destroy the world.<br>
Currently there are enough of them to go around. :-(<br>
<p>
With a future where there's a risk of a techno elite to hold the power,<br>
there's a big chance of unsatisfied masses instead of minorities, with an<br>
abundance of unsatisfied "loosers" willing to press the button in the hope<br>
of some kind of chance.<br>
<p>
We need to adress this!<br>
<p>
There's no worse enemy than an enemy with nothing to loose.<br>
<p>
<p>
MAX M Rasmussen<br>
New Media Director<br>
<p>
Private: maxmcorp@inet.uni-c.dk<br>
         <a href="http://inet.uni-c.dk/~maxmcorp">http://inet.uni-c.dk/~maxmcorp</a><br>
<p>
Work:    maxm@novavision.dk<br>
         <a href="http://www.novavision.dk/">http://www.novavision.dk/</a><br>
<p>
This is my way cool signature message!!<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1624.html">The Low Willow: "Re: Venus Automorphs Into Great Mathematician, Loses Psyche"</a>
<li> <b>Previous message:</b> <a href="1622.html">Ray Peck: "Re: Evolution: Bipedalism and Baldness(was the aquatic ape)"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="1689.html">Anders Sandberg: "Re: Humanrintelligences' motivation (Was: Superintelligences' motivation)"</a>
<li> <b>Reply:</b> <a href="1689.html">Anders Sandberg: "Re: Humanrintelligences' motivation (Was: Superintelligences' motivation)"</a>
<!-- reply="end" -->
</ul>
