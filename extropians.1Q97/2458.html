<!-- received="Mon Feb 10 12:57:43 1997 MDT" -->
<!-- sent="Mon, 10 Feb 1997 11:33:32 -0800" -->
<!-- name="Eric Watt Forste" -->
<!-- email="arkuat@pobox.com" -->
<!-- subject="Re: The Meaning of Life" -->
<!-- id="199702101933.LAA27332@idiom.com" -->
<!-- inreplyto="The Meaning of Life" -->
<title>extropians: Re: The Meaning of Life</title>
<h1>Re: The Meaning of Life</h1>
Eric Watt Forste (<i>arkuat@pobox.com</i>)<br>
<i>Mon, 10 Feb 1997 11:33:32 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2458">[ date ]</a><a href="index.html#2458">[ thread ]</a><a href="subject.html#2458">[ subject ]</a><a href="author.html#2458">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2459.html">Eric Watt Forste: "Re: Universal Schelling points (was SPORT: Ready? . . . Break!)"</a>
<li> <b>Previous message:</b> <a href="2457.html">Eric Watt Forste: "Re: Eurocracy (was Re: Immortality and Resources)"</a>
<li> <b>Maybe in reply to:</b> <a href="2594.html">John K Clark: "The Meaning of Life"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2469.html">Lee Daniel Crocker: "Design vs. Use (was Meaning of Life)"</a>
<li> <b>Reply:</b> <a href="2469.html">Lee Daniel Crocker: "Design vs. Use (was Meaning of Life)"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
Eliezer Yudkowsky writes:<br>
<i> &gt;Well?  What is it?  I'm waiting?  I realize that a human being is more</i><br>
<i> &gt;complex than a 386, but what difference does it make?  A 386 can</i><br>
<i> &gt;maximize about as well as a human, even if it has less resources to do</i><br>
<i> &gt;it with.</i><br>
<p>
Eliezer, you're being simply obtuse. There are several different<br>
differences. A 386 cannot make love to you. A 386 cannot discuss<br>
epistemology with you. A 386 can't compose anything half as good<br>
as a Mozart piece, at least not yet. A 386 cannot send chills down<br>
my spine the way Tori Amos can. Listing obvious observations about<br>
the differences between human beings and toasters grows tiresome...<br>
surely you are capable of observing on your own. Of course, you<br>
will now come back at me and claim that these differences make no<br>
difference, and you will be wrong. These differences perhaps make<br>
no difference *to you*, but as I said, nihilism is not a philosophy,<br>
nihilism is an emotional disorder. These things make a difference<br>
to *me*.<br>
<p>
<i> &gt;Again.  Why does something acting to maximize X make it valuable, and</i><br>
<i> &gt;why is X valuable if a human acts to maximize it but not if a 386 acts</i><br>
<i> &gt;to maximize it?</i><br>
<p>
When did I claim that something is not valuable if a 386 acts to<br>
maximize it? You're putting words in my mouth. But there are certain<br>
things (such as truth, beauty, justice, love, etc.) that human beings<br>
often act towards maximizing, that you cannot (yet) program a simple von<br>
neumann machine to maximize. When you show me a "simple von neumann<br>
machine" programmed so as to maximize these things, I'll be happy to<br>
recognize it as a person. Or perhaps not. Perhaps we'll have new<br>
characteristic poorly-understood abstractions around which our society is<br>
built by then. And again, you're divorcing values from valuers. These<br>
things are valuable to *me*. I cannot make them valuable to you by means<br>
of argument. That's what the fact-value dichotomy is about. If you want<br>
to use the knife of reason to destroy your own values, that's your own<br>
business.<br>
<p>
<i> &gt;I've stated that there's no ethical difference 'tween a human maximizing</i><br>
<i> &gt;something and a thermostat following the same strategy.  Respond, remain</i><br>
<i> &gt;silent, or claim I'm "obviously wrong", refuse to back it up, and change</i><br>
<i> &gt;your name to Searle.</i><br>
<p>
How about I just avoid your company in the future? That's my usual<br>
response to people for whom human life is cheap. I happen to think it is<br>
a rational response.<br>
<p>
<i> &gt;&gt; Assertions prove nothing, Eliezer. How would you like to go about</i><br>
<i> &gt;&gt; demonstrating to us the arguments behind your conviction that value</i><br>
<i> &gt;&gt; is independent of valuers? If this is your axiom, then you are</i><br>
<i> &gt;&gt; simply assuming what it is that you are setting out to prove: that</i><br>
<i> &gt;&gt; life has no value.</i><br>
<i> &gt;</i><br>
<i> &gt;I'm setting out to prove no such thing.  Nor does it follow from</i><br>
<i> &gt;observer-independent value that life is valueless.  My claim is that</i><br>
<i> &gt;observer-dependent value in the direct, pleasure-center sense requires a</i><br>
<i> &gt;definition of "value" which is not intuitively compatible with what</i><br>
<i> &gt;*anyone* means, including all participants in this dicussion, by asking</i><br>
<i> &gt;whether life is worth living.  Thus making observer-dependent values no</i><br>
<i> &gt;values at all.</i><br>
<p>
Who said anything about "the direct, pleasure-center sense"? This<br>
is *not* what I'm talking about. Sometimes unpleasant things are<br>
valuable, and sometimes pleasant things have negative utility.<br>
Life's being worth living is not a *given*, it is not an answer to<br>
a question which can be proved or demonstrated. Life's being worth<br>
living is a consequence of action.  There's nothing sillier than<br>
someone mooning around asking whether or not life is worth living,<br>
when they could be going out and *making* their life worth living.<br>
You're doing the usual silly thing that people who try to *prove*<br>
free will instead of *choosing* free will do.<br>
<p>
<i> &gt;&gt; Perhaps it's the extra fuss that makes the difference? A simple</i><br>
<i> &gt;&gt; computer can declare "I will resist if you turn me off." You turn</i><br>
<i> &gt;&gt; it off and nothing happens. I, on the other hand, can declare "I</i><br>
<i> &gt;&gt; will resist if you try to take away my raincoat." and you will find</i><br>
<i> &gt;&gt; it a considerably harder task to take away my raincoat than to turn</i><br>
<i> &gt;&gt; off the simple computer.</i><br>
<i> &gt;</i><br>
<i> &gt;Now you're citing something with a vague bit of justification:  You</i><br>
<i> &gt;state that *meaningful* maximization requires *resistance* to</i><br>
<i> &gt;de-maximizing forces.</i><br>
<p>
I made no such statement. I was a making a suggestion, and I didn't say<br>
anything about "requiring"... I was merely talking about "making a<br>
difference". You are oversimplifying my suggestion and cramming it into<br>
some weird crystal-edged intellectual pigeonhole of yours again.<br>
<p>
<i> &gt;Someday, computer security systems may put up quite a fight if you try</i><br>
<i> &gt;to turn them off over the 'Net.  This is not the place to retype GURPS</i><br>
<i> &gt;Cyberpunk, but possible actions range from severing connections to</i><br>
<i> &gt;repairing damaged programs to rewriting entrance architectures to impose</i><br>
<i> &gt;new password-challenges.  I could write, right now, an application that</i><br>
<i> &gt;resisted death in a dozen ways, from trying to copy itself elsewhere to</i><br>
<i> &gt;disabling the commonly used off-switch.  If someone can't resist, do</i><br>
<i> &gt;they lose their right to live?  This is the "fetuses/computers have no</i><br>
<i> &gt;rights because they are dependent on us" absurdity all over again.  If I</i><br>
<i> &gt;make you dependent on me, even for blood transfusions, do you lose your</i><br>
<i> &gt;rights as a sentient being?</i><br>
<p>
Yes, the standard distinction between persons and non-persons is<br>
breaking down, and we're going to have come up with new heuristics for<br>
making that distinction. What else is new? But the fact that we will<br>
soon no longer be able to make the glib equation "person = human" has no<br>
impact on the question of whether or not life can be worth living.<br>
<p>
<i> &gt;There are dozens of far-from-sentient classical AIs that will attempt to</i><br>
<i> &gt;achieve goals using combinations of actions.  From where I sit, I see a</i><br>
<i> &gt;folder called "shrdlu" which contains a computer program that will</i><br>
<i> &gt;attempt to stack up blocks and uses chained actions to that effect. </i><br>
<i> &gt;Does this program have any ethical relevance?  Within its world, do</i><br>
<i> &gt;stacked blocks have meaning?</i><br>
<p>
Not to me it doesn't. But please make up your own mind on this question.<br>
I don't think there's any screaming need for consensus yet on this<br>
particular issue. If some people want to treat present-day AI programs<br>
as people, that doesn't bother me any. It's good practice for the<br>
future, and not much weirder than the most radical animal-rights<br>
activists.<br>
<p>
<i> &gt;Don't give me that "sad that you can't tell the difference" copout. </i><br>
<i> &gt;Tell me what the fundamental difference is, and why "maximizing X" is</i><br>
<i> &gt;not sufficient to make X valuable, and how humans maximizing X includes</i><br>
<i> &gt;the key ingredient.</i><br>
<p>
Do your own research, buddy. I'm not trying to sell you anything, I'm<br>
just trying to steer you away from suicide-talk, because I would be<br>
pissed off at you if you killed yourself over some ridiculous<br>
abstraction that you had set up in your brain in such a way that it had<br>
a negative effect on your brain chemistry. These things do happen!<br>
Abstractions make good tools and poor masters. The meaning of life is<br>
either right at the tips of your nerves, in the experiences that you are<br>
having right now, or it is not at all. You are scaring yourself with<br>
ghosts. The talk you are talking is death-memes, information patterns<br>
that get into people's heads and sometimes kill them. It's lemming talk<br>
even worse than the most virulent Christian fundamentalist memes.<br>
Perhaps you are just playing intellectual games, and you're not really<br>
*feeling* the sadness that comes with such ridiculous beliefs as the<br>
ones you are pretending to espouse. But I've felt it enough that I have<br>
very little patience for these ideas, and if you don't like my lack of<br>
patience, tough! Go tell it to Kurt Cobain.<br>
<p>
<i> &gt;Name as many as you want, because I can PROVE that a giant</i><br>
<i> &gt;hashtable-thermostat can maximize anything a computational mind can. </i><br>
<i> &gt;I.e. REALLY BIIG (but less than 3^^^3) lookup table, duplicates inputs</i><br>
<i> &gt;and outputs, no mind, but works as well.  Again, what difference does it</i><br>
<i> &gt;make what does the maximizing?</i><br>
<p>
Okay, bring me a thermostat that maximizes beauty. I want it next<br>
Thursday. Thanks.<br>
<p>
<i> &gt;Why is tickling the pleasure centers significant?  You're simply ducking</i><br>
<i> &gt;the question.  Evolution makes a lot of things *pleasurable*, this</i><br>
<i> &gt;causes us to assign them value, but I want to know if things *really*</i><br>
<i> &gt;have value that isn't just a trick of our genes!  If our genes made us</i><br>
<i> &gt;think that sadism was inherently valuable, would it be?  You've named</i><br>
<i> &gt;things that we think of as not merely pleasurable, but *holy* - but John</i><br>
<i> &gt;K Clark would laugh at quite a few "holy" values.</i><br>
<p>
Significant of what? What question is it, precisely, that I'm ducking<br>
here? Besides, you don't even know what you mean by "the pleasure<br>
centers". You are scaring yourself with a lot of voodoo talk, building<br>
frightening theories about things that have not yet been scientifically<br>
investigated with any good results. "The pleasure centers" could turn<br>
out to have as much or as little meaning as phlogiston or caloric. If<br>
you are sincerely interested in these precise questions, you should<br>
take up empirical neuroscience research, instead of pretending to be<br>
able to come up with answers by mere introspection like Descartes and<br>
Leibniz. Answers to such questions come from laboratories, not from<br>
mailing lists.<br>
<p>
<i> &gt;Now, it is possible that pleasure has inherent value.  David Pearce</i><br>
<i> &gt;thinks so.  He could even be right; subjective conscious pleasure could</i><br>
<i> &gt;be inherently valuable.  In which case the logical course would be</i><br>
<i> &gt;wireheading on the greatest possible scale, using only the Power</i><br>
<i> &gt;necessary to "turn on" the Universe and then devoting all those useless</i><br>
<i> &gt;thinking circuits to pleasure-center emulation.  And yet, some deep part</i><br>
<i> &gt;of us says:  "This isn't meaningful; this is a dead end."</i><br>
<p>
This presumes that the phrase "inherent value" means anything<br>
independent of a particular valuer, which it does not! Value is a<br>
relationship between the valuer and the valued.  And there are no<br>
rules to tell you what it is. This is precisely what it means to<br>
be free. You get to decide what is right and what is wrong: you<br>
are free. Thou art God. If you mess things up for your cohorts,<br>
they will mess things up for you, so you might want to choose to<br>
refrain from force and fraud (I usually do).<br>
<p>
You aren't going to get freedom from choice, no matter how much you<br>
crave it. As the old Devo song goes, freedom of choice is what you've<br>
got.<br>
<p>
<i> &gt;I've been at peace with the world around me, while listening to Bach, if</i><br>
<i> &gt;at few other times.  It moves me.  I admit it.  But I still have the</i><br>
<i> &gt;moral courage to defy my own sense of outrage and say, "So what?" </i><br>
<i> &gt;Simply because I assign value to a thing does not necessarily make it</i><br>
<i> &gt;valuable!  Why is assigning value to Bach better than assigning value to</i><br>
<i> &gt;cats?  And if the latter is wrong, why not the first as well?</i><br>
<p>
Such a declaration is a degradation and cheapening of yourself as a<br>
valuer. It is a repudiation of your own freedom to create values. You<br>
are free to act, to create, to enjoy, to love. If that's not enough for<br>
you, perhaps you can find something more in a basement universe with<br>
different laws of physics.<br>
<p>
<i> &gt;Your intuitive definition of "value" is "All pleasurable activities that</i><br>
<i> &gt;I have not categorized as 'dead ends'."  My response:  "So what? </i><br>
<i> &gt;Justify!"</i><br>
<p>
Not all valuable things are pleasurable, for starters. Perhaps you<br>
are so confused because you have oversimplified here (hedonism is<br>
always an oversimplification, although perhaps David Pearce did it<br>
right this time). But I'm not espousing simple hedonism here. And I<br>
don't have to justify myself to you, buddy. (Moral philosophy always<br>
ends up in rudeness, it seems.)<br>
<p>
<i> &gt;So it could be true that I'd be a lot different if brought up in Korea. </i><br>
<i> &gt;But the evidence available to me suggests that innate ability levels do</i><br>
<i> &gt;an awful lot to determine what philosophy you choose, perhaps to the</i><br>
<i> &gt;point of simple genetic determinism.</i><br>
<p>
Now you are begging the question that innate ability levels are<br>
determined by your genes and not by your first-five-years environment,<br>
which is precisely the claim that I was attacking!<br>
<p>
<pre>
--
Eric Watt Forste ++ arkuat@pobox.com ++ <a href="http://www.pobox.com/~arkuat/">http://www.pobox.com/~arkuat/</a> 
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2459.html">Eric Watt Forste: "Re: Universal Schelling points (was SPORT: Ready? . . . Break!)"</a>
<li> <b>Previous message:</b> <a href="2457.html">Eric Watt Forste: "Re: Eurocracy (was Re: Immortality and Resources)"</a>
<li> <b>Maybe in reply to:</b> <a href="2594.html">John K Clark: "The Meaning of Life"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2469.html">Lee Daniel Crocker: "Design vs. Use (was Meaning of Life)"</a>
<li> <b>Reply:</b> <a href="2469.html">Lee Daniel Crocker: "Design vs. Use (was Meaning of Life)"</a>
<!-- reply="end" -->
</ul>
