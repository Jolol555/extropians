<!-- received="Fri Feb  7 16:10:09 1997 MDT" -->
<!-- sent="Fri, 07 Feb 1997 01:34:48 -0600" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The Meaning of Life" -->
<!-- id="v01530500af20eb47029d@[204.212.59.164]" -->
<!-- inreplyto="The Meaning of Life" -->
<title>extropians: Re: The Meaning of Life</title>
<h1>Re: The Meaning of Life</h1>
Eliezer Yudkowsky (<i>sentience@pobox.com</i>)<br>
<i>Fri, 07 Feb 1997 01:34:48 -0600</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2359">[ date ]</a><a href="index.html#2359">[ thread ]</a><a href="subject.html#2359">[ subject ]</a><a href="author.html#2359">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2360.html">Kathryn Aegis: "Re: Venus wonders what all the fuss is about"</a>
<li> <b>Previous message:</b> <a href="2358.html">Regina Pancake: "Re: WIRED ARTICLE ON JULIAN SIMON by ED REGIS"</a>
<li> <b>Maybe in reply to:</b> <a href="2594.html">John K Clark: "The Meaning of Life"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2431.html">Ray Peck: "Re: WIRED ARTICLE ON JULIAN SIMON by ED REGIS"</a>
<li> <b>Reply:</b> <a href="2431.html">Ray Peck: "Re: WIRED ARTICLE ON JULIAN SIMON by ED REGIS"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<i>&gt; 1)I am the one doing the choosing if I have free will.  </i><br>
<i>&gt; 2)I have free will if I can not predict what I will do next. </i><br>
<i>&gt; 3)I am doing the choosing.</i><br>
<p>
This man is a Turing computationalist?<br>
<p>
Lemme 'splain something, friend.  The thing about Turing machines is<br>
that all forces within the computation originate from clearly defined<br>
causal units.  What I'm trying to say is that whatever choice you make,<br>
it *will* have a causal explanation within the computational framework.<br>
<p>
Remember - "The choices are written in our brains, but we are the<br>
writing?"  Ya can't lift yourself up to some pure ethical plane and say<br>
that your "choice" is made of your own "free will" while a thermostat<br>
does not choose to keep the temperature at a certain setting.  After<br>
all, you know a lot more about yourself than a thermostat does, so your<br>
will should be much less free.<br>
<p>
The point I'm trying to make is that you'll assign meaning to something,<br>
for causal reasons explicable entirely by Turing-based forces.  Either<br>
that, or start chanting, to use your phrase.<br>
<p>
Now.  The question is - will you assign meaning based on "what you<br>
want", which is a fancy way of saying you'll let evolution do it for<br>
you?  Or will you assign meaning based on a genuinely true chain of<br>
logic?  In short, will you reason it out or accept the dictates of your<br>
genes?<br>
<p>
<i>&gt; You don't want the meaning of life, you want a way to always know what is the </i><br>
<i>&gt; best way to maximize value, well I don't blame you, I want that too, it's </i><br>
<i>&gt; called infinite intelligence.</i><br>
<p>
Ah.  I don't quite understand.  Either things have value, or everything<br>
is valueless.  I'm not saying that there's only one "Meaning" which is<br>
the best way to maximize value.  I'm asking if anything has any value at<br>
all.<br>
<p>
And saying "Yes, it has value, to me" is simply ducking the point.  You<br>
say it has value?  Well, I can take apart your brain right down to the<br>
level of individual atoms and demonstrate that you assigned it value<br>
simply because it tickled your pleasure centers.  And I am not<br>
impressed.  If I build a computer programmed to spew out:  "Cats have<br>
value to me", fundamentally, nothing happens.  So what?<br>
<p>
I simply deny that value is an observer-dependent thing.  You can have,<br>
in memory, a declarative statement that "X has value".  So what?  A<br>
CD-ROM can do the same thing with a lot less fuss.  Certainly this<br>
statement will have no causal influence on X - and what's more, it won't<br>
have any logical force.  And if I code it with RSA, does X suddenly lose<br>
its value?  If I lose the encryption key, does X lose its value?<br>
<p>
My position is that we can't alter the Platonic structure of the<br>
Universe through wishful thinking.  We can't make things have value just<br>
by declaring they do.  Our declaration that "X has value" will cause us<br>
to act in such a way as to maximize X.  So what?  What good is it,<br>
unless X really *is* valuable?<br>
<p>
Are you going to tell me that "value" is defined with respect to a<br>
particular individual, and equals whatever that person acts to<br>
maximize?  That 74 degrees is the Meaning of Life for a thermostat? <br>
Well, I am jumping out of the system.  I'm saying that to *me*, acting<br>
to maximize X *has* *no* *value*.<br>
<p>
You see, you are arguing for a Meaning of Life, just a silly one. <br>
You're saying that the Meaning of Life is a certain sort of behavior,<br>
acting to maximize things.  I don't see how this is any more reasonable<br>
than saying that the Meaning of Life is "cats" or "worshipping God" or<br>
"tuna loaf".  So you're born, you live, you act to maximize something,<br>
and you die.  I see no place for intelligence, consciousness, brains,<br>
neurons, thinking, subjectivity, or really much of anything in this<br>
worldview.<br>
<p>
In short, John K Clark, your ideal world is run by thermostats, who act<br>
to maximize things with a purity far exceeding that of any human, and<br>
have completely free will through their complete lack of<br>
self-knowledge.  Where I come from, we call this a "reductio ad<br>
absurdum".  I'm sure you disagree, though.  At this point in our<br>
discussions, you usually change the rules by subtly redefining a term. <br>
My guess is that it will be "value".  Well?<br>
<pre>
-- 
         sentience@pobox.com      Eliezer S. Yudkowsky
          <a href="http://tezcat.com/~eliezer/singularity.html">http://tezcat.com/~eliezer/singularity.html</a>
           <a href="http://tezcat.com/~eliezer/algernon.html">http://tezcat.com/~eliezer/algernon.html</a>
Disclaimer:  Unless otherwise specified, I'm not telling you
everything I think I know.
</pre>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2360.html">Kathryn Aegis: "Re: Venus wonders what all the fuss is about"</a>
<li> <b>Previous message:</b> <a href="2358.html">Regina Pancake: "Re: WIRED ARTICLE ON JULIAN SIMON by ED REGIS"</a>
<li> <b>Maybe in reply to:</b> <a href="2594.html">John K Clark: "The Meaning of Life"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2431.html">Ray Peck: "Re: WIRED ARTICLE ON JULIAN SIMON by ED REGIS"</a>
<li> <b>Reply:</b> <a href="2431.html">Ray Peck: "Re: WIRED ARTICLE ON JULIAN SIMON by ED REGIS"</a>
<!-- reply="end" -->
</ul>
