<!-- received="Sat Feb  8 22:27:49 1997 MDT" -->
<!-- sent="Sat, 8 Feb 1997 21:10:31 -0800 (PST)" -->
<!-- name="John K Clark" -->
<!-- email="johnkc@well.com" -->
<!-- subject="The Meaning of Life" -->
<!-- id="199702090510.VAA07996@well.com" -->
<!-- inreplyto="" -->
<title>extropians: The Meaning of Life</title>
<h1>The Meaning of Life</h1>
John K Clark (<i>johnkc@well.com</i>)<br>
<i>Sat, 8 Feb 1997 21:10:31 -0800 (PST)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2410">[ date ]</a><a href="index.html#2410">[ thread ]</a><a href="subject.html#2410">[ subject ]</a><a href="author.html#2410">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2411.html">Eliezer Yudkowsky: "Goodbye 5."</a>
<li> <b>Previous message:</b> <a href="2409.html">J. Daugherty: "Memory and Morphic Resonance"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
-----BEGIN PGP SIGNED MESSAGE-----<br>
<p>
On Fri, 07 Feb 1997 Eliezer Yudkowsky &lt;sentience@pobox.com&gt; Wrote:<br>
<p>
<i>        &gt;The thing about Turing machines is that all forces within the         </i><br>
<i>        &gt;computation originate from clearly defined causal units.          </i><br>
<p>
<p>
Thank you Guru Eliezer for those words of wisdom. Your pride is justified,  <br>
only someone who has achieved the 800'th level of SAT enlightenment could  <br>
have deduced that a Turing Machine was deterministic even if it is not  <br>
predictable. Help me follow in your glorious footsteps with my own humble  <br>
query. Do you think that maybe that's why they call a Turing Machine a  <br>
machine?       <br>
<p>
<p>
<i>        &gt;This man is a Turing computationalist?        </i><br>
<i>        &gt;Lemme 'splain something, friend.              </i><br>
<p>
<p>
No, let me explain something to you buddy boy. I expect and want people to <br>
emphasize the weak points in my arguments, I rather like sarcasm even if it's <br>
directed at me, and insults don't bother me much, but deliberately  <br>
misrepresenting my position is going way over the line. And and this was not <br>
just a misunderstanding, you're not dumb enough to think that a deterministic  <br>
machine would be a revelation to me.<br>
              <br>
<p>
<i>        &gt;What I'm trying to say is that whatever choice you make, it *will*          </i><br>
<i>        &gt;have a causal explanation within the computational framework.</i><br>
<p>
<p>
Within a computational framework sure,  but not within my computational  <br>
framework, and that makes all the difference. I don't understand myself,  <br>
I don't know what I will do next and this is why I feel free. This feeling  <br>
is not an illusion, I  really do feel free.<br>
              <br>
<p>
<i>        &gt;Ya can't lift yourself up to some pure ethical plane and say that         </i><br>
<i>        &gt;your "choice" is made of your own "free will" while a thermostat        </i><br>
<i>        &gt;does not choose to keep the temperature at a certain setting.  </i><br>
<p>
<p>
I said  "A being has free will if and only if he can not predict what he <br>
will do next", the only part of this definition that is vague is "a being". <br>
If  you insist that a thermostat is a being then I would have to say that <br>
being had free will.<br>
              <br>
<p>
<i>        &gt;After all, you know a lot more about yourself than a thermostat does </i><br>
        <br>
<p>
That could be true but is far from obvious. I am astronomically smarter <br>
than a thermostat but the thing I'm trying to understand, myself, is <br>
astronomically  more complex.<br>
           <br>
<p>
<i>        &gt;The point I'm trying to make is that you'll assign meaning to         </i><br>
<i>        &gt;something, for causal reasons explicable entirely by Turing-based         </i><br>
<i>        &gt;forces.  </i><br>
<p>
<p>
A keen grasp of the obvious.<br>
             <br>
<p>
<i>        &gt;The question is - will you assign meaning based on "what you want"         </i><br>
<p>
<p>
Yes, but your way is much better, you will assign meaning in ways you don't  <br>
want.<br>
<p>
<p>
<i>       &gt;which is a fancy way of saying you'll let evolution do it for you?         </i><br>
       <br>
<p>
The reason I have the personality I do is because  something caused it to be <br>
that way, my genes and my environment. I'm trying very hard to figure out  <br>
why this should be depressing, but am not having much luck. Of course,  <br>
if different causes, different reasons, had acted on me then I would it find<br>
determinism depressing and would have been a great fan of the only  <br>
alternative, randomness.<br>
             <br>
<p>
<i>        &gt;Or will you assign meaning based on a genuinely true chain of logic?          </i><br>
<p>
<p>
Which is a fancy way of saying you'll let the meaning of life do it for you.         <br>
           <br>
<p>
<i>        &gt;In short, will you reason it out or accept the dictates of your        </i><br>
<i>        &gt;genes?</i><br>
<p>
<p>
So you want to reason it out, you want to find the reasons, the causes, that <br>
effects the machine that calls itself Eliezer Yudkowsky to act in the way it <br>
does. That sounds like a worthwhile thing to do to me, but understand,  <br>
you'll be able to find some of the causes, but Turing proved you can't find  <br>
them  all.       <br>
           <br>
<p>
<i>        &gt;I'm asking if anything has any value at all.</i><br>
<p>
<p>
Yes, a dollar has the value of 100 cents,  PI has the value 3.14149...<br>
<p>
Value is the amount of a specific measurement, in this thread the specific <br>
measurement I'm talking about is the desire to maximize. <br>
                   <br>
<p>
<i>        &gt;You say it has value?  Well, I can take apart your brain right down         </i><br>
<i>        &gt;to the level of individual atoms and demonstrate that you assigned         </i><br>
<i>        &gt;it value simply because it tickled your pleasure centers.  </i><br>
<p>
<p>
No shit Sherlock! <br>
              <br>
<p>
<i>        &gt;And saying "Yes, it has value, to me" is simply ducking the point.          </i><br>
            <br>
<p>
How is that ducking the point? How does that prove that it  doesn't have <br>
value for me?<br>
<p>
<p>
<i>        &gt;And I am not impressed.  If I build a computer programmed to spew         </i><br>
<i>        &gt;out:  "Cats have value to me", fundamentally, nothing happens.          </i><br>
<p>
<p>
It's not important if you're impressed or not, you're not the one who likes  <br>
cats, it's only important if the computer is impressed or not.<br>
<p>
<p>
<i>        &gt;So what?</i><br>
<p>
<p>
So cats have value to the computer.<br>
<p>
<p>
<i>        &gt;I simply deny that value is an observer-dependent thing.          </i><br>
<p>
<p>
Then you have fallen so in love with your own theory that you simply deny the  <br>
results of experiments that are in conflict with it. That has never worked in  <br>
Science in the past and is certainly not a path to the truth. Ask a thousand  <br>
people what they value and what they don't and you will get a thousand <br>
different answers.<br>
               <br>
<p>
<i>        &gt;You can have, in memory, a declarative statement that "X has value".          </i><br>
<i>        &gt;So what?  A CD-ROM can do the same thing with a lot less fuss.  </i><br>
<p>
<p>
At least a thermostat changes its state, a CD-ROM is static, and a mind that <br>
is static is not a mind.<br>
              <br>
<p>
<i>        &gt;We can't make things have value just by declaring they do.  </i><br>
<p>
<p>
Why not? We can and do make value judgments,  and we do so a thousand times  <br>
a day. You say we error when we do so, but what are the consequences of that  <br>
error? Where is the collapsed bridge? Exactly what is it that goes wrong?<br>
                    <br>
<p>
<i>        &gt;Our declaration that "X has value" will cause us to act in such a         </i><br>
<i>        &gt;way as to maximize X.           </i><br>
        <br>
Yes.<br>
<p>
<p>
<i>        &gt;So what?  </i><br>
<p>
So we maximize X and are happy.<br>
<p>
<p>
<i>        &gt;What good is it, unless X really *is* valuable?</i><br>
<p>
<p>
According to my theory, an anvil is valuable to a blacksmith but it is not  <br>
valuable to a man trying to swim across the English Channel. According to  <br>
your theory the anvil would be of equal value to both. I'm right, you're not.<br>
<p>
        <br>
<i>        &gt;74 degrees is the Meaning of Life for a thermostat? </i><br>
<p>
<p>
74? Ridiculous! It's 42.<br>
<p>
<p>
<i>         &gt;I am jumping out of the system.           </i><br>
<i>         &gt;I'm saying that to *me*, acting to maximize X *has* *no* *value*.</i><br>
<p>
<p>
You are not jumping out of THE system, you are just jumping out of YOUR <br>
system. If you really believe in the above for any X, then your life would  <br>
indeed have no value to YOU. You're still alive so I rather doubt that is  <br>
the case.<br>
<p>
Things are very different for me. I think many things have value , but <br>
If  X = "The Meaning of Life" then to *me*, acting to maximize X *has* *no*  <br>
*value* to me.<br>
                <br>
<p>
<i>        &gt;I see no place for intelligence, consciousness, brains, neurons,         </i><br>
<i>        &gt;thinking, subjectivity, or really much of anything in this worldview.</i><br>
<p>
<p>
I see no need for anything to give its value seal of approval to <br>
consciousness. I do see a very great need for consciousness to give its <br>
value seal of approval to other things.<br>
<p>
<p>
<i>        &gt;At this point in our discussions, you usually change the rules by         </i><br>
<i>        &gt;subtly redefining a term. My guess is that it will be "value". Well? </i><br>
<p>
<p>
I've already given you my definition of value, I am still happy with it and <br>
see no need to redefine it. I can't very well redefine your definition of <br>
value because you never gave me one.          <br>
<p>
                                              John K Clark    johnkc@well.com<br>
                                              <br>
PS: I probably should have waited an hour or two before I wrote this reply,     <br>
    it wouldn't have had such an ill tempered tone, but there were reasons    <br>
    that I acted as I did, even if I don't know what they were.<br>
<p>
-----BEGIN PGP SIGNATURE-----<br>
Version: 2.6.i<br>
<p>
iQCzAgUBMv1ZiX03wfSpid95AQEZiwTw0rfGo1+bhQ4Nt9Cn+Vd0TmqeNdHfluZI<br>
jABXfeopHUngmjVr41zvBmPh5Ms3CJqL36ZG+u/pulGN9kho4aJynDCHRgYH8lMg<br>
naQdQVDUEzB9bvi/OmSUtscxkbiDPwVDJLzgoeK7nq7FkqMi5XYyFLaoAua/eNbE<br>
LskdmDrkKU0ot89eVl2Ke9zqRc+39OOlz5sRZPKugqOgjyPdkPg=<br>
=p+X6<br>
-----END PGP SIGNATURE-----<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2411.html">Eliezer Yudkowsky: "Goodbye 5."</a>
<li> <b>Previous message:</b> <a href="2409.html">J. Daugherty: "Memory and Morphic Resonance"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
