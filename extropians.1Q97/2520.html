<!-- received="Wed Feb 12 10:15:20 1997 MDT" -->
<!-- sent="Wed, 12 Feb 1997 09:11:25 -0800" -->
<!-- name="Max More" -->
<!-- email="maxmore@primenet.com" -->
<!-- subject="Re: PHIL/AI: Humongous Lookup Table" -->
<!-- id="3.0.32.19970212091012.00c176e8@mailhost.primenet.com" -->
<!-- inreplyto="PHIL/AI: Humongous Lookup Table" -->
<title>extropians: Re: PHIL/AI: Humongous Lookup Table</title>
<h1>Re: PHIL/AI: Humongous Lookup Table</h1>
Max More (<i>maxmore@primenet.com</i>)<br>
<i>Wed, 12 Feb 1997 09:11:25 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2520">[ date ]</a><a href="index.html#2520">[ thread ]</a><a href="subject.html#2520">[ subject ]</a><a href="author.html#2520">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2521.html">Max More: "Re: Platonic materialism"</a>
<li> <b>Previous message:</b> <a href="2519.html">Natasha V. Mor: "Re: Explaining Extropy to mundanes"</a>
<li> <b>Maybe in reply to:</b> <a href="2477.html">Hal Finney: "PHIL/AI: Humongous Lookup Table"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2526.html">CurtAdams@aol.com: "Re: PHIL/AI: Humongous Lookup Table"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
At 04:27 PM 2/12/97 +1000, Mitch wrote:<br>
<i>&gt;My $.02: the lookup table presents no problem if you regard passing</i><br>
<i>&gt;the Turing Test as a likely indication of consciousness or intelligence, </i><br>
<i>&gt;but not as a defining criterion of intelligence.</i><br>
<i>&gt;</i><br>
<i>&gt;Consider the "Thermometer Test" for the presence of heat. If you</i><br>
<i>&gt;stick a thermometer in an unknown liquid and the mercury shoots up</i><br>
<i>&gt;the column, you have reason to believe that the liquid is hot.</i><br>
<i>&gt;But "being hot" is not the same thing as "being disposed to cause </i><br>
<i>&gt;mercury to expand", and there are other ways to make mercury rise.</i><br>
<i>&gt;</i><br>
<i>&gt;In other words: why would anyone think that the Turing Test was</i><br>
<i>&gt;foolproof? [snip] (Actually,</i><br>
<i>&gt;I would have thought that a functionalist would agree on this</i><br>
<i>&gt;count, since functionalism cares about internal causation.)</i><br>
<p>
Well, I'm a functionalist and I agree. For me, the Turing Test can only be<br>
an indicator, as you say. It really doesn't contain any theory of<br>
intelligence, except maybe of the kind "if it looks like a dog from the<br>
outside then it is a dog" kind. The Turing Test was proposed in the days of<br>
behaviorism. It may be the best we can do for now, but eventually we'll<br>
have a convincing theory of intelligence and awareness. <br>
<p>
Max<br>
<p>
<p>
Max More, Ph.D.<br>
more@extropy.org<br>
<a href="http://www.primenet.com/~maxmore">http://www.primenet.com/~maxmore</a><br>
President, Extropy Institute, Editor, Extropy<br>
exi-info@extropy.org, <a href="http://www.extropy.org">http://www.extropy.org</a><br>
(310) 398-0375<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2521.html">Max More: "Re: Platonic materialism"</a>
<li> <b>Previous message:</b> <a href="2519.html">Natasha V. Mor: "Re: Explaining Extropy to mundanes"</a>
<li> <b>Maybe in reply to:</b> <a href="2477.html">Hal Finney: "PHIL/AI: Humongous Lookup Table"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2526.html">CurtAdams@aol.com: "Re: PHIL/AI: Humongous Lookup Table"</a>
<!-- reply="end" -->
</ul>
