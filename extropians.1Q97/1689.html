<!-- received="Thu Jan 30 06:43:35 1997 MDT" -->
<!-- sent="Thu, 30 Jan 1997 14:40:55 +0100 (MET)" -->
<!-- name="Anders Sandberg" -->
<!-- email="nv91-asa@nada.kth.se" -->
<!-- subject="Re: Humanrintelligences' motivation (Was: Superintelligences' motivation)" -->
<!-- id="11193325010271@jax.gulfnet.com" -->
<!-- inreplyto="199701292356.AAA20246@inet.uni-c.dk" -->
<title>extropians: Re: Humanrintelligences' motivation (Was: Superintelligences' motivation)</title>
<h1>Re: Humanrintelligences' motivation (Was: Superintelligences' motivation)</h1>
Anders Sandberg (<i>nv91-asa@nada.kth.se</i>)<br>
<i>Thu, 30 Jan 1997 14:40:55 +0100 (MET)</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#1689">[ date ]</a><a href="index.html#1689">[ thread ]</a><a href="subject.html#1689">[ subject ]</a><a href="author.html#1689">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1690.html">Omega: "Re: Von Neumann's Blunder"</a>
<li> <b>Previous message:</b> <a href="1688.html">Omega: "Re: Free Will vs Group Think"</a>
<li> <b>In reply to:</b> <a href="1623.html">Max M: "Re: Humanrintelligences' motivation (Was: Superintelligences' motivation)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
On Thu, 30 Jan 1997, Max M wrote:<br>
<p>
<i>&gt; We don't need advanced AI or IA but just plain simple exponential growth to</i><br>
<i>&gt; give extremists and minorites acces to weaons of massive destruction.</i><br>
<i>&gt; Somehow we need to change the goals and motivations of these dangerous</i><br>
<i>&gt; minorities.</i><br>
<p>
This is a big problem, but changing goals and motivations of people is<br>
very hard, most likely much harder than nanotech or AI. Convincing *many*<br>
people that causing mass destruction is bad is quite possible and has<br>
obviously worked, the trouble is that those who would cause it usually<br>
have memes that shield them from "foreign propaganda" or "satanic lies". <br>
<p>
<i>&gt; But it only takes one madman</i><br>
<i>&gt; with the recipe for Gray Goo to destroy the world.</i><br>
<i>&gt; Currently there are enough of them to go around. :-(</i><br>
<p>
Exactly. And the problem will get worse as technology advances. Since the<br>
power of destructive devices is increasing, while the price of any<br>
technology once developed tends to fall, and the percentage of madmen<br>
seems to remain finite the expectation value of madmen-induced disasters<br>
will increase. In short: sooner or later even local loonies can play with<br>
gengineering anthrax bacteria. <br>
<p>
The problem is that you cannot get rid of the "loonies", we can just <br>
decrease the number of them by education, brainwashing, niceness nanites <br>
in the drinking water and whatnot, not bring it down to zero - that would <br>
imply that everyone was rational, tolerant and devoted to the continued <br>
existence of homo sapiens and its descendant species. Let's not forget <br>
incompetence ("Oops! I forgot to add a halt instruction to the <br>
replicator!") and different values ("Everyone will thank us for removing <br>
the revenge instinct"). <br>
<p>
So, what other possibilities are there? You can of course try to halt the<br>
growth of technology, but that will most likely cause very dangerous other<br>
effects. You can try to prevent the development of nasty inventions, but<br>
that is hard to predict beforehand (who could tell in the ninteenth<br>
century what the investigations of the fluoroscence of some heavy minerals <br>
would lead to?). You can try to monitor everyone and everything to <br>
prevent loonies from doing something nasty, but that will not always work <br>
and you will end up with "Who watches the watchmen?". Another possibility <br>
would be an automated "immune system" against nanotech, but even if it <br>
would work well, there are always other dangers. <br>
<p>
Most likely the only stable way of preventing widespread disaster is <br>
dispersion.<br>
<p>
<i>&gt; With a future where there's a risk of a techno elite to hold the power,</i><br>
<i>&gt; there's a big chance of unsatisfied masses instead of minorities, with an</i><br>
<i>&gt; abundance of unsatisfied "loosers" willing to press the button in the hope</i><br>
<i>&gt; of some kind of chance.</i><br>
<p>
True. Spreading education and optimism is a good idea for many reasons, <br>
not just this one. <br>
<p>
-----------------------------------------------------------------------<br>
Anders Sandberg                                      Towards Ascension!<br>
nv91-asa@nada.kth.se         <a href="http://www.nada.kth.se/~nv91-asa/main.html">http://www.nada.kth.se/~nv91-asa/main.html</a><br>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="1690.html">Omega: "Re: Von Neumann's Blunder"</a>
<li> <b>Previous message:</b> <a href="1688.html">Omega: "Re: Free Will vs Group Think"</a>
<li> <b>In reply to:</b> <a href="1623.html">Max M: "Re: Humanrintelligences' motivation (Was: Superintelligences' motivation)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
