<!-- received="Mon Feb 10 10:42:42 1997 MDT" -->
<!-- sent="Mon, 10 Feb 1997 09:05:45 -0800" -->
<!-- name="Hal Finney" -->
<!-- email="hal@rain.org" -->
<!-- subject="Re: The meaning of Life" -->
<!-- id="199702101705.JAA22719@crypt.hfinney.com" -->
<!-- inreplyto="The meaning of Life" -->
<title>extropians: Re: The meaning of Life</title>
<h1>Re: The meaning of Life</h1>
Hal Finney (<i>hal@rain.org</i>)<br>
<i>Mon, 10 Feb 1997 09:05:45 -0800</i>
<p>
<ul>
<li> <b>Messages sorted by:</b> <a href="date.html#2456">[ date ]</a><a href="index.html#2456">[ thread ]</a><a href="subject.html#2456">[ subject ]</a><a href="author.html#2456">[ author ]</a>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2457.html">Eric Watt Forste: "Re: Eurocracy (was Re: Immortality and Resources)"</a>
<li> <b>Previous message:</b> <a href="2455.html">Gray.D: "Re: Venus wonders what all the fuss is about"</a>
<li> <b>Maybe in reply to:</b> <a href="2464.html">John K Clark: "The meaning of Life"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2461.html">CurtAdams@aol.com: "Re: The meaning of Life"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
From: John K Clark &lt;johnkc@well.com&gt;<br>
<i>&gt; In general, anything that could make the slightest claim to be intelligent, </i><br>
<i>&gt; even a simple computer program, can not predict what it will do.</i><br>
<p>
There are some things about John's definition of free will which are unclear<br>
to me.  First, would simple computer programs really be said to have free<br>
will?<br>
<p>
I think John's original definition referred to "beings" with free<br>
will, and he explained that before asking whether a program had free<br>
will we should first ask if it is a being, which I gather would<br>
imply consciousness at a minimum.  Since simple computer programs<br>
aren't conscious, then this would mean that they don't have free will.<br>
(Perhaps John in the above meant a program which was relatively "simple"<br>
for a conscious computer program, something which would be very complex<br>
by our standards.)<br>
<p>
I also am not clear on how to define what it means to make a prediction.<br>
Suppose we ask a conscious program, "What will you do if X happens?"  The<br>
program runs a simulation of itself where X happens, and comes up with<br>
the result, which it reports (perhaps with a probabilistic component).<br>
<p>
Now we might say that this is not an accurate prediction, because if X<br>
then actually does happen, the program won't be in the state it thought<br>
it was going to be in when it made the prediction, simply by virtue of<br>
having made the prediction.  So when X then does happen, the program might<br>
do something else.  This would illustrate the program's inability to predict<br>
what it would do.<br>
<p>
But I don't think this is fair to the program.  It was asked to predict<br>
what it would do when X happened.  We should have asked it, "What will<br>
you do if you are asked to predict what you will do when X happens, and<br>
then X actually happens?"  In that case the computer could have accurately<br>
predicted what it would do in these circumstances.<br>
<p>
Now of course if X happens after the computer is asked _this_ question,<br>
again it might do something different from the prediction.  But once<br>
again the problem is not that the computer program is unpredictable<br>
(to itself or anyone else), but rather that the question being asked<br>
does not correspond to the circumstances.  We would have had to ask it,<br>
"What will you do if you are asked to predict what will happen under the<br>
circumstances where you predict what you will do when X happens, and then<br>
X happens, and then after making that prediction, X actually happens?"<br>
<p>
This is obviously an infinite regress.  But again I would not conclude<br>
from this that the computer is unable to predict its future actions, but<br>
rather that it is impossible to usefully word certain types of questions<br>
about future actions to a program with a memory of the questions it<br>
was asked.  If the wording is supposed to imply that the program had<br>
never heard the question, then the circumstances being asked about are<br>
never going to occur.<br>
<p>
You could ask it this: "What would you do if X happens after your memory<br>
has been reset to the state it was in before I asked this question?"<br>
In that case the program could accurately and reliably predict its<br>
behavior.  In principle, a person with access to an accurate neural-level<br>
description of his brain could do so as well (although obviously the<br>
memory resetting would be more difficult in that case).<br>
<p>
Would people's ability to answer such questions imply that they don't<br>
have free will?  Do we really want to rely on the fact that the memory of<br>
questions changes the state of the system to show that it has free will?<br>
To me this is a somewhat incidental characteristic of conscious (and many<br>
unconscious) systems and does not seem fundamental enough to distinguish<br>
which ones have free will.<br>
<p>
Hal<br>
<!-- body="end" -->
<hr>
<p>
<ul>
<!-- next="start" -->
<li> <b>Next message:</b> <a href="2457.html">Eric Watt Forste: "Re: Eurocracy (was Re: Immortality and Resources)"</a>
<li> <b>Previous message:</b> <a href="2455.html">Gray.D: "Re: Venus wonders what all the fuss is about"</a>
<li> <b>Maybe in reply to:</b> <a href="2464.html">John K Clark: "The meaning of Life"</a>
<!-- nextthread="start" -->
<li> <b>Next in thread:</b> <a href="2461.html">CurtAdams@aol.com: "Re: The meaning of Life"</a>
<!-- reply="end" -->
</ul>
