<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>extropians: Singularity and AIs (was: We're stuck with each other)</title>
<meta name="Author" content="Robert J. Bradbury (bradbury@aeiveos.com)">
<meta name="Subject" content="Singularity and AIs (was: We're stuck with each other)">
<meta name="Date" content="2002-01-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Singularity and AIs (was: We're stuck with each other)</h1>
<!-- received="Sat Jan 26 01:09:52 2002" -->
<!-- isoreceived="20020126080952" -->
<!-- sent="Sat, 26 Jan 2002 00:09:49 -0800 (PST)" -->
<!-- isosent="20020126080949" -->
<!-- name="Robert J. Bradbury" -->
<!-- email="bradbury@aeiveos.com" -->
<!-- subject="Singularity and AIs (was: We're stuck with each other)" -->
<!-- id="Pine.LNX.4.10.10201252257330.20660-100000@server.aeiveos.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3C521CAA.4040303@cox.rr.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Robert J. Bradbury (<a href="mailto:bradbury@aeiveos.com?Subject=Re:%20Singularity%20and%20AIs%20(was:%20We're%20stuck%20with%20each%20other)"><em>bradbury@aeiveos.com</em></a>)<br>
<strong>Date:</strong> Sat Jan 26 2002 - 01:09:49 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1885.html">Hubert Mania: "Re: Hawking's Doom and Gloom debunked..."</a>
<li><strong>Previous message:</strong> <a href="1883.html">Spike Jones: "Re: Singularity Fun Theory (was: Ethical basics)"</a>
<li><strong>In reply to:</strong> <a href="1876.html">Dan Clemmensen: "Re: We're stuck with each other"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1886.html">Eugene Leitl: "Re: Singularity and AIs (was: We're stuck with each other)"</a>
<li><strong>Reply:</strong> <a href="1886.html">Eugene Leitl: "Re: Singularity and AIs (was: We're stuck with each other)"</a>
<li><strong>Reply:</strong> <a href="1891.html">Samantha Atkins: "Re: Singularity and AIs (was: We're stuck with each other)"</a>
<li><strong>Reply:</strong> <a href="1897.html">Dan Clemmensen: "Re: Singularity and AIs (was: We're stuck with each other)"</a>
<li><strong>Maybe reply:</strong> <a href="1992.html">Emlyn O'regan: "RE: Singularity and AIs (was: We're stuck with each other)"</a>
<li><strong>Maybe reply:</strong> <a href="1993.html">Emlyn O'regan: "RE: Singularity and AIs (was: We're stuck with each other)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1884">[ date ]</a>
<a href="index.html#1884">[ thread ]</a>
<a href="subject.html#1884">[ subject ]</a>
<a href="author.html#1884">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Fri, 25 Jan 2002, Dan Clemmensen wrote:
<br>
<p><em>&gt; Yes, you should be very careful with that prediction. it was arbitrary:
</em><br>
<em>&gt; In 1996 I predicted that the singularity would happen within ten years.
</em><br>
<p>Arbitrary! -- Arbitrary!  Lord, I can't believe an esteemed extropian like
<br>
Dan Clemmensen (whose google quotient may exceed my own) made an arbitrary
<br>
prediction about when the singularity would occur.  It such a shame that
<br>
I'm halfway around the world from the &quot;wailing wall&quot;.
<br>
<p><em>&gt; However, if you accept the concept that the singularity is driven by
</em><br>
<em>&gt; exponential change, then you must realize that the actual data will not 
</em><br>
<em>&gt; become obvious to most people until the singularity is nearly upon us.
</em><br>
<p>I wonder whether that is really the case.  The curves for CPU speed,
<br>
data storage densities, and communications bandwidth are following
<br>
fairly predicatble, though different, paths.  I could cite DNA sequenced and
<br>
protein crystal structures in the Genbank and PDB databases as additional
<br>
examples.  If there are discontinuities, they appear to be in the earlier
<br>
parts of the curves rather than the later parts.  For example, there
<br>
is going to be a discontinuity in the &quot;proteomics&quot; information curve.
<br>
Up until 2000, most of the protein-protein interactions were determined
<br>
by relatively primitive lab experiments.  From 2000 - 2002, Cellzome
<br>
(www.cellzome.de) built an engine to apply robotic MassSpec analysis
<br>
to the problem.  Now they have 1/3 of the yeast proteome done.
<br>
Now that proteomics is on a curve, I don't really expect to see
<br>
any huge jumps in the development rate though.
<br>
<p>I'll note from my own experience that putting myself back in 1996, we
<br>
are slightly ahead of where I would have expected us to be from
<br>
a genomics/biological knowledge standpoint.
<br>
<p>So, the problem isn't that people aware of these concepts will
<br>
not see it coming, the problem is that &quot;most people&quot; don't
<br>
understand the concept of the singularity and don't know
<br>
what signs to look for.
<br>
<p><em>&gt; If you prefer the &quot;phase change&quot; model, then the situation is even 
</em><br>
<em>&gt; worse. In this model, we have built a substrate that provides the
</em><br>
<em>&gt; resources needed to create an SI, needing only some single breakthrough
</em><br>
<em>&gt; to bring it into existence. With this model, the SI can be brought into
</em><br>
<em>&gt; existence without warning by an individual or a small group.
</em><br>
<p>If its human-level you want, Blue Gene should be functional before 2006.
<br>
*But* even if you have access to such hardware, its a roomful of
<br>
hardware.  You have to make a strong case that (a) significant
<br>
acceleration is possible using only software modifications [Ray and Hans
<br>
seem to suggest we may be limited to 1-2 orders of magnitude with clever
<br>
algorithms]; or (b) there is a rapid advancement of matter as software.
<br>
If you don't get (b) the SI hits a ceiling that can't be breached
<br>
without humans supplying it with more advanced hardware.  That
<br>
presupposes that we would have the technology base at that time
<br>
to manufacture such hardware.
<br>
<p>So there could be a number of bumps in the road.
<br>
<p>The only smooth path I could envision for the singularity would
<br>
be an underground breakout that takes advantage of the WWW.
<br>
You would have to be able to co-opt a significant amount of
<br>
of underutilized resources to be able to manage an exponential
<br>
growth path.  Ultimately you still face the requirement for
<br>
matter compilers.  I don't think this will be feasible until
<br>
you have many high-bandwidth connections to a large fraction
<br>
of the installed computronium base -- the intelligence constraints
<br>
on low bandwidth connections between fractional human brain
<br>
equivalents seems to be a strong barrier.
<br>
<p>If humans realize what is going on and decide they don't want
<br>
it to happen you run the risk that they will disconnect their
<br>
computers from the net.  So you have the additional handicap
<br>
that it may only be able to sneak up on you if it operates
<br>
in a severely constrained stealth mode.
<br>
<p><em>&gt; As the substrate improves, The size of the needed breakthrough decreases.
</em><br>
<p>This certainly is true.  By 2010-2015, when human mind equivalent
<br>
computational capacity becomes available to small groups, the
<br>
probability of self-evolving AIs goes up significantly.
<br>
<p><em>&gt; I feel that the substrate is already very rich and is rapidly getting 
</em><br>
<em>&gt; richer, as measured in available computing capacity.
</em><br>
<p>*If* people allow the access.  The opportunities for a Trojan
<br>
Horse are there.  The possibility of someone sneaking self-evolving
<br>
AI code into SETI@Home, Folding@Home, etc. are something we should be
<br>
concerned with.  There may be an argument here for ExI to become
<br>
involved in engaging Distributed Computing developers in such
<br>
discussions.  While the Singularity Institute may have laudable
<br>
goals, Al Qaeda certainly does not.
<br>
<p>It gives you pause -- 1 billion muslims around the world.
<br>
10% of them devoting their computers to a DC project to
<br>
evolve not a &quot;Friendly AI&quot; but an AI dedicated to advancing
<br>
a radical muslim hegemony.
<br>
<p>That, IMO, is one of the problems with pushing AI technology.
<br>
Unlike the situation with most humans, there may be no &quot;built-in&quot;
<br>
human empathic perspectives in AIs.  I'm deeply suspicious
<br>
of any arguments that only beneficent AIs may produced.
<br>
If malevolent AIs are as easy as friendly AIs then we may
<br>
have some serious problems.
<br>
<p><em>&gt; Conclusion: 2006 is a guess, but it is not completely ridiculous.
</em><br>
<p>No, certainly not.  But I think getting a smooth fit to the
<br>
curve seems to require either a late start (lots of underutilized
<br>
resources available) or robust technologies that allow compiling
<br>
matter as software.
<br>
<p>Personally, I'd say 2016 is a better date than 2006.
<br>
<p>Robert
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1885.html">Hubert Mania: "Re: Hawking's Doom and Gloom debunked..."</a>
<li><strong>Previous message:</strong> <a href="1883.html">Spike Jones: "Re: Singularity Fun Theory (was: Ethical basics)"</a>
<li><strong>In reply to:</strong> <a href="1876.html">Dan Clemmensen: "Re: We're stuck with each other"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1886.html">Eugene Leitl: "Re: Singularity and AIs (was: We're stuck with each other)"</a>
<li><strong>Reply:</strong> <a href="1886.html">Eugene Leitl: "Re: Singularity and AIs (was: We're stuck with each other)"</a>
<li><strong>Reply:</strong> <a href="1891.html">Samantha Atkins: "Re: Singularity and AIs (was: We're stuck with each other)"</a>
<li><strong>Reply:</strong> <a href="1897.html">Dan Clemmensen: "Re: Singularity and AIs (was: We're stuck with each other)"</a>
<li><strong>Maybe reply:</strong> <a href="1992.html">Emlyn O'regan: "RE: Singularity and AIs (was: We're stuck with each other)"</a>
<li><strong>Maybe reply:</strong> <a href="1993.html">Emlyn O'regan: "RE: Singularity and AIs (was: We're stuck with each other)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1884">[ date ]</a>
<a href="index.html#1884">[ thread ]</a>
<a href="subject.html#1884">[ subject ]</a>
<a href="author.html#1884">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Fri Nov 01 2002 - 13:37:36 MST
</em></small></p>
</body>
</html>
