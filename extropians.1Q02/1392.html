<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>extropians: Re: Jaron Lanier Got Up My Shnoz on AI</title>
<meta name="Author" content="Steve Nichols (steve@multisell.com)">
<meta name="Subject" content="Re: Jaron Lanier Got Up My Shnoz on AI">
<meta name="Date" content="2002-01-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Jaron Lanier Got Up My Shnoz on AI</h1>
<!-- received="Sat Jan 19 16:51:52 2002" -->
<!-- isoreceived="20020119235152" -->
<!-- sent="Sat, 19 Jan 2002 23:53:51 -0000" -->
<!-- isosent="20020119235351" -->
<!-- name="Steve Nichols" -->
<!-- email="steve@multisell.com" -->
<!-- subject="Re: Jaron Lanier Got Up My Shnoz on AI" -->
<!-- id="002501c1a144$d5c83380$2ee2193e@leeds.ac.uk" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="Jaron Lanier Got Up My Shnoz on AI" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Steve Nichols (<a href="mailto:steve@multisell.com?Subject=Re:%20Jaron%20Lanier%20Got%20Up%20My%20Shnoz%20on%20AI"><em>steve@multisell.com</em></a>)<br>
<strong>Date:</strong> Sat Jan 19 2002 - 16:53:51 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1393.html">jeff davis: "Zingers re Spike's physique"</a>
<li><strong>Previous message:</strong> <a href="1391.html">Samantha Atkins: "Re: Skeptics and extropians"</a>
<li><strong>Maybe in reply to:</strong> <a href="0871.html">Colin Hales: "Jaron Lanier Got Up My Shnoz on AI"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1392">[ date ]</a>
<a href="index.html#1392">[ thread ]</a>
<a href="subject.html#1392">[ subject ]</a>
<a href="author.html#1392">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Date: Fri, 18 Jan 2002 08:58:44 -0800
<br>
From: James Rogers &lt;<a href="mailto:jamesr@best.com?Subject=Re:%20Jaron%20Lanier%20Got%20Up%20My%20Shnoz%20on%20AI">jamesr@best.com</a>&gt;
<br>
Subject: Re: Jaron Lanier Got Up My Shnoz on AI
<br>
<p>On 1/17/02 8:05 AM, &quot;Steve Nichols&quot; &lt;<a href="mailto:steve@multisell.com?Subject=Re:%20Jaron%20Lanier%20Got%20Up%20My%20Shnoz%20on%20AI">steve@multisell.com</a>&gt; wrote:
<br>
<em>&gt; But the brain processes are nothing like von Neumann/ finite state computers!
</em><br>
<em>&gt; For a start the synapses can rewire (in hardware, infinite-state!) in response
</em><br>
<em>&gt; to situations &amp; problems. Do you deny this? And what structures in the brain
</em><br>
<em>&gt; resemble anything like CPU or RAM chips? Not to be found, because brains and
</em><br>
<em>&gt; brain processes are nothing like ordinary computers (doh).
</em><br>
<p><p>They human brain doesn't have to operate like modern silicon, only be
<br>
mappable to it.  There are some basic mathematical criteria for what can and
<br>
can't be mapped to silicon, and the brain definitely looks like it can be
<br>
mapped.  It is utterly irrelevant if it uses very different hardware because
<br>
the results are all that matter.
<br>
<p>*new* That is like saying that a drawing of the pyramids is the same
<br>
thing as the actual pyramids! Remember, this discussion ios about sentience,
<br>
not computation. It seems fairly obvious that glial cells &amp;c in brains
<br>
play some role in felt experience, but these have no discernible
<br>
role in computation &amp; information processing.
<br>
<p><em>&gt;&gt; Input and output results are all you need to duplicate ANY finite state
</em><br>
<em>&gt;&gt; machinery.  Universal FSM copying algorithms create structures that look very
</em><br>
<em>&gt;&gt; similar to neural networks, not like the more conventional algorithms that
</em><br>
<em>&gt;&gt; most people who work with software are familiar with.  With &quot;weight states&quot;
</em><br>
<em>&gt;&gt; and everything.
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt; What about hidden units????????
</em><br>
<p><p>Hidden units are again irrelevant.  If they don't modify the input at the
<br>
output, then they are computationally null.  If they ARE modifying the
<br>
output, then they will be modelled automatically just like everything else.
<br>
What is the value of a hidden unit if it has no measurable impact on the
<br>
process?
<br>
<p>*new* The hidden units are a good example of how serial emulation of
<br>
massively parallel processing FAILS to be able to replicate all the
<br>
processes. Algorithmically you can just look at outputs, inputs and
<br>
architecture, but cannot actually duplicate the processing. It is wrong
<br>
to say hidden units have no measurable impact, because if absent the
<br>
neural net will not function (the same/ as well).
<br>
<p><p><em>&gt; All serial computers can do is model or simulate mpd function ... the
</em><br>
<em>&gt; lock-step mechanism cannot be replicated exactly in serial because in parallel
</em><br>
<em>&gt; all nodes are updated AT ONCE, this is why it is called parallel, whereas
</em><br>
<em>&gt; lock-step simulation in serial just does one process at a time, linearly. And,
</em><br>
<em>&gt; again, what about hidden units?????
</em><br>
<em>&gt; 
</em><br>
<p>There are an infinite number of algorithms that can implement any process.
<br>
Inputs mapping identically to outputs is the definition of an equivalent
<br>
algorithmic form.  Hardware and software architecture are irrelevant.
<br>
<p>*New* This is your big problem. I might agree with you if we are
<br>
talking about &quot;computation&quot; but we are discussing potential for
<br>
sentience, so you are completely off the mark! The whole point I am
<br>
making is that a parallel/ wetware system can compute more flexibly,
<br>
and switch between what information is handled as neural information
<br>
(software if you must) ... virtual  ... and what problems are tackled by 
<br>
more fundamental resetting of logic, more permanent changes to
<br>
system, by reorganising hardware/ physical connections.
<br>
<p>Serial computers just cannot do this, they just run algorithms
<br>
keeping their hardware architecture stable and distinct. Sure, both
<br>
systems can solve the same problems, but finite-state, digital
<br>
systems don't have any 'awareness' or self-referentialness. Self-
<br>
referential systems can modify THEMSELVES .... ** end New**
<br>
<p>Incidentally, modelling a parallel process done serially checkpoints at the
<br>
same exact state as the truly parallel model.  The only difference between
<br>
parallel and serial in this case is that the state transitional steps are
<br>
computed differently.  Therefore, the results are computationally
<br>
indistinguishable.  
<br>
<p>*New* So what ... computational equivalence isn't the issue.
<br>
You seem to be saying that a robot-actor in a play, whose
<br>
behaviour is entirely conditioned as responses to programming,
<br>
is in fact identical to the human-actor in the same role, because they 
<br>
both deliver the same roles! Is this really what you think, are you
<br>
a behaviourist ???????? *end new*
<br>
<p>It may go against whatever intuition you have, but if
<br>
you actually study the problem you'll find that parallel and serial
<br>
computation are computationally equivalent and interchangeable.
<br>
<p>*New* So you only arguing the limited case for COMPUTATIONAL
<br>
equivalence (which I dispute as well, neural nets can interpolate,
<br>
learn and extrapolate from new information not met previously).
<br>
You make no case for &quot;experiential&quot; equivalence, which is what 
<br>
we are talking about ... so my point is proved. *end New*
<br>
<p><p><em>&gt; Not only just very different, but nothing like! You can't go the other way and
</em><br>
<em>&gt; extract the &quot;algorithm&quot; being used by brains because the brain changes its
</em><br>
<em>&gt; wiring, not just weight states. Also, how can u reproduce graceful degradation
</em><br>
<em>&gt; in serial? If the bit stream is disrupted u r f**c**d!
</em><br>
<p><p>Again, the computational process is irrelevant.  Equality of outcome
<br>
demonstrates equivalence.
<br>
<p>* New* Obviously you don't understand graceful degradation then, because
<br>
it has nothing to do with computation, you fool! It has to do with resilience
<br>
of hardware functioning, parallel systems can re-route to avoid damaged
<br>
pathways, yet still retain optimum computation despite damage. If a
<br>
serial CPU is damaged it stops working or goes bonkers ...... *end new*
<br>
<p><p><em>&gt; All you can get from improved serial is more speed ... but speed of
</em><br>
<em>&gt; calculation has absolutely zilch to do with sentience/ felt experience. My old
</em><br>
<em>&gt; z80 is slower by a magnitude than a P4, but is not a jot more or less
</em><br>
<em>&gt; sentient!
</em><br>
&nbsp;
<br>
<p>While I'm not necessarily disagreeing with you, can you prove that your P4
<br>
isn't more sentient than your Z80?  What is your criteria?
<br>
<p>* The onus is on you to demonstrate that either has the slightest glimmer
<br>
of consciousness. I argue that neither have any, and won't in the future,
<br>
however fast you clock them. *end new*
<br>
<p>Silicon is just the computational machinery; the software is way more
<br>
important.  The human brain is what would be considered software implemented
<br>
in hardware.  It doesn't have to be that way but nature apparently has
<br>
difficulty evolving proper von neumann machines.
<br>
<p>*New* von Neumann machines are not evolvable systems. Software
<br>
requires outside helpers, programmers &amp;c, MPS can LEARN. 
<br>
<p>Hope this makes it clearer for you James. 
<br>
Best
<br>
www.steve-nichols.com  *end new*
<br>
<p><p><em>      &gt;All you can get from improved serial is more speed=20
</em><br>
<p>&nbsp;&nbsp;Besides that Mrs. Lincoln how did you like the play?
<br>
<p><em>      &gt;My old z80 is slower by a magnitude than a P4, but is not a jot =
</em><br>
more
<br>
<em>      &gt;or less sentient!
</em><br>
<p>&nbsp;&nbsp;You seem very sure, how did you find out?
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;John K Clark       <a href="mailto:jonkc@att.net?Subject=Re:%20Jaron%20Lanier%20Got%20Up%20My%20Shnoz%20on%20AI">jonkc@att.net</a>
<br>
<p>* new*  Given long enough the z80 could probably do anything the faster
<br>
P4 does ...  both are baby Turing engines aren't they? And if you are
<br>
claiming that toasters, calculators, ordinary PC's and so on have
<br>
self-referential mentation in the very same way that you do, then you 
<br>
must be a very limited sort of being! I don't consider that they do, but
<br>
accept that my cat IS sentient .... I have no parts of a brain she does
<br>
not have (just large prefrontal cortex) ... the cat has REM, so probably
<br>
has dreams ... this counts for me as sentience. P4 is just a lump
<br>
of sand &amp; metal with no more feeling than sand on the beach.
<br>
<p>Best,
<br>
www.steve-nichols.com
<br>
Adeptus
<br>
*********
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1393.html">jeff davis: "Zingers re Spike's physique"</a>
<li><strong>Previous message:</strong> <a href="1391.html">Samantha Atkins: "Re: Skeptics and extropians"</a>
<li><strong>Maybe in reply to:</strong> <a href="0871.html">Colin Hales: "Jaron Lanier Got Up My Shnoz on AI"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1392">[ date ]</a>
<a href="index.html#1392">[ thread ]</a>
<a href="subject.html#1392">[ subject ]</a>
<a href="author.html#1392">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Fri Nov 01 2002 - 13:37:35 MST
</em></small></p>
</body>
</html>
