<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>extropians: Re: Understanding CFAI</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Understanding CFAI">
<meta name="Date" content="2002-02-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Understanding CFAI</h1>
<!-- received="Thu Feb  7 02:22:47 2002" -->
<!-- isoreceived="20020207092247" -->
<!-- sent="Thu, 07 Feb 2002 04:22:44 -0500" -->
<!-- isosent="20020207092244" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Understanding CFAI" -->
<!-- id="3C624764.3AA9D834@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="EB5DDEEFC7B4D411AD3B00508BDFF3E20231919C@1upmc-msx7.isdip.upmc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Understanding%20CFAI"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Feb 07 2002 - 02:22:44 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2528.html">Samantha Atkins: "Re: Ashcroft Antics"</a>
<li><strong>Previous message:</strong> <a href="2526.html">Cory Przybyla: "Re: Ashcroft Antics"</a>
<li><strong>In reply to:</strong> <a href="2517.html">Smigrodzki, Rafal: "Understanding CFAI"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2527">[ date ]</a>
<a href="index.html#2527">[ thread ]</a>
<a href="subject.html#2527">[ subject ]</a>
<a href="author.html#2527">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&quot;Smigrodzki, Rafal&quot; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I just finished reading CFAI and GISAI and here are some questions and
</em><br>
<em>&gt; comments:
</em><br>
<p>I nominate Rafal's post for &quot;Post of the Month&quot;.
<br>
<p><em>&gt; &gt; Of course, SIAI &lt;../GISAI/meta/glossary.html&gt; knows of only one current
</em><br>
<em>&gt; &gt; project advanced enough to even begin implementing the first baby steps
</em><br>
<em>&gt; &gt; toward Friendliness - but where there is one today, there may be a dozen
</em><br>
<em>&gt; &gt; tomorrow.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### Which other AI projects are on the right track?
</em><br>
<p>I didn't say that I *ever* knew of any AI project on the &quot;right track&quot;. 
<br>
My phrasing was &quot;advanced enough to even begin&quot;.  And now that Webmind has
<br>
gone down, there aren't any AI projects left even in that category - that
<br>
I know about, anyway.  Peter Voss has earned my respect, but he hasn't
<br>
said enough (that I know of) about the proposed architecture of his AI
<br>
project for me to judge whether it would be capable of representing a
<br>
Friendly goal system.
<br>
<p><em>&gt; &gt; That is, conscious reasoning can replace the &quot;damage signal&quot; aspect of
</em><br>
<em>&gt; &gt; pain. If the AI successfully solves a problem, the AI can choose to
</em><br>
<em>&gt; &gt; increase the priority or devote additional computational power to
</em><br>
<em>&gt; &gt; whichever subheuristics or internal cognitive events were most useful in
</em><br>
<em>&gt; &gt; solving the problem, replacing the positive-feedback aspect of pleasure.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### Do you think that the feedback loops focusing the AI on a particular
</em><br>
<em>&gt; problem might (in any sufficiently highly organized AI) give rise to
</em><br>
<em>&gt; qualia analogous to our feelings of &quot;intellectual unease&quot;, and &quot;rapture
</em><br>
<em>&gt; of an insight&quot;?
</em><br>
<p>I do not pretend to understand qualia.  But what focuses the AI on a
<br>
particular problem is not a low-level feedback loop, but a deliberately
<br>
implemented feedback loop.  The AI controls the feedback.  The feedback
<br>
doesn't control the AI.  Unless an FAI deems it necessary to shift to the
<br>
human pleasure-pain architecture to stay Friendly, I can't see vis mental
<br>
state ever becoming that closely analogous to human emotions.
<br>
<p><em>&gt; &gt; The absoluteness of &quot;The end does not justify the means&quot; is the result
</em><br>
<em>&gt; &gt; of the Bayesian Probability Theorem &lt;../../GISAI/meta/glossary.html&gt;
</em><br>
<em>&gt; &gt; applied to internal cognitive events. Given the cognitive event of a
</em><br>
<em>&gt; &gt; human thinking that the end justifies the means, what is the probability
</em><br>
<em>&gt; &gt; that the end actually does justify the means? Far, far less than 100%,
</em><br>
<em>&gt; &gt; historically speaking. Even the cognitive event &quot;I'm a special case for
</em><br>
<em>&gt; &gt; [reason X] and am therefore capable of safely reasoning that the end
</em><br>
<em>&gt; &gt; justifies the means&quot; is, historically speaking, often dissociated with
</em><br>
<em>&gt; &gt; external reality. The rate of hits and misses is not due to the
</em><br>
<em>&gt; &gt; operation of ordinary rationality, but to an evolutionary bias towards
</em><br>
<em>&gt; &gt; self-overestimation. There's no Bayesian binding
</em><br>
<em>&gt; &gt; &lt;../../GISAI/meta/glossary.html&gt; between our subjective experience of
</em><br>
<em>&gt; &gt; feeling justified and the external event of actually being justified, so
</em><br>
<em>&gt; &gt; our subjective experience cannot license actions that would be dependent
</em><br>
<em>&gt; &gt; on being actually justified.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### I think I can dimly perceive a certain meaning in the above
</em><br>
<em>&gt; paragraph, with which I could agree (especially in hindsight after
</em><br>
<em>&gt; reading 3.4.3: Causal validity semantics &lt;design/structure/causal.html&gt;
</em><br>
<em>&gt; ). Yet, without recourse to the grand finale, this paragraph is very
</em><br>
<em>&gt; cryptic and in some interpretations for me unacceptable.
</em><br>
<p>My apologies.  Just because CFAI is 910K long doesn't mean that it wasn't
<br>
written in a tearing hurry.
<br>
<p>For the record, what I was trying to talk about was the internal use of
<br>
the Bayesian theorem for meta-rationality - deciding to what extent the
<br>
thought &quot;X&quot; implies the real-world state X.  Any imperfect organism can
<br>
use meta-rationality to correct for internal errors - not just evolved
<br>
biases, but also things like insufficient cognitive resources.
<br>
<p>However, Bayesian reflectivity is most important for (a) correcting human
<br>
biases, (b) understanding the ethical heuristics that humans use to
<br>
correct those biases that they view as invalid, (c) understanding the
<br>
social heuristics that humans use to spot uncorrected biases in others,
<br>
and (d) making principled statements about whether such social heuristics
<br>
should be generalized to AIs.
<br>
<p><em>&gt; &gt; I think that undirected evolution is unsafe, and I can't think of any
</em><br>
<em>&gt; &gt; way to make it acceptably safe. Directed evolution might be made to
</em><br>
<em>&gt; &gt; work, but it will still be substantially less safe than
</em><br>
<em>&gt; &gt; self-modification. Directed evolution will also be extremely unsafe
</em><br>
<em>&gt; &gt; unless pursued with Friendliness in mind and with a full understanding
</em><br>
<em>&gt; &gt; of non-anthropomorphic minds. Another academically popular theory is
</em><br>
<em>&gt; &gt; that all people are blank slates, or that all altruism is a child goal
</em><br>
<em>&gt; &gt; of selfishness - evolutionary psychologists know better, but some of the
</em><br>
<em>&gt; &gt; social sciences have managed to totally insulate themselves from the
</em><br>
<em>&gt; &gt; rest of cognitive science, and there are still AI people who are getting
</em><br>
<em>&gt; &gt; their psychology from the social sciences.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### Is altruism something other than a child goal of selfishness?
</em><br>
<p>Within a given human, altruism is an adaptation, not a subgoal.  This is
<br>
in the strict sense used in CFAI, i.e. Tooby and Cosmides's &quot;Individual
<br>
organisms are best thought of as adaptation-executers rather than as
<br>
fitness-maximizers.&quot;
<br>
<p><em>&gt; It was
</em><br>
<em>&gt; my impression that evolutionary psychology predicts the emergence of
</em><br>
<em>&gt; altruism as a result of natural selection.
</em><br>
<p>Yes.  (Strictly speaking EP explains known altruism in terms of natural
<br>
selection, and makes predictions about further details, but &quot;altruism&quot; was
<br>
a known phenomenon before EP or even Darwin, and shouldn't count as a
<br>
successful prediction.)
<br>
<p><em>&gt; Since natural selection is
</em><br>
<em>&gt; not goal-oriented, the goals defining the base of the goal system (at
</em><br>
<em>&gt; least if you use the derivative validity rule), are the goals present
</em><br>
<em>&gt; in the subjects of natural selection - selfish organisms, which in the
</em><br>
<em>&gt; service of their own survival have to develop (secondarily) altruistic
</em><br>
<em>&gt; impulses. As the derivative validity rule is itself the outcome of
</em><br>
<em>&gt; ethical reasoning , one could claim that it cannot invalidate the goals
</em><br>
<em>&gt; from which it is derived, thus sparing us a total meltdown of the goal
</em><br>
<em>&gt; system and shutdown.
</em><br>
<p>If the rule of derivative validity can't be tail-ended through an
<br>
objective morality, then the point at which we will be forced to make a
<br>
controlled exception (for ourselves, and for our AIs) will lie somewhere
<br>
around the evolution of altruism.
<br>
<p>The derivative validity instinct becomes complex when partially thwarted. 
<br>
Where we choose to place our reluctant exceptions will then likely be
<br>
based on the degree to which evolution is seen as playing puppeteer or
<br>
conduit.  For example, in the case of when a human feels that he or she is
<br>
&quot;entitled&quot; to something, evolution is clearly playing puppet master.  In
<br>
the case of mathematics, evolution is clearly playing unbiased conduit. 
<br>
The question then becomes whether the rule of derivative validity, or the
<br>
semantics of objectivity, are more like self-righteousness or
<br>
mathematics.  I would say they are more like mathematics.
<br>
<p><em>&gt; &gt; The semantics of objectivity are also ubiquitous because they fit very
</em><br>
<em>&gt; &gt; well into the way our brain processes statements; statements about
</em><br>
<em>&gt; &gt; morality (containing the word &quot;should&quot;) are not evaluated by some
</em><br>
<em>&gt; &gt; separate, isolated subsystem, but by the same stream of consciousness
</em><br>
<em>&gt; &gt; that does everything else in the mind. Thus, for example, we cognitively
</em><br>
<em>&gt; &gt; expect the same kind of coherence and sensibility from morality as we
</em><br>
<em>&gt; &gt; expect from any other fact in our Universe
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### It is likely that there are specialized cortical areas, mainly in
</em><br>
<em>&gt; the frontopolar and ventromedial frontal cortices, involved in the
</em><br>
<em>&gt; processing of ethics-related information.
</em><br>
<p>I agree.  But emotions modulate the stream of consciousness; they don't
<br>
reimplement the stream of consciousness under different rules.  There is
<br>
not one hippocampus used by moral thinking and a separate hippocampus used
<br>
by motorcycle maintenance.  To have two completely different systems, you
<br>
would need two completely different brains.
<br>
<p><em>&gt; Many of us are perfectly
</em><br>
<em>&gt; capable of double- and triple-thinking about ethical issues, as your
</em><br>
<em>&gt; examples of self-deception testify, while similar feats of mental
</em><br>
<em>&gt; juggling are not possible in the arena of mathematics or motorcycle
</em><br>
<em>&gt; maintenance.
</em><br>
<p>Possible, but not likely - possible if mathematics or motorcycle
<br>
maintenance becomes a political issue.  Our emotions are capable of
<br>
influencing any mental &quot;cause&quot; that has a moral &quot;effect&quot;.  To the extent
<br>
that motorcycle maintenance is rarely biased, it is because motorcycle
<br>
maintenance rarely becomes a moral/social/political issue.  Let two people
<br>
get in an argument about motorcycle maintenance and each may begin to
<br>
tolerate flawed arguments that happen to argue in favor of their own
<br>
positions, or reject correct arguments that argue against their positions,
<br>
where in private they would have evenly weighed both sides of the issue.
<br>
<p><em>&gt; &gt; Actually, rationalization does not totally disjoint morality and
</em><br>
<em>&gt; &gt; actions; it simply gives evolution a greater degree of freedom by
</em><br>
<em>&gt; &gt; loosely decoupling the two. Every now and then, the gene pool or the
</em><br>
<em>&gt; &gt; memetic environment spits out a genuine altruist; who, from evolution's
</em><br>
<em>&gt; &gt; perspective, may turn out to be a lost cause. The really interesting
</em><br>
<em>&gt; &gt; point is that evolution is free to load us with beliefs and adaptations
</em><br>
<em>&gt; &gt; which, if executed in the absence of rationalization, would turn us into
</em><br>
<em>&gt; &gt; total altruists ninety-nine point nine percent of the time. Thus, even
</em><br>
<em>&gt; &gt; though our &quot;carnal&quot; desires are almost entirely observer-centered, and
</em><br>
<em>&gt; &gt; our social desires are about evenly split between the personal and the
</em><br>
<em>&gt; &gt; altruistic, the adaptations that control our moral justifications have
</em><br>
<em>&gt; &gt; strong biases toward moral symmetry, fairness, truth, altruism, working
</em><br>
<em>&gt; &gt; for the public benefit, and so on.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### In my very personal outlook, the &quot;moral justifications&quot; are the
</em><br>
<em>&gt; results of advanced information processing applied in the service of
</em><br>
<em>&gt; &quot;carnal&quot; desires, supplemented  by innate, evolved biases.
</em><br>
<p>By computation in the service of &quot;carnal&quot; desires, do you mean computation
<br>
in the service of evolution's goals, or computation that has been skewed
<br>
by rationalization effects toward outcomes that the thinker finds
<br>
attractive?  In either case the effective parent goals are not limited to
<br>
&quot;carnal&quot; desires.
<br>
<p><em>&gt; The initial
</em><br>
<em>&gt; supergoals are analyzed, their implications for action under various
</em><br>
<em>&gt; conditions are explored, and the usual normative human comes to
</em><br>
<em>&gt; recognize the superior effectiveness of fairness, truth, etc., for
</em><br>
<em>&gt; survival in a social situation.
</em><br>
<p>I think this is a common misconception from the &quot;Age of Game Theory&quot; in
<br>
EP.  (By the &quot;Age of Game Theory&quot; I mean the age when a game-theoretical
<br>
explanation was thought to be the final step of an analysis; we still use
<br>
game theory today, of course.)  Only a modern-day human, armed with
<br>
declarative knowledge about Axelrod and Hamilton's results for the
<br>
iterated Prisoner's Dilemna, would employ altruism as a strict subgoal. 
<br>
And even then the results would be suboptimal because people instinctively
<br>
mistrust agents who employ altruism as a subgoal rather than &quot;for its own
<br>
sake&quot;... but that's a separate issue.  A human in an ancestral environment
<br>
may come to see virtue rewarded and wickedness punished, or more likely,
<br>
witness the selective reporting of virtuous rewards and wicked follies. 
<br>
However, this memetic effect only reinforces an innate altruism instinct. 
<br>
It does not construct a cultural altruism strategy from scratch.
<br>
<p><em>&gt; As a result the initial supergoals are
</em><br>
<em>&gt; overwritten by new content (at least to some degree, dictated by the
</em><br>
<em>&gt; ability to deceive others). As much as the imprint of my 4-year old self
</em><br>
<em>&gt; in my present mind might object, I am forced to accept the higher
</em><br>
<em>&gt; Kohlberg stage rules. Do you think that the Friendly AI will have some
</em><br>
<em>&gt; analogue of such (higher) levels? Can you hypothesize about the
</em><br>
<em>&gt; supergoal content of such level? Could it be translated back for
</em><br>
<em>&gt; unenhanced humans, or would it be only accessible to highly improved
</em><br>
<em>&gt; uploads?
</em><br>
<p>I'm not sure I believe in Kohlberg, but that aside:  From the perspective
<br>
of a human, an FAI would most closely resemble Kohlberg 6, and indeed
<br>
could not be anything but Kohlberg 6, because an FAI cannot be influenced
<br>
by threat of punishment, threat of disapproval, someone else's opinion, or
<br>
society's opinion, except insofar as the FAI decides that these events
<br>
represent valid signals about vis target goal content.
<br>
<p><em>&gt; &gt; We want a Meaning of Life that can be explained to a rock, in the same
</em><br>
<em>&gt; &gt; way that the First Cause (whatever it is) can be explained to
</em><br>
<em>&gt; &gt; Nothingness. We want what I call an &quot;objective morality&quot; - a set of
</em><br>
<em>&gt; &gt; moral propositions, or propositions about differential desirabilities,
</em><br>
<em>&gt; &gt; that have the status of provably factual statements, without derivation
</em><br>
<em>&gt; &gt; from any previously accepted moral propositions. We want a tail-end
</em><br>
<em>&gt; &gt; recursion to the rule of derivative validity. Without that, then yes -
</em><br>
<em>&gt; &gt; in the ultimate sense described above, Friendliness is unstable
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### I do agree with the last sentence. A human's self-Friendliness is
</em><br>
<em>&gt; inherently unstable, too.
</em><br>
<p>Yes, that's the point.  The design requirement is that Friendliness should
<br>
be at least as stable as the moral structure and altruistic content of any
<br>
human.  If the human structure disapproves of itself, so will
<br>
Friendliness.  If the human structure reluctantly decides to keep the
<br>
human structure for lack of a better alternative, so should Friendliness.
<br>
<p><em>&gt; &gt; Even if an extraneous cause affects a deep shaper, even deep shapers
</em><br>
<em>&gt; &gt; don't justify themselves; rather than individual principles justifying
</em><br>
<em>&gt; &gt; themselves - as would be the case with a generic goal system protecting
</em><br>
<em>&gt; &gt; absolute supergoals - there's a set of mutually reinforcing deep
</em><br>
<em>&gt; &gt; principles that resemble cognitive principles more than moral
</em><br>
<em>&gt; &gt; statements, and that are stable under renormalization. Why &quot;resemble
</em><br>
<em>&gt; &gt; cognitive principles more than moral statements&quot;? Because the system
</em><br>
<em>&gt; &gt; would distrust a surface-level moral statement capable of justifying
</em><br>
<em>&gt; &gt; itself!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### Can you give examples of such deep moral principles?
</em><br>
<p>Heh.  Such principles in CFAI are more described than seen, because they
<br>
mostly fall under the catgory of &quot;Friendship content&quot; rather than
<br>
&quot;Friendship structure&quot;.  The deep principles can be discovered by
<br>
examination of a human, as long as you start out with the Friendship
<br>
structure needed to deduce moral shapers from moral statements.
<br>
<p>Two deep principles that do make a cameo in CFAI are the rule of
<br>
derivative validity and the semantics of objectivity, both of which, when
<br>
applied to themselves, unflinchingly judge themselves to be flawed.  The
<br>
semantics of objectivity are derived from social selection pressures as
<br>
well as objective truths; the rule of derivative validity is ultimately
<br>
caused by evolution, even though it is more in the nature of mathematics
<br>
than self-righteousness.  Both are stable under renormalization as long as
<br>
they are the optimum *achievable*, even though they may not be the optimum
<br>
*imaginable*.
<br>
<p>But at any rate they are not self-protecting; there is no reason why they
<br>
would be.
<br>
<p><em>&gt; &gt; Humanity is diverse, and there's still some variance even in the
</em><br>
<em>&gt; &gt; panhuman layer, but it's still possible to conceive of description for
</em><br>
<em>&gt; &gt; humanity and not just any one individual human, by superposing the sum
</em><br>
<em>&gt; &gt; of all the variances in the panhuman layer into one description of
</em><br>
<em>&gt; &gt; humanity. Suppose, for example, that any given human has a preference
</em><br>
<em>&gt; &gt; for X; this preference can be thought of as a cloud in configuration
</em><br>
<em>&gt; &gt; space. Certain events very strongly satisfy the metric for X; others
</em><br>
<em>&gt; &gt; satisfy it more weakly; other events satisfy it not at all. Thus,
</em><br>
<em>&gt; &gt; there's a cloud in configuration space, with a clearly defined center.
</em><br>
<em>&gt; &gt; If you take something in the panhuman layer (not the personal layer) and
</em><br>
<em>&gt; &gt; superimpose the clouds of all humanity, you should end up with a
</em><br>
<em>&gt; &gt; slightly larger cloud that still has a clearly defined center. Any point
</em><br>
<em>&gt; &gt; that is squarely in the center of the cloud is &quot;grounded in the panhuman
</em><br>
<em>&gt; &gt; layer of humanity&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### What if the shape of superposition turns out to be more complicated,
</em><br>
<em>&gt; with the center of mass falling outside the maximum values of the
</em><br>
<em>&gt; superposition? In that case implementing a Friendliness focused on this
</em><br>
<em>&gt; center would have outcomes distasteful to all humans, and finding
</em><br>
<em>&gt; alternative criteria for Friendliness would be highly nontrivial.
</em><br>
<p>Well, what would *you* do in a situation like that?
<br>
What would you want a Friendly AI to do?
<br>
<p>It seems to me that problems like these are also subject to
<br>
renormalization.  You would use the other principles to decide what to do
<br>
about the local problem with panhuman grounding.
<br>
<p>If that's not the answer you had in mind, could you please give a more
<br>
specific example of a problem?  It's hard to answer questions when things
<br>
get this abstract.
<br>
<p><em>&gt; ---
</em><br>
<em>&gt; ### And a few more comments:
</em><br>
<em>&gt; I wonder if you read Lem's &quot;Golem XIV&quot;?
</em><br>
<em>&gt; Oops, Google says you did read it. Of course.
</em><br>
<p>I've read some Lem, but not that one.
<br>
<p><em>&gt; In a post on Exilist you say that uploading is a post-Singularity
</em><br>
<em>&gt; technology.
</em><br>
<p>Yes, and I still hold to this.
<br>
<p><em>&gt; While I intuitively feel that true AI will be built well
</em><br>
<em>&gt; before the computing power becomes available for an upload, I would
</em><br>
<em>&gt; imagine it should be possible to do uploading without AI. After all, you
</em><br>
<em>&gt; need just some improved scanning methods, and with laser tissue
</em><br>
<em>&gt; machining, quantum dot antibody labeling and high-res confocal
</em><br>
<em>&gt; microscopy, as well as the proteome project, this might be realistic in
</em><br>
<em>&gt; as little as 10 years (a guess). With a huge computer but no AI the
</em><br>
<em>&gt; scanned data would give you a human mind in a box, amenable to some
</em><br>
<em>&gt; enhancement.
</em><br>
<p>The statement about post-Singularity technology reflects relative rates of
<br>
development, not an absolute technological impossiblity.  In other words,
<br>
you might be able to carry out uploading in 30 years (10 is a bit much),
<br>
but the precursor technologies are such as to permit the construction of
<br>
an AI, using the knowledge of cognitive science gained from snooping on
<br>
human neural processes.  For that matter, uploading precursor technologies
<br>
would allow you to construct a 64-human grid with broadband
<br>
brain-to-computer-to-brain links.  Uploading does not require SI; it
<br>
requires knowledge and technology that could also be used to follow much
<br>
shorter paths to strong transhumanity.
<br>
<p><em>&gt; What do you think about using interactions between a nascent AI and the
</em><br>
<em>&gt; upload(s), with reciprocal rounds of enhancement and ethical system
</em><br>
<em>&gt; transfer, to develop Friendliness?
</em><br>
<p>Using a system with a human component should only be necessary if
<br>
Friendliness turns out to be substantially more &quot;human&quot; (nonportable) than
<br>
I currently expect.  I would consider it as, if not a &quot;last&quot; resort, then
<br>
at least not the default resort.  Hopefully it will be pretty clear that
<br>
the AI has &quot;gotten&quot; Friendliness and is now inventing vis own, clearly
<br>
excellent ideas about Friendliness, so that the thought of the AI needing
<br>
a human to supply some extra &quot;kick&quot; seems unlikely, and in any case we
<br>
would trust the AI to notice on vis own if a situation like that
<br>
developed.
<br>
<p>I suppose that during the Singularity someone could volunteer to be
<br>
uploaded, copied, and have the copy/original go along with the AI on vis
<br>
path to superintelligence, as an advisor, as long as it's clear that this
<br>
presents *no* additional threat of takeover by a transcending upload. 
<br>
Necessary conditions for &quot;no additional threat&quot; are that the AI-born SI
<br>
remains smarter, remains in control of the computing system, and does not
<br>
accept the upload-born SI's advice uncritically.
<br>
<p><em>&gt; And, by the way, I do think that CFAI and GISAI are wonderful
</em><br>
<em>&gt; intellectual achievements.
</em><br>
<p>Thank you kindly, good sir.  No doubt your message will someday appear
<br>
under the dictionary entry for &quot;constructive criticism&quot;.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2528.html">Samantha Atkins: "Re: Ashcroft Antics"</a>
<li><strong>Previous message:</strong> <a href="2526.html">Cory Przybyla: "Re: Ashcroft Antics"</a>
<li><strong>In reply to:</strong> <a href="2517.html">Smigrodzki, Rafal: "Understanding CFAI"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2527">[ date ]</a>
<a href="index.html#2527">[ thread ]</a>
<a href="subject.html#2527">[ subject ]</a>
<a href="author.html#2527">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Fri Nov 01 2002 - 13:37:38 MST
</em></small></p>
</body>
</html>
