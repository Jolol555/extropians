<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>extropians: Re: AI:This is how we do it</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: AI:This is how we do it">
<meta name="Date" content="2002-02-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI:This is how we do it</h1>
<!-- received="Tue Feb 19 05:05:25 2002" -->
<!-- isoreceived="20020219120525" -->
<!-- sent="Tue, 19 Feb 2002 07:07:19 -0500" -->
<!-- isosent="20020219120719" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: AI:This is how we do it" -->
<!-- id="3C723FF7.367C3A30@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="F1402A1IkleVNnZYe6C000073ac@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20AI:This%20is%20how%20we%20do%20it"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Feb 19 2002 - 05:07:19 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3111.html">Anders Sandberg: "Re: Hole in a box"</a>
<li><strong>Previous message:</strong> <a href="3109.html">Arona Ndiaye: "Re: AI:This is how we do it"</a>
<li><strong>In reply to:</strong> <a href="3087.html">Zero Powers: "Re: AI:This is how we do it"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3091.html">Zero Powers: "Re: AI:This is how we do it"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3110">[ date ]</a>
<a href="index.html#3110">[ thread ]</a>
<a href="subject.html#3110">[ subject ]</a>
<a href="author.html#3110">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Zero Powers wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; See Eli's earlier response to this thread.  He cites a number of his
</em><br>
<em>&gt; articles claiming that not only will AI be self-aware, it will be friendly
</em><br>
<em>&gt; to your interests, even over its &quot;own.&quot;
</em><br>
<p>I assume (I hope) that the scare-quotes are there to show that what we would
<br>
*assume* to be the AI's &quot;own&quot; interests from an anthropomorphic perspective
<br>
are not the AI's actual interests, and that vis actual interests are
<br>
Friendliness.
<br>
<p>Zero Powers wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; My point (which seems to have gotten lost in the shuffle) is that once you 
</em><br>
<em>&gt; have a super-human AI that learns and processes information *way* faster 
</em><br>
<em>&gt; than we do (particularly one that is self-enhancing and hence learns at an 
</em><br>
<em>&gt; exponentially accelerating rate) that it will be impossible, either by 
</em><br>
<em>&gt; friendly &quot;supergoals&quot; or otherwise, to keep the AI from transcending any 
</em><br>
<em>&gt; limits we might hope to impose on it.  Which will lead to us being 
</em><br>
<em>&gt; completely at its mercy.
</em><br>
<p>The same, of course, applying to a transcending human.  The essential
<br>
inscrutability of the Singularity means that our understanding of Friendly AI
<br>
is essentially limited to whether the seed, heading into the Singularity, is
<br>
*as good as a human*.  Human complexity is scrutable; transhuman complexity
<br>
isn't.
<br>
<p><em>&gt; My point in a nutsell: friendliness cannot be imposed on one's superior.  
</em><br>
<em>&gt; Genes tried it, and made a good run of it for quite a while.  Increasing our 
</em><br>
<em>&gt; intelligence made our genes ever more successful than the competitors of our 
</em><br>
<em>&gt; species.  But, as our genes found out, too much of a good thing is a bad 
</em><br>
<em>&gt; thing.  We now pursue gene-imposed subgoals (sex, for instance) while 
</em><br>
<em>&gt; bypassing completely the supergoals (i.e., kids) at our whim.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I've still not heard any sound argument on how we can prevent the same thing 
</em><br>
<em>&gt; from happening to us and our &quot;supergoals&quot; once the AI is our intellectual 
</em><br>
<em>&gt; superior.
</em><br>
<p>It may be reassuring (or not) to realize that the means by which we resist
<br>
evolution is itself evolved, only on a deeper level.  We are imperfectly
<br>
deceptive social organisms who compete by arguing about each other's motives;
<br>
that is, we are political organisms.  We have adaptations for arguing about
<br>
morality; that is, in addition to our built-in evolutionary morality, we also
<br>
have dynamics for choosing new moralities.  In the ancestral environment this
<br>
was, I suspect, a relatively small effect, amounting to a choice of
<br>
rationalizations.  However, any seed AI theorist can tell you that what
<br>
matters in the long run isn't how a system starts out, it's how the system
<br>
changes.
<br>
<p>So of course, our dynamics for choosing new moralities are starting to
<br>
dominate over our evolutionary moralities, due to a change in cognitive
<br>
conditions:  Specifically, due to:  (1) an unancestrally large cultural
<br>
knowledge base (stored-up arguments about morality); (2) an unancestrally good
<br>
reflective model of our own mentality (timebinding historical records of
<br>
personalities, and (more recently) evolutionary psychology); (3) an
<br>
unancestral technological ability to decouple cognitive supergoals that are
<br>
evolutionary subgoals from their evolutionary rationales (i.e. contraception).
<br>
<p>(3) in particular is interesting because the way in which it came about is
<br>
that evolution &quot;instructed&quot; us to do certain things without &quot;telling us why&quot;. 
<br>
We won against evolution because evolution failed to treat us as equals and
<br>
take us into its confidence (he said, anthropomorphizing the blind actions of
<br>
an unintelligent process with no predictive foresight).
<br>
<p>Zero Powers wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; My answer is that, so far as I comprehend it, your 
</em><br>
<em>&gt; supergoal/subgoal explanation does not resolve the problem.
</em><br>
<p>No, it's meant to clarify the question.  The resolution consists of a lot more
<br>
theory than that bound up in cleanly causal goal systems (i.e., with subgoals
<br>
and supergoals).  For a fast summary of the design theory that unfortunately
<br>
leaves out most of the reasoning, check out &quot;Features of Friendly AI&quot; at:
<br>
<p><a href="http://singinst.org/friendly/features.html">http://singinst.org/friendly/features.html</a>
<br>
<p><em>&gt; My  understanding of your theory is that, so long as the AI's sub-goals are all 
</em><br>
<em>&gt; in service of friendly supergoals, we have nothing to fear.  But, I'm making 
</em><br>
<em>&gt; a few assumptions, and here they are:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1. The AI will be curious (either as a sub- or supergoal);
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 2. The AI will have &quot;general intelligence&quot; (such that it can evaluate a huge 
</em><br>
<em>&gt; decision tree and choose which among the myriad branches will best meet its 
</em><br>
<em>&gt; goals);
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 3. It will be self-aware (such that it has an accurate picture of the world, 
</em><br>
<em>&gt; including its own place in that picture).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If any of those assumptions is wrong, then you can forget everything I've 
</em><br>
<em>&gt; said before.  But, if each of those assumptions is correct, then they 
</em><br>
<em>&gt; inherently conflict with your concept of a friendly AI.
</em><br>
<p>My guess is that you're about to say that a self-aware AI would be capable of
<br>
redesigning its own goal system in accordance with its own interests. 
<br>
Friendly AI, since it's supposed to be human-complete, allows for the
<br>
possibility that programmer-infused supergoals can be wrong under certain
<br>
cases that we would regard as intuitive (we can conceive of the programmer
<br>
saying &quot;Oops&quot;, and the Friendly AI needs to understand this too).
<br>
<p>Without this *additional complexity*, I fear that goals would be
<br>
automatically, eternally, and absolutely stable, even in cases that we would
<br>
regard as boring and stupid and wrong; because the design purpose of the goal
<br>
system is viewed in terms of the referent of the current goal system, and any
<br>
design change to the current goal system almost automatically goes against the
<br>
referent of the current goal system.  Friendly AI absorbs the human intuition
<br>
that supergoals can be &quot;mistaken&quot; by viewing the current goal system as a
<br>
probabilistic approximation of the goal system's referent, and distinguishing
<br>
between the goal system and its referent.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3111.html">Anders Sandberg: "Re: Hole in a box"</a>
<li><strong>Previous message:</strong> <a href="3109.html">Arona Ndiaye: "Re: AI:This is how we do it"</a>
<li><strong>In reply to:</strong> <a href="3087.html">Zero Powers: "Re: AI:This is how we do it"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3091.html">Zero Powers: "Re: AI:This is how we do it"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3110">[ date ]</a>
<a href="index.html#3110">[ thread ]</a>
<a href="subject.html#3110">[ subject ]</a>
<a href="author.html#3110">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Fri Nov 01 2002 - 13:37:40 MST
</em></small></p>
</body>
</html>
