<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>extropians: Re: The Politics of Transhumanism</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: The Politics of Transhumanism">
<meta name="Date" content="2002-01-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Politics of Transhumanism</h1>
<!-- received="Sun Jan  6 16:28:18 2002" -->
<!-- isoreceived="20020106232818" -->
<!-- sent="Sun, 06 Jan 2002 18:28:12 -0500" -->
<!-- isosent="20020106232812" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The Politics of Transhumanism" -->
<!-- id="3C38DD8C.B2618E5D@pobox.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="NFBBLPCEGLEKJKPPBAIJMEGBCKAA.jhughes@changesurfer.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20Politics%20of%20Transhumanism"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 06 2002 - 16:28:12 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0444.html">Adrian Tymes: "Re: A great new year"</a>
<li><strong>Previous message:</strong> <a href="0442.html">Jacques Du Pasquier: "Re: Quoting Nietzsche is a perilous business"</a>
<li><strong>In reply to:</strong> <a href="0439.html">J. Hughes: "The Politics of Transhumanism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0445.html">Eliezer S. Yudkowsky: "Re: The Politics of Transhumanism"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#443">[ date ]</a>
<a href="index.html#443">[ thread ]</a>
<a href="subject.html#443">[ subject ]</a>
<a href="author.html#443">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&quot;J. Hughes&quot; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; <a href="http://www.changesurfer.com/Acad/TranshumPolitics.htm">http://www.changesurfer.com/Acad/TranshumPolitics.htm</a>
</em><br>
<p>In which he wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; There are occasional discussions on the extropian list
</em><br>
<em>&gt; about the potential downsides or catastrophic
</em><br>
<em>&gt; consequences of emerging technologies, but these
</em><br>
<em>&gt; are generally waved off as being either easily
</em><br>
<em>&gt; remediable or acceptable risks given the tremendous
</em><br>
<em>&gt; rewards. This form of argumentation is more
</em><br>
<em>&gt; understandable since most extropians have adopted a
</em><br>
<em>&gt; form of millennial apocalyptic which they call “the
</em><br>
<em>&gt; Singularity.”
</em><br>
<p>I deal with the unjustified use of the prejudicial term &quot;millennial
<br>
apocalyptic&quot; in greater detail below.  However, the above contention (that
<br>
belief in the Singularity leads to &quot;waving off&quot; potential downsides) is
<br>
directly contradicted by historical observation of sides taken on the
<br>
Extropian list.  For example, during the goo prophylaxis debate partially
<br>
archived at
<br>
&nbsp;<a href="http://www.nickbostrom.com/old/nanotechnology.html">http://www.nickbostrom.com/old/nanotechnology.html</a>
<br>
I think I may legitimately lay claim to having had the most worried
<br>
outlook.
<br>
<p><em>&gt; The extropians’ Singularity is a coming
</em><br>
<em>&gt; abrupt rupture in social life, brought about by some
</em><br>
<em>&gt; confluence of genetic, cybernetic and nano
</em><br>
<em>&gt; technologies.
</em><br>
<p>My personal experience is that, while this definition of the Singularity
<br>
is quite popular among outsiders looking in on transhumanism, it is much
<br>
rarer among actual transhumanists.  It is a vague and even eschatological
<br>
definition.  The canonical definition for the hard-core Singularity
<br>
interest group represented by the Singularity Institute is that the
<br>
Singularity comprises those events following from the rise of
<br>
greater-than-human intelligence in any form.  Nanotechnology is thus
<br>
relevant only insofar as it is an enabling technology of AI or
<br>
intelligence enhancement.
<br>
<p><em>&gt; The concept of the Singularity was
</em><br>
<em>&gt; first proposed by science fiction author Vernor
</em><br>
<em>&gt; Vinge in a 1993 essay, referring specifically to the
</em><br>
<em>&gt; apocalyptic consequences of the emergence of
</em><br>
<em>&gt; self-willed artificial intelligence.
</em><br>
<p>Again, Vinge's original Singularity concept refers to the emergence of
<br>
greater-than-human intelligence in *any* form.
<br>
<p><em>&gt; But the roots of the
</em><br>
<em>&gt; Singularity idea are in the transcultural millenarian
</em><br>
<em>&gt; impulse; the Singularity is a vision of techno-Rapture
</em><br>
<em>&gt; for secular, alienated, relatively powerless,
</em><br>
<em>&gt; techno-enthusiasts (Bozeman, 1997).
</em><br>
<p>Your comments as they stand are reminiscent of Smalley on nanotechnology
<br>
or Lewontin on evolutionary psychology.
<br>
<p>An examination of Bozeman 1997 shows that he presents no evidence
<br>
whatsoever for the assertion that the Singularity bears any current or
<br>
historical relation to the millenarian impulse; he simply happens to cite
<br>
the Singularity, in passing and without supporting evidence, in the same
<br>
paper as that describing other ideas which he believes to bear the same
<br>
flaw.
<br>
<p>Proponents of the Singularity cite specific and technical arguments as
<br>
reasons for their beliefs; you must discuss the claimed sufficiency of
<br>
these specific and technical arguments before you can legitimately
<br>
postulate that religious fervor is a necessary ingredient for belief. 
<br>
That people once believed certain things could be done by magic or
<br>
theology is no reason to suppose that they cannot be achieved
<br>
technologically; otherwise the belief in winged angels would have been
<br>
sufficient to prevent the Wright Brothers from leaving the ground.  Humans
<br>
have always wanted to fly, and invented many emotionally satisfying
<br>
stories in which this desire was granted; eventually the always-existent
<br>
desire was coupled to new technological capabilities, and the impulse was
<br>
satisfied in reality as well as myth.  The same may later prove to hold
<br>
true of the human wish to live forever.  That Singularitarians wish to
<br>
bring about a massive worldwide change for the better is not evidence of
<br>
millenarian apocalyptism unless it can be demonstrated that we are
<br>
mistaken in asserting that this goal has now become technologically
<br>
feasible.
<br>
<p>Given that the Singularity concept first rose to popularity among a social
<br>
group composed primarily of scientifically literate aggressive
<br>
rationalists, your assertion above is (a) extreme fighting words in that
<br>
cultural mileu, and (b) not very likely to be correct.  If you don't like
<br>
the Singularity, present substantive arguments against it.  A dismissal in
<br>
passing, as if the issue were already settled, is not constructive.
<br>
<p>And if I may speak to the purposes behind your paper, may I also remind
<br>
you that advocates of the Singularity are not exclusively drawn from the
<br>
Extropian mailing list, and that your potshots at the Singularity are not
<br>
satisfying punches delivered to the &quot;evil reactionary&quot; Extropians but
<br>
rather punches delivered to an entirely separate interest group in
<br>
transhumanism.  That you have chosen to attack the Singularity in this
<br>
paper makes it clear that you believe the Singularity to be an ideological
<br>
support of &quot;evil reactionary&quot; Extropianism.  It is not.  The Singularity
<br>
interest group comprises an entirely separate subsector of transhumanism,
<br>
an interest group which includes &quot;good progressive&quot; transhumanists such as
<br>
Damien Broderick and Ben Goertzel.
<br>
<p>(The above paragraph should not be taken as tacitly condoning the style of
<br>
argument in which the universe revolves around the opponents and
<br>
supporters of liberalism.  I find the paper's use of this style to be
<br>
appallingly parochial, but I'm just here to handle the Singularity end of
<br>
it.)
<br>
<p><em>&gt; The appeal of
</em><br>
<em>&gt; the Singularity for libertarians such as the extropians
</em><br>
<em>&gt; is that, like the Second Coming, it does not require
</em><br>
<em>&gt; any specific collective action;
</em><br>
<p>I beg your pardon?  What are we doing here at the Singularity Institute,
<br>
knitting sweaters?
<br>
<p><em>&gt; it is literally a deus ex
</em><br>
<em>&gt; machina. Ayn Rand envisioned society sinking into
</em><br>
<em>&gt; chaos once the techno-elite withdrew into their
</em><br>
<em>&gt; Valhalla. But the Singularity will elevate the
</em><br>
<em>&gt; techno-savvy elite while most likely wiping out
</em><br>
<em>&gt; everybody else.
</em><br>
<p>An assertion contradicted by the Singularity Institute's literature, the
<br>
Singularitarian Principles, the moral content proposed in &quot;Creating
<br>
Friendly AI&quot;, discussions on the SL4 mailing list, and the stated
<br>
humanistic reasons of most individuals explaining their personal
<br>
involvement in the Singularity.
<br>
<p><em>&gt; Working individually to avidly stay
</em><br>
<em>&gt; on the cutting edge of technology, transforming
</em><br>
<em>&gt; oneself into a post-human, is one’s best insurance of
</em><br>
<em>&gt; surviving and prospering through the Singularity.
</em><br>
<p>As D. den Otter's example illustrates, such sentiments are expressed very
<br>
rarely and when expressed have found very little agreement (I can only
<br>
think of two cases).  And of course D. den Otter's case also illustrates
<br>
another effect; that those who profess such beliefs will tend to be
<br>
rapidly marginalized out of Singularity communities because of their own
<br>
professed disinterest in group action.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0444.html">Adrian Tymes: "Re: A great new year"</a>
<li><strong>Previous message:</strong> <a href="0442.html">Jacques Du Pasquier: "Re: Quoting Nietzsche is a perilous business"</a>
<li><strong>In reply to:</strong> <a href="0439.html">J. Hughes: "The Politics of Transhumanism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0445.html">Eliezer S. Yudkowsky: "Re: The Politics of Transhumanism"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#443">[ date ]</a>
<a href="index.html#443">[ thread ]</a>
<a href="subject.html#443">[ subject ]</a>
<a href="author.html#443">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Fri Nov 01 2002 - 13:37:33 MST
</em></small></p>
</body>
</html>
