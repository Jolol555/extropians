<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>extropians: Understanding CFAI</title>
<meta name="Author" content="Smigrodzki, Rafal (SmigrodzkiR@msx.upmc.edu)">
<meta name="Subject" content="Understanding CFAI">
<meta name="Date" content="2002-02-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Understanding CFAI</h1>
<!-- received="Wed Feb  6 21:52:45 2002" -->
<!-- isoreceived="20020207045245" -->
<!-- sent="Wed, 6 Feb 2002 23:52:12 -0500 " -->
<!-- isosent="20020207045212" -->
<!-- name="Smigrodzki, Rafal" -->
<!-- email="SmigrodzkiR@msx.upmc.edu" -->
<!-- subject="Understanding CFAI" -->
<!-- id="EB5DDEEFC7B4D411AD3B00508BDFF3E20231919C@1upmc-msx7.isdip.upmc.edu" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Smigrodzki, Rafal (<a href="mailto:SmigrodzkiR@msx.upmc.edu?Subject=Re:%20Understanding%20CFAI"><em>SmigrodzkiR@msx.upmc.edu</em></a>)<br>
<strong>Date:</strong> Wed Feb 06 2002 - 21:52:12 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2518.html">Smigrodzki, Rafal: "RE: Bye-Bye to the &gt;H Right Wing"</a>
<li><strong>Previous message:</strong> <a href="2516.html">Smigrodzki, Rafal: "RE: more about growth hormone and the decline of years"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2527.html">Eliezer S. Yudkowsky: "Re: Understanding CFAI"</a>
<li><strong>Reply:</strong> <a href="2527.html">Eliezer S. Yudkowsky: "Re: Understanding CFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2517">[ date ]</a>
<a href="index.html#2517">[ thread ]</a>
<a href="subject.html#2517">[ subject ]</a>
<a href="author.html#2517">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I just finished reading CFAI and GISAI and here are some questions and
<br>
comments:
<br>
Of course, SIAI &lt;../GISAI/meta/glossary.html&gt; knows of only one current
<br>
project advanced enough to even begin implementing the first baby steps
<br>
toward Friendliness - but where there is one today, there may be a dozen
<br>
tomorrow.
<br>
### Which other AI projects are on the right track?
<br>
------- 
<br>
That is, conscious reasoning can replace the &quot;damage signal&quot; aspect of pain.
<br>
If the AI successfully solves a problem, the AI can choose to increase the
<br>
priority or devote additional computational power to whichever subheuristics
<br>
or internal cognitive events were most useful in solving the problem,
<br>
replacing the positive-feedback aspect of pleasure.
<br>
### Do you think that the feedback loops focusing the AI on a particular
<br>
problem might (in any sufficiently highly organized AI) give rise to qualia
<br>
analogous to our feelings of &quot;intellectual unease&quot;, and &quot;rapture of an
<br>
insight&quot;?
<br>
-------
<br>
The absoluteness of &quot;The end does not justify the means&quot; is the result of
<br>
the Bayesian Probability Theorem &lt;../../GISAI/meta/glossary.html&gt; applied to
<br>
internal cognitive events. Given the cognitive event of a human thinking
<br>
that the end justifies the means, what is the probability that the end
<br>
actually does justify the means? Far, far less than 100%, historically
<br>
speaking. Even the cognitive event &quot;I'm a special case for [reason X] and am
<br>
therefore capable of safely reasoning that the end justifies the means&quot; is,
<br>
historically speaking, often dissociated with external reality. The rate of
<br>
hits and misses is not due to the operation of ordinary rationality, but to
<br>
an evolutionary bias towards self-overestimation. There's no Bayesian
<br>
binding &lt;../../GISAI/meta/glossary.html&gt; between our subjective experience
<br>
of feeling justified and the external event of actually being justified, so
<br>
our subjective experience cannot license actions that would be dependent on
<br>
being actually justified.
<br>
### I think I can dimly perceive a certain meaning in the above paragraph,
<br>
with which I could agree (especially in hindsight after reading 3.4.3:
<br>
Causal validity semantics &lt;design/structure/causal.html&gt; ). Yet, without
<br>
recourse to the grand finale, this paragraph is very cryptic and in some
<br>
interpretations for me unacceptable.
<br>
------
<br>
I think that undirected evolution is unsafe, and I can't think of any way to
<br>
make it acceptably safe. Directed evolution might be made to work, but it
<br>
will still be substantially less safe than self-modification. Directed
<br>
evolution will also be extremely unsafe unless pursued with Friendliness in
<br>
mind and with a full understanding of non-anthropomorphic minds. Another
<br>
academically popular theory is that all people are blank slates, or that all
<br>
altruism is a child goal of selfishness - evolutionary psychologists know
<br>
better, but some of the social sciences have managed to totally insulate
<br>
themselves from the rest of cognitive science, and there are still AI people
<br>
who are getting their psychology from the social sciences.
<br>
### Is altruism something other than a child goal of selfishness? It was my
<br>
impression that evolutionary psychology predicts the emergence of altruism
<br>
as a result of natural selection. Since natural selection is not
<br>
goal-oriented, the goals defining the base of the goal system (at least if
<br>
you use the derivative validity rule),  are the goals present in the
<br>
subjects of natural selection - selfish organisms, which in the service of
<br>
their own survival have to develop (secondarily) altruistic impulses. As the
<br>
derivative validity rule is itself the outcome of ethical reasoning , one
<br>
could claim that it cannot invalidate the goals from which it is derived,
<br>
thus sparing us a total meltdown of the goal system and shutdown.
<br>
------
<br>
The semantics of objectivity are also ubiquitous because they fit very well
<br>
into the way our brain processes statements; statements about morality
<br>
(containing the word &quot;should&quot;) are not evaluated by some separate, isolated
<br>
subsystem, but by the same stream of consciousness that does everything else
<br>
in the mind. Thus, for example, we cognitively expect the same kind of
<br>
coherence and sensibility from morality as we expect from any other fact in
<br>
our Universe
<br>
### It is likely that there are specialized cortical areas, mainly in the
<br>
frontopolar and ventromedial frontal cortices, involved in the processing of
<br>
ethics-related information. Many of us are perfectly capable of double- and
<br>
triple-thinking about ethical issues, as your examples of self-deception
<br>
testify, while similar feats of mental juggling are not possible in the
<br>
arena of mathematics or motorcycle maintenance.
<br>
----- 
<br>
Actually, rationalization does not totally disjoint morality and actions; it
<br>
simply gives evolution a greater degree of freedom by loosely decoupling the
<br>
two. Every now and then, the gene pool or the memetic environment spits out
<br>
a genuine altruist; who, from evolution's perspective, may turn out to be a
<br>
lost cause. The really interesting point is that evolution is free to load
<br>
us with beliefs and adaptations which, if executed in the absence of
<br>
rationalization, would turn us into total altruists ninety-nine point nine
<br>
percent of the time. Thus, even though our &quot;carnal&quot; desires are almost
<br>
entirely observer-centered, and our social desires are about evenly split
<br>
between the personal and the altruistic, the adaptations that control our
<br>
moral justifications have strong biases toward moral symmetry, fairness,
<br>
truth, altruism, working for the public benefit, and so on.
<br>
### In my very personal outlook, the &quot;moral justifications&quot; are the results
<br>
of advanced information processing applied in the service of &quot;carnal&quot;
<br>
desires, supplemented  by innate, evolved biases. The initial supergoals are
<br>
analyzed, their implications for action under various conditions are
<br>
explored, and the usual normative human comes to recognize the superior
<br>
effectiveness of fairness, truth, etc., for survival in a social situation.
<br>
As a result the initial supergoals are overwritten by new content (at least
<br>
to some degree, dictated by the ability to deceive others). As much as the
<br>
imprint of my 4-year old self in my present mind might object, I am forced
<br>
to accept the higher Kohlberg stage rules. Do you think that the Friendly AI
<br>
will have some analogue of such (higher) levels? Can you hypothesize about
<br>
the supergoal content of such level? Could it be translated back for
<br>
unenhanced humans, or would it be only accessible to highly improved
<br>
uploads?
<br>
-----
<br>
An AI's complete mind-state at any moment in time is the result of a long
<br>
causal chain. We have, for this moment, stopped speaking in the language of
<br>
desirable and undesirable, or even true and false, and are now speaking
<br>
strictly about cause and effect. Sometimes the causes described may be
<br>
beliefs existing in cognitive entities, but we are not obliged to treat
<br>
these beliefs as beliefs, or consider their truth or falsity; it suffices to
<br>
treat them as purely physical events with purely physical consequences. 
<br>
This is the physicalist perspective, and it's a dangerous place for humans
<br>
to be. I don't advise that you stay too long. The way the human mind is set
<br>
up to think about morality, just imagining the existence of a physicalist
<br>
perspective can have negative emotional effects. I do hope that you'll hold
<br>
off on drawing any philosophical conclusions until the end of this topic at
<br>
the very least
<br>
### If I understand this paragraph correctly, my way of thinking about
<br>
myself has been physicalist for the past 10 - 15 years, yet it failed to
<br>
produce negative emotional effects. I am an information processing routine,
<br>
with the current self-referential goal of preserving its continued
<br>
existence, all in the context of  15x10e9 years of the Universe's evolution.
<br>
Even this self-referential supergoal can be explicitly renounced if it
<br>
becomes advantageous for the routine's survival (as in joining the Borg to
<br>
escape the Klingons).
<br>
<pre>
----
The rule of derivative validity - &quot;Effects cannot have greater validity than
their causes.&quot; - contains a flaw; it has no tail-end recursion. Of course,
so does the rule of derivative causality - &quot;Effects have causes&quot; - and yet,
we're still here; there is Something rather than Nothing. The problem is
more severe for derivative validity, however. At some clearly defined point
after the Big Bang, there are no valid causes (before the rise of
self-replicating chemicals on Earth, say); then, at some clearly defined
point in the future (i.e., the rise of homo sapiens sapiens) there are valid
causes. At some point, an invalid cause must have had a valid effect. To
some extent you might get around this by saying that, i.e., self-replicating
chemicals or evolved intelligences are pattern-identical with (represent)
some Platonic valid cause - a low-entropy cause, so that evolved
intelligences in general are valid causes - but then there would still be
the question of what validates the Platonic cause. And so on. 
The rule of derivative validity is embedded very deeply in the human mind.
It's the ultimate drive behind our search for the Meaning of Life. It's the
reason why we instinctively dislike circular logic. It's a very powerful
shaper(!). Just violating it arbitrarily, to trick the AI into doing
something, or in the belief that it doesn't really matter... well, that
wouldn't be safe (4), because that kind of &quot;selfishness&quot; is designated as an
extraneous cause by quite a few deeper shapers. Of course, I'm omitting the
possibility that the programmer personally believes that kind of logic is
okay (i.e., would use it herself), in which case things would probably come
out okay, though I personally would worry that this programmer, or her
shaper network, had too high a tolerance for circular logic...
### I think it's possible for a human to have both a limited use of
derivative validity and circular (or self-referential) basis for the goal
system. See my comments above.
---
We want a Meaning of Life that can be explained to a rock, in the same way
that the First Cause (whatever it is) can be explained to Nothingness. We
want what I call an &quot;objective morality&quot; - a set of moral propositions, or
propositions about differential desirabilities, that have the status of
provably factual statements, without derivation from any previously accepted
moral propositions. We want a tail-end recursion to the rule of derivative
validity. Without that, then yes - in the ultimate sense described above,
Friendliness is unstable
### I do agree with the last sentence. A human's self-Friendliness is
inherently unstable, too
---
As General Intelligence and Seed AI describes a seed AI capable of
self-improvement, so Creating Friendly AI describes a Friendly AI capable of
self-correction. A Friendly AI is stabilized, not by objective morality -
though I'll take that if I can get it - but by renormalization, in which the
whole passes judgement on the parts, and on its own causal history.
### This seems to summarize my goal system functioning, with the importance
of personal history, use of a wide range of cognitive tools to derive rules,
and to slowly change the goal system. 
----
. Even if an extraneous cause affects a deep shaper, even deep shapers don't
justify themselves; rather than individual principles justifying themselves
- as would be the case with a generic goal system protecting absolute
supergoals - there's a set of mutually reinforcing deep principles that
resemble cognitive principles more than moral statements, and that are
stable under renormalization. Why &quot;resemble cognitive principles more than
moral statements&quot;? Because the system would distrust a surface-level moral
statement capable of justifying itself! 
### Can you give examples of such deep moral principles?
-----
Humanity is diverse, and there's still some variance even in the panhuman
layer, but it's still possible to conceive of description for humanity and
not just any one individual human, by superposing the sum of all the
variances in the panhuman layer into one description of humanity. Suppose,
for example, that any given human has a preference for X; this preference
can be thought of as a cloud in configuration space. Certain events very
strongly satisfy the metric for X; others satisfy it more weakly; other
events satisfy it not at all. Thus, there's a cloud in configuration space,
with a clearly defined center. If you take something in the panhuman layer
(not the personal layer) and superimpose the clouds of all humanity, you
should end up with a slightly larger cloud that still has a clearly defined
center. Any point that is squarely in the center of the cloud is &quot;grounded
in the panhuman layer of humanity&quot;.
### What if the shape of superposition turns out to be more complicated,
with the center of mass falling outside the maximum values of the
superposition? In that case implementing a Friendliness focused on this
center would have outcomes distasteful to all humans, and finding
alternative criteria for Friendliness would be highly nontrivial.
--- 
### And a few more comments:
I wonder if you read Lem's &quot;Golem XIV&quot;?
Oops, Google says you did read it. Of course.
In a post on Exilist you say that uploading is a post-Singularity
technology. While I intuitively feel that true AI will be built well before
the computing power becomes available for an upload, I would imagine it
should be possible to do uploading without AI. After all, you need just some
improved scanning methods, and with laser tissue machining, quantum dot
antibody labeling and high-res confocal microscopy, as well as the proteome
project, this might be realistic in as little as 10 years (a guess). With a
huge computer but no AI the scanned data would give you a human mind in a
box, amenable to some enhancement.
What do you think about using interactions between a nascent AI and the
upload(s), with reciprocal rounds of enhancemnt and ethical system transfer,
to develop Friendliness?
And, by the way, I do think that CFAI and GISAI are wonderful intellectual
achievements.
Rafal
 
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2518.html">Smigrodzki, Rafal: "RE: Bye-Bye to the &gt;H Right Wing"</a>
<li><strong>Previous message:</strong> <a href="2516.html">Smigrodzki, Rafal: "RE: more about growth hormone and the decline of years"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2527.html">Eliezer S. Yudkowsky: "Re: Understanding CFAI"</a>
<li><strong>Reply:</strong> <a href="2527.html">Eliezer S. Yudkowsky: "Re: Understanding CFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2517">[ date ]</a>
<a href="index.html#2517">[ thread ]</a>
<a href="subject.html#2517">[ subject ]</a>
<a href="author.html#2517">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Fri Nov 01 2002 - 13:37:38 MST
</em></small></p>
</body>
</html>
